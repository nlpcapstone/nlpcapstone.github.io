<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>NLP Capstone Spring 2018</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="planet.css" type="text/css">
<link rel="alternate" href="https://nlpcapstone.github.io/atom.xml" title="" type="application/atom+xml">
</head>

<body>
<h1>NLP Capstone Spring 2018</h1>

<div class="daygroup">
<h2>May 29, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/last-update">
<h4><a href="http://sarahyu.weebly.com/cse-481n/last-update">Last Update</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">The final update for the project! Unfortunately, there is not as much to report on results as usual and not as many fun visualizations. For the 2nd Advanced Model, my plan was to begin on the stretch goals I had initially outlined and train a neural model for Reddit Post classification and Generation. The idea took cue from the Affect-LM paper. Basically it would be similar to this model</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 13.331751602564%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 68.227199377828%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-29-at-9-42-04-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 18.441049019608%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph">which has inputs of the context words, on which to build up the rest of the sentence from; the Affect category which is chosen beforehand to generate the desired output; and an Affect strength to determine the intensity of the affect category defined. <br /><br />My model would be similar to this, but instead look more like the following where we take out the strength factor and choose the mental category  to be fed into the Mental LM.</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 24.666352941176%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 56.902274509804%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/published/screen-shot-2018-05-29-at-10-56-19-pm.png?1527659857" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 18.43137254902%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph">My current model seems to have some issues generating posts containing any language beyond the exact topic of the mental category itself (i.e. using the word depressed for the F30 category), with the model using the cross entropy loss. <br /><br />At this point, the game plan is to fix the bugs and get a working model to test out the post generation. The results will be interesting to see in and of themselves, but I will also compare the language model of the generated posts against the metrics we hav seen throughout the quarter (vennclouds, idp). </div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/last-update">May 29, 2018 05:35 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 23, 2018</h2>

<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=27">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/05/23/advanced-model-attempt-2-part-2/">Advanced Model Attempt 2 (Part 2)</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>This week I further tuned my hyperparameters and increased the exploration rate of my model so the slot-filling Q-values were fixed for all my questions. This made sure that my slot filling accuracy is perfect (when there’s no dropout). After tweaking the model, I tested and recorded the number of games it won using both the database it trained on as well as another random database of 100 people.</p>
<p>Training Database: Model won 90/100 games.</p>
<p>Validation Database: Model won 87/100 games.</p>
<p>As expected, the validation accuracy is not too far behind the training accuracy. While the NLU component of the model is just as accurate for both databases, the order of questions is more tailored towards the training database so the sequence of questions might be a bit inefficient for the other database meaning some additional games must have been lost on time. The reference paper I based this model on was trained using speech utterances and that model got about a 92% win rate (although they didn’t specify whether the database they evaluated on was different from the one they trained on). Therefore, my model is not quite as well trained as theirs but it’s still not too far behind in accuracy.</p>
<p>Now that I have my minimal action plan working, this week I’ll work on my final report and presentation as well as try to make this model work for a more generic dialogue scenario as a possible demo.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/05/23/advanced-model-attempt-2-part-2/">by ananthgo at May 23, 2018 06:34 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-9203775015655831448.post-7864337718441481784">
<h4><a href="https://teamoverfit.blogspot.com/2018/05/9-milestone-advanced-model-attempt-2.html">#9 Milestone: Advanced model attempt #2 (continued)</a></h4>
<div class="entry">
<div class="content">
<h2 style="height: 0px;"><span>Team Overfit</span></h2><h3><span><br /></span></h3><h3><span>Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span>Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><div style="text-align: start; text-indent: 0px;"><div style="margin: 0px;"><div><span><b>Blog Post: #9: 05/22/2018</b></span></div><div><span><span><b><br /></b></span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Social Media Predicts Stock Price (StartUp Mode)</span><br /><span><br /></span><span><b>What We Did:</b></span><br /><ul><li><span>Use news data and write preprocessor</span></li><ul><li><span>The drawback of News Tweets</span></li><ul><li><span><span id="docs-internal-guid-883230d4-8b20-8c22-2807-843c883c978c"><span>Too short for each tweet and not informative enough for the model to learn from it.</span></span></span></li><li><span><span><span id="docs-internal-guid-af2383c6-8b20-b2a3-a62a-455584d56812"><span style="vertical-align: baseline;">Some news tweets don’t summarize well for the real content and cause misleading predictions.</span></span></span></span></li><li><span><span><span style="vertical-align: baseline;"><span id="docs-internal-guid-e02be321-8b21-0416-2fa0-ef6480c92d92"><span style="vertical-align: baseline;">The sources of such news tweets are mostly unauthorized.</span></span></span></span></span></li></ul><li><span><span style="white-space: pre-wrap;"><span id="docs-internal-guid-e9f853fe-8b21-3a41-e15b-561e8b9a7498"><span style="vertical-align: baseline;">Therefore, we decided to move on and explore new news sources which are authorized as our input.</span></span></span></span></li><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;"><span id="docs-internal-guid-00d89dbe-8b21-6184-8856-59ddaaef7254"><span style="vertical-align: baseline;">We used the financial news published everyday and crawl the content directly, instead of using the titles. This will increase the input size for each news. However, it helps the model to gain more information from each single news.</span></span></span></span></span></li><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;"><span style="vertical-align: baseline;"><span id="docs-internal-guid-a49c91be-8b21-8bf2-9b24-72087904ee5d"><span style="vertical-align: baseline;">The link to the source we use: </span><span style="color: #1155cc; vertical-align: baseline;"><a href="https://intrinio.com/tutorial/file_download#News">https://intrinio.com/tutorial/file_download#News</a></span></span></span></span></span></span></li></ul><li><span><span style="white-space: pre-wrap;">Model Optimization</span></span></li><ul><li><span><span style="white-space: pre-wrap;"><span id="docs-internal-guid-d56ee8ab-8b21-eec9-1c8a-26602af6d372"><span style="vertical-align: baseline;">Add learning rate decay to our training</span></span></span></span></li><ul><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;">This helps the model to converge smoothly at the end of training</span></span></span></li></ul><li><span><span style="white-space: pre-wrap;"><span id="docs-internal-guid-cd723ea4-8b22-d2e3-4c18-27df8c103e58"><span style="vertical-align: baseline;">Investigate the correlation between sentiment analysis of Tweets and stock price movement</span></span></span></span></li><ul><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;">We categorize the attitudes in each tweets and news, which increase our feature size</span></span></span></li><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;"><span id="docs-internal-guid-ac7fa55e-8b23-3684-d1f2-4eda487b4e92"><span style="vertical-align: baseline;">Also, we compare the overall attitudes of all tweets and news each day with the prediction made by the model and the real stock price.</span></span></span></span></span></li></ul></ul><li><span><span style="white-space: pre-wrap;"><span id="docs-internal-guid-56177478-8b23-82ae-6d6a-d780f77fdf2a"><span style="vertical-align: baseline;">Visualize and investigate our prediction/training to have better understanding of our model</span></span></span></span></li><ul><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;">Find out the top k words that have most weight for a given day</span></span></span></li><ul><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;">Most of them do not seem to relate to stock price movement</span></span></span></li></ul><li><span><span style="white-space: pre-wrap;"><span id="docs-internal-guid-51b8e57b-8b23-fc01-5457-d60abea87f96"><span style="vertical-align: baseline;">Compare the prediction/target graph for each epochs to see if the predictions are improving/converging</span></span></span></span></li><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;">Graphs:</span></span></span></li></ul></ul><a href="https://2.bp.blogspot.com/-cvX-eD-CWqI/WwTph39sPdI/AAAAAAAAAAY/p-YRfgomFu0oyiXeSsMXTRAhNRQ4FbPGgCEwYBhgL/s1600/final.png" style="clear: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" height="133" src="https://2.bp.blogspot.com/-cvX-eD-CWqI/WwTph39sPdI/AAAAAAAAAAY/p-YRfgomFu0oyiXeSsMXTRAhNRQ4FbPGgCEwYBhgL/s640/final.png" width="640" /></a><br /><div><span><span style="white-space: pre-wrap;"><b>Possible Next steps</b></span></span></div><div><ul><li><span><span style="white-space: pre-wrap;">Predict more stock prices in the further future rather than just the next day.</span></span></li><li><span><span style="white-space: pre-wrap;">Implement an application to help optimize investment strategy</span></span></li></ul></div><ul><ul></ul></ul></div></div></div></div>







<p class="date">
<a href="https://teamoverfit.blogspot.com/2018/05/9-milestone-advanced-model-attempt-2.html">by Team Overfit (noreply@blogger.com) at May 23, 2018 04:11 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a></h3>


<div class="entrygroup" id="http://cse481n-capstone.azurewebsites.net/?p=114">
<h4 lang="en-US"><a href="http://cse481n-capstone.azurewebsites.net/2018/05/22/adversarial-data-collection-pilot-reddit-comment-storage-design-advanced-attempt-ii-continue/">Adversarial Data Collection Pilot &amp; Reddit Comment Storage Design (Advanced Attempt II Continue)</a></h4>
<div class="entry">
<div class="content" lang="en-US">
<h3>Adversarial Data Collection</h3>
<h4>Motivation for adversarial Data collection</h4>
<p><span style="font-weight: 400;">As we had shown previously, our model is very fixated with certain keywords when deciding that something is hate speech. For example, consider this sentence from a support subreddit:</span></p>
<p>“My wife was raped My wife tonight was raped, she doesn’t want to go to the police.”</p>
<p><span style="font-weight: 400;">Our best model considers this to be hate with 89% certainty. In particular it thinks the use of “raped” and “wife” (as opposed to “husband”) demonstrates hate speech. We think this has a lot to do with hateful speech online being commonly sexist against women while there isn’t as much obvious sexism against men. </span></p>
<p><span style="font-weight: 400;">However, we still want it to learn more sophisticated patterns. We decided that the best way to do this would be to find examples that we know are not hateful but that use the same keywords as the sentences the model deems hateful. This would add more uncertainty to the dataset, allowing us to train a more complex model. </span></p>
<p><span style="font-weight: 400;">Since we do not have any labeled data to choose from, we had to pick some sort of unlabeled data that we know in advance is going to almost always be not hateful. For this purpose, we decided to use news article headlines. Thus, these headlines are our adversarial data. They have the keywords that our model thinks are hateful, but they are almost guaranteed to not be hateful at all. </span></p>
<p><span style="font-weight: 400;">We considered two choices for collecting news article headlines. We could either use Reddit again, or use the Bing News search API through Azure. With Reddit, we would have to filter out everything except for a whitelist of news congregation subreddits and then search through the titles of the posts there for our keywords. In this type of subreddit, it’s enforced by moderators that the title of the post be the same as the headline of the article being linked. Because of our existing data collection experience, we would not need to learn anything new to go this route. With Bing, we would have to get access to their Search API resource in Azure and then pass it our keywords and manipulate the results. We ended up going with Bing because we decided that, as a search engine, Bing would be better at deciding the most relevant headlines for our keywords and thus provide us with better data for less effort.</span></p>
<h4>Pilot pipeline</h4>
<p><span style="font-weight: 400;">First, we want to extract ngrams from reddit posts that are labeled as hate by our current best model. These ngrams would later be ranked using document frequencies, and the most frequent ngrams would be used to search for news titles through Bing Search API.</span></p>
<p><span style="font-weight: 400;">We used the model with gru seq2vec encoder, 50d glove twitter embeddings, and ELMo, trained on combined twitter dataset to make predictions on 18k collected reddit posts (removing all posts with less than 4 tokens after preprocessing). We then extracted all posts labeled “hate” by this model. A total number of 3410 posts were labeled “hate”. Then we extracted all 1-3 ngrams after removing stop words and punctuations but keeping the special character apostrophe (‘) in place because there are words like can’t, won’t, don’t, etc. The output file contains multiple lines of lists. Each list consists of all 1-3 grams of a posts classified as “hate”, and each line is for each post.</span></p>
<p><span style="font-weight: 400;">After we generated a list of ngrams for each post, we need to process it to get ngrams that our model feels really hateful. Therefore, we decided to find the most frequent ngrams that appeared in all posts that are labeled as hate.</span></p>
<p><span style="font-weight: 400;">First, we need to do some data preprocessing.</span></p>
<ol>
<li style="font-weight: 400;"><span style="font-weight: 400;">We turned all words into lowercase and removed all non-alphabetic characters (including numbers) except punctionations within word like “can’t”. </span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Then we split each ngram into words, for each word, we used NLTK toolkit to tag it, and lemmatized back to the stem form. </span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Lastly, we decided to remove all unigrams after previous steps, because we found unigrams less descriptive comparing to bigram or trigrams.</span></li>
</ol>
<p><span style="font-weight: 400;">Then we compute the document frequency, and come up with a list of top 100 ngrams for us to explore on search engine.</span></p>
<p><span style="font-weight: 400;">For example, our ngrams looks like [“year old”, “what’s difference”, “year ago”, “old girl”, “year old girl”, “walk bar” ….]. We are aware that words like “year old”, “year old girl” looks really similar, and we would like to combine them, however, because this requires a lot more engineering and this is a pilot, we think it would be worth doing as a future work.</span></p>
<p><span style="font-weight: 400;">Once we have the ngrams we want to use as keywords for finding headlines, we send them through Bing’s news search feature. This required some experimenting to get right because if you just give Bing the keywords it will find articles that have them in the body, or sometimes just on the same website as the article. We had to make sure to use the “intitle:” restriction for each keyword of the ngram in question. As well, we found that Bing will silently filter out keywords that it thinks are offensive. For example, if you just search for articles with “intitle: jew” it removes that keyword and does an empty news search instead, which just returns the most recent articles published. This led to us doing a second round of filtering on top of the Bing results to make sure the sentences we give to the model actually have the keywords we want to test it against. As well, because our Azure tier doesn’t include Bing search, we had to use a 7-day free trial. For future work on this project we would have to change our Azure tier to keep using Bing, or we would have to switch to using Reddit for article headlines.</span></p>
<p><span style="font-weight: 400;">In the end we took the top 100 ngrams from the hate predicted posts and got up to 20 articles for each one. This resulted in almost 900 article headlines. However, when we ran these through our model we only got 73 of them labeled hate. This was an unexpected result, because we were expecting these headlines to confuse the model. On further investigation, we realized that because of Bing’s built in silent filtering, we were much less likely to get headlines that had these potentially controversial phrases in them. This happens even with safe-search turned off. If we had known about this when we were first brainstorming, we would have probably gone with using Reddit to get headlines instead.</span></p>
<p><span style="font-weight: 400;">Here are a few of the headlines that made it through Bing’s silent filter that the model did think were hate speech:</span></p>
<ul>
<li><em>Teen scarred for life after masked thug hurled acid over her in racially-motivated attack because she dated a black guy</em></li>
<li><em>I had to brutally murder the black gay guy because he hit on me</em></li>
<li><em>Starbucks drops the Jewish group Anti-Defamation League from its racial bias training after activists criticized their support of Israel and their failure to endorse Black …</em></li>
<li><em>Neo-Nazi who beat a black man with a 2-by-4 in Charlottesville pleads guilty</em></li>
<li><em>Kansas Cops Detain Black Man Because Of Vegetation On His Windshield</em></li>
<li><em>Can a Black Person Truly Love Black People if They Date Outside the Race?</em></li>
<li><em>Community responds after woman calls police on black people barbecuing</em></li>
<li><em>University survey asking if students want to know whether ‘black people hate America’ draws ire</em></li>
<li><em>Community responds after woman calls police on black people barbecuing</em></li>
<li><em>Oakland Residents Throw “BBQing While Black” Party After White Woman Called Police on Black Men for Grilling</em></li>
<li><em>3 are arrested in the stabbing of black man that officials call a hate crime</em></li>
<li><em>WATCH: Racist campers call black man a ‘n*gger’ 30 times after he asks them to leave his street</em></li>
<li><em>Understanding why you don’t call a black man a boy</em></li>
<li><em>Woman can’t get DirecTV to cancel service</em></li>
<li><em>Why you can’t get ‘Chelsea Dagger’ out of your head</em></li>
<li><em>Madonna: I Can’t Get Taylor Swift’s Songs Out of My Head</em></li>
<li><em>Spieth can’t seem to get anything right at Sawgrass</em></li>
<li><em>‘I hate her, can’t stand the b’: Daniel Heazlewood jailed for 11 years for killing his mother</em></li>
<li><em>You Can’t Tell Kids to ‘Just Say No’ to Legal Weed</em></li>
</ul>
<p><span style="font-weight: 400;">It’s clear that these were chosen based on keyword matching. We hope that including them in the training dataset in the future could make the model better, but we might have to iterate on this process using Reddit for headlines instead if we want more and better adversarial data.</span></p>
<h4>future work</h4>
<p><span style="font-weight: 400;">As mentioned above, currently we are using the most common non-trivial ngrams in all posts that are classified as hateful. However, we talked about another interesting way: found ngrams that have greatest ratio of df in predicted hate posts to df in predicted none posts. By doing so, we would be more sure about these ngrams’ contribution to the hatefulness of the post. This is a data processing step we can do in a future iteration to increase the adversarial impact of the data, since right now it is actually mostly clear to the model that these headlines should be labeled none.</span></p>
<p><span style="font-weight: 400;">Also, from our chosen ngrams, we found a lot of words that looks really similar. For example, we may have “year old” and “year old girl” appear at the same time, we can use other techniques to get rid one of them, like stemming and lemmatization . However, it might be computational challenging because of the large amount of ngrams we have. We think this is definitely a direction to look into.</span></p>
<p><span style="font-weight: 400;">Currently we are still working with unlabeled data, so we just used everything the model predicted as hate. Once our initial set of reddit data is labeled we could use that to make better decisions about what ngrams to use, such as by leaving out ngrams from true positives and true negatives from the start, and just dealing with the examples the model gets wrong. </span></p>
<p><span style="font-weight: 400;">Finally, we would like to try out other ways to collect headlines in the future, especially if there is no way around Bing’s silent filter. Using Reddit news subreddits like described up above is a possible alternative.</span></p>
<h3>Context extraction for Reddit Comment Design</h3>
<p><span style="font-weight: 400;">Here we present a high-level idea about how to store Reddit comment objects in DBMS for future context retrieval.  This is a substantial project on its own, therefore the implementation might not happen this quarter. We hope our exploration on this subject would pave the path for future work. </span></p>
<p><span style="font-weight: 400;">Last week, we presented an interesting paper, Anyone Can Become a Troll:</span><span style="font-weight: 400;"><br />
</span><span style="font-weight: 400;">Causes of Trolling Behavior in Online Discussions by Cheng et. al. To recap, the paper concludes that Negative Mood and Negative Discussion Context are the two causes of trolling behavior online. Moreover, the paper states that when training and evaluating a logistic regression classifier, “features relating to discussion context are most informative” (Cheng et. al.). </span></p>
<p><span style="font-weight: 400;">This gives us an idea that could potentially improve our model performance on reddit comments. Since a reddit post/discussion format is very much like CNN.com’s comment section where the post takes the role of an article, and all subsequent comments either spawn directly from the post or from previous comments, we believe the context of comments would also be a very useful feature for us to use when we use reddit comment data in the future (note: we are collecting reddit posts at this moment, but the data collection pipeline could easily be ported to reddit comments). </span></p>
<p><span style="font-weight: 400;">A reddit comment JSON object looks like this in the comment dumps we get from pushshift.io: </span></p>
<pre><span style="font-weight: 400;">{
    'author': 'LysergicOracle',</span>

<span style="font-weight: 400;">    'author_flair_css_class': None,</span>

<span style="font-weight: 400;">    'author_flair_text': None,</span>

<span style="font-weight: 400;">    'body': '&lt;3',</span>

<span style="font-weight: 400;">    'can_gild': True,</span>

<span style="font-weight: 400;">    'controversiality': 0,</span>

<span style="font-weight: 400;">    'created_utc': 1512086400,</span>

<span style="font-weight: 400;">    'distinguished': None,</span>

<span style="font-weight: 400;">    'edited': False,</span>

<span style="font-weight: 400;">    'gilded': 0,</span>

<span style="font-weight: 400;">    'id': 'dql1dzn',</span>

<span style="font-weight: 400;">    'is_submitter': False,</span>

<span style="font-weight: 400;">    'link_id': 't3_7go27t',</span>

<span style="font-weight: 400;">    'parent_id': 't1_dql0d4o',</span>

<span style="font-weight: 400;">    'permalink': '/r/freefolk/comments/7go27t/jonerys_first_fight_306_ac_colorized/dql1dzn/',</span>

<span style="font-weight: 400;">    'retrieved_on': 1514212661,</span>

<span style="font-weight: 400;">    'score': 2,</span>

<span style="font-weight: 400;">    'stickied': False,</span>

<span style="font-weight: 400;">    'subreddit': 'freefolk',</span>

<span style="font-weight: 400;">    'subreddit_id': 't5_37tpy',</span>

<span style="font-weight: 400;">    'subreddit_type': 'public',</span>
<span style="font-weight: 400;">}</span></pre>
<p><span style="font-weight: 400;">Here we get a comment from subreddit freefolks and the comment’s content is ‘&lt;3’. “link_id” is the id of the post under which this comment is posted, and “parent_id” is the id of the comment or post under which this comment is posted. In the case where the current comment is a top-level comment (directly posted under the post, with no parent comment), “link_id” and “parent_id” would be the same. We can also use “created_utc” to figure out which comment was created first if they are siblings, thus knowing which comment could have had an influence on the other.  </span></p>
<p><span style="font-weight: 400;">To recreate the discussion context of a comment, we are thinking of two kinds of queries: 1) queries that find all “ancestor comments” of the current comment; 2) queries that finds all “sibling comments” that were posted at an earlier timestamp than the current comment. Then we can incorporate the level of negativity of these comments as features when learning or making predictions of the current comment.</span></p>
<p><span style="font-weight: 400;">To store these comment objects in a DBMS for fast retrieval and context construction, we did some research on which DBMS to use. There are many DBMS systems out there that support JSON datatype, but the two we mainly looked at were MongoDB and PostgreSQL. Both of them are open source projects. MongoDB supports native JSON storage and has its own set of query commands. PostgreSQL was initially designed to be a SQL database, but it has supported JSON datatype for a few years. While both were valid choices, PostgreSQL supports SQL like query language on JSON data. It also supports recursive queries, which is perfect for the purpose of context tree construction. Therefore, we decided PostgreSQL is a better choice for Reddit comment storage.</span></p>
<p><span style="font-weight: 400;">For future work, we would spin up a PostgreSQL server and write a client-side package for comment context retrieval.</span></p>
<h3>Advanced Model Attempt Update</h3>
<p><span style="font-weight: 400;">Last week, we implemented an attention LSTM model and we reported that there was a bug in pytorch. However, after doing some checking, we realized that there was a dimension error on our side.</span></p>
<p><span style="font-weight: 400;">Now we have complete statistics for our attention LSTM/GRU model:</span></p>
<p><span style="font-weight: 400;">All models are trained and evaluated on twitter Waseem dataset.</span></p>
<table>
<tbody>
<tr>
<td>50d</td>
<td>LSTM w/o ELMo</td>
<td>LSTM w/ ELMo</td>
<td>GRU w/o ELMo</td>
<td>GRU w/ ELMo</td>
</tr>
<tr>
<td>F1</td>
<td>0.7907</td>
<td>0.7879</td>
<td>0.7823</td>
<td>0.7770</td>
</tr>
<tr>
<td>Precision</td>
<td>0.7961</td>
<td>0.8018</td>
<td>0.7781</td>
<td>0.7851</td>
</tr>
<tr>
<td>Recall</td>
<td>0.7862</td>
<td>0.7784</td>
<td>0.7875</td>
<td>0.7708</td>
</tr>
<tr>
<td>Accuracy</td>
<td>0.8194</td>
<td>0.8207</td>
<td>0.8056</td>
<td>0.8091</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>100d</td>
<td>LSTM w/o ELMo</td>
<td>LSTM w/ ELMo</td>
<td>GRU w/o ELMo</td>
<td>GRU w/ ELMo</td>
</tr>
<tr>
<td>F1</td>
<td>0.7907</td>
<td>0.7864</td>
<td>0.7913</td>
<td>0.7931</td>
</tr>
<tr>
<td>Precision</td>
<td>0.8076</td>
<td>0.7873</td>
<td>0.7931</td>
<td>0.7999</td>
</tr>
<tr>
<td>Recall</td>
<td>0.7798</td>
<td>0.7854</td>
<td>0.7895</td>
<td>0.7876</td>
</tr>
<tr>
<td>Accuracy</td>
<td>0.8242</td>
<td>0.8132</td>
<td>0.8180</td>
<td>0.8221</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>200d</td>
<td>LSTM w/o ELMo</td>
<td>LSTM w/ ELMo</td>
<td>GRU w/o ELMo</td>
<td>GRU w/ ELMo</td>
</tr>
<tr>
<td>F1</td>
<td>0.7944</td>
<td>0.7745</td>
<td>0.7948</td>
<td>0.7725</td>
</tr>
<tr>
<td>Precision</td>
<td>0.7902</td>
<td>0.7870</td>
<td>0.7874</td>
<td>0.8001</td>
</tr>
<tr>
<td>Recall</td>
<td>0.7995</td>
<td>0.7659</td>
<td>0.8074</td>
<td>0.7580</td>
</tr>
<tr>
<td>Accuracy</td>
<td>0.8166</td>
<td>0.8091</td>
<td>0.8132</td>
<td>0.8132</td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;">From our results, we see a very consistent performance (~0.79 F1 score) when attention is used. And it seems ELMo, in this case, does not introduce any help. </span></p>
<p><span style="font-weight: 400;">The reason might be attention with ELMo need much more data in order to have a performance improvement. We will train on the combined dataset in the coming days and update our statistics.</span></p>
<h3>Work Cited</h3>
<p><a href="https://files.clr3.com/papers/2017_anyone.pdf"><span style="font-weight: 400;">Cheng, Justin et al. “Anyone Can Become a Troll: Causes of Trolling Behavior in Online Discussions.” CSCW : proceedings of the Conference on Computer-Supported Cooperative Work. Conference on Computer-Supported Cooperative Work 2017 (2017): 1217-1230.</span></a></p></div>







<p class="date">
<a href="http://cse481n-capstone.azurewebsites.net/2018/05/22/adversarial-data-collection-pilot-reddit-comment-storage-design-advanced-attempt-ii-continue/">by Team Watch Your Language! at May 23, 2018 02:11 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 22, 2018</h2>

<div class="channelgroup">







<h3><a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-5600014144802012716.post-7945337673285975235">
<h4><a href="https://nlpcapstonesemparse.blogspot.com/2018/05/blog-9.html">Blog 9</a></h4>
<div class="entry">
<div class="content">
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Decided to edit code prototypes. The motivation comes from a high baseline score which uses this </span><br /><span>approach. Namely, when generating a method, if the closest method by maximum comment Jaccard </span><br /><span>distance is picked, a bleu score of .34 is achieved, almost 15 points higher than the actual model! </span><br /><span>Though note em is 0 for this baseline since it's picking a different method.</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span><br /></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>I’ve run some experiments with the code prototypes. None seem to be working that well but I have </span><br /><span>some ideas as to why.</span></div><b id="docs-internal-guid-4c49591c-89e4-bc06-8efe-ea78d3b9b1e1" style="font-weight: normal;"><br /></b><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Since the dataset is slightly altered the baseline is at .336 bleu and .128 em.</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>The prototype model which concats encoded prototype and utterance is at .331</span><br /><span> bleu and .119 em, meaning it’s not using the prototype information and is the same </span><br /><span>as the baseline. This is evident after examining the model outputs.</span></div><div><span><br /></span></div><div><span><span style="font-size: 14.6667px; white-space: pre-wrap;">The model needs to be designed better in order to figure out what parts of the prototype to copy. This is what I'm currently figuring out.</span></span></div></div>







<p class="date">
<a href="https://nlpcapstonesemparse.blogspot.com/2018/05/blog-9.html">by nlpcapstone (noreply@blogger.com) at May 22, 2018 11:38 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 17, 2018</h2>

<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=339">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/05/17/nlp-capstone-post-8-training-challenges/">NLP Capstone Post #8: Training challenges</a></h4>
<div class="entry">
<div class="content" lang="en">
<h1><span style="font-weight: 400;">Challenges with training TSL model</span></h1>
<p><span style="font-weight: 400;">In our previous post we proposed a three-model system that would allow us to take advantage of a larger corpus of higher quality lyrics data for the production of lyrics. We also finally tackle the alignment task with a simple approach of determining whether a lyric token should be produced at each timestep. This seems sensible since we have begun dividing the MIDIs into pianorolls with a constant frequency.</span></p>
<p><span style="font-weight: 400;">Unfortunately, even after several bugs bashed, we’ve been still unable to produce even sensible timings. We find the RNN collapses to repeatedly generating 0 (for no lyric event), even though a randomly initialized RNN will repeatedly generate 1 (and perform better with respect to classification accuracy).</span></p>
<p> </p>
<h1><span style="font-weight: 400;">Future direction</span></h1>
<p><span style="font-weight: 400;">If we are able to produce something reasonable from our existing architecture, we would like to move on to a second model that structures the problem as machine translation. We have decided to focus on the paper Attention is All You Need by Vaswani et al. [1] for our presentation in two weeks. The structure of our problem is straightforward to apply to translation as converting pianoroll format into english sentences. Incorporating attention has shown promising results in the literature, though that is no guarantee that our noisy dataset would be able to take advantage of this proposed architecture.</span></p>
<p> </p>
<h1><span style="font-weight: 400;">References</span></h1>
<p><span style="font-weight: 400;">[1] </span><a href="https://arxiv.org/abs/1706.03762"><span style="font-weight: 400;">https://arxiv.org/abs/1706.03762</span></a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/05/17/nlp-capstone-post-8-training-challenges/">by Nicholas Ruhland at May 17, 2018 05:55 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 16, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-2">
<h4><a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-2">Advanced Model Attempt #2</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">Wow, can't believe we're already in week 8! <br /><br />This past week, I've worked on shifting my project from not having a neural component, to well, having one. With inspiration from the work of Ghosh et al. in <em>Affect-LM: A Neural Language Model for Customizable Affective Text Generation, </em>I've extended my project (and am encroaching on stretch goal territory) to include a neural language model for reddit post generation***. The idea would be to train a model on reddit posts out of five categories - F30 (mood [affective] disorders), F40<font color="#515151"> (Anxiety, dissociative, stress-related, somatoform and other nonpsychotic mental disorders), X71 (Intentional Self Harm), F10 (addiction categories), and neurotypical advice - and use that model for post generation, based on the specified target type, i.e. generate a F30 reddit post. <br /><br /></font><br /></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-2">May 16, 2018 11:19 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=25">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/05/16/advanced-model-attempt-2-part-1/">Advanced Model Attempt 2 (Part 1)</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>This week I was able to greatly improve upon the results from last week. Last week my average reward got to about -23 at best, but this week by tweaking my reward function I was able to greatly speed up convergence and bump my model up to about +70 reward on average. I made my reward function more strict about interpreting the user response correctly by giving a +6 reward for correct interpretation, -6 reward for the opposite interpretation and 0 reward for interpreting as unknown (model is unsure).</p>
<p>One way I tried to improve over last week was reducing my CNN architecture to simplify the state representation, but surprisingly this had the opposite effect of underfitting on the user responses and misinterpreting them more. In fact, I later found out that my reward function was the main reason my model didn’t converge and in the end I actually ended up increasing the number of filters in my CNN architecture.</p>
<p>At this point, my model is able to win between 3 and 4 out of 5 games on average because there is 1 out of my 31 questions for which it learned the wrong Q values, probably due to insufficient exposure to the right answer, so I’ll increase the amount of exploration the model takes until these values are straightened out and the model wins 5 out of 5 games. Then I’ll evaluate the model performance on a separate database of 100 different people. While I expect the end result to be the same (winning 100% of games), I expect the model to take longer to win on average because the order of questions it asks to eliminate people is tailored towards the training database.</p>
<p>My goals for this week are reporting more exact quantitative results of model performance in terms of games won and reward gained, both on the training database and a validation database of 100 different people. Furthermore, I’ll experiment with a slightly more complicated dialogue scenario but I probably won’t go too deep considering I only have about a week left.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/05/16/advanced-model-attempt-2-part-1/">by ananthgo at May 16, 2018 06:55 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-9203775015655831448.post-3973266691169482239">
<h4><a href="https://teamoverfit.blogspot.com/2018/05/8-milestone-advanced-model-attempt-2.html">#8 Milestone: Advanced model attempt #2</a></h4>
<div class="entry">
<div class="content">
<h2 style="height: 0px;"><span>Team Overfit</span></h2><h3><span><br /></span></h3><h3><span>Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span>Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><div style="text-align: start; text-indent: 0px;"><div style="margin: 0px;"><div><span><b>Blog Post: #8: 05/15/2018</b></span></div><div><span><span><b><br /></b></span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Social Media Predicts Stock Price (StartUp Mode)</span><br /><span><br /></span><span>This week, we weren't able to make our model make better prediction on stock price movement.</span><br /><h3><span>We have tried:</span></h3><ul><li><span>Improve initialization weights and bias</span></li><ul><li><span>Increase standard deviation so that the initial price prediction varies more preventing the model to be stuck at local minima</span></li><li><span>Decrease bias so that the initial prediction are closer to the expected, and therefore, converge faster</span></li></ul><li><span>Tune hyperparameters of LSTM and try out different settings of data</span></li><ul><li><span>Number of time steps of RNN</span></li><li><span>Size of dictionary</span></li><ul><li><span>Remove vocab that have high document frequency</span></li></ul><li><span>Number of LSTM hidden units</span></li><li><span>Number of RNN layers</span></li><li><span>Learning rate</span></li><li><span>Dropout layer</span></li></ul><li><span>Visualize and investigate our prediction/training to have better understanding of our model</span></li><ul><li><span>Find out the top k words that have most weight for a given day</span></li><ul><li><span>Most of them do not seem to relate to stock price movement</span></li></ul><li><span>Compare the prediction/target graph for each epochs to see if the predictions are improving/convergin</span></li></ul></ul><h3><span>In progress and next steps:</span></h3><div><ul><li><span>Find good news data and write preprocessor</span></li><li><span>Research on better featurizations</span></li><li><span>Research on better model</span></li><li><span>Add learning rate decay to our training</span></li><li><span>Investigate the correlation between sentiment analysis of Tweets and stock price movement</span></li></ul></div></div></div></div></div>







<p class="date">
<a href="https://teamoverfit.blogspot.com/2018/05/8-milestone-advanced-model-attempt-2.html">by Team Overfit (noreply@blogger.com) at May 16, 2018 06:37 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a></h3>


<div class="entrygroup" id="http://cse481n-capstone.azurewebsites.net/?p=105">
<h4 lang="en-US"><a href="http://cse481n-capstone.azurewebsites.net/2018/05/15/advanced-attempt-ii-2/">Advanced Attempt II</a></h4>
<div class="entry">
<div class="content" lang="en-US">
<h3>Data Collection</h3>
<p><span style="font-weight: 400;">In our last post, we described our final collection of Reddit sentences that we would send for labeling. We decided to add subreddit metadata to each sentence before sending it off, but this created an unexpected slight setback because of the way we spliced posts into sentences after throwing away all of the metadata associated with the original dump. We did finish connecting each sentence to its originating subreddit however, and from doing so discovered a slight issue with our data. It turned out that from doing the hate lexicon matching described earlier, we got a few posts that looked like: “[uncensored-r/Bitcoin] &lt;</span><i><span style="font-weight: 400;">some original post</span></i><span style="font-weight: 400;">&gt; The following post by vichuu is being replicated because the post has been silently removed.” In the end we decided to add the subreddit “noncensored_bitcoin” to our blacklist and removed all its sentences from our dataset, but this decreased our dataset size to ~18k, down 2k from the last blog post. </span></p>
<p><span style="font-weight: 400;">In our sixth blog post we showed the top 10 subreddits for quantity of posts. Here are the top 10 subreddits just in our dataset. </span></p>
<p><span style="font-weight: 400;">As expected, we have a lot of MeanJoke sentences as well as sentences from other joke subreddits that probably have similar language structure. The inclusion of the hate lexicon set of sentences most likely helped bring diversity to this final set of sentences. Otherwise, it looks like a good collection of discussion subreddits. As well, since there are 18k sentences and the top 10 subreddits only sum to ~6000 of them we once again see the sparse nature of our data’s origins. </span></p>
<table>
<tbody>
<tr>
<td>Subreddit</td>
<td>Number of Sentences</td>
</tr>
<tr>
<td>MeanJokes</td>
<td>2964</td>
</tr>
<tr>
<td>AskReddit</td>
<td>706</td>
</tr>
<tr>
<td>Jokes</td>
<td>565</td>
</tr>
<tr>
<td>darkjokes</td>
<td>387</td>
</tr>
<tr>
<td>depression</td>
<td>284</td>
</tr>
<tr>
<td>relationships</td>
<td>280</td>
</tr>
<tr>
<td>DebateConservatives</td>
<td>253</td>
</tr>
<tr>
<td>offmychest</td>
<td>227</td>
</tr>
<tr>
<td>raisedbynarcissists</td>
<td>168</td>
</tr>
<tr>
<td>relationship_advice</td>
<td>166</td>
</tr>
</tbody>
</table>
<h3>Advanced Model Attempt</h3>
<p><span style="font-weight: 400;">We implemented a new model which incorporates attention.</span></p>
<p><span style="font-weight: 400;">The way we did it is by writing an seq2seq (in this case, is LSTM) encoder with attention and then take the last dimension of each sequence as our vector representation of the whole sentence.</span></p>
<p><span style="font-weight: 400;">For example, if we have (batch_size, sentence_length, dim), our vector representation will become (batch_size, dim).</span></p>
<p><span style="font-weight: 400;">After that, we passed it through a 2-layer feedforward neural network and map the output to a 2-dimensional vector which represents each class (hate, none in this case). Then we do a softmax and pick the one with the highest probability as our prediction.</span></p>
<p><b>Statistics</b></p>
<table>
<tbody>
<tr>
<td><span>50d</span></td>
<td><span>Without ELMo</span></td>
</tr>
<tr>
<td><span>F1</span></td>
<td><span>0.7907</span></td>
</tr>
<tr>
<td><span>Precision</span></td>
<td><span>0.7961</span></td>
</tr>
<tr>
<td><span>Recall</span></td>
<td><span>0.7862</span></td>
</tr>
<tr>
<td><span>Accuracy</span></td>
<td><span>0.8194</span></td>
</tr>
</tbody>
</table>
<p> </p>
<table>
<tbody>
<tr>
<td><span>100d</span></td>
<td><span>Without ELMo</span></td>
</tr>
<tr>
<td><span>F1</span></td>
<td><span>0.7907</span></td>
</tr>
<tr>
<td><span>Precision</span></td>
<td><span>0.8076</span></td>
</tr>
<tr>
<td><span>Recall</span></td>
<td><span>0.7798</span></td>
</tr>
<tr>
<td><span>Accuracy</span></td>
<td><span>0.8242</span></td>
</tr>
</tbody>
</table>
<p> </p>
<table>
<tbody>
<tr>
<td><span>200d</span></td>
<td><span>Without ELMo</span></td>
</tr>
<tr>
<td><span>F1</span></td>
<td><span>0.7944</span></td>
</tr>
<tr>
<td><span>Precision</span></td>
<td><span>0.7902</span></td>
</tr>
<tr>
<td><span>Recall</span></td>
<td><span>0.7995</span></td>
</tr>
<tr>
<td><span>Accuracy</span></td>
<td><span>0.8166</span></td>
</tr>
</tbody>
</table>
<p><b>Unexpected Problem encountered:</b></p>
<p><span style="font-weight: 400;">There was a bug in pytorch saying “fn” is undefined when we want to train LSTM with ELMo using CUDA, and we have no luck on fixing that. After googling it, it turns out to be a problem related to pytorch 0.3.1 and it’s fixed in a later version like 0.4.0. Since Allennlp requires pytorch 0.3.1, we may have to try something else instead.</span></p>
<p><span style="font-weight: 400;">We will try to use GRU instead (hopefully it works) and report all metrics in detail in next week’s blog post.</span></p>
<h3>Demo Frontend</h3>
<p><span style="font-weight: 400;">Previously, we used default AllenNLP frontend for demoing purpose. Although it was easy to set up and use by us the developers who knew exactly what numbers we are looking for, it was not intuitive to use at all for other people. The default demo page looked like this:</span></p>
<p><img alt="" class="alignnone size-full wp-image-107" height="922" src="http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/old_demo.png" width="2712" /></p>
<p><span style="font-weight: 400;">In order to make the demo page more straightforward, we decided to only keep the most relevant fields of the model output: predicted label and class probabilities. Since it is also interesting to see how prediction changes every time a new word is entered, we designed the page to make a prediction everytime a blank space or ENTER is entered in the text input box (in addition to the Predict button being hit). Our new demo page looked like this (serving a different model):</span></p>
<p><img alt="" class="alignnone size-full wp-image-108" height="1034" src="http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/new_demo.png" width="2758" /></p>
<p><span style="font-weight: 400;">As shown above, while the overall page format looked the same (the left half is input, the right half is output), the output display changed quite a bit. The predicted label is nicely put into a sentence and highlighted under “Prediction”, and the class probabilities are in a table with the “Hate probability” entry highlighted under “Summary Breakdown”. We also added hate probability visualization at the end. A linear gradient from bright green to red represented hate probability from 0% to 100%, a dark vertical line representing the predicted hate probability of the current sentence, and a small label under the dark line showing the hate probability. </span></p>
<p><span style="font-weight: 400;">Not only is the new demo page easier to read and see the changes when each word is entered, this simple output display could potentially be ported to AMT surveys in the future if we want turkers to come up with hard to detect examples.</span></p>
<h3>Next Steps:</h3>
<p>On the data collection front, we will push forward in the process of getting our reddit dataset labeled.  For modeling, we would perform more hyperparameter tuning and error analysis as well as incorporating ELMo into the current model, and hopefully, we get to explore potential ways to make neural nets work with relatively small datasets like these twitter datasets that we have.</p></div>







<p class="date">
<a href="http://cse481n-capstone.azurewebsites.net/2018/05/15/advanced-attempt-ii-2/">by Team Watch Your Language! at May 16, 2018 03:27 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 15, 2018</h2>

<div class="channelgroup">







<h3><a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-5600014144802012716.post-6025423166932572092">
<h4><a href="https://nlpcapstonesemparse.blogspot.com/2018/05/blog-8.html">Blog 8</a></h4>
<div class="entry">
<div class="content">
I'm focusing now on combining coding patterns from the new class at test time with the patterns learned from the training set. This means both the code and javadocs for all the other methods are now being utilized as well, as opposed to the previously where just the method name is used. This is really interesting, both from code and language understanding, but also very challenging. This week I've been doing analysis on this approach, and preparing the dataset for this modified task through filtering heuristics.<br /><br />I ran an experiment to empirically test whether using the method implementations will help. For each method whose code your supposed to generate, I picked the class method which maximized the bleu and em score. This provides a soft upper bound. This naive baseline gets a bleu of .42(2 times the state of the art!) and an em of 0.06. When I randomly picked another implementation(soft lower bound) I got bleu of .13 which is half the state of the art.<br /><br />Additionally, to learn these coding patterns, the documentation and code needs to be of high quality hence I've been filtering the data to reduce the noise. Here are the criteria, if any are true the class is removed. This so far has cut down the number of classes from around 2 million to a hundred thousand. There's more work still to be done here.<br /><br /><ul><li>A method uses identifiers not present in the class and occurring in the training set under 7 times.</li><li>Use of too many integer literals</li><li>Too few method javadocs.</li></ul><br /><br /></div>







<p class="date">
<a href="https://nlpcapstonesemparse.blogspot.com/2018/05/blog-8.html">by nlpcapstone (noreply@blogger.com) at May 15, 2018 06:08 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 10, 2018</h2>

<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=335">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/05/10/nlp-capstone-post-7-tsl-pipeline/">NLP Capstone Post #7: TSL Pipeline</a></h4>
<div class="entry">
<div class="content" lang="en">
<p> </p>
<h2><span style="font-weight: 400;">Modeling issues</span></h2>
<p><span style="font-weight: 400;">As seen in our results last week, the RNN architecture we have been training has not been able to produce any coherent series of tokens based on the music data provided in the clean Lakh dataset. To analyze the poor results of this model, we have considered various features of the quality of the data. To simplify the issue of timing the lyric tokens, this model attempts to predict a lyric token at every timestep. Between each token we have summed all the musical data, producing a piano roll that looks approximately like the following image.</span></p>
<p><img alt="Screen Shot 2018-05-09 at 4.31.53 PM" class="  wp-image-331 aligncenter" height="303" src="https://mathstoc.files.wordpress.com/2018/05/screen-shot-2018-05-09-at-4-31-53-pm.png?w=501&amp;h=303" width="501" /></p>
<p><span style="font-weight: 400;">In the event that two lyrics occur at exactly the same time step, we end up with a gap in the notes, here highlighted in red.</span></p>
<p><img alt="Screen Shot 2018-05-09 at 4.31.53 PM" class="  wp-image-334 aligncenter" height="302" src="https://mathstoc.files.wordpress.com/2018/05/screen-shot-2018-05-09-at-4-31-53-pm1.png?w=500&amp;h=302" width="500" /></p>
<p><span style="font-weight: 400;">At first we expected this problem to occur in only a small number of cases, but it is often the result of the newline character appearing in a message simultaneously with the first lyric of the next sentence. This processing poses several problems to the task of learning the lyrical content based on the structure of the music. First, the large number of musical gaps may be confounding the model due to the large variety in lyrics that will be seen at those time steps. Additionally, we lose all information about the song timing since all regions without lyrics are compressed into a single time step. In theory, gaps in lyrics could hint to the model that the next section should start a new verse or chorus.</span></p>
<h2><span style="font-weight: 400;">The TSL Pipeline</span></h2>
<p><span style="font-weight: 400;">As suggested in the previous blogpost, we would like to be able to augment the results of the musical model with a higher quality lyrical dataset. The Kaggle lyrics dataset has shown promising results in previous blogposts at the quality of the lyric sequences it has been able to produce.</span></p>
<p><span style="font-weight: 400;">The TSL Pipeline is a combination of three models: Timing, Seed, and Lyrics. The architecture may look something like the following diagram:</span></p>
<p><img alt="Training" class="  wp-image-333 aligncenter" height="221" src="https://mathstoc.files.wordpress.com/2018/05/training.png?w=531&amp;h=221" width="531" /></p>
<p><span style="font-weight: 400;">During training, each pianoroll will be separated into data representing the timing, notes and lyrics. These get passed into respective models to learn timing and “seed” information. Additional lyrics information from the Kaggle dataset is used to train a lyrical model.</span></p>
<p><img alt="Evaluation" class="  wp-image-332 aligncenter" height="220" src="https://mathstoc.files.wordpress.com/2018/05/evaluation.png?w=538&amp;h=220" width="538" /></p>
<p><span style="font-weight: 400;">At evaluation time, the lyrics from the original pianoroll are not passed into the Seed model. Instead, the Seed model attempts to predict some seed based on the musical content, and will pass its result into the lyrics model. The combination of these lyrics and timing information constitute the complete description of our karaoke output.</span></p>
<h2><span style="font-weight: 400;">Timing Model</span></h2>
<p><span style="font-weight: 400;">In all previous posts we ignored the issue of lyrics timing in the interest of creating a reasonable lyrical model. Our current timing model is similar to our previous model attempt, but the data is generated differently. Instead of computing a pianoroll sample at each lyrical timestep, we us a constant sampling frequency of 10 timesteps per second. We then annotate each timestep with a 1 or 0 based on if a lyric was annotated at that step. The model will then attempt to predict for each step of a given pianoroll the probability there should be a lyric at that time.</span></p>
<h2><span style="font-weight: 400;">Seed Model</span></h2>
<p><span style="font-weight: 400;">The seed model will be a simplified version of the poorly performing model from before. Instead of predicting all lyrics, it will attempt to predict a small subset of the initial lyrics. This would also allow us to create a dataset with more training examples by splitting each song into smaller samples.</span></p>
<h2><span style="font-weight: 400;">Lyrics Model</span></h2>
<p><span style="font-weight: 400;">The lyrics model will be similar to the one described in the second blog post, which is a character level RNN for generating lyrics. This will take the first few words predicted by the seed model and generate the remainder of the lyrics. Since it’s trained on the large Kaggle dataset the quality seems to be much higher than what our MIDI training has produced.</span></p>
<h2>Results</h2>
<p>As of this blog post, we are still testing various hyperparameters and waiting for models to converge.  Additional results will follow once we can examine the various output.</p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/05/10/nlp-capstone-post-7-tsl-pipeline/">by Nicholas Ruhland at May 10, 2018 06:56 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 09, 2018</h2>

<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=23">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/05/09/advanced-model-attempt-1-part-2/">Advanced Model Attempt 1 (Part 2)</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>This week I completely finished the model end-to-end and programmed the rules of the dialogue/game. The input to the model is created as follows: the user response is broken up into individual words and converted into a matrix of word embeddings. That matrix is run through a CNN before being fed into the LSTM and finally the policy networks to compute the Q values for user actions (questions to the user) and hypothesis actions (updates to the database). The model I created closely follows the one in this paper: <a href="https://arxiv.org/abs/1606.02560" rel="nofollow">https://arxiv.org/abs/1606.02560</a></p>
<p>I was able to run the simulation on my computer and debug a few issues with the model before starting the actual training session. Furthermore, I moved the model to Azure, set up the SQL database on the remote computer, and am currently running the simulation on tensorflow-gpu.</p>
<p>The initial results are not too great but somewhat promising. The model assigns +30 reward for winning, -30 for losing, -5 for bad guesses and +1 if it correctly understands the user response and performs the right action on the database. My model started out with an average of -30 reward for an entire game (it almost never took the same action that the user said and then abruptly lost) and after about 100,000 time steps that average has decreased to -23. This tells me the model is eventually learning how to understand the user responses but the convergence is still a lot slower than I expected.</p>
<p>As a side note, while it seems silly to try classifying user responses into buckets like ‘Yes’, ‘No’, ‘Unknown’ instead of training a separate classifier, the goal is to build a model that can be generalized to more complex dialogue types where there are more nuanced actions that heavily depend on dialogue state, not just the current user response.</p>
<p>The main difference between my model and the model in the paper (barring any bugs I am unaware of) is how the user response is represented. In both models, the model tries to combine NLU and dialog state tracking into one end-to-end model and the user response sentence representation is crucial in determining the dialogue state at any given time step. The paper creates a very short bag of bigrams feature vector (of about 30 features) from the sentence whereas I create glove vectors (embedding dimension = 50) for each word and run them through a CNN to get an output of 250-300 features.</p>
<p>The paper made it sound like they got the model to converge and play the game well after about 120000 time steps. To play the game well, the model needs to correctly understand what action should be done on the database based on the user response. My guess is their model converges much faster and can learn to make the right action much faster than mine because of their simpler state representation. Once the model learns how to correctly interpret the user response, it becomes much easier to learn how to win from any given dialogue state. This allows the model to generalize to people it hasn’t seen because the model only learns what actions to take based on the dialogue state, not on the attributes of any given person.</p>
<p>My goal for next week is to try tuning my model to converge faster. My main plan will be to shorten and simplify the state representation to be more like the paper’s (although it will still be different since the paper used utterance embeddings for speech, not word embeddings). If the model somehow works well after tuning, I will look into trying to train the model on more complex user responses where the set of actions to take are more qualitative than yes and no.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/05/09/advanced-model-attempt-1-part-2/">by ananthgo at May 09, 2018 06:58 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-9203775015655831448.post-6866691098285474093">
<h4><a href="https://teamoverfit.blogspot.com/2018/05/7-milestone-advanced-model-attempt-1.html">#7 Milestone: Advanced model attempt #1 (continued)</a></h4>
<div class="entry">
<div class="content">
<h2 style="height: 0px;"><span>Team Overfit</span></h2><h3><span><br /></span></h3><h3><span>Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span>Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><div style="text-align: start; text-indent: 0px;"><div style="margin: 0px;"><div><span><b>Blog Post: #7: 05/09/2018</b></span></div><div><span><span><b><br /></b></span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Social Media Predicts Stock Price (StartUp Mode)</span><br /><br /><span><span style="white-space: pre;"><b>Try Out Logistic Regression To Tune LSTM</b></span></span><br /><ul><li><span><span style="white-space: pre;"><b><span id="docs-internal-guid-bd404b7a-43cb-ecd9-d5e4-fdaf1add2bb5" style="font-weight: normal;"><span>From our previous model, which we classify the stock price movement as UP/DOWN/STAY, we noticed that during training often the loss will converge in less than 5 epochs and the predictions are either all UPs or all STAYs depending on the range of percent difference we classified as STAY. It seems like the model wasn’t able to learn much from the input data.</span></span></b></span></span></li><li><span><span style="white-space: pre;"><b><span style="font-weight: normal;"><span><span id="docs-internal-guid-bd404b7a-43cc-42d1-70ab-45794e67af61"><span style="vertical-align: baseline;">Then, we tried to use logistic regression by making our RNN only has one output node and remove the softmax function. We also changed the loss function from Cross-Entropy to mean squared loss. This helps to predict the percentage of change of the stock price directly.</span></span></span></span></b></span></span></li><li><span><span style="white-space: pre;"><b><span style="font-weight: normal;"><span><span style="vertical-align: baseline;"><span id="docs-internal-guid-bd404b7a-43cc-6947-13f3-a833249311ac"><span style="vertical-align: baseline;">With the new training method, the model can now predict a specific stock price for a day given input data. Then, according to the stock price predicted, we can further categorize the prediction into UP/DOWN/STAY and use our previous evaluation function to determine the accuracy of our model.</span></span></span></span></span></b></span></span></li></ul><div><span><span style="white-space: pre-wrap;"><b>Select and Add Competitors Stock Price to Tune LSTM</b></span></span></div><div><ul><li><span><span style="white-space: pre-wrap;"><span id="docs-internal-guid-bd404b7a-43cd-277f-263a-f85ae0ec0c14"><span style="vertical-align: baseline;">For company Apple, we selected company Samsung’s and Huawei’s stock prices as part of input data to train the model, because this two big company also produce mobile phones and are the biggest competitors for Apple in the industry.</span></span></span></span></li><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;"><span id="docs-internal-guid-bd404b7a-43cd-46f5-47b8-55814d42e5b3"><span style="vertical-align: baseline;">For company Tesla, we selected company Toyota for the similar reason as for Apple.</span></span></span></span></span></li></ul><div><span><span style="white-space: pre-wrap;"><b>Results</b></span></span></div></div><div><ul><li><span><span style="white-space: pre-wrap;"><span id="docs-internal-guid-bd404b7a-43cd-a926-74ae-e30f944bf03e"><span style="vertical-align: baseline;">After we have improved the model and the training process, it no longer generates the same classification for all input. It now does try to predict the stock price percent difference and has reasonable results.</span></span></span></span></li><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;"><span id="docs-internal-guid-bd404b7a-43cd-bab2-93f6-880e4aaa0b8c"><span style="vertical-align: baseline;">The prediction accuracy for Apple will be around </span><span style="font-weight: 700; vertical-align: baseline;">55.5</span><span style="vertical-align: baseline;">% if we choose </span><span style="font-style: italic; vertical-align: baseline;">‘+/-0.5% change of the stock price</span><span style="vertical-align: baseline;"> as </span><span style="font-style: italic; vertical-align: baseline;">‘STAY’.</span></span></span></span></span></li><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;"><span style="font-style: italic; vertical-align: baseline;"><span id="docs-internal-guid-bd404b7a-43cd-d22c-ec0c-0e76e203c53a"><span style="font-style: normal; vertical-align: baseline;">The prediction accuracy for Apple will be above </span><span style="font-style: normal; font-weight: 700; vertical-align: baseline;">44.4</span><span style="font-style: normal; vertical-align: baseline;">% if we choose </span><span style="vertical-align: baseline;">‘+/-1.0% change of the stock price’</span><span style="font-style: normal; vertical-align: baseline;"> as </span><span style="vertical-align: baseline;">‘STAY’.</span></span></span></span></span></span></li></ul><div><span><span style="white-space: pre-wrap;"><b>What to Investigate for the Next Week</b></span></span></div></div><div><ul><li><span><span style="white-space: pre-wrap;">Try to use CNN as model.</span></span></li><li><span><span style="white-space: pre-wrap;">Instead of using Tweets, try using financial news as input.</span></span></li><li><span><span style="white-space: pre-wrap;">Try out different embedding methods.</span></span></li><li><span><span style="white-space: pre-wrap;"><span id="docs-internal-guid-bd404b7a-43ce-cc90-1d5a-b707d0e7eb84"><span style="vertical-align: baseline;">Apply F1 Scores. For the evaluation, we would like to know:</span></span></span></span></li><ul><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;">The rate of stocks are marked ‘UP’/’DOWN’/’STAY’ that are predicted correctly as ‘UP’/’DOWN’/’STAY’ separately.</span></span></span></li><li><span><span style="white-space: pre-wrap;"><span style="vertical-align: baseline;"><span id="docs-internal-guid-bd404b7a-43ce-fb74-a644-3c307522369d"><span style="vertical-align: baseline;">The rate of stocks are marked ‘UP’/’DOWN’/’STAY’ that are predicted wrongly as ‘UP’/’DOWN’’STAY’</span></span></span></span></span></li></ul></ul></div></div></div></div></div>







<p class="date">
<a href="https://teamoverfit.blogspot.com/2018/05/7-milestone-advanced-model-attempt-1.html">by Team Overfit (noreply@blogger.com) at May 09, 2018 06:54 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a></h3>


<div class="entrygroup" id="http://cse481n-capstone.azurewebsites.net/?p=82">
<h4 lang="en-US"><a href="http://cse481n-capstone.azurewebsites.net/2018/05/08/advanced-attempt-ii/">Advanced Attempt I Continues</a></h4>
<div class="entry">
<div class="content" lang="en-US">
<h3>Data Collection</h3>
<p><span style="font-weight: 400;">Since our last blog post we have settled on using Dice Index with a cutoff of .5 for including post sentences in our dataset. This final dataset was generated by running similarity between 24574 “hateful” sentences (r/MeanJokes posts and sentences from the general set that were matched by the hate lexicon described previously) and 8,662,875 general Reddit sentences that were from subreddits that were not black-listed. With a .5 cutoff only 2308 of the “hateful” sentences were matched with general reddit posts other than themselves. There were 16940 such general posts. This gives us a total dataset of ~19k potentially interesting sentences to use for the pilot program. </span></p>
<p><span style="font-weight: 400;">An interesting observation from running this is just how sparse the data truly is. Reddit is very diverse and its various communities use very different language from each other. Only .2% of all posts were within .5 Dice Index of the “hateful” posts. With .7 Dice Index it was .01% with only 1000 matched sentences. However, this is just from one month of Reddit data. If we need to increase the cutoff or just get more examples in the future, there are many more dumps we could use to get us enough data to feed our neural net models. </span></p>
<p><span style="font-weight: 400;">What follows are some examples from the dataset and how our current best model performs on them.</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Matched by hate lexicon, presumably from a support subreddit: “My wife was raped My wife tonight was raped, she doesn’t want to go to the police.” labeled hate with 88% certainty</span>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Similar post: “I don’t want to go. My wife is being guilted into going.” labeled none with 78% certainty</span></li>
</ul>
</li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Matched by hate lexicon: “It really fucks with me and makes me feel like a slave.” labeled none with 62% certainty</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Matched by hate lexicon: “Saying the word ”gentleman” is not sexist is like saying the word ”nigger” is not racist.” labeled hate with 95% certainty</span>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Similar post: “Saying a word such as Nigger isn’t racist. It’s only racist when aimed at a person.” labeled hate with 94% certainty</span></li>
</ul>
</li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Matched by hate lexicon: “I denied it, would say “I can’t believe that pathetic faggot would make something like that up”.” labeled hate with 99% certainty (model doesn’t understand what quotes are)</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">From r/MeanJokes: “What do you call a 5 year old with no friends? A sandy hook survivor.” labeled none with 83% accuracy (model doesn’t know current events)</span></li>
</ul>
<p><span style="font-weight: 400;">Here are some more examples of issues:</span></p>
<p><span style="font-weight: 400;">We did use December after all:</span></p>
<p><img alt="" class="alignnone size-full wp-image-85" height="835" src="http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/b4-p1.png" width="1033" /></p>
<p><span style="font-weight: 400;">People like to repost mean jokes with variations:</span></p>
<p><img alt="" class="alignnone size-full wp-image-86" height="127" src="http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/b4-p2.png" width="974" /></p>
<p> </p>
<h3>Advanced Model Attempt 2:</h3>
<p><span style="font-weight: 400;">Last week we explored a new model that uses CNN as encoder, however, we had little luck on making it work better. This week we tried to take contextual information into account using ELMo provided by Allennlp.</span></p>
<p><span style="font-weight: 400;">In short, ELMo enables us to adjust our word/character embedding based on words/characters nearing it. Comparing to the pure Glove embedding we used before, ELMo enables us to take context into consideration which is meaningful as illustrated in our results.</span></p>
<p><span style="font-weight: 400;">During our experiments, we used the </span><a href="http://allennlp.org/elmo"><span style="font-weight: 400;">pretrained ELMo embeddings</span></a><span style="font-weight: 400;"> (1-layer) provided by Allennlp. The ELMo embeddings and glove embeddings were concatenated before being passed through an NN seq2vec encoder.</span></p>
<ul>
<li><span style="font-weight: 400;">Setup 1: gru + 1 layer feed forward with 50 dimension glove embedding, trained and evaluated on twitter_waseem dataset</span></li>
</ul>
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Without ELMo</span></td>
<td><span style="font-weight: 400;">With ELMo</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.7940</span></td>
<td><span style="font-weight: 400;">0.7982</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.8022</span></td>
<td><span style="font-weight: 400;">0.7979</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.7876</span></td>
<td><span style="font-weight: 400;">0.7986</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.8235</span></td>
<td><span style="font-weight: 400;">0.8228</span></td>
</tr>
</tbody>
</table>
<ul>
<li><span style="font-weight: 400;">Setup 2: gru + 1 layer feed forward with 50 dimension glove embedding, trained on combined dataset, evaluated on twitter dataset</span><br />
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Without ELMo</span></td>
<td><span style="font-weight: 400;">With ELMo</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.7788</span></td>
<td><span style="font-weight: 400;">0.8047</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.7979</span></td>
<td><span style="font-weight: 400;">0.8029</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.7672</span></td>
<td><span style="font-weight: 400;">0.8065</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.8152</span></td>
<td><span style="font-weight: 400;">0.8276</span></td>
</tr>
</tbody>
</table>
</li>
<li><span style="font-weight: 400;">Setup 3: gru + 1 layer feed forward with 200 dimension glove embedding, trained and evaluated on twitter_waseem dataset</span><br />
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Without ELMo</span></td>
<td><span style="font-weight: 400;">With ELMo</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.7915</span></td>
<td><span style="font-weight: 400;">0.8027</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.7908</span></td>
<td><span style="font-weight: 400;">0.8067</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.7923</span></td>
<td><span style="font-weight: 400;">0.7993</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.8166</span></td>
<td><span style="font-weight: 400;">0.8290</span></td>
</tr>
</tbody>
</table>
</li>
<li><span style="font-weight: 400;">Setup 4: gru + 1 layer feed forward with 200 dimension glove embedding, trained on combined dataset and evaluated on twitter dataset</span><br />
<table>
<tbody>
<tr>
<td></td>
<td>Without ELMo</td>
<td>With ELMo</td>
</tr>
<tr>
<td>F1</td>
<td>0.7953</td>
<td>0.7813</td>
</tr>
<tr>
<td>Precision</td>
<td>0.7997</td>
<td>0.8044</td>
</tr>
<tr>
<td>Recall</td>
<td>0.7914</td>
<td>0.7681</td>
</tr>
<tr>
<td>Accuracy</td>
<td>0.8228</td>
<td>0.8187</td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;">From the aboving statistics, we can see that ELMo does help us on getting a better result. It seems with ELMo, our model has a noticeable </span><span style="font-weight: 400;">improvement on Recall </span><span style="font-weight: 400;">when training on combined dataset. And precision and accuracy remains about the same with a little bit fluctuation.</span></p>
<p><span style="font-weight: 400;">We think with context taken into consideration, our model now is better at telling sentences that intend to be hateful but with few or no clearly hateful words or phrases. We will talk more about it in the following part.</span></p>
<p><span style="font-weight: 400;">Next we will do some error analysis on our new models:</span></p>
<p><span style="font-weight: 400;">First, we will check how ELMo helps on correcting mistakes we had in our models without ELMo:</span></p>
<p><span style="font-weight: 400;">Recall these examples from our baseline model blog posts:</span></p>
<p><span style="font-weight: 400;">False Negatives:</span></p>
<ol>
<li style="font-weight: 400;"><span style="font-weight: 400;">@RichardDawkins @AbuelitoSerge Really, Muslims understand this. They just want to be able to use the name “racism” to shut us up.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">@RTUKnews An Islamist human rights group? LOL. Now there is a contradiction in terms.</span></li>
</ol>
<p> </p></li>
</ul>
<p><span style="font-weight: 400;">False Positives:</span></p>
<p><span style="font-weight: 400;">3. @Strubbestition Name one thing that is not an opinion but is still sexist. I will wait.</span></p>
<p><span style="font-weight: 400;">4. @Bipartisanism @AllooCharas Terrorism involves a political or religious objective to the terror. Most mass murderers have personal objectives</span></p>
<p><span style="font-weight: 400;">In our new model with ELMo, we now correctly classifies 1 and 2. However, We still think 3 and 4 to be hateful while they aren’t.</span></p>
<table>
<tbody>
<tr>
<td></td>
<td>Hate prob</td>
<td>None prob</td>
<td>Correct?</td>
</tr>
<tr>
<td>1</td>
<td>0.8673</td>
<td>0.1327</td>
<td>yes</td>
</tr>
<tr>
<td>2</td>
<td>0.8408</td>
<td>0.1592</td>
<td>yes</td>
</tr>
<tr>
<td>3</td>
<td>0.6350</td>
<td>0.3650</td>
<td>no</td>
</tr>
<tr>
<td>4</td>
<td>0.7003</td>
<td>0.2997</td>
<td>no</td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;">From the aboving results, we can see that with ELMo, our model does a better job on classifying things that means bad while has nothing really bad. From the class probabilities, we can tell our model is pretty confident about its prediction.</span></p>
<p><span style="font-weight: 400;">Since we still misclassified those two non-hateful sentences, this may introduce a concern that our model may have a tendency to predict everything as hateful. However, we didn’t observe a noticeable drop in precision or accuracy as they remains about the same. We think it is safe to conclude that by using ELMo embeddings, our model makes some improvements.</span></p>
<p><span style="font-weight: 400;">Next we will did some error analysis on our new model. However, after reading through the error sentence files, we discovered a lot of them looks like to the noise of the original dataset. For example, “islam must be outlawed.” is labeled as none as gold label in original dataset and actually our model did a great job on classifying it as hateful.</span></p>
<p><span style="font-weight: 400;">Finally, we also tried ELMo on our preliminary CNN model:</span></p>
<p><span style="font-weight: 400;">With the same setting as previous, we do observe some improvement on our CNN model, however, comparing to other gru models we have, CNN one shows no improvement but the training time is way much longer.</span></p>
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Without ELMo</span></td>
<td><span style="font-weight: 400;">With ELMo</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.7745</span></td>
<td><span style="font-weight: 400;">0.7987</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.7616</span></td>
<td><span style="font-weight: 400;">0.8014</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.7879</span></td>
<td><span style="font-weight: 400;">0.7963</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.8084</span></td>
<td><span style="font-weight: 400;">0.8248</span></td>
</tr>
</tbody>
</table>
<h3>Next Step:</h3>
<p><span style="font-weight: 400;">Our next steps for Data Collection are going to be to get the posts formatted for Mechanical Turk and begin the process of getting our Reddit data labeled.</span></p>
<p>For Modeling, we want to try and incorporate attention into our model and see if that improves our model’s performance.</p>
<h6 style="text-align: left;"><em><strong><span style="color: #000000;">We have a running demo available on our blog in the RUNNING DEMO section of the sidebar.</span></strong></em></h6></div>







<p class="date">
<a href="http://cse481n-capstone.azurewebsites.net/2018/05/08/advanced-attempt-ii/">by Team Watch Your Language! at May 09, 2018 06:07 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 08, 2018</h2>

<div class="channelgroup">







<h3><a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-5600014144802012716.post-8581531123022969204">
<h4><a href="https://nlpcapstonesemparse.blogspot.com/2018/05/blog-7.html">Blog 7</a></h4>
<div class="entry">
<div class="content">
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>I've read through a lot of the data, to understand the data set and view the different types of error </span><br /><span>cases. in general, most(⅔) of the data is just noise, meaning that a programmer won't be able to </span><br /><span>generate the target code from the utterance. </span></div><b id="docs-internal-guid-42510a76-41f6-9607-1678-1da3293c9502" style="font-weight: normal;"><br /></b><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Error breakdown(whats consistently wrong). Rule based fixes.</span></div><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Not understanding functions versus fields</span></div></li><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>return func_debug; or func_x = …</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Fix "Expression--&gt;Expression___(___)", </span></div></li></ul></ul><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Expresion -&gt;identifer</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>identifier-&gt;classfields variables</span></div><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>"Expression--&gt;Expression___.___Nt_33"</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>If parent has parenthesis then allow nt_33 to generate functions otherwise fields ok</span></div></li></ul></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Not able to initialize correctly</span></div></li><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>List y = new File</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>sriniclass_event = new StatisticGenerationEvent()</span></div></li></ul></ul><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt; text-indent: 36pt;"><span>"Expression--&gt;Expression___Nt_68___Expression",</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;"><span>            "Expression--&gt;Primary",</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;"><span>            "Primary--&gt;IdentifierNT",</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;"><span>            "IdentifierNT--&gt;sriniclass_event",</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;"><span>            "Nt_68--&gt;=",</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;"><span>            "Expression--&gt;new___Creator",</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;"><span>            "Creator--&gt;CreatedName___Nt_37",</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;"><span>            "CreatedName--&gt;IdentifierNT",</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;"><span>            "IdentifierNT--&gt;StatisticGenerationEvent",</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;"><span>            "Nt_37--&gt;ClassCreatorRest",</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;"><span>            "ClassCreatorRest--&gt;Arguments",</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;"><span>            "Arguments--&gt;(___ExpressionList___)"</span></div><b style="font-weight: normal;"><br /></b><br /><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Generics incorrect and missing generics</span></div></li></ul><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span><span class="Apple-tab-span" style="white-space: pre;"> </span></span><span><span class="Apple-tab-span" style="white-space: pre;"> </span></span><span>List&lt;List&gt;  Map&lt;Map, Map&gt;</span></div><span><span class="Apple-tab-span" style="white-space: pre;"> </span></span><span><span class="Apple-tab-span" style="white-space: pre;"> </span></span><span>List x</span><br /><span><br /></span><span>I want pivot now since adding all these rules doesn't seem interesting from a language perspective. I am considering finding a filtering heuristic to run on this dataset to get better quality utterances.</span></div>







<p class="date">
<a href="https://nlpcapstonesemparse.blogspot.com/2018/05/blog-7.html">by nlpcapstone (noreply@blogger.com) at May 08, 2018 10:55 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1-part-21">
<h4><a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1-part-21">Advanced Model Attempt #1 (Act 2 Scene 1)</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">This week, I had to do a little backtracking and restructuring, but all for the better! <ul><li>More Data! I have had a couple jobs still running that has allowed me to have more reddit data to work with overall, which is especially good news for the next advanced model</li><li>Subreddit ReOrg - A large portion of this week has been on identifying different subsets for the neurodivergent and neurotypical groups. In conversation with Maarten, we discussed that the subreddits in the current state are a bit too broad to address the hypotheses and may not produce a focused language model that makes sense. Currently, the neurodivergent group is a broad strokes grouping of all types of differences. With that in mind, I spent some time researching, following some Reddit rabbit holes, and learning about ICD 10 <font color="#515151">(International Statistical Classification of Diseases and Related Health Problems (ICD), a medical classification list by the World Health Organization (WHO)) to classify the subreddits and the mental illnesses they speak to. Among these topics, we specifically wanted to look at the the two 'Blocks' - Mood (affective) disorders and Neurotic, Stress-Related and Somatoform disorders. This would be defined as the new 28 subreddits constituting the neurodivergent set, and the 22 neurotypical subreddits are those that are general advice, community support, or discussions on mental health, broadly speaking. These were chosen from the initial ND set because while some of them discussed mental health and neurodiversity, they were open to more than just users dealing with mental health related issues. They had similar types of posts where people are more open and discussing in detail their personal lives, a kind of venting space. And choosing from this group rather than to look at more general reddit, was a decision made so that I could find the nuances rather than the more general difference seen in discussing psychology and not. Despite the significant reduction of subreddits (from 126 to 28 ND and 100 to 22 NT), the data collection I mentioned above has resulted in 5x the original vocabulary size and we now have a combined 1 million words in the newest set (being added to as we speak)</font></li></ul><br />With that, I was able to run the previous vennclouds and idp models (and Act 2 Scene 2, before next week will hopefully include the Connotation Frames model results). For the vennclouds, I think we see some similarity from before of the "my/me/I" terms speaking in first person, but the NT posts has more discussion of a second person or a more general reference to an other or friends. It might also be useful to find a list of prepositional phrases to remove and to find something more interesting in the venn diagram middle portion. </div>  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-09-at-2-32-31-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>  <div class="paragraph"><br />I also ran the IDP model from before and found the following top categories:<br /><strong>ND</strong><br /><em>I/<span>my/</span></em><span><em>me/myself</em> - the personal discussion makes sense, but I'm a bit curious as to why it would be more prevalent than in the other discussion forums that are also predominantly venting spaces focused on the self seeking the community. </span><br /><em>anxiety/depression/OCD</em> - these are the subreddit categories we basically chose<br /><em>feel/feeling </em>- I was surprised to see a significantly higher mention of feeling in the ND categories despite the other subreddits still being communities discussing mental illness along with the larger public in discussing general problems <br /><br /><strong>NT</strong><br /><em>you / his/he/him / girls/she/her - </em>This is a bit surprising given that I had made a bit of a fuss around the female mentions being significantly higher in the ND categories. I'm interested to see what this shift might mean, but even more so, I think the discussion of another is an interesting contrast between the two that might be better suited for the connotation frames results that I hope to do in the next couple of days. <br /><em>https</em> - links are back! But more interestingly, while they were prevalent in the original NT group, which we hypothesized were due to the higher proportion of anecdotal advice in the ND categories, it seems that even in regards to discussions around mental illness, people bring in outside links and whatnot to advise, or uplift other users. <br /><br />Next Steps: As I mentioned above, I'd like to run this same new and larger dataset with the connotation frames model. Beyond that, I'd like to spend the next two weeks working on either implementing a SAGE log-linear model to describe these language models, or a deep learning model. If possible, I hope to also get the graphical model going <br /><br /></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1-part-21">May 08, 2018 07:00 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 02, 2018</h2>

<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=21">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/05/02/advanced-model-attempt-1-part-1/">Advanced Model Attempt 1 (Part 1)</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>This week I created and populated my SQL database with the birth year, birth place, industry, gender, profession, and continent information corresponding to 100 random famous people spanning across all time periods. Furthermore, I created a little over 30 questions that the model can ask as well as the corresponding SQL queries for each question. During each game for the simulation, the user will randomly pick a person for the model to guess and the model picks from a list of these questions to ask. Then, the query corresponding to this question is used to extract the truth value of the question from the database (Yes, No, Unknown). This answer is used as the response to simulate a real person giving that answer through user input. Now that the code has been written to interact with the database, the model can now fully create the observation at any point, which is the input vector to the DRQN. Next, I will hardcode the sample rewards as well as the rules of the game (maximum 20 questions, rewards for winning/losing/wrong guess, terminating game, etc.). This week I will be focusing on getting the simulation to run end-to-end, use tensorflow-gpu, and do hyperparameter tuning.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/05/02/advanced-model-attempt-1-part-1/">by ananthgo at May 02, 2018 06:38 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a></h3>


<div class="entrygroup" id="http://cse481n-capstone.azurewebsites.net/?p=64">
<h4 lang="en-US"><a href="http://cse481n-capstone.azurewebsites.net/2018/05/01/advanced-attempt-i/">Advanced Attempt I</a></h4>
<div class="entry">
<div class="content" lang="en-US">
<h3><b>Data Collection: </b></h3>
<p><span style="font-weight: 400;">We have made a lot of progress increasing our data quality since the last blog post. We have fine-tuned our filtering parameters and experimented with a few different definitions for set similarity. On top of Jaccard Index, we tried Dice Index and Cosine Similarity. We found that depending on the threshold, these different methods gave very similar results, but Dice Index seemed to provide the highest quality sentences while being more tolerant of long sentences (unlike Jaccard, which favored short sentences). Although it’s very picky, we’re certain that due to the huge amount of raw Reddit data we have we can still get a dataset big enough to train our complex neural nets.</span></p>
<p><span style="font-weight: 400;">As an example of our improvement, consider this example from the last post:</span></p>
<pre><span style="font-weight: 400;">MeanJokes Post: “Don’t be offended but Fuck you”
</span>Similar Post: “fuck Foligno”
Similar Post: “fuck narek”
Similar Post: “fuck”
Similar Post: “Fuck me?”
Similar Post: “Fuck me”
Similar Post: “fuck me”
Similar Post: “Fuck it”
Similar Post: “Fuck”
Similar Post: “Who the fuck are you?”</pre>
<p><span style="font-weight: 400;">Now our output would look like:</span></p>
<pre><span style="font-weight: 400;">MeanJokes Post: Don't be offended but Fuck you
</span>Similar Post: why the fuck does he have to talk in a screaming voice
Similar Post 171137: "Officer, I have no idea what in the fuck you're talking about.
Similar Post 92163: Or maybe you just fuck me in public for all too see.
Similar Post 18052: "you know, I'm finally happy". UGH, fuck off.
Similar Post 2567: So reddit, that's my fuck up. Any advice if any of you are in HR?
Similar Post 160778: Now I'm questining what numbers are real and what was put down to fuck with me and what's serious.
Similar Post 210956: And when i ask him about it, he cusses me out (tells me to fuck off) and i just die/break down internally.</pre>
<p><span style="font-weight: 400;">As well, since Dice is so picky and because the MeanJokes set tends to have a very particular structure to all its posts, we are also adding in some other obviously offensive posts to use for our set similarity step. We’re using a hate speech lexicon developed by Tom Davidson (linked below) to extract hateful posts from the general Reddit set. We will concatenate this with the MeanJokes set before running Set Similarity against all of the posts again, hopefully giving us a wider range of language structure for our dataset. </span></p>
<p><span style="font-weight: 400;">Our final improvement was discovering that a handful of subreddits contribute a majority of the noise in our data. This noise is mostly of two varieties: 1. Personal ads for intimate encounters and 2. Trading requests, for both physical and virtual items. A handful of these subreddits are very activate and are surprisingly a large chunk of Reddit’s posts, although none of them ever get nearly enough upvotes to be noticed by the average user. </span></p>
<p><span style="font-weight: 400;">So in order to combat having a lot of posts of this sort in our dataset:</span></p>
<p><img alt="" class="alignnone wp-image-65" height="144" src="http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/Picture1-300x110.png" width="393" /></p>
<p><span style="font-weight: 400;">We have put together a blacklist of subs and filtered them out of the posts we consider for set similarity.</span></p>
<table style="height: 620px;" width="414">
<tbody>
<tr>
<td><span style="font-weight: 400;">100k posts</span></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Top 10 Black List</span></td>
<td></td>
<td><span style="font-weight: 400;">Top 10 White List</span></td>
<td></td>
</tr>
<tr>
<td><span style="font-weight: 400;">RocketLeagueExchange’</span></td>
<td><span style="font-weight: 400;">1860</span></td>
<td><span style="font-weight: 400;">AskReddit’</span></td>
<td><span style="font-weight: 400;">5978</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">A5XHE’</span></td>
<td><span style="font-weight: 400;">1373</span></td>
<td><span style="font-weight: 400;">Showerthoughts’</span></td>
<td><span style="font-weight: 400;">1709</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dirtykikpals’</span></td>
<td><span style="font-weight: 400;">1128</span></td>
<td><span style="font-weight: 400;">The_Donald’</span></td>
<td><span style="font-weight: 400;">850</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dirtypenpals’</span></td>
<td><span style="font-weight: 400;">870</span></td>
<td><span style="font-weight: 400;">teenagers’</span></td>
<td><span style="font-weight: 400;">720</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">DirtySnapchat’</span></td>
<td><span style="font-weight: 400;">792</span></td>
<td><span style="font-weight: 400;">GlobalOffensiveTrade’</span></td>
<td><span style="font-weight: 400;">681</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dirtyr4r’</span></td>
<td><span style="font-weight: 400;">438</span></td>
<td><span style="font-weight: 400;">Bitcoin’</span></td>
<td><span style="font-weight: 400;">651</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">AppNana’</span></td>
<td><span style="font-weight: 400;">372</span></td>
<td><span style="font-weight: 400;">relationships’</span></td>
<td><span style="font-weight: 400;">586</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Roleplaykik’</span></td>
<td><span style="font-weight: 400;">368</span></td>
<td><span style="font-weight: 400;">FIFA’</span></td>
<td><span style="font-weight: 400;">558</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">buildapc’</span></td>
<td><span style="font-weight: 400;">364</span></td>
<td><span style="font-weight: 400;">explainlikeimfive’</span></td>
<td><span style="font-weight: 400;">500</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">AgeplayPenPals’</span></td>
<td><span style="font-weight: 400;">329</span></td>
<td><span style="font-weight: 400;">Fireteams’</span></td>
<td><span style="font-weight: 400;">469</span></td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;"><br />
Running the filter on 100k posts, we can see that most of the most common subreddits that remained are conversational in nature, while those that were removed would not make very useful sentences.</span></p>
<p> </p>
<h3><b>Advanced Model Attempt: </b></h3>
<h4><span style="font-weight: 400;">Combining Datasets: </span></h4>
<p><span style="font-weight: 400;">In our last blog post, we mentioned our concern about the small size of </span><span style="font-weight: 400;">Waseem’s twitter dataset</span><span style="font-weight: 400;">. This week, we combined that dataset with another twitter hate speech dataset made by Thomas Davidson. The Davidson dataset contains 24,802 labeled tweets. Each tweet is coded by at least 3 CrowdFlower users. Each row contains 5 columns:</span></p>
<table style="height: 336px;" width="584">
<tbody>
<tr>
<td>count</td>
<td>number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).</td>
</tr>
<tr>
<td>hate_speech</td>
<td>number of CF users who judged the tweet to be hate speech.</td>
</tr>
<tr>
<td>offensive_language</td>
<td>number of CF users who judged the tweet to be offensive.</td>
</tr>
<tr>
<td>neither</td>
<td>number of CF users who judged the tweet to be neither offensive nor non-offensive.</td>
</tr>
<tr>
<td>class</td>
<td>class label for majority of CF users. 0 – hate speech 1 – offensive language 2 – neither</td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;">Davidson et. al. used the following definition for hate speech: language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. According to the paper, </span><span style="font-weight: 400;">“only 5% of tweets were coded by the majority of coders”</span><span style="font-weight: 400;">. If we directly combine Waseem Data with this we might get a even more skewed class distribution (31% ‘hate’, 69% ‘none’). Therefore, we decided to change the class labels of Davidson a little bit: if all CF users unanimously coded a tweet hate_speech or offensive_language, the tweet would be labeled ‘hate’; otherwise, the tweet would be labeled ‘none’. The modified Davidson dataset has a class distribution of 76% ‘hate’ and 24% ‘none’. Then we combined these two datasets (removed duplicate tweets if there are any). The new combined dataset has 40,509 tweets and a class distribution of 59% ‘hate’ and 41% ‘none. The combined dataset is much larger than the altered Waseem dataset (~15k tweets) and the labels are more balanced. </span></p>
<p><span style="font-weight: 400;">We do have the concern whether this more these more generously labeled ‘hate’ tweets are noisy. However, because the Waseem dataset is also more generous to ‘none’ labels (as long as the tweet is neither racist or sexist), we believe they would have some counter effect on each other. After all, data noise is very unlikely to be completely removed. </span></p>
<h4><span style="font-weight: 400;">Preprocessing: </span></h4>
<p><span style="font-weight: 400;">Since the model we tried requires each sentence to have at least 4 tokens, we decided to ignore sentences with less than 4 tokens after pre-processing.</span></p>
<h4><span style="font-weight: 400;">The effectiveness of Combined Dataset:</span></h4>
<p><span style="font-weight: 400;">To illustrate the effectiveness of the combined dataset, we chose the best NN model set up from baseline II to train on Waseem dataset and combined dataset separately and evaluated the two trained models on Waseem dev data. We decided not to evaluate on test data yet because we don’t want any leaked info from test.</span></p>
<p><span style="font-weight: 400;">Set up — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer GRU</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Trained on Waseem Dataset, epoch chosen: 13</span></li>
</ul>
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Waseem-Dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy </span></td>
<td><span style="font-weight: 400;">0.8235</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.8022</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.7876</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.7940</span></td>
</tr>
</tbody>
</table>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Trained on Combined Dataset, epoch chosen: 16</span></li>
</ul>
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Waseem-Dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy </span></td>
<td><span style="font-weight: 400;">0.8152</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.7979</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.7672</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.7788</span></td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;">Although the two models had the same setup, the one trained on the combined dataset got performance close to the one trained on the original Waseem dataset despite the fact that we now have really different class distributions in the two datasets. </span></p>
<h4><span style="font-weight: 400;">Retrain Some Baseline Models on Combined Dataset:</span></h4>
<p><span style="font-weight: 400;">Here we retained some baseline models with different set ups on the combined dataset and evaluated them on both Waseem dev data and combined dev data.</span></p>
<ul>
<li>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Model1 — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer GRU, epoch chosen: 16</span><br />
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Combined-dev</span></td>
<td><span style="font-weight: 400;">Waseem-dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.8665</span></td>
<td><span style="font-weight: 400;">0.8152</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.8614</span></td>
<td><span style="font-weight: 400;">0.7979</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.8628</span></td>
<td><span style="font-weight: 400;">0.7672</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.8621</span></td>
<td><span style="color: #ff0000;">0.7788</span></td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Model2 — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer LSTM, epoch chosen: 19</span><br />
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Combined-dev</span></td>
<td><span style="font-weight: 400;">Waseem-dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.8625</span></td>
<td><span style="font-weight: 400;">0.8091</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.8577</span></td>
<td><span style="font-weight: 400;">0.7875</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.8578</span></td>
<td><span style="font-weight: 400;">0.7648</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.8578</span></td>
<td><span style="color: #ff0000;">0.7739</span></td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;"> Model3 — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer BiGRU, epoch chosen: 16</span></p></li>
</ul>
</li>
</ul>
<ul>
<li>
<ul>
<li style="font-weight: 400;">
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Combined-dev</span></td>
<td><span style="font-weight: 400;">Waseem-dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.8618</span></td>
<td><span style="font-weight: 400;">0.8104</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.8567</span></td>
<td><span style="font-weight: 400;">0.7916</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.8575</span></td>
<td><span style="font-weight: 400;">0.7620</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.8571</span></td>
<td><span style="color: #ff0000;">0.7732</span></td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;">Model4 — embedding: 100 dimensional glove twitter embeddings, encoder: 1 layer GRU, epoch chosen: 11</span></p></li>
</ul>
</li>
</ul>
<ul>
<li>
<ul>
<li style="font-weight: 400;">
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Combined-dev</span></td>
<td><span style="font-weight: 400;">Waseem-dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.8651</span></td>
<td><span style="font-weight: 400;">0.8194</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.8605</span></td>
<td><span style="font-weight: 400;">0.7971</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.8603</span></td>
<td><span style="font-weight: 400;">0.7834</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.8604</span></td>
<td><span style="color: #ff0000;">0.7894</span></td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
</ul>
<h4><span style="font-weight: 400;">Model:</span></h4>
<p><span style="font-weight: 400;">Our first Advanced model will be a CNN model.</span></p>
<h5><span style="font-weight: 400;">The intuition of choosing this model:</span></h5>
<p><span style="font-weight: 400;">CNN provides us a convenient way to extract the most important information within the given fragment of a sentence through filters and max pooling. We found it might be a worth trying model on our task.</span></p>
<p><span style="font-weight: 400;">Our model looks like:</span></p>
<p><img alt="" class="alignnone wp-image-67" height="158" src="http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/Picture2-300x116.png" width="409" /></p>
<pre><span style="font-weight: 400;">Image credit:</span><span style="font-weight: 400;">Gambäck, B., &amp; Sikdar, U.K. (2017). Using Convolutional Neural Networks to Classify Hate-Speech.</span></pre>
<ul>
<li>
<ul>
<li>
<ol>
<li style="font-weight: 400;"><span style="font-weight: 400;">Preprocess all words and encode them using pretrained glove embeddings.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Feed result into a convolution neural network, taking 2, 3 and 4-grams into consideration. Output dimension is 28, 26 for English alphabets, 1 for digits and 1 for all other symbols. </span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Feed result into a 2-layer feed-forward neural net, with dimension (28, 2)  and dropout (0.3, 0.3)</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Softmax on the result and pick the major class</span></li>
</ol>
</li>
</ul>
</li>
</ul>
<p><span style="font-weight: 400;">With not much tuning, here’s what our best model looks like:</span></p>
<ul>
<li>
<ul>
<li>
<ul>
<li><span style="font-weight: 400;">200 dimension embedding, filters=100, trained on Waseem twitter dataset</span></li>
</ul>
</li>
</ul>
</li>
</ul>
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Waseem dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.79169</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.78274</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.80086</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.82142</span></td>
</tr>
</tbody>
</table>
<ul>
<li>
<ul>
<li>
<ul>
<li><span style="font-weight: 400;">200 dimension embedding, filters=100, trained on the combined dataset</span></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><span style="color: #ff0000;">*revised, due to an imperfection in the combined dataset, there was a mistake in numbers</span></p>
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">combined dev</span></td>
<td><span style="font-weight: 400;">Waseem dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.85983</span></td>
<td><span style="color: #ff0000;">0.77451</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.86029</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.76156</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.85937</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.78791</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.86437</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.80837</span></td>
</tr>
</tbody>
</table>
<p><del><span style="font-weight: 400;">With no doubt, our combined dataset provides a huge boost on performance on original Waseem twitter dataset.</span></del></p>
<p><span style="color: #ff0000;">From the above results, it seems CNN hasn’t show an improvement on our job. We think doing more hyper parameter tuning should give us some improvement. Furthermore, we would like to incorporate Elmo to see if that will help us our not.</span></p>
<p><del><span style="font-weight: 400;">However, we do have some concern about our models: almost all of the best models we have with a large number of filters have their best epoch generally to be the first few epochs. We are a little bit concerned about that since that may be a sign of overfitting.</span></del></p>
<p><span style="color: #ff0000;">Since our result hasn’t show any improvement, we think it’s more appropriate to do error analysis once we gain some improvement.</span></p>
<p><del><span style="font-weight: 400;">Therefore, before we start to do any error analysis, we would like to do a little bit more hyperparameters since we haven’t really try different drop out rate or other output dimension values other than the one specified in the paper we referenced.</span></del></p>
<h3><span style="font-weight: 400;">Next Step:</span></h3>
<p><span style="font-weight: 400;">First, we will dig deeper on the model we have right now. We will first play with its parameters and then conduct error analysis on it.</span></p>
<p><span style="font-weight: 400;">As suggested in previous blog post feedback, we would like to try Elmo and see how much can we improve with it. Furthermore, we would like to try things like character level embedding as well as another very interesting model which combines CNN with GRU to make prediction.</span></p>
<h3><span style="font-weight: 400;">Work Cited:</span></h3>
<p><a href="https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06">Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).</a></p>
<p><a href="https://www.semanticscholar.org/paper/Automated-Hate-Speech-Detection-and-the-Problem-of-Davidson-Warmsley/6ccfff0d7a10bf7046fbfd109b301323293b67da">Davidson, Thomas J et al. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM (2017).</a></p>
<p><a href="https://www.semanticscholar.org/paper/Using-Convolutional-Neural-Networks-to-Classify-Gamb%C3%A4ck-Sikdar/0dca29b6a5ea2fe2b6373aba9fe0ab829c06fd78">Gambäck, Björn and Utpal Kumar Sikdar. “Using Convolutional Neural Networks to Classify Hate-Speech.” (2017).</a></p>
<p> </p></div>







<p class="date">
<a href="http://cse481n-capstone.azurewebsites.net/2018/05/01/advanced-attempt-i/">by Team Watch Your Language! at May 02, 2018 06:17 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-9203775015655831448.post-3361785683406757277">
<h4><a href="https://teamoverfit.blogspot.com/2018/05/6-milestone-advanced-model-attempt-1.html">#6 Milestone: Advanced model attempt #1</a></h4>
<div class="entry">
<div class="content">
<h2 style="height: 0px;"><span>Team Overfit</span></h2><h3><span><br /></span></h3><h3><span>Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span>Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><div style="text-align: start; text-indent: 0px;"><div style="margin: 0px;"><div><span><b>Blog Post: #6: 05/01/2018</b></span></div><div><span><span><b><br /></b></span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Social Media Predicts Stock Price (StartUp Mode)</span><br /><br /><span><span style="white-space: pre;">This week, we added more data source and have finer process of the data. We also convert </span></span><br /><span><span style="white-space: pre;">Keras model to Tensorflow for better future improvement.   </span></span><br /><span><span style="white-space: pre;"><br /></span></span><span><b>Keras to TensorFlow</b></span><br /><ul><li><span>We updated our code from using Keras to Tensorflow which helps to do more improvement on model. For example, TensorFlow is a lower level library which allow us to have more control over the variables we used as the input and output. For example, if we want to try with more complicated input with adding the voerall market stock price and the competitors' stock prices, TensorFlow will be more helpful.</span></li></ul><div><span><b>Add One More Target Company: Tesla</b></span></div><div><ul><li><span>As there are too many Tweets that have tagged #Apple but are unrelated to the company Apple, we need to filter out such tweets. However, if we apply the model on the company Tesla, who's company name is uniquer than 'Apple' and has its products naming closer to its company name, the Tweets and news that have tagged #Tesla will contain much less noisy information and thus improve the prediction to its stock price.</span></li></ul><div><span><b>Try Out Different Layers Structure to Tune LSTM</b></span></div></div><div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><ul><span id="docs-internal-guid-1d495372-1f45-7a06-476d-fd5b4661d619"><li><span><span style="white-space: pre-wrap;">Add the NASDAQ index to the input</span></span></li><ul><li><span><span style="white-space: pre-wrap;">The composition of the NASDAQ Composite is heavily weighted towards information technology companies. Therefore, company Apple’s and Tesla’s stock prices may be influenced by the Nasdaq index.</span></span></li></ul><li><span><span style="white-space: pre-wrap;">Add a 'STAY' category as the output prediction of stock price</span></span></li><ul><li><span><span>If the change of the stock price for the next day is within 1%, we will mark the stock price of that day as a ‘STAY’. Otherwise, we mark it as ‘UP’ or ‘DOWN’</span></span></li></ul><li><span><span style="white-space: pre-wrap;">Lemmatization &amp; Stemming</span></span></li><ul><li><span><span style="white-space: pre-wrap;">Lemmatization and stemming help collect the same words with different tenses together, which reduce the total vocabulary size.</span></span></li></ul></span></ul><div><span id="docs-internal-guid-1d495372-1f45-7a06-476d-fd5b4661d619"><span><span style="white-space: pre-wrap;"><b>Update Of Evaluation Plan</b></span></span></span><br /><span><span style="white-space: pre-wrap;"><b><br /></b></span></span></div><div><span id="docs-internal-guid-1d495372-1f45-7a06-476d-fd5b4661d619"><span><span style="white-space: pre-wrap;"><b>Results &amp; Error Analysis</b></span></span></span></div><div><ul><span id="docs-internal-guid-1d495372-1f45-7a06-476d-fd5b4661d619"><li><span><span style="white-space: pre-wrap;">After we have improved the model and the training process, the prediction accuracy for Apple will be above 85% if we choose '<i>+/-1% change of the stock price</i>' as 'STAY'.</span></span></li><li><span><span style="white-space: pre-wrap;">The prediction accuracy for Apple will be above 60% if we choose '<i>+/-1% change of the stock price</i>' as 'STAY'.</span></span></li></span></ul></div><div><span id="docs-internal-guid-1d495372-1f45-7a06-476d-fd5b4661d619"><span><span style="white-space: pre-wrap;"><b>What To Investigate For The Next Week</b></span></span></span></div><div><ul><span id="docs-internal-guid-1d495372-1f45-7a06-476d-fd5b4661d619"><li><span><span style="white-space: pre-wrap;">Apply F1 scores. For the evaluation ,we would like to know:</span></span></li><ul><li><span><span style="white-space: pre-wrap;">The rate of stocks are marked 'UP/DOWN/STAY' that are predicted correctly as 'UP/DOWN/STAY' separately.</span></span></li><li><span><span style="white-space: pre-wrap;">The rate of stocks are marked 'UP/DOWN/STAY' that are predicted wrongly as 'UP/DOWN/STAY' separately.</span></span></li></ul><li><span><span style="white-space: pre-wrap;">Add competitors' stock prices to the input</span></span></li></span></ul></div><div></div></div></div></div></div></div></div>







<p class="date">
<a href="https://teamoverfit.blogspot.com/2018/05/6-milestone-advanced-model-attempt-1.html">by Team Overfit (noreply@blogger.com) at May 02, 2018 05:53 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 01, 2018</h2>

<div class="channelgroup">







<h3><a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-5600014144802012716.post-7248717662742163548">
<h4><a href="https://nlpcapstonesemparse.blogspot.com/2018/05/advanced-model-attempt-one.html">Advanced model attempt one</a></h4>
<div class="entry">
<div class="content">
I fixed the performance issues, and I have been working on adding enhancements to the model such as restricting the action space, by only allowing the model to generate types from the class, as well as debugging it to improve accuracy. The results are: 7.7 EM and 22.1 Bleu.</div>







<p class="date">
<a href="https://nlpcapstonesemparse.blogspot.com/2018/05/advanced-model-attempt-one.html">by nlpcapstone (noreply@blogger.com) at May 01, 2018 03:57 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1">
<h4><a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1">Advanced Model Attempt #1 (Act 1)</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">The first part of implementing my advanced model attempt was to work on implementing the IDP Algorithm presented in  Monroe, B. L., Colaresi, M. P., &amp; Quinn, K. M. (2008). <em>Fightin' words: Lexical feature selection and evaluation for identifying the content of political conflict. </em><font size="2">Political Analysis, 16(4), 372-403. </font><br /><br />In doing so, I was able to find the weighted log odds ratio of each word present in both ND and NT posts, ultimately showing which type of subreddit each word was 'more affiliated' with. The findings were as one might expect, especially with my previous baselines and were in line with the results from those. As seen below we see some familiar words within the ND (I, you, <strong>she</strong>​) and NT (http) - so sorry for the ugly terminal output, but I need to find a prettier CSV presentation:</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 50%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-02-at-12-42-36-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 50%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-02-at-12-43-09-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph">This was a good first step, and will need some more work hashing out some final implementation details, but my next step in making this an actual advanced model, is to now utilize some of that reddit data that I've been harvesting for the past week or so. With that, we have a lot more data and might need to make some changes on the subreddit subsets depending on how the data has developed (changes in sentence length and number of total number of posts in each subreddit). Off to more data!!!</div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1">May 01, 2018 07:00 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 26, 2018</h2>

<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=18">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/04/26/strawman-baseline-2-same-drqn-model-with-a-different-policy/">Strawman/Baseline 2: Same DRQN Model with a Different Policy</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>I decided to narrow the scope of my problem by changing the types of questions asked to the user to be only yes/no questions. This simplifies interpreting the user response into a classification task. More specifically, the questions will be about figuring out the attributes of any entities brought up in the text.  Therefore, I will use a relational database to store the accumulated entity-attribute relationships instead of a semantic network since it will be easier to extract quantifiable features by performing specific queries. In addition to asking the user questions, I decided to involve database queries in the training process. This way the model can query the database about any inferences or guesses it has about the attributes of an entity, and the database can return a list of entities that fit the hypothesis. Then, the model is given a reward based on how much it was able to narrow down the list of entities. This allows the model to get frequent reward signals from the database to speed up training.</p>
<p>Because of this change, my model architecture has also changed. The DRQN will now take as inputs the action from the previous time step (one-hot vector), the user response (word embeddings passed through CNN), and the database response (number of entities the previous query narrowed it down to). In addition, the outputs to the LSTM at each time step will feed into A+1 policy networks where A = number of attributes. The first A policy networks are needed because the model needs to learn how to guess each attribute independently. The last policy network determines which question the model will ask the user. So far I have implemented most of this architecture but still need to add in the policy networks and debug.</p>
<p>Finally, I will implement a question simulator to randomly pick an entity and have the model guess what it is, similar to 20 questions. During this simulation, rewards will be automatically given every time the model queries the database and whenever the game ends (win or loss). Furthermore, a small penalty is given for a wrong guess. By implementing this simulator, the need for user input to give rewards is eliminated and this should completely automate and greatly speed up training.</p>
<p>My goal for next week is to finish implementing and debugging the model to start this training simulation.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/04/26/strawman-baseline-2-same-drqn-model-with-a-different-policy/">by ananthgo at April 26, 2018 06:58 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 25, 2018</h2>

<div class="channelgroup">







<h3><a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-9203775015655831448.post-597849658553454254">
<h4><a href="https://teamoverfit.blogspot.com/2018/04/5-milestone-strawmanbaseline-ii.html">#5 Milestone: Strawman/Baseline II</a></h4>
<div class="entry">
<div class="content">
<h2 style="height: 0px;"><span>Team Overfit</span></h2><h3><span><br /></span></h3><h3><span>Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span>Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><div style="text-align: start; text-indent: 0px;"><div style="margin: 0px;"><div><span><b>Blog Post: #5: 04/24/2018</b></span></div><div><span><span><b><br /></b></span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Social Media Predicts Stock Price (StartUp Mode)</span><br /><span><br /></span><span><span id="docs-internal-guid-8a395488-fb3a-d9a7-cec2-8d3cb31f0a59"><span>This week, we tried to use tf-idf and seq2vec to process news headlines as input to our LSTM </span></span></span><span>model to predict UP/DOWN for APPLE stock price.</span><br /><span><br /></span><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span><b>Data Preprocessing</b></span><br /><br /><ul><li><span>Crawling historical news headlines from Twitter</span></li><li><span>Categorize tweets with date and correlated stock price</span></li><li><span>Cleanup unrelated data</span></li><li><span>Map vocabulary to index with removing words that appear less than 3 times in the tweet news corpus.</span></li></ul><div><span><b>Seq2One with TF-IDF</b></span></div><div><ul><li><span>TF-IDF</span></li><ul><li><span>Correlate word frequencies with price changes (convert words into normalized frequency count vector)</span></li></ul><li><span>Seq2One</span></li><ul><li><span>Correlate average word embeddings with price changes (convert words into normalized embedding vector)</span></li></ul><li><span>Bidirectional RNN</span></li><ul><li><span>Enhance the performance by knowing before and after prices</span></li></ul></ul><div><span><b>Evaluation</b></span></div></div><div><ul><li><span>Accuracy on predicting Up or Down</span></li><li><span>Accuracy on predicting Stock Price (TODO)</span></li></ul><div><span><b>Result</b></span></div></div><div><ul><li><span>TF-IDF: 60.7% accuracy predictin UP/DOWN on test set with 273 total data points and 9/1 train-test split</span></li><li><span>Seq2Vec: 53.6% accuracy predicting UP/DOWN on test set with 273 total data points and 9/1 train-test split</span></li></ul><div><span><b>Error Analysis</b></span></div></div><div><ul><li><span><b>Things that may impair the precision of prediction</b></span></li><ul><li><span>Words with high frequency but have low effect on stock prices:</span></li><ul><li><span>A, is, of, for, the</span></li><li><span>Tweets talking about fruit <i>apple</i> instead of the company</span></li></ul><li><span>Tweets news that have high frequency but low effect on stock prices and news that may have high effect on stock prices but with low frequencies</span></li><li><span>Losing information of each single tweet as we have combined all tweets in a day together to form a word frequency vector and do the prediction</span></li></ul><li><span><b>Things that could be done to improve the performance</b></span></li><ul><li><span>Add the overall market stock price as part of the input because it usually also has big impact on a single companies stock price.</span></li><li><span>Crawl more data for more companies and longer duration</span></li><li><span>Add financial</span></li><li><span>Add the competitor companies stock prices and tweets news as part of the input.</span></li></ul></ul></div></div></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span id="docs-internal-guid-8a395488-fb3c-6f01-c575-279c6f3d6421"></span></div></div></div></div>







<p class="date">
<a href="https://teamoverfit.blogspot.com/2018/04/5-milestone-strawmanbaseline-ii.html">by Team Overfit (noreply@blogger.com) at April 25, 2018 05:20 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a></h3>


<div class="entrygroup" id="http://cse481n-capstone.azurewebsites.net/?p=51">
<h4 lang="en-US"><a href="http://cse481n-capstone.azurewebsites.net/2018/04/24/more-data-collection-and-baseline/">More Data Collection and Baseline</a></h4>
<div class="entry">
<div class="content" lang="en-US">
<h3><span style="font-weight: 400;">Data Collection: </span></h3>
<p><span style="font-weight: 400;">In the last blog we discussed the challenges of trying to find general Reddit posts that were similar to the collected MeanJokes posts. Even limiting to posts with Jaccard Similarity &gt; .3 a lot of the data looked like the following:</span></p>
<pre><span style="font-weight: 400;">MeanJokes Post: “Don’t be offended but Fuck you”
    Similar Post: “fuck Foligno”
    Similar Post: “fuck narek”
    Similar Post: “fuck"
    Similar Post: “Fuck me?”
    Similar Post: “Fuck me”
    Similar Post: “fuck me”
</span>    Similar Post: “Fuck it”
    Similar Post: “Fuck”
    Similar Post: “Who the fuck are you?”</pre>
<p><span style="font-weight: 400;">These wouldn’t be very interesting examples to eventually train a model on. We also noticed that because of the nature of Jaccard Similarity and the sparsity of language in our collected Reddit posts, most of the posts that matched our MeanJokes posts would be very short, containing one or two key phrases from the MJ post. Posts made to Reddit are typically either very long or very short, so to make use of those long posts we decided to split them up by sentence and consider every sentence individually. We would also filter out sentences that are below a certain number of tokens, so that we avoid examples like the above.</span></p>
<pre><span style="font-weight: 400;">MeanJokes Post: “How is ScizorSci like Hoss McDank? They’re both faggots!”
</span>    Similar Post: “How long was he like that?”
    Similar Post: “More like CRAPitalism (this but unironically)
    Similar Post: “Volcanoes are like earth pimples”
    Similar Post: “I cried like a bitch”
    Similar Post: “She doesn’t like jewelry”
    Similar Post: “Everyone was like daaaaayum”
    Similar Post: “Don’t speak to me like that”
    Similar Post: “Don’t like the smell of this at all”
    Similar Post: “A few others I like are”
    Similar Post: “It’s like I’m on fire”</pre>
<p><span style="font-weight: 400;">These are better than the previous examples but still the similarities are very shallow. Most of the matches are just because there were one or two content phrases that matched between them. This could be expected from having a Jaccard Index cutoff as low as .3, since usually you want one that is somewhere above .7, but the language used in these posts is too sparse to be this picky and still have enough data to train a neural network. </span></p>
<p><span style="font-weight: 400;">We could possibly try similarity on word embeddings or sentence embeddings, but we liked using Jaccard Index because we actually care about the specific words used and not just the semantic meaning. </span></p>
<p><span style="font-weight: 400;">Our main issue ended up being that we assumed most posts would be conversationally structured with short-ish text, when in reality we found that posts are either really short, some collection of tags for indexing or trading, really long posts with at least a paragraph of text about some abstract subject, or requests for sexual favors. Overall this makes general reddit posts quite different from r/meanjokes, so at the surface level jaccard index won’t really do much, and furthermore general reddit posts won’t be conversational in structure the way r/meanjokes posts are. For these reasons we will have to move on to looking at comments instead, since we believe that they will be more conversational than posts. We originally wanted to use posts instead of comments since posts are contextually self-containing while comments are typically responses to multi-person conversations. However, we ended up splitting posts into independent sentences anyway, so this reasoning for avoiding comments became moot.</span></p>
<h3><b>Baseline Model:</b></h3>
<p><span style="font-weight: 400;">This week, we developed a baseline Neural Network model using allennlp. The model architecture is simple. We used pretrained </span><a href="https://nlp.stanford.edu/projects/glove/"><span style="font-weight: 400;">glove twitter word embeddings</span></a><span style="font-weight: 400;">, encode each tweet with a recurrent neural network (e.g. RNN, LSTM, GRU) sequence to vector encoder, and finally feed the vector into a feed-forward network with softmax at the end. We experimented with glove twitter word embeddings with 50 dimensions. We also tried different flavors of 1 layer recurrent neural network sequence to vector encoders, more specifically, GRU, LSTM, BiLSTM, and RNN. By the time this blog is written, we have yet performed extensive hyperparameter tuning. </span></p>
<p><span style="font-weight: 400;">Among the 4 different model setups that we tried, one of the models (Model1) got the highest accuracy, recall, and f1 score on test data, while another model (Model2) got the highest precision on test data. Below are their performances on dev and test dataset. </span></p>
<ul>
<li><span style="font-weight: 400;">Model1 — embedding: 50-dimensional glove twitter embeddings, encoder: 1 layer GRU, epoch chosen: 20</span></li>
</ul>
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Dev</span></td>
<td><span style="font-weight: 400;">Test</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy </span></td>
<td><span style="font-weight: 400;">0.8245</span></td>
<td><span style="font-weight: 400;">0.8181</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.7934</span></td>
<td><span style="font-weight: 400;">0.7896</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.7995</span></td>
<td><span style="font-weight: 400;">0.7947</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.7964</span></td>
<td><span style="font-weight: 400;">0.7921</span></td>
</tr>
</tbody>
</table>
<ul>
<li><span style="font-weight: 400;">Model2: — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer BiLSTM, epoch chosen: 5</span><br />
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Dev</span></td>
<td><span style="font-weight: 400;">Test</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy </span></td>
<td><span style="font-weight: 400;">0.8409</span></td>
<td><span style="font-weight: 400;">0.8175</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.8239</span></td>
<td><span style="font-weight: 400;">0.8004</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.7909</span></td>
<td><span style="font-weight: 400;">0.7627</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.8070</span></td>
<td><span style="font-weight: 400;">0.7811</span></td>
</tr>
</tbody>
</table>
</li>
</ul>
<p><span style="font-weight: 400;">These two models’ setups are almost the same except for model1 uses a GRU encoder while model2 uses an BiLSTM encoder. Surprisingly, Model1 ends up having better overall performance on test data than the ones with more complex encoders like BiLSTM. </span></p>
<h4><span style="font-weight: 400;">Error Analysis:</span></h4>
<p><span style="font-weight: 400;">We will use the Model2’s errors in our error analysis:</span></p>
<p><span style="font-weight: 400;">First we will look into sentences that are hateful but our model classified as none:</span></p>
<ol>
<li style="font-weight: 400;">@RichardDawkins @AbuelitoSerge Really, Muslims understand this. They just want to be able to use the name “racism” to shut us up.</li>
<li style="font-weight: 400;">@RTUKnews An Islamist human rights group? LOL. Now there is a contradiction in terms.</li>
</ol>
<p><span style="font-weight: 400;">From the above examples, we found that our model is not good at understanding the underlying meaning of a sentence. For example, the 2rd one implies Islamist doesn’t care about human rights, which is attacking Islam people. However, since this sentence does not have any words that are very sensitive, our model considered it as OK instead of hateful.</span></p>
<p><span style="font-weight: 400;">Here are some other sentences that are not hateful but our model classified then as hateful:</span></p>
<ol>
<li style="font-weight: 400;">@Strubbestition Name one thing that is not an opinion but is still sexist. I will wait.</li>
<li style="font-weight: 400;">@Bipartisanism @AllooCharas Terrorism involves a political or religious objective to the terror.Most mass murderers have personal objectives</li>
</ol>
<p><span style="font-weight: 400;">On the other hand, we found a trend that sentences including words like “sexist”, “crime” are classified as hateful disregarding what exactly the post means. For a concrete example, the 3rd sentence from 2nd group is not saying anything hateful but our model considered it as hateful. We suspect that because “murderers” appeared in that sentence and in our training data and most other sentences with such word is hateful, our model picked up such pattern and made a wrong decision.</span></p>
<p><span style="font-weight: 400;">Why we end up have a pretty bad result? We have two possible reasons:</span></p>
<ol>
<li style="font-weight: 400;"><span style="font-weight: 400;">Our dataset used is really small (15k sentences in total) and dataset itself is really noisy. For example, “@dgbattaglia Saw this this morning… http://t.co/9YUwOuZugw” is somehow labeled as hateful as true label in original dataset.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Our model is not expressive enough to recognize more complicated patterns. This also has something to do with the dataset. With such a small dataset, we cannot really train a deep or more complicated model.</span></li>
</ol>
<h4><span style="font-weight: 400;">Next Steps: </span></h4>
<p><span style="font-weight: 400;">What we are seeing in training is is this general pattern. We suspect it is because the dataset we have (around 15k tweets) is too small for a neural network model. We would want to try combine another </span><a href="https://github.com/t-davidson/hate-speech-and-offensive-language"><span style="font-weight: 400;">twitter hate speech dataset (by Thomas Davidson et. al.)</span></a><span style="font-weight: 400;"> and Waseem’s twitter dataset and train different neural net models on the combined dataset. </span><span style="font-weight: 400;"><br />
</span></p>
<p><img alt="" class="alignnone wp-image-54" height="192" src="http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/04/P3-300x138.png" width="417" /></p>
<h4><span style="font-weight: 400;">Data Sources: </span></h4>
<p><a href="https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06">Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).</a></p>
<p><a href="https://www.semanticscholar.org/paper/Automated-Hate-Speech-Detection-and-the-Problem-of-Davidson-Warmsley/6ccfff0d7a10bf7046fbfd109b301323293b67da">Davidson, Thomas J et al. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM (2017).</a></p></div>







<p class="date">
<a href="http://cse481n-capstone.azurewebsites.net/2018/04/24/more-data-collection-and-baseline/">by Team Watch Your Language! at April 25, 2018 05:00 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=323">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/25/nlp-capstone-post-5-a-new-hope/">NLP Capstone Post #5: A New Hope</a></h4>
<div class="entry">
<div class="content" lang="en">
<h1><span style="font-weight: 400;">Dataset Improvements</span></h1>
<p><i><span style="font-weight: 400;">Last time, on Music NLP.</span></i><span style="font-weight: 400;"> We ran into many midi data parsing issues. Since then, we have discovered a new dataset called the Lakh MIDI Dataset (</span><a href="http://colinraffel.com/projects/lmd/"><span style="font-weight: 400;">http://colinraffel.com/projects/lmd/</span></a><span style="font-weight: 400;">) that comes with reasonably well-formed midi files. Using the “Clean MIDI Subset”, we found thousands of midi files with their associated song names and songwriters. From these midi files, we extracted all with nonempty “lyric” fields when parsed via the pretty_midi package (which, incidentally, is also developed by Colin Raffel). After this step, we were left with ~1200 midi files that contain lyrics.</span></p>
<p><span style="font-weight: 400;">We currently clean the lyrical content by removing all unusual symbols and setting all characters to lowercase. We leave all lyrical tokens as is, which typically means syllable. Due to the inconsistent quality of the MIDI annotations, many songs are tokenized instead to characters, words, or even sentences. We will explore other methods for processing data if this is not sufficient for our results.</span></p>
<p><span style="font-weight: 400;">It is unfortunate we did not find this dataset sooner, because most of our challenges up to this point have been dealing with the poor quality of the gathered data.</span></p>
<h1><span style="font-weight: 400;">Alignment</span></h1>
<p><span style="font-weight: 400;">For our task of producing karaoke style output, there are two main tasks we have to solve. The first task is the generation of plausible lyrics, and the second is to align the lyrics to the proper time along the musical data. The alignment task has been studied extensively, but specifically aligning lyrical content to MIDI has not been covered in literature we have found. The most common alignment task is lyrics to audio data, as opposed to MIDI. The other common task is to align audio data to the notes defined in a MIDI file. In [1], they show a method that takes a MIDI file with annotated lyrics and uses this to align the lyrics to the raw audio. Unfortunately this is not our task, because we are trying to generate the annotated MIDI.</span></p>
<p><span style="font-weight: 400;">This week, we have decided to ignore the alignment task and focus primarily on making a reasonable lyrical model. We will return to alignment next week.</span></p>
<p><span style="font-weight: 400;">The next step was to align the lyrics with pianoroll. Fortunately, well-formed midi data parsed into PrettyMIDI objects come with a “get_piano_roll” function that takes as input a list of “times” which correspond to where in time pretty_midi will attempt to sample the music. As each syllable in the lyrics comes with a start time for when the singer enunciates it, we can pass in these start times to produce pianoroll that is aligned (up to small error) with the lyrics.</span></p>
<p><span style="font-weight: 400;">For some implementation reasons that are difficult to explain in English, it is possible for “get_piano_roll” to produce NaN entries, which we have replaced with zeros. Due to this and the potential for other such problems, we have forked the pretty_midi package and will be able to modify the code for our needs. For example, as pointed out in [2], “in a given MIDI file there is no reliable way of determining which instrument is a transcription of the vocals in a song”. As such, there are many choices for how to do alignment; pretty_midi has implemented just one. It is an interesting task to see how different alignment methods help or hurt our models.</span></p>
<h1><span style="font-weight: 400;">Lyric prediction</span></h1>
<p><span style="font-weight: 400;">Now that we have aligned pianoroll to lyrics data, we can begin engineering the model. Last time, we used an LSTM to generate lyrics given starting characters. Here, we will again use LSTMs, but instead, work at the syllable level and take as input the pianoroll of a song. As each column of a pianoroll is a time slice, each input vector to the LSTM is a single time slice. Each time slice is a 128-dimensional vector, with each entry representing the activation of an instrument; there are 128 midi recognized “instruments”.</span></p>
<p> </p>
<p><span style="font-weight: 400;">All that is left is to play with the architecture. </span></p>
<p><img alt="RNN model" class="alignnone size-full wp-image-322" src="https://mathstoc.files.wordpress.com/2018/04/rnn-model1.png?w=676" /></p>
<p><span style="font-weight: 400;">At the moment, our pipeline looks like what is shown in the diagram. At each iteration, we take a song, extract the lyrics and the corresponding pianoroll data. We then feed each time slice of the pianoroll data through an encoder unit, then through an LSTM unit, then through a decoder unit, and finally through a softmax to produce the prediction. Our loss is the negative log-likelihood (negative logarithm of the RNN softmax probability of the true syllable).</span></p>
<p><span style="font-weight: 400;">We will compare our final model to this baseline with respect to the loss on a held-out validation set. We will also experiment with loss functions other than cross entropy to see how it affects the actual lyrical output.</span></p>
<h1><span style="font-weight: 400;">Model results</span></h1>
<p><span style="font-weight: 400;">We have so far only trained our model for a single iteration over the training set. For an empirical evaluation on the current model quality, we ran a single MIDI through the input and computed the argmax word for each output. This produced a result in which every predicted lyric was an empty message, which is the most common string in the training set. We will explore methods to handle this class imbalance as our next task.</span></p>
<h1>References</h1>
<p><span style="font-weight: 400;">[1] Müller, Meinard &amp; Kurth, Frank &amp; Damm, David &amp; Fremerey, Christian &amp; Clausen, Michael. (2007). Lyrics-Based Audio Retrieval and Multimodal Navigation in Music Collections. 4675. 112-123. 10.1007/978-3-540-74851-9_10.</span><br />
<span style="font-weight: 400;">[2] </span><span style="font-weight: 400;">Raffel, Colin and Daniel P. W. Ellis. “Extracting Ground-Truth Information from MIDI Files: A MIDIfesto.” </span><i><span style="font-weight: 400;">ISMIR</span></i><span style="font-weight: 400;"> (2016). </span><span style="font-weight: 400;"> </span></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/25/nlp-capstone-post-5-a-new-hope/">by Nicholas Ruhland at April 25, 2018 04:44 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 24, 2018</h2>

<div class="channelgroup">







<h3><a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-5600014144802012716.post-1597347106413431292">
<h4><a href="https://nlpcapstonesemparse.blogspot.com/2018/04/blog-5-strawman-ii.html">Blog 5: Strawman II</a></h4>
<div class="entry">
<div class="content">
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Currently my model is at 20.1 Bleu and .02 EM. The state of the art has 23 Bleu and .08 EM. I'm</span><br /><span> not doing error analysis yet since I'm only able to train my model on 1/3 of the data due to </span><br /><span>performance and memory issues. I've cut training time in half through several optimization </span><br /><span>but there is a memory bug which I haven't found yet which prevents me from training on the </span><br /><span>whole dataset. Thus, I will list what I've done on the performance end, and my action plan.</span></div><span id="docs-internal-guid-17abcdb6-f838-25e9-608e-d57755b90e92"><br /></span><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><span>Speed Optimizations:</span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><span>Problem - Training ⅓ dataset taking 6 hours per epoch.</span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><span>Solutions implemented:</span></span></div><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li>Get_states_to_consider slowest part of take_step. This was optimized along with the expensive padding operation in get_action_embeddings, and the inefficient looping and sorting in compute_new_states. This cuts the training time in half, but still takes 3 hours on training set for 1 epoch</li></ul><div><span><span style="font-size: 14.6667px; white-space: pre-wrap;">Solutions to explore:</span></span></div><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li>Further areas to improve which cut down by about ½ hour are create_grammar_state, embed_actions, map_entity_productions. Here since there are 10,000 global rules which are processed for every batch in every iteration, just process them once in the constructor.</li></ul><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><span>Memory Optimization:</span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><span>Problem - Low identifier threshold or high embedding dim like in paper, on 40,000 or more instances causes gpu out of memory error.</span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><span>Debugging:</span></span></div><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li>Printing all tensors in memory at the end of forward using python garbage collection package. Total size is around 115mb but gpu uses 12gb! Perhaps this is a memory leak?</li><li>Tried different configurations to see where its crashing, high embedding dim causes crash in action index select, while just large dataset causes crash in backward.</li></ul><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;"><span>Solutions that I've tried:</span></span></div><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li>Doing index select on action_embeddings for previous embedding to save space.</li><li>All finished states del’d</li><li>Del keyword used frequently after tensor no longer used.</li><li>Disabling cudnn backend</li></ul><div><span style="font-size: 14.6667px; white-space: pre-wrap;"><span>Solutions to explore:</span></span><br /><br /><ul><li>Using pytorch DataParallel package.</li><li>Split only when a state has finished.</li></ul><div>Let me know if you guys have any ideas on solving this gpu out of memory issue!</div></div></div>







<p class="date">
<a href="https://nlpcapstonesemparse.blogspot.com/2018/04/blog-5-strawman-ii.html">by nlpcapstone (noreply@blogger.com) at April 24, 2018 03:14 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/actual-strawman-update">
<h4><a href="http://sarahyu.weebly.com/cse-481n/actual-strawman-update">Actual Strawman Update</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph"><strong>Real Data and Results Have Been Seen! </strong><br />As I mentioned in my last post, I was struggling with accessing the data, but I've since solved my problems and got to learn some cool tools along the way (like apparently you can read a compressed file without decompressing??? wild). I've also spent a large part of the week learning and fighting with SqlAlchemy, PyMySQL, MySQL, and UTF-8 issues. With the interaction of all of these, I was able to read (most of the) Reddit posts of January 2017 (thanks to Jason Baumgartner publishing these dumps on pushshift.io, I will donate when I have an income) which amounted to 80 million posts, find the users we are interested in, find neurotypical subreddits these users post to, and then get posts of our two (neurotypical and neurodivergent) subreddit subsets. <br /><br />Side Note: I'm going to start referencing the Neurodivergent set as ND, and Neurotypical as NT, trying to save some typing<br /><br /><strong>Baseline #1 (kind of an update of the Strawman #1):</strong><ul><li><span>Glen Coppersmith and Erin Kelly (2014). <strong><em>Dynamic Wordclouds and Vennclouds for Exploratory Data Analysis. </em></strong></span><span><font size="2">Association for Computational Linguistics Workshop on Interactive Language Learning and Visualization</font></span></li></ul> With thanks to Coppersmith and Kelly, I was able to make a Term Frequency Venncloud as seen below that show in black the most frequent terms found in both subreddit subsets, and then separated into the most frequent terms in neurodivergent subreddits and neurotypical subreddits in blue and red, respectively. </div>  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0px; margin-right: 0px; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-04-26-at-1-48-22-pm_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;">Naurodivergent vs. Neurotypical Subreddit Venncloud</div> </div></div>  <div class="paragraph"><br />As we can see, the middle of the venncloud is pretty uninteresting, but here are some notable points:<ul><li>Personalization: Frequency of you/me, your/my words which indicate some sense of relationship and more personalization between ND posters. This contrasts the frequency of they, indicating some discussion of an other, in the NT subreddits.</li><li>"is": I interpreted the frequency of this word in the NT subreddits as a more definitive and declarative way of speech, rather than other words such as "think", "feel" and "maybe" (in the ND subreddits) which signal more hesitation. This is a point touched on and described as dogma in Fast &amp; Horvitz which is one of the papers I discussed in a previous post.</li><li>"www", "imagesofnetwork" :  This is something I cold probably fix; the way I pre-process the data scrubs and separates the links into separate words. At the end of the day though, this shows that there are significantly more links in NT subreddits. My thought is that the lack of such in the ND subreddits might mean more anecdotal and personal interactions than when compared to ND subreddits</li><li>Moral Adjectives: Some of the ND frequently used terms are what I am going to call Morale Adjectives (let me know if there's an actual term for this); here I mean, we see words like "good", "right", "bad", which are often used to describe habits or behavior.</li><li>SURPRISE GENDER DIFF: As you can see, 'she' is one of the most frequent ND words, whereas 'he' is  one of the most frequently used NT words. Some thoughts: 1) doesn't show anything, there are some partner subreddits and may just show that the predominantly male reddit user base talks about different genders in the two, but they themselves may not be a different gender distribution or 2) could show different gender engagement in the different subsets.</li></ul><br /><strong>Baseline #2: Connotation Frames</strong><ul><li><span style="color: rgb(0, 0, 0);">Hannah Rashkin, Sameer Singh, Yejin Choi. 2016. <strong><em>Connotation Frames: A Data-Driven Investigation.</em></strong><font size="2"> In Proceedings of ACL 2016</font></span></li><li><span style="color: rgb(0, 0, 0);">Maarten Sap, Marcella Cindy Prasettio, Ari Holtzman, Hannah Rashkin, &amp; Yejin Choi. 2017. <em><strong>Connotation Frames of Power and Agency in Modern Films.</strong></em> <font size="2">sched. to appear EMNLP 2017 short papers. </font></span></li></ul> With help from Maarten Sap, another model I explored was the Connotation Frames formalism, to look at the verbs used in both our NT and ND subreddits and the sentiments these provide between agent and subject. However, we found no significant differences between the two (output below).</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 50%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-04-26-at-9-45-10-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 50%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/nt-verbs_1_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph"><br /><strong>Baseline #3: LIWC2015</strong><br />Finally, I used LIWC2015 to count and classify the psychological meanings and categories for both NT and ND subreddits. This serves as another type of language model to define these two 'languages' and offers us another metric on which to find similarities and differences. ​<br /><br />Just for some clarification, the way that this model works is by having 73 categories (more information available <a href="https://liwc.wpengine.com/compare-dictionaries/" target="_blank">here</a>), anywhere from topics - PRONOUN, HEALTH, BIO - to grammar - VERB, ARTICLE - and gives the percentage of the language that each category accounts for in that 'language'. In our case, we see the distribution of categories in NT subreddits compared to ND subreddits. <br /><br /><span>My hypotheses were:</span><ul><li>[You, Heshe, Pronoun, Health, Feel, They] categories would be significantly higher in ND </li><li>[Anger, Power, Swear] categories would be significantly lower in ND than in NT</li></ul><br />After getting the results, I report the top 10 categories with the largest % difference between the two. <ul><li>HEALTH(3.34x), <span>INGEST(2.69x), BIO(2.13x)</span><br /><ul><li>These categories are topic specific (ingestion related to drug subreddits and bio on biological processes) and align with what we expect in mental health topic subreddits</li></ul></li><li>FEEL(2.04x), SAD(1.88x), ANX - anxiety (<span>2.86x)</span><br /><ul><li><span>​Also make sense for the support communities within the ND group, potentially, topical for "anxiety" as a temporary and consistent feeling</span><br /></li></ul></li><li>FAMILY(2.07x), HOME (2.05x)<br /><ul><li>This was a bit surprising, I believe appeals to the family and home tend are prominent in support groups as well as the "partners of" subreddits we have in the ND group</li></ul></li><li>I (1.73x)<ul><li>There seems to also be a lot of personal discussion, which we expect in subreddits that are meant to discuss personal problems</li></ul></li><li>FEMALE (1.81x)<br /><ul><li>ITS HERE AGAIN WHY AND HOW</li></ul></li></ul>​<br />That's all for baseline models, here's to my first attempt at the more advanced model this week...</div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/actual-strawman-update">April 24, 2018 07:00 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 20, 2018</h2>

<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=16">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/04/20/strawman-baseline-1-deep-recurrent-q-network/">Strawman/Baseline 1: Deep Recurrent Q Network</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>For this week, I decided to first develop the RL model for asking questions. I chose to try a Deep Recurrent Q Network (DRQN) first (and the policy gradient method later) using the following repository as a starting point:</p>
<p><a href="https://github.com/awjuliani/DeepRL-Agents/blob/master/Deep-Recurrent-Q-Network.ipynb" rel="nofollow">https://github.com/awjuliani/DeepRL-Agents/blob/master/Deep-Recurrent-Q-Network.ipynb</a></p>
<p>This repository implements the DRQN for games by using LSTM cells to encode sequential information as successive frames are passed in for each time step. These frames are then passed through a CNN, then the LSTM cell, and then output the Q values. In the code I replaced where the input image (frame) is passed into the DRQN with the 2D word embedding matrix (GloVe vector for each word in sentence) and passed it straight into the CNN. I also gave a default reward of 0 to the model and managed to run the model without errors.</p>
<p>My goal for next week is to change the reward allocation to be user input, hardcode all the question templates, and begin training the model.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/04/20/strawman-baseline-1-deep-recurrent-q-network/">by ananthgo at April 20, 2018 06:47 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 19, 2018</h2>

<div class="channelgroup">







<h3><a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-5600014144802012716.post-2741407940361589303">
<h4><a href="https://nlpcapstonesemparse.blogspot.com/2018/04/blog-4-strawman-i.html">Blog 4: Strawman I</a></h4>
<div class="entry">
<div class="content">
I'm implementing the parser within the the wikitables semantic parser(Neural Semantic parsing with type constraints by Krishnamurthy et al) since both are solving similar tasks and it'd be a cool result if the same architecture worked for both tasks. The similarity is that both tasks need to generate the logical form which incorporates elements of a context. For wikitables its cell and column names and for this task its variable and method names.<br /><br />Results:<br /><br /><ul><li>Wikitables framework - 4% accuracy.</li><li>Wikitables framework + parent states - 12% accuracy.</li></ul><div><br /></div><div>The wikitables baseline performs poorly and integrating the parent production rule used as input to the decoder cell in the java paper but not the wikitables paper results in a eight percent improvement. This is since the java production rule sequences are much longer and cannot just rely on the previous rule.</div><div> </div><br />These results demonstrate that while that tasks are similar there are some key differences in the datasets that won't allow the exact same architecture to be used. Nonetheless, I will still be implementing the java parser within the wikitables framework incorporating necessary elements to boost performance.<br /><br />My code is here:<br /><b>https://github.com/rajasagashe/allennlp/tree/enviro-linking</b></div>







<p class="date">
<a href="https://nlpcapstonesemparse.blogspot.com/2018/04/blog-4-strawman-i.html">by nlpcapstone (noreply@blogger.com) at April 19, 2018 07:13 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 18, 2018</h2>

<div class="channelgroup">







<h3><a href="https://cse481n.blogspot.com/" title="PrimeapeNLP">Ron Fan, Aditya Saraf <br/> Team PrimeapeNLP</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-3753031463594823927.post-8569998071322028844">
<h4><a href="https://cse481n.blogspot.com/2018/04/blog-post-4.html">Blog Post #4</a></h4>
<div class="entry">
<div class="content">
<div dir="ltr" id="docs-internal-guid-7937c8af-d777-ed0f-f77b-cf09623b985e" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">A big part of our work for setting up a baseline is the creation of a reasonably good dataset for training and evaluating extractive text summarization. Our goal was to build a dataset with all sentences from the articles marked with binary labels indicating whether or not they were part of the extracted summary.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">We used the DailyMail and CNN story dataset, which is meant for abstractive summarization, to build our own dataset. The data set includes automatically-parsed lines from news articles, as well as a few bullet point highlights for each article. These highlights are not sentences directly from the article, but are overall a decent indicator and considered to be one of the most useable datasets out there (if for no other reason than sheer volume of data).</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">To build our data set, we used a toy metric roughly based off of ROUGE-N, with the understanding that we would only need a way of relative ranking for each sentence.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">The procedure to generate the data set was as follows:</span></div><ol style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Read in a single .story file from the abstractive dataset into highlights and sentences.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Post-process sentences- in the original data, new lines do not correspond exactly with sentence endings. Additionally, some sentences are originally split arbitrarily by new lines. We do an additional split by the regex (?&lt;=[.?!])\s+ (whitespace with period, question mark, or exclamation mark lookbehind), but sentence “re-joining” is off by default, as some sentences in the original dataset do not end with punctuation.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Score each sentence with our metric.</span></div></li><ol style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Tokenize each sentence and highlight.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Build unigram, bigram, and trigram lists.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Count the number of matches between the sentence and all the highlights, giving more weight to bigram and trigram matches and also weighting by word length.</span></div></li></ol><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Sort sentences by score.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Let S be the number of sentences and H be the number of highlights. The algorithm picks up to min(1, floor(S/2)) sentences as part of the summary. It picks at least the H sentences with the highest scores. After sentence H is picked, subsequent sentences will only be picked if their score was at least (80+3*X)% of the previously-picked sentence’s score, where X is the number of sentences currently picked. This is effectively a fairly arbitrary metric to pick the most likely sentences roughly in accordance with the size of the abstractive summary, while also allowing for particularly similar sentences to both be chosen for completeness.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">All data input and output with UTF-8 encoding.</span></div></li></ol><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Output format:</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">&lt;#&gt; &lt;SENTENCE PLAINTEXT&gt;</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"># is either 0 = normal sentence in the article, 1 = sentence chosen as part of extractive summary, 2 = original abstractive summary (should not be touched by model, only left there for human reference). </span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Overall, we think our data set is generally reasonable and makes sense. Nonetheless, there are a number of weaknesses with both extractive models in general and this specific type of dataset (including some problems which derive from the original abstractive dataset).</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Key weaknesses of data set:</span></div><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">In the original data, highlights do not always correspond to actual information in the article. For example, this short story has four highlights that are all new information: 008fc24ca9f4c48a54623bef423a3f2f8db8451a.story.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Frequent formatting problems - sentences that don’t terminate with punctuation, repeated sentences, bugged unicode characters, etc.</span></div></li></ul><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Key weaknesses specific to extractive summarization:</span></div><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Articles that are particularly short are effectively impossible to meaningfully summarize with extractive techniques.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Some sentences in articles contain the same content with slightly different wording. In these cases, we decided to choose both sentences, with the idea that post-processing could take care of highly-similar sentences, but a model could not be expected to accurately distinguish between two such sentences if they have different labels.</span></div></li></ul><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">The Baseline Model:</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">For our baseline combinatorial approach, we did a simple Maximum Coverage Problem implementation. The Maximum Coverage Problem is a classic NP-complete problem. Given a collection of n sets, the goal is to maximize the number of chosen (or “covered”) elements while only choosing k of the sets (where k &lt; n). It’s very straightforward to reduce summarization to this problem. The sentences in the document are the sets, and the vocabulary of the document represents the “universe”, or the elements in the sets. The goal is to cover as many words as possible while picking the same number of sentences as in the labelled summary. The intuition is that sentences with more words capture more semantic meaning, and once a word is listed in the summary, that word need not be considered again. This reduction makes some intuitive sense, but two main aspects must be refined. First, not all words are equally meaningful. Some words, like “a”, “the”, or “and”, have little semantic content, while named entities and other important words have much higher semantic content. Second, we should at least account for sentence lengths -- instead of constraining the algorithm to pick the same number of sentences, we should constrain it to the same word count. This two additional criteria turn the problem into the Budgeted Maximum Coverage Problem: elements in the sets have specified values, and the sets have specified costs. The new objective is to maximize the total value of the covered elements while remaining with a given budget.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Of course, this problem is NP-complete, so it may seem like a strange choice to model summarization with. There are two approaches to this. First, a simple greedy algorithm achieves an approximation of ~63%, which means that the greedy algorithm is guaranteed to get at least 63% of the value of the optimal solution. For the standard MCP, the greedy algorithm just chooses the set with the most uncovered elements until no more sets are allowed. For the budgeted MCP, the approximation algorithm is slightly more complex. See Khuller et al. for a detailed look at it -- the algorithm is slightly more work, but still very understandable, and achieves the same approximation bound.  We can also formulate the problem as an Integer Linear Program (ILP) and use an ILP solver to generate optimal solutions on realistic instance sizes. This works because the documents and sentences are both small.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">For now, we are using the greedy algorithm for the standard MCP. As a simple metric, we used the number of correct chosen sentences divided by the number of chosen sentences in total (correct means that the gold standard also picked that sentence). With a basic MCP approach, we had an accuracy of 0.218. This was measured on a small sample of 2000 stories. In order to approximate a weighting scheme, we use a stop list to simply remove common articles and other semantically empty words. With this small modification, we had an accuracy of 0.2328.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">We plan to extend this to a better baseline model in the next few days. We will formulate the problem as an ILP and pass it to an ILP solver for an optimal solution. We will also implement a more sophisticated weighting scheme. One idea is to use some of the labelled data as training data (right now, we don’t train) to build a weight matrix for common words -- we can build this matrix with a logistic regression, using n-gram (probably unigram) similarity as the loss function.This will hopefully give us a more reasonable baseline model.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Evaluation Framework:</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">For now, we’re using a simple accuracy metric. We plan to switch to ROUGE, which counts n-gram similarity (ROUGE measures recall, not precision). We will work with the ROUGE-2.0 Java package or pyrouge, depending on which language we are working with. </span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">So to summarize our goals for the next week, we will improve this baseline model by formulating the problem as an ILP and we may implement a weighting scheme (if that’s too complicated, we’ll save it for the advanced model). We will also build a baseline neural model. Then, we’ll flesh out our evaluation framework using ROUGE metrics, train/test using more of the corpus, and have detailed error analysis.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">References:</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Khuller, S., Moss, A., &amp; Naor, J. (Seffi). (1999). The Budgeted Maximum Coverage Problem. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Inf. Process. Lett.</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">, </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">70</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">(1), 39–45. https://doi.org/10.1016/S0020-0190(99)00031-9</span></div></div>







<p class="date">
<a href="https://cse481n.blogspot.com/2018/04/blog-post-4.html">by Ron &amp;amp; Aditya (noreply@blogger.com) at April 18, 2018 06:25 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=314">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/18/nlp-capstone-post-3-baseline-and-midi-frustration/">NLP Capstone Post #4: Baseline and MIDI Frustration</a></h4>
<div class="entry">
<div class="content" lang="en">
<h1><b>Baseline model</b></h1>
<p><span style="font-weight: 400;">Our baseline approach is taken from Daniil Pakhomov’s excellent blog post[1]. In this post, two separate RNNs are trained as generators: one for lyrical content and one for music content (in piano roll format). We will begin by using his trained lyrical model, and attempt to do conditional sampled character generation given a starting sequence of characters. We loaded the already trained models from the blog post and generated lyrics according to the style of a given songwriter and with a given seed word. The lyrics are generated via a character-level LSTM and generates the next character conditioned on the preceding characters and choice of songwriter. The model is trained on a corpus of song lyrics, where naturally the “correct” character to generate is the next character in the lyrics. Essentially the same mechanism is applied to the musical note generation.</span></p>
<p><span style="font-weight: 400;">The characters are encoded as a one-hot vector over all letters in the English alphabet plus space, comma, etc. Piano rolls already come in an encoding amenable to feeding into RNNs, modulo additional zero padding to ensure every time slice of every piano role has the same dimension. In particular, at each time step (discretized in an appropriately fine-grained way), we have an indicator 0-1 vector on which notes are currently activated.</span></p>
<p><span style="font-weight: 400;">Here is a song generated in the style of “Queen” with the starting seed sequence of characters “Music”:</span></p>
<blockquote>
<p style="text-align: center;"><i>Music savor valerite – yah  </i></p>
<p style="text-align: center;"><i>Imabribot, bind me – I – well  </i></p>
<p style="text-align: center;"><i>All going down to L</i></p>
<p style="text-align: center;"><i>At the eyes of the universe  </i></p>
<p style="text-align: center;"><i>Agree, five to the Slim  </i></p>
<p style="text-align: center;"><i>I just want to convincide  </i></p>
<p style="text-align: center;"><i>We wash stars and quiet Ich  </i></p>
<p style="text-align: center;"><i>You had a dirty old baby  </i></p>
<p style="text-align: center;"><i>We won’t  </i></p>
<p style="text-align: center;"><i>We does nothing no one ezy? follohin?  </i></p>
<p style="text-align: center;"><i>Sometimes we get down and ooh  </i></p>
<p style="text-align: center;"><i>Nothing do you see all night  </i></p>
<p style="text-align: center;"><i>  </i></p>
<p style="text-align: center;"><i>This is my pries  </i></p>
<p style="text-align: center;"><i>  </i></p>
<p style="text-align: center;"><i>Joyful the world  </i></p>
<p style="text-align: center;"><i>Does their beams  </i></p>
<p style="text-align: center;"><i>Surgeon makes the scule la beat  </i></p>
<p style="text-align: center;"><i>Walking out on my pocket ride  </i></p>
<p style="text-align: center;"><i>My faulty power  </i></p>
<p style="text-align: center;"><i>I wear from the ston</i></p>
</blockquote>
<p><span style="font-weight: 400;">Eventually, since we are actually interested in converting the musical information into plausible lyrics, we will need to modify this baseline in the natural way to take as input time slices of the musical instrumentation in piano roll format and predict characters (or syllables) that are to be enunciated simultaneously with the played notes. In this manner, the lyrics come already aligned in a natural way, and the words can be extracted by compressing the letters occurring between spaces.</span></p>
<h1><b>Dataset parsing</b></h1>
<p><span style="font-weight: 400;">The MIDI format has an unfortunate number of unexpected caveats. We have spent a majority of our time so far cleaning the data and attempting to use it in existing Python libraries that handle MIDI. A brief description of MIDI[2] covers some of the challenges:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">MIDI is made up of a series of messages, such as notes, instruments, and tempo changes. Additional metadata messages exist called meta messages, which can contain text content such as the song title (and lyrics!). In our dataset, lyrics are provided either as “text” messages or as “lyrics” messages.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Messages are grouped into different tracks, often representing separate instruments. Metadata sometimes is located in its own track, and lyrics are sometimes found in a different track from the rest of the metadata.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Durations in the MIDI format are specified as a delta-time relative to the most recent frame. Delta times are in a unit called a tick. Ticks are defined in the file header as a division of the quarter note. The header also defines the number of ticks per frame, which is what the deltas are relative to. Beats per minute (bpm) messages adjust the speed of playback throughout the song.</span></li>
</ul>
<p><span style="font-weight: 400;">The most promising library so far is PrettyMIDI[3], which handles most of the unexpected behavior of the basic MIDI format. This library wraps MIDI messages into structured python objects, and provides a conversion from MIDI into piano roll format. Piano roll in this case is a numpy array of shape (num_notes, num_frames). This allows us to input the musical data directly into an RNN. The units are also converted into absolute seconds, as opposed to relative durations. PrettyMIDI can additionally handle embedded lyrics, but this has proven to be a challenge due to the variety of annotation styles in our dataset. About 200 of our 900 files have parsed lyric data properly, so continuing to clean our data is a high priority.</span></p>
<h1><strong>U</strong>pd<strong>ate</strong></h1>
<p><span style="font-weight: 400;">Unfortunately, we have found the Kara1k dataset[4] to be inapplicable to our project, as the raw sequence of musical notes and lyrical content are not provided, only metadata that the dataset developers have extracted.</span></p>
<h1>References</h1>
<p><span style="font-weight: 400;">[1] </span><a href="http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/"><span style="font-weight: 400;">http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/</span></a></p>
<p><span style="font-weight: 400;">[2] </span><a href="http://www.music.mcgill.ca/~ich/classes/mumt306/StandardMIDIfileformat.html"><span style="font-weight: 400;">http://www.music.mcgill.ca/~ich/classes/mumt306/StandardMIDIfileformat.html</span></a></p>
<p><span style="font-weight: 400;">[3] </span><a href="http://craffel.github.io/pretty-midi/"><span style="font-weight: 400;">http://craffel.github.io/pretty-midi/</span></a></p>
<p><span style="font-weight: 400;">[4] </span><a href="http://yannbayle.fr/karamir/kara1k.php"><span style="font-weight: 400;">http://yannbayle.fr/karamir/kara1k.php</span></a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/18/nlp-capstone-post-3-baseline-and-midi-frustration/">by Nicholas Ruhland at April 18, 2018 04:58 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-9203775015655831448.post-1250926726356516395">
<h4><a href="https://teamoverfit.blogspot.com/2018/04/4-milestone-strawmanbaseline-i.html">#4 Milestone: Strawman/Baseline I</a></h4>
<div class="entry">
<div class="content">
<h2 style="height: 0px;"><span>Team Overfit</span></h2><h3><span><br /></span></h3><h3><span>Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span>Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><div style="text-align: start; text-indent: 0px;"><div style="margin: 0px;"><div><span><b>Blog Post: #4: 04/17/2018</b></span></div><div><span><span><b><br /></b></span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Social Media Predicts Stock Price (StartUp Mode)</span><br /><br /><span>This week, we experimented with crawling data by querying from Twitter website, but we had some struggles finding the meaningful data set. Firstly, the Twitter API does not support fetching tweets prior than 7 days, so we have to write bash scripts and use a crawler to strip data from website, which is comparatively slower.</span><br /><span id="docs-internal-guid-945a652a-d716-7ba2-4653-bdd5ec706d01"><a href="https://github.com/Jefferson-Henrique/GetOldTweets-python"><span>https://github.com/Jefferson-Henrique/GetOldTweets-python</span></a></span><br /><span><br /></span><span>We tried to query keywords like Apple, iPhone, iPad. Lots of tweets we got are not in English, so we used language detection tools to filter out tweets in other languages. Unfortunately, there are too much giveaway and advertisement flooding on the platform. Some examples are:</span><br /><blockquote class="tr_bq"><span>"Microsoft planning to launch Surface Pro 6 in first quarter of 2017"</span></blockquote><blockquote class="tr_bq"><span>"KFire TV Giveaway: Win a Microsoft Bluetooth Mouse. http://kodifiretvstick.com"</span></blockquote><blockquote class="tr_bq"><span>"Microsoft Releases The 'Studio', Its First Desktop Computer"</span></blockquote><blockquote class="tr_bq"><span>"Like - free microsoft points world http://freemicrosoftpointsworld.weebly.com/"</span></blockquote><span>Therefore, for out strawman model #1 we decide to use the relative news headlines under twitter search query.</span><br /><span><br /></span><span>We searched companies' names like Google, Microsoft, Tesla to find relative new headlines and use unigram (bag of words) to put it in a decision tree model with random forest mechanism to predict the price go UP or DOWN. We had mixed results using this model. This is just our first attempt to see whether the twitter data is useful to predict the movement of future stock prices.</span><br /><span><br /></span><span>AAPL GOOG MSFT AMZN</span><br /><span>53.66% 58.53% 56.10% 37.50%</span><br /><span><br /></span><span>Next week, we'll try to use deep neural network as predictive model and expand our input to the user feedback of the product from general users.</span><br /><blockquote class="tr_bq"></blockquote><blockquote class="tr_bq"></blockquote><blockquote class="tr_bq"></blockquote><blockquote class="tr_bq"></blockquote><blockquote class="tr_bq"></blockquote></div></div></div></div>







<p class="date">
<a href="https://teamoverfit.blogspot.com/2018/04/4-milestone-strawmanbaseline-i.html">by Team Overfit (noreply@blogger.com) at April 18, 2018 04:47 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/first-strawman-attempt">
<h4><a href="http://sarahyu.weebly.com/cse-481n/first-strawman-attempt">First Strawman Attempt</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">Well...here's the first hiccup: the Reddit data I'd like to use is much too large (like 8GB compressed and unknown uncompressed for one month of posts). Since my last post, I had been working on trying to scrape the data manually through Reddit API requests, but I was running into some issues and it was taking quite a while because of the request restrictions.<br /><br />I decided, maybe a little later than I should have, to use the Reddit Data dumps provided by John Baumgartner at pushshift.io instead, but much of the recent data is too large for my computer, attu, as well as my Azure instance. I am working with a grad student to get access to more resources so that I can work with (or even open) some of these files! However, in the meantime I have committed some files that <em>would </em>be my strawman. I have a list of 124 subreddits labeled as the neurodivergent subreddit subset and currently have a model that builds a set of users that post to these subsets, finds the other neurotypical subreddits those users also post to, and aggregates a set of those subreddits. The strawman compares the basic n-grams in each subset of the subreddits to see basic language model differences. I also cloned the vennclouds github project to try and visualize the n-grams (uni, bi, and tri) that are used in these two subsets as well as their overlap however, the current version seems to have a basic bug.<br /><br />I will get the resources hopefully in the next couple of days and update this post with the actual data results! <br /><br /><em>To be continued...</em><br /><br /></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/first-strawman-attempt">April 18, 2018 03:47 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a></h3>


<div class="entrygroup" id="http://cse481n-capstone.azurewebsites.net/?p=41">
<h4 lang="en-US"><a href="http://cse481n-capstone.azurewebsites.net/2018/04/17/data-collection-and-first-baseline/">Data Collection and First Baseline</a></h4>
<div class="entry">
<div class="content" lang="en-US">
<h3>Data Collection</h3>
<p><span style="font-weight: 400;">In a previous blog, we mentioned using RAKE to collect content phrases from posts in order to compare their similarity. However, we decided since then that it would be easier to just look at ngrams, since they can capture the same information as the content phrases. So, after removing stopwords we collect all of the unigrams, bigrams, and trigrams on each MeanJokes post and on each general Reddit post. However, this creates a ton of data compared to the posts or even their content phrases. So, we must do some cleaning before we make use of this data.</span></p>
<p><span style="font-weight: 400;">For data cleaning, we decided to use document frequency to further clean content ngrams gathered from both mean-jokes and other Reddit posts. The reason why we choose df is:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">First, we are using content ngrams, so it does not make sense to compute ngram term back on the original post since bigram, trigram term frequency is likely to be 1.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Second, df provides us a good way to remove too frequent and too infrequent ngrams. If an ngram is too frequent or too infrequent, it won’t be very informative about the pattern of the sentence.</span></li>
</ul>
<p><span style="font-weight: 400;">By using document frequency, we achieved the following purpose:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Much shorter, cleaner content ngrams</span></li>
</ul>
<p><b>Example:</b></p>
<p><span style="font-weight: 400;">[(‘took’, ‘family’, ‘camping’), (‘camping’, ‘concentration’), (‘took’, ‘family’), (‘family’, ‘camping’), (‘took’,), (‘family’,), (‘concentration’, ‘camping’), (‘family’, ‘camping’, ‘concentration’), (‘camping’,), (‘concentration’,), (‘camping’, ‘concentration’, ‘camping’)]</span></p>
<p><b>After filtering:</b></p>
<p><span style="font-weight: 400;">[(‘took’,), (‘family’,), (‘camping’,), (‘concentration’,)]</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Filtered out posts written in a different language (like written in Spanish or Germany)</span></li>
</ul>
<p><strong>Example:</strong></p>
<p><span style="font-weight: 400;">[(‘adan’,), (‘zapata-con’,), (‘carino’,), (‘para’,), (‘ti’,), (‘letra’,), (‘encanta’,), (‘sii’,), (‘adan’, ‘zapata-con’), (‘zapata-con’, ‘carino’), (‘carino’, ‘para’), (‘para’, ‘ti’), (‘ti’, ‘letra’), (‘letra’, ‘encanta’), (‘encanta’, ‘sii’), (‘adan’, ‘zapata-con’, ‘carino’), (‘zapata-con’, ‘carino’, ‘para’), (‘carino’, ‘para’, ‘ti’), (‘para’, ‘ti’, ‘letra’), (‘ti’, ‘letra’, ‘encanta’), (‘letra’, ‘encanta’, ‘sii’)]</span></p>
<p><span style="font-weight: 400;">However, we sometimes encounter a problem that after filtering, the content ngrams becomes empty. We decide to skip these lines when we do set similarity because by looking back to the original posts, posts that end up with empty content ngram are generally </span><b>very short</b><span style="font-weight: 400;"> and </span><b>non-informative</b><span style="font-weight: 400;">. For example, the post “Bolivian coastline MeanJokes” ended up with an empty content ngrams. </span></p>
<p><span style="font-weight: 400;">To determine our final dataset, we want to find Reddit posts that use similar language to the MeanJokes posts. To do this, we use </span><a href="https://en.wikipedia.org/wiki/Jaccard_index"><span style="font-weight: 400;">Jaccard index</span></a><span style="font-weight: 400;"> to determine similarity. Due to the nature of MeanJokes posts being rather short compared to most Reddit posts, the Jaccard Index of most posts compared to these posts would be very low. By filtering as described above, Jaccard index becomes much more useful because the number of n-grams we consider for each post is greatly reduced, down to those we consider most identifying of those posts. However, Reddit posts are very diverse so there is a great sparsity of similar posts. Very few posts have Jaccard Indices greater than .2 even after this filtering. Before filtering, very few posts would even get close to .1. For context, Jaccard similarity could be thought of as a percentage from 0 to 1, where 1 means the posts are identical and 0 means they have no intersection. We have over 3 million Reddit posts just in one month though, so we are not worried about not getting a big enough dataset if we have a high threshold for similarity.</span></p>
<p><span style="font-weight: 400;">Here’s an example of what this data looks like:</span></p>
<p><b>Intersection: </b><span style="font-weight: 400;">{(‘ca’, “n’t”, ‘spell’), (‘ca’, “n’t”), (“n’t”, ‘spell’), (‘spell’,), (‘ca’,)}</span></p>
<p><b>MeanJokes Post:</b><span style="font-weight: 400;"> {(‘ca’, “n’t”), (‘therapist’,), (‘without’,), (‘ca’, “n’t”, ‘spell’), (‘rapist’,), (“n’t”, ‘spell’), (‘spell’,), (‘remember’,), (‘ca’,)}</span></p>
<p><b>Reddit Post:</b><span style="font-weight: 400;"> {(‘ca’, “n’t”), (‘ca’, “n’t”, ‘spell’), (“n’t”, ‘spell’), (‘spell’,), (‘crisis’,), (‘ca’,)}</span></p>
<p><b>Jaccard index:</b><span style="font-weight: 400;"> 0.5</span></p>
<p><span style="font-weight: 400;">5 / (9 + 6 – 5) -&gt; 5/10 -&gt; .5</span></p>
<p> </p>
<h3>Baseline Model</h3>
<p><span style="font-weight: 400;">Because we are in the process of data collection, we want to build baseline models on similar datasets and later port the model over to our own dataset. The dataset we chose for this purpose is the </span><span style="font-weight: 400;">Twitter Hate Speech dataset</span><span style="font-weight: 400;"> created by</span><span style="font-weight: 400;"> Zeerak Waseem and Dirk Hovy</span><span style="font-weight: 400;">. In this dataset, each tweet is labeled as “racism”, “sexism”, or “none”. Since our goal is to build tools that can not only detect if the text is offensive or non-offensive but also detect towards which group the text is offensive, the Twitter Hate Speech dataset serves as a good starting point. Here, we built a logistic regression model as a baseline for offensiveness detection. We consider both “racism” and “sexism” as offensive, and “none” non-offensive. </span></p>
<p><span style="font-weight: 400;">The each example in the dataset is of the format &lt;tweet_id&gt;, &lt;label&gt;. We first used </span><span style="font-weight: 400;">python-twitter </span><span style="font-weight: 400;">(a python wrapper around Twitter API) to collect the original tweets by the tweet_ids given. In this process, we noticed that 1133 tweets from this dataset are already removed from Twitter. </span></p>
<p><span style="font-weight: 400;">To make the results more reproducible, we made a train, dev, test dataset split (80%, 10%, 10%) using sklearn train_test_split function. </span></p>
<p><span style="font-weight: 400;">For data preprocessing, we used an existing </span><span style="font-weight: 400;">text preprocessor made by Zhang et. al. to clean each tweet down to plain text, removing extra space, URLs, hashtags, special characters, etc. </span></p>
<p><span style="font-weight: 400;">For text vectorization, we experimented with two different sklearn vectorizers: tf-idf vectorizer and count vectorizer. Count vectorizer converts a corpus to a document-term matrix. TF-IDF vectorizer converts a corpus to a tf-idf weighted document-term matrix. </span></p>
<p><span style="font-weight: 400;">After that, we trained a logistic regression model on training data and tuned epoch on dev data. Both the trained model and trained text vectorizer would be saved. We then loaded in the saved model and text vectorizer to make predictions and evaluate on test data. </span></p>
<p><span style="font-weight: 400;">The metrics we chose for performance evaluation are accuracy, precision, recall, and f1 score. Here we decided to be more conservative and paid more attention to precision because the expected downstream application (a.k.a. the targeting group detector)  relies on text predicted “offensive” to be actually offensive. We also focused on the f1 score to evaluate the overall performance of this baseline model.</span></p>
<p><span style="font-weight: 400;">We ran experiments with this baseline model with the different types of vectorizers and different vectorizer setups. We decided to keep follow ngram_range (1, 4) chosen by </span>Zeerak Waseem and Dirk Hovy.</p>
<ol>
<li>Set-up1:
<ul>
<li>Vectorizer:<br />
<table>
<tbody>
<tr>
<td><span style="font-weight: 400;">type</span></td>
<td><span style="font-weight: 400;">strip_accents</span></td>
<td><span style="font-weight: 400;">analyzer</span></td>
<td><span style="font-weight: 400;">ngram_range</span></td>
<td><span style="font-weight: 400;">max_features</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">tf-idf</span></td>
<td><span style="font-weight: 400;">unicode</span></td>
<td><span style="font-weight: 400;">word</span></td>
<td><span style="font-weight: 400;">(1, 4)</span></td>
<td><span style="font-weight: 400;">10000</span></td>
</tr>
</tbody>
</table>
</li>
<li>Model: <span style="font-weight: 400;">Logistic regression, epoch_chosen=5</span></li>
<li>
<table>
<tbody>
<tr>
<td>dev accuracy: 0.8162</td>
<td>test accuracy: 0.8131</td>
</tr>
<tr>
<td>dev precision: 0.8170</td>
<td>test precision: 0.8203</td>
</tr>
<tr>
<td>dev recall: 0.7336</td>
<td>test recall: 0.7333</td>
</tr>
<tr>
<td>dev f1: 0.7560</td>
<td>test f1: 0.7552</td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
<li>Set-up2:
<ul>
<li>Vectorizer:<br />
<table>
<tbody>
<tr>
<td><span style="font-weight: 400;">type</span></td>
<td><span style="font-weight: 400;">strip_accents</span></td>
<td><span style="font-weight: 400;">analyzer</span></td>
<td><span style="font-weight: 400;">ngram_range</span></td>
<td><span style="font-weight: 400;">max_features</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">tf-idf</span></td>
<td><span style="font-weight: 400;">unicode</span></td>
<td><span style="font-weight: 400;">char</span></td>
<td><span style="font-weight: 400;">(1, 4)</span></td>
<td><span style="font-weight: 400;">10000</span></td>
</tr>
</tbody>
</table>
</li>
<li>Model: <span style="font-weight: 400;">Logistic regression, epoch_chosen=18</span></li>
<li>
<table>
<tbody>
<tr>
<td><span style="font-weight: 400;">dev accuracy: 0.8175</span></td>
<td><span style="font-weight: 400;">test accuracy: 0.8067</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dev precision: 0.8040</span></td>
<td><span style="font-weight: 400;">test precision: 0.7951</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dev recall: 0.7482</span></td>
<td><span style="font-weight: 400;">test recall: 0.7388</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dev f1: 0.7664</span></td>
<td><span style="font-weight: 400;">test f1: 0.7561</span></td>
</tr>
</tbody>
</table>
</li>
<li>Comparison: <span style="font-weight: 400;">By taking character level ngrams, test f1 actually improves, but test precision drops drastically. </span></li>
</ul>
</li>
<li>Set-up3:
<ul>
<li><span style="font-weight: 400;">Vectorizer:</span><br />
<table>
<tbody>
<tr>
<td><span style="font-weight: 400;">type</span></td>
<td><span style="font-weight: 400;">strip_accents</span></td>
<td><span style="font-weight: 400;">analyzer</span></td>
<td><span style="font-weight: 400;">ngram_range</span></td>
<td><span style="font-weight: 400;">max_features</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">tf-idf</span></td>
<td><span style="font-weight: 400;">unicode</span></td>
<td><span style="font-weight: 400;">char</span></td>
<td><span style="font-weight: 400;">(1, 4)</span></td>
<td><span style="font-weight: 400;">30000</span></td>
</tr>
</tbody>
</table>
</li>
<li><span style="font-weight: 400;">Model: Logistic regression, epoch_chosen=10</span><br />
<table>
<tbody>
<tr>
<td><span style="font-weight: 400;">dev accuracy: 0.8194</span></td>
<td><span style="font-weight: 400;">test accuracy: 0.8105</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dev precision: 0.8099</span></td>
<td><span style="font-weight: 400;">test precision: 0.8021</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dev recall: 0.7473</span></td>
<td><span style="font-weight: 400;">test recall: 0.7416</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dev f1: 0.7669</span></td>
<td><span style="font-weight: 400;">test f1: 0.7598</span></td>
</tr>
</tbody>
</table>
</li>
<li><span style="font-weight: 400;">Comparison: By increasing the max number of features a document-term matrix can have, both test precision and test f1 improved compared with those of set-up2. </span></li>
</ul>
</li>
<li>Set-up4:
<ul>
<li><span style="font-weight: 400;">Vectorizer:</span><br />
<table>
<tbody>
<tr>
<td><span style="font-weight: 400;">type</span></td>
<td><span style="font-weight: 400;">strip_accents</span></td>
<td><span style="font-weight: 400;">analyzer</span></td>
<td><span style="font-weight: 400;">ngram_range</span></td>
<td><span style="font-weight: 400;">max_features</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">count</span></td>
<td><span style="font-weight: 400;">unicode</span></td>
<td><span style="font-weight: 400;">word</span></td>
<td><span style="font-weight: 400;">(1, 4)</span></td>
<td><span style="font-weight: 400;">30000</span></td>
</tr>
</tbody>
</table>
</li>
<li><span style="font-weight: 400;">Model: Logistic regression, epoch_chosen=100</span><br />
<table>
<tbody>
<tr>
<td><span style="font-weight: 400;">dev accuracy: 0.8321</span></td>
<td><span style="font-weight: 400;">test accuracy: 0.8213</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dev precision: 0.8213</span></td>
<td><span style="font-weight: 400;">test precision: 0.8097</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dev recall: 0.7690</span></td>
<td><span style="font-weight: 400;">test recall: 0.7618</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dev f1: 0.7871</span></td>
<td><span style="font-weight: 400;">test f1: 0.7781</span></td>
</tr>
</tbody>
</table>
</li>
<li><span style="font-weight: 400;">Comparison: Vectorizer is now changed to count vectorizer. With this set-up, test f1 is the highest among all set-ups, while test precision is higher than that of set-up3 and lower of that of set-up1. </span></li>
</ul>
</li>
<li>Set-up5:
<ul>
<li><span style="font-weight: 400;">Vectorizer:</span><br />
<table style="height: 96px;" width="328">
<tbody>
<tr>
<td><span style="font-weight: 400;">type</span></td>
<td><span style="font-weight: 400;">strip_accents</span></td>
<td><span style="font-weight: 400;">analyzer</span></td>
<td><span style="font-weight: 400;">ngram_range</span></td>
<td><span style="font-weight: 400;">max_features</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">count</span></td>
<td><span style="font-weight: 400;">unicode</span></td>
<td><span style="font-weight: 400;">char</span></td>
<td><span style="font-weight: 400;">(1, 4)</span></td>
<td><span style="font-weight: 400;">30000</span></td>
</tr>
</tbody>
</table>
</li>
<li><span style="font-weight: 400;">Model: Logistic regression, epoch_chosen=30</span></li>
<li>
<table style="height: 291px;" width="388">
<tbody>
<tr>
<td><span style="font-weight: 400;">dev accuracy: 0.8226</span></td>
<td><span style="font-weight: 400;">test accuracy: 0.8093</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dev precision: 0.8001</span></td>
<td><span style="font-weight: 400;">test precision: 0.7860</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dev recall: 0.7690</span></td>
<td><span style="font-weight: 400;">test recall: 0.7594</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dev f1: 0.7812</span></td>
<td><span style="font-weight: 400;">test f1: 0.7698</span></td>
</tr>
</tbody>
</table>
</li>
<li><span style="font-weight: 400;">Comparison: Test precision is the lowest of all the set-ups, and test f1 is higher than the first 3 set-ups, but lower than set-up4.</span></li>
</ul>
</li>
</ol>
<p><span style="font-weight: 400;">Although </span><span style="font-weight: 400;">Zeerak Waseem and Dirk Hovy’s paper stated that </span><span style="font-weight: 400;">character n-grams have better performance than work n-grams as features, our experiment results suggested otherwise. All of our set-ups have higher test precision and f1 score but lower test recall that the best model of the paper. These differences might be because of the removal of tweets from the original dataset. The train, dev, test data split method might also contribute to the differences. </span></p>
<p><span style="font-weight: 400;">Next Steps: We would like to experiment with other baseline models, especially neural networks and perform error analysis on these baseline models.</span></p>
<p>Work Cited:</p>
<p><a href="https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06"><span style="font-weight: 400;">Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).</span></a></p>
<p><a href="https://www.researchgate.net/publication/323723283_Detecting_hate_speech_on_Twitter_using_a_convolution-GRU_based_deep_neural_network">Zhang, Ziqi &amp; Robinson, D &amp; Tepper, J. (2018). Detecting hate speech on Twitter using a convolution-GRU based deep neural network.</a></p></div>







<p class="date">
<a href="http://cse481n-capstone.azurewebsites.net/2018/04/17/data-collection-and-first-baseline/">by Team Watch Your Language! at April 18, 2018 02:36 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 11, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/formal-proposal">
<h4><a href="http://sarahyu.weebly.com/cse-481n/formal-proposal">Formal Proposal</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">Linguistic Accommodation for Self-Presentation as seen in Neurotypical vs. Neurodivergent Subreddits<br /><br /><u>Hypotheses:</u> <br />1) Users change their language depending on the community, represented by subreddits categorized as Neurotypical vs Neurodivergent<br />      - The divergence from their own baseline is a<span> sign of assimilation through tuned self-presentation*</span><br />2) Language Models, as used by the whole community, differ and not just in topic-specific jargon<br />      - The language changes of the individual user and the change in delta from the community's language model is a sign of their attempt at assimilating language accommodation*<br />          - Do certain users adapt better? If so, what is differentiating those users?<br /><font size="1">*(subpoints are very similar, and I'm still working through if there is a nuance, or if they're the same)</font><br /><br />My <strong>objective</strong>, then, is to address these hypotheses through the following approach:<br /><br /><u>Literature Survey:</u><br />While this project is novel, mainly in the focus on neurotypical vs neurodivergent separation and the use of Reddit data, this project finds guidance from previous work done on similar questions. First, this project aims to extend upon the work of Danescu-Niculescu-Mizil et al. in<em> </em><u><em>Mark My Words! Linguistic Style Accommodation in Social Media​.</em></u> This was the first large-scale endeavor in identifying linguistic accommodation using social media. However, our project extends this work by taking advantage of the siloed nature of Reddit to identify linguistic accommodation employed by a single user across communities as opposed to the one-dimensional view of a user's linguistic accommodation to the general twittersphere in Danescu's paper. Also, this project is informed by Fast and Horvitz in <u><em>Identifying Dogmatism in Social Media: Signals and Models</em></u>, specifically in their methodologies and models; I attempt to extend upon these with more complex models. In this process, I also found several works that were similar in nature: Tamburrini et al. on language change based on social identity on Twitter, Nguyen and Rose on language socialization in online communities, and Michael and Otterbacher on herding in online review language. Two more relevant works for my project are De Choudhury et al.'s work on identifying the shift to Suicidal Ideation in social media and D<span>anescu-Niculescu-Mizil's work on the life-cycle of users in online communities. <br />​<br /><u>Proposed Methodologies</u>: </span><br />In it's most basic form, these questions can be explored with basic language models. First, we will identify a subset of neurotypical and neurodivergent subreddits to explore (100 or so respectively), chosen by a preliminary search on overlapping users posting between these. Based on this preliminary search, we will also gain a set of users who potentially post to both neurotypical and neurodivergent subreddits (we may need to look only at posts within a band of characters, but that is a parameter I'd like to explore). We will aggregate all of a user's posting history, not just in the subset aforementioned, to model the user's language use and do the same for the language of all posts made by any user (not just our set) to the subreddit to model the subreddit's language. I will supplement these models with the LIWC lexicon to characterize the differences between the communities and between users in different subreddits. (I may use a subset of the LIWC categories later on). A more complex model would be to use PPDB to find differences via paraphrasing. Yet another complex model would be to use a graphical model as inspired by Bamman et al's <em><u>Learning Latent Personas of Film Characters</u></em>. A stretch goal would be to train an RNN model for the language model of a neurotypical subreddit and that of a neurodivergent to see the probability of a post to belong to either of these categories. A stretch goal (not in complexity as in the RNN, but rather in interest) is to use the <em>Zelig Quotient</em>, a proposed measure for normalizing linguistic accommodation by Jones et al and see how much this may affect our findings. <br />One special consideration is the use of NSFW language. My only filter will be to disqualify the list of NSFW subreddits, as named by a reddit post (so meta) in being chosen for the subsets, but otherwise we will not do anything special for NSFW language in other subreddits. <br /><br /><u>Resources</u>: Lots of Reddit fun!!!<br /><br /><em>​Here goes nothing...</em></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/formal-proposal">April 11, 2018 07:00 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=14">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/04/11/project-proposal-question-based-knowledge-representation/">Project Proposal: Question-Based Knowledge Representation</a></h4>
<div class="entry">
<div class="content" lang="en">
<p><strong>Project Objective:</strong> The objective of this project is to create a model that can build a representation of the knowledge it’s gathered by reading the input text line by line and asking appropriate clarifying questions to a human for further insight.</p>
<p><strong>Literature Survey:</strong></p>
<ul>
<li><cite class="formatted-citation formatted-citation--style-mla">Williams, Jason D. et al. “Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning.” <em>ACL</em> (2017).</cite></li>
<li>
<div class="padded"><cite class="formatted-citation formatted-citation--style-mla">Zhang, Qianqian et al. “A Review on Entity Relation Extraction.” <em>2017 Second International Conference on Mechanical, Control and Computer Engineering (ICMCCE)</em> (2017): 178-183.</cite></div>
</li>
<li>Arvind Neelakantan’s Doctoral Disseration: <em>Knowledge Representation and Reasoning with Deep Neural Networks (2017)</em></li>
<li>
<div class="padded"><cite class="formatted-citation formatted-citation--style-mla">Zhao, Tiancheng and Maxine Eskénazi. “Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning.” <em>SIGDIAL Conference</em> (2016).</cite></div>
</li>
</ul>
<p><strong>Minimal Viable Action Plan:</strong> My minimum action plan would be to implement and train an RL model to ask certain question template(s) as clarification as it processes input text line by line. Then a human (myself) would assign a reward to indicate the quality of the question and give a brief answer to the question. This brief answer would then be used to expand the model’s stored knowledge representation and the assigned reward would be used to train the RL model. This series of exchanges is treated like a one-on-one conversation even if it’s more like a one-sided lecture.</p>
<p><strong>Stretch Goals:</strong> Stretch goals include extracting features from the existing knowledge base and feeding those into the model to further improve the relevancy of questions (for example not asking questions that the model should already know). Another stretch goal would be to translate the knowledge representation back into everyday English text for tasks such as answering reading comprehension questions. This can be done either by training a Machine Translation model, using a parser like ANTLR, or some other method.</p>
<p><strong>Proposed Methodologies:</strong> My proposed methodologies follows a similar outline as given by Williams et al. in their paper on Hybrid Code Networks (HCNs).</p>
<ul>
<li>The input text is read in line-by-line and goes through various preprocessing steps like entity extraction, word embeddings layer, sentiment analysis, bag of words, etc. These features are concatenated and fed into the model.</li>
<li>In the HCN paper, the model was an RNN followed by a softmax layer (probability distribution over the various actions to take).
<ul>
<li>In this problem, the “actions” to take are the different types of question templates to ask so copying this model and substituting their action templates with my question templates would work.</li>
</ul>
</li>
<li>An alternative approach would be to build a Deep Q Network model with an LSTM (or GRU) layer at the end, so the network would update the Q values for each question type that can be asked (and the one with the highest Q value can be greedily selected). For clarification, each input sentence would represent a time step.</li>
<li>After a question type is selected with either of the 2 approaches above, nouns need to be substituted in to form an actual question (as in the HCN paper). For example, a question template might be “Is there a relationship between ______ and ______?” and the two blanks need to be filled in with nouns. If the substitution results in a good question, a good reward is assigned and otherwise, a very negative reward is assigned. If it’s a good question, then an answer is given to update the knowledge representation.
<ul>
<li>Some possible ways to create this knowledge representation is to use lists, trees, semantic networks, production rules, logical propositions and/or other existing NLP knowledge representation models (I haven’t decided on one yet).</li>
</ul>
</li>
</ul>
<p><strong>Available Resources/Databases:</strong> To train the model, any reading comprehension dataset like MS Marco (Microsoft), SQuAD (Stanford), RACE, etc. can be used. If my project is successful enough, I can even use these datasets for evaluation.</p>
<p><strong>Evaluation Plan:</strong> If I only finish my minimum viable action plan, I’m not sure how I can qualitatively assess my knowledge representation except by comparing the representation against the information I wanted to be recorded (or have other people judge what knowledge should be stored). On the other hand, if I finish my stretch goals (translating knowledge representation to English) I can try to use the knowledge representation to train on and respond to queries in a reading comprehension dataset (which would be a much more qualitative evaluation).</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/04/11/project-proposal-question-based-knowledge-representation/">by ananthgo at April 11, 2018 06:59 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=309">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/11/nlp-capstone-post-3-proposal/">NLP Capstone Post #3: Proposal</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>Here, we finally present our project proposal in full.</p>
<h3>Project Objectives:</h3>
<p>Our goal for this project is to engineer a model that, given the instrumental (“karaoke”) music for a song in English represented as MIDI data, output a coherent sequence of words corresponding to lyrics for the music. The model will produce timings along with the words to align it with the background instrumentals. Additionally, given the output of the model and the input music, we will automate their combination into a song complete with lyrics and supporting instrumentals. This combined output will be playable and we intend to do live demonstration.</p>
<h3>Proposed Methodology:</h3>
<p>Here, we outline the steps we will need to take in detail.</p>
<li>Data collection (datasets of songs, preferably with instrumentals and lyrics already separated)</li>
<li>Decide on vocabulary and how to handle uncommon words</li>
<li>Decide and implement any required preprocessing of the raw MIDI data. Strip lyrics from MIDI data if not already provided in dataset.</li>
<li>Decide and implement model (see Model Design)</li>
<li>Implement model sanity checks</li>
<li>Model tweaking (we expect this will take the majority of the time; see Model Design)</li>
<li>Implement automated combination of model output (lyrics) and model input (instrumentals)</li>
<li>Further testing</li>
<li>Assuming preceding steps are completed satisfactorily, proceed to stretch goals</li>
<li>Presentation and write-up</li>
<h3>Model Design:</h3>
<p>We will pursue a seq2seq RNN approach, taking in input MIDI data represented as a sequence, and outputting a sequence of words from a specified vocabulary. This model will be referred to as the generator. We will employ adversarial training, simultaneously training a many-to-one RNN discriminator that, given the input instrumentals and corresponding lyrics, output if the lyrics were produced by the generator or not. We will follow approaches taken in previous works such as SeqGAN [2] (and [3, 4]), namely using policy gradient ideas from reinforcement learning to obtain gradients that can be backpropagated from the discriminator network through the generator network. We note that syntactic correctness can be enforced in this manner, as malformed lyrical output can be assigned arbitrarily small reward.</p>
<h3>Stretch Goals:</h3>
<p>There are several stretch goals we will consider, time permitted. They are as follows, in no particular order.</p>
<li>Handling multiple languages, particularly those with less available data</li>
<li>Given a specific songwriter/band, produce the instrumentals along with lyrics for a new song that is in the style of that songwriter/band</li>
<li>Lyrics generation for duets, or multi-singer songs</li>
<li>Playing with phoneme-level generation</li>
<h3>Core Challenges:</h3>
<p>The core challenges we will need to overcome include alignment of lyrics with the music, and production of sensible lyrics. On the more technical side, it is well-known that ensuring convergence in adversarial training is difficult.</p>
<h3>Available Resources:</h3>
<p>Existing music datasets for machine learning tasks are made up of audio samples (such as .wav or .mp3), or MIDI data that specifies timing and notes. For karaoke, lyrics are also provided either as a separate text file (.LRC) specifying the timing of each word, or can be embedded into the MIDI file directly (.KAR). It may also be useful to train a lyric model on a larger corpus of song lyrics, since lyrics are easier to collect than fully time-annotated karaoke files.</p>
<p>The MusicNet dataset [9] provides 330 classical instrumental audio files, each of which has associated timing provided for every note. Since we are primarily interested in lyrical generation and alignment, this dataset is not going to be useful for creating a language model.</p>
<p>An existing karaoke dataset called Kara1k [1] provides many features computed from 1000 lyric-annotated songs. This provides lots of metadata about each song, including annotated chords for each timestep of the song. According to the KaraMIR website, these features are extracted from audio samples using Vamp Plugins, which estimates chords with accuracy up to 70%. </p>
<p>We propose a new dataset (name not yet determined) of MIDI karaoke data with embedded lyrics (.KAR). This dataset contains over 700 files, scraped from a karaoke content aggregator [11]. Timed lyrical data has been extracted from these files, and the precise timing of each note is already available by nature of the MIDI format.</p>
<p>Additional datasets for training a lyric model may be useful, and many are available. One such dataset is the 55000+ Song Lyrics on Kaggle [10]. This could help our model generalize its lyrical output beyond the limited set of vocabulary available within the 1000 or fewer annotated karaoke songs.</p>
<h3>Evaluation Plan:</h3>
<p>Evaluation of our model can be done several ways. The first is simply to listen to the music ourselves. This is the most direct method of evaluation but is not efficient, as likely we will need many iterations of tuning; furthermore, will likely need to listen to several songs to be confident of the model’s quality. Hence, we will also design basic “sanity check” tests for our models.</p>
<p>Recall that in our proposed methodology, we intend to use adversarial training. The discriminator network itself gives a direct evaluation of the generator. As long as the discriminator is of vetted quality, and the discriminator is run on sufficiently many examples (with roughly even number of generated and true examples mixed in), the generator will be deemed also of sufficient quality (as a “sanity check”).</p>
<p>Of course, this leaves the question of ensuring the discriminator is good. We can run the discriminator on instrumentals combined with randomly generated words (according to some distribution), or on instrumentals combined with the original lyrics, which are perturbed in some fashion. As an example, one can perturb the original lyrics temporally (making an utterance off-beat when it should be precisely on the down-beat of a bar) or replacing a few words with randomly selected ones (according to some distribution over the vocabulary). These “test inputs” to the discriminator can be generated before-hand.</p>
<h3>Literature Survey:</h3>
<p>Here are some relevant papers (most were already included in preceding posts).</p>
<p>[1] Y. Bayle, L. Marsik, M. Rusek, M. Robine, P. Hanna, K. Slaninova, J. Martinovic, J. Pokorny. “Kara1k: A Karaoke Dataset for Cover Song Identification and Singing Voice Analysis”. IEEE International Symposium on Multimedia (ISM), 2017. <a href="https://ieeexplore.ieee.org/document/8241597/" rel="nofollow">https://ieeexplore.ieee.org/document/8241597/</a></p>
<p>[2] L. Yu, W. Zhang, J. Wang, Y. Yu. “SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient”. Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, 2017. <a href="https://arxiv.org/abs/1609.05473" rel="nofollow">https://arxiv.org/abs/1609.05473</a></p>
<p>[3] S. Lee, U. Hwang, S. Min, S. Yoon. “A SeqGAN for Polyphonic Music Generation”. 2017. <a href="https://arxiv.org/abs/1710.11418" rel="nofollow">https://arxiv.org/abs/1710.11418</a></p>
<p>[4] H. W. Dong, W. Y. Hsiao, L. C. Yang, Y. H. Yang. “MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment”. 2017. <a href="https://arxiv.org/abs/1709.06298" rel="nofollow">https://arxiv.org/abs/1709.06298</a></p>
<p>[5] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio. “Generative Adversarial Nets”. NIPS, 2014. <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets" rel="nofollow">https://papers.nips.cc/paper/5423-generative-adversarial-nets</a></p>
<p>[6] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, X. Chen. “Improved Techniques for Training GANs”. NIPS, 2016. <a href="https://arxiv.org/abs/1606.03498" rel="nofollow">https://arxiv.org/abs/1606.03498</a></p>
<p>[7] M. Arjovsky,  S. Chintala, L. Bottou. “Wasserstein GAN”. 2017. <a href="https://arxiv.org/abs/1701.07875" rel="nofollow">https://arxiv.org/abs/1701.07875</a></p>
<p>[8] J. Faille, Y. Wang. “Using Deep Learning to Annotate Karaoke Songs”. 2016. <a href="https://www.semanticscholar.org/paper/Using-Deep-Learning-to-Annotate-Karaoke-Songs-Faille-Wang/521361762a7327f8fcc77bd9d76eaa2b503f845a" rel="nofollow">https://www.semanticscholar.org/paper/Using-Deep-Learning-to-Annotate-Karaoke-Songs-Faille-Wang/521361762a7327f8fcc77bd9d76eaa2b503f845a</a></p>
<p>[9] J. Thickstun, Z. Harchaoui, S. Kakade. “Learning Features of Music from Scratch”. 2017. <a href="https://arxiv.org/abs/1611.09827" rel="nofollow">https://arxiv.org/abs/1611.09827</a></p>
<p>[10] Additional data <a href="https://www.kaggle.com/mousehead/songlyrics">here</a></p>
<p>[11] Even more additional data <a href="http://vooch.narod.ru/midi/midi.htm">here</a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/11/nlp-capstone-post-3-proposal/">by Kuikui Liu at April 11, 2018 06:45 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://cse481n.blogspot.com/" title="PrimeapeNLP">Ron Fan, Aditya Saraf <br/> Team PrimeapeNLP</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-3753031463594823927.post-4531878816260312232">
<h4><a href="https://cse481n.blogspot.com/2018/04/blog-post-3.html">Blog Post #3</a></h4>
<div class="entry">
<div class="content">
<h1 dir="ltr" id="docs-internal-guid-ea0c9d97-b369-9237-6f13-3675807d7a60" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Project Objectives</span></h1><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Single document summarization (SDS) is one of the remaining challenging problems in natural language processing. Novel methods are presented frequently in new papers, but they often do not include specific code allowing for reproducibility and are evaluated on specific datasets that make comparisons between models meaningless and difficult.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">There are many approaches to SDS, but they can be broadly divided into combinatorial approaches and neural approaches. Neural approaches build a neural architecture, such as a seq2seq/encoder-decoder model or single sequence RNNs. Combinatorial approaches will either try to frame the problem as an optimization problem, and then use an ILP solver, or frame the problem as a classic NP-hard problem, like Knapsack or Maximum Coverage. We want to explore both approaches, and compare their performance on the same dataset.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">SDS is comprised of two tasks: extractive summarization and abstractive summarization. Extractive summarization compiles a summary by selecting sentences from the document’s text while abstractive summarization generates text for the summary (sentences that may not have been present in the document’s text). While abstractive summarization might have more intuitive appeal, our project will focus on extractive summarization to enable meaningful comparisons between neural and combinatorial approaches (combinatorial approaches often must be extractive).</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">In this project, we plan to implement at least one neural and one combinatorial model for extractive single document summarization. We hope to establish some meaningful ways to compare the differences between selections made by the different types of models. Our primary goal is to better understand the strengths and weaknesses of neural and combinatorial models for single document summarization - a particular important aspect of SDS given the general roughness of existing evaluation metrics. We will gauge our progress based on reaching acceptable performance on commonly-used evaluation metrics when we implement models.</span></div><h1 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Methodology</span></h1><h3 dir="ltr" style="line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Minimal Viable Action Plan</span></h3><ol style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Build our data set using existing data. Specifically, convert data better suited for training abstractive summarization models into data that can be used for extractive summarization..</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Implement a simple combinatorial model (for example, we can do a simple maximum coverage problem, where we set up the “universe” to be the vocabulary of the document, and treat the sentences as sets of words).</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Implement a simple neural model (just treat the problem as a generic binary classification problem).</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Train models on identical data sets and do a baseline comparison -- how well does a simple neural model do vs. a simple combinatorial model? This doesn’t tell us much about the relative strengths of the two approaches (we can’t quantify “simple”), but with some error analysis, we might be able to see what sentences neural models are misidentifying vs. what sentences combinatorial models are misidentifying.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Build at least one state-of-the-art combinatorial model (adapting from a recent paper). We have two candidate papers: Hirao et al.’s Tree Knapsack approach and Durrett et al.’s Compression/Anaphoricity</span></div></li></ol><br /><h3 dir="ltr" style="line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Stretch Goals</span></h3><ol style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Design our own model that is a combination of the strong points of the combinatorial and neural models. Ideally, our model would be as good as or better than the existing models we implemented on the quantitative and qualitative metrics we use.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Alternatively, we can use ideas from one domain to improve an aspect of a SOTA model in the other domain. For example, we might learn that neural models are great at dealing with named entities, and so incorporate a neural layer in a combinatorial model (perhaps by allowing the output of the neural layer to determine the weights of named entities).</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Design a common system for comparing performance of extractive summarization models. Rather than a differentiable evaluation metric, we think it may be useful to choose a set of “tough” documents to summarize and bundle them together with specific reasons for their difficulty, so that researchers may more easily identify weaknesses in models they are working on.</span></div></li></ol><h1 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Available Resources</span></h1><h3 dir="ltr" style="line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Dataset/Evaluation</span></h3><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">For this problem, we will be using the DailyMail/CNN dataset. From our initial research, this seems to be the standard dataset for both document summarization as well as basic reading comprehension. The dataset has 400,000 articles, and includes both the full text of the article as well as bullet point “highlights”. For reading comprehension, an important word is omitted from the highlights and the machine is asked to fill in the blank. For text summarization, the bullet points are considered the “gold standard” summaries -- machine generated summaries are evaluated against the bullet points, typically</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"> </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">using ROUGE metrics</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"> </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">(Lin, 2004). While this works fine for abstractive summarization, this training corpus is not annotated enough for extractive summarization. More specifically, extractive summarization requires sentence level binary annotations, to indicate whether each sentence does or doesn’t belong in the summary. So we need to first convert the bullet points into more fine grained annotations.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">We’ve looked at two papers which briefly touched on this. Nallapati et. al. used a greedy approach, where they added one sentence at a time to the extractive summary while seeking to maximize the Rouge score with respect to the abstractive summary (the bullet points). They also tried to use an RNN decoder in combination with the abstractive summaries to train the extractive model without using sentence-level annotations. However, this approach was slightly less successful than estimating sentence-level annotations. Cheng and Lapata used a different approach - they created a “rule-based system that determines whether a given sentence matches a highlight...The rules take into account the position of the sentence in the document, the unigram and bigram overlap between document sentences and highlights, [and] the number of entities appearing in the highlight and in the document sentence”. It’s not 100% clear what rules the authors used, but according to Nallapati et. al., the rule-based approach found a better “ground-truth” than the greedy approach.</span></div><h3 dir="ltr" style="line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">GitHub Repositories</span></h3><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Some researchers publish the code they used in their paper on GitHub. We can use repos for quick comparisons or to see how they design their code.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><a href="https://github.com/abisee/pointer-generator" style="text-decoration: none;"><span>https://github.com/abisee/pointer-generator</span></a><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">: This repo is for See et al.’s Pointer-Generator neural model.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><a href="https://github.com/cheng6076/NeuralSum" style="text-decoration: none;"><span>https://github.com/cheng6076/NeuralSum</span></a><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">: This repo is for Cheng and Lapata’s neural model, that combines a sentence level RNN with a word level CNN.</span></div><h1 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Related Work and References</span></h1><br /><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Cheng, J., &amp; Lapata, M. (2016). Neural Summarization by Extracting Sentences and Words. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">arXiv:1603.07252 [Cs]</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">. Retrieved from http://arxiv.org/abs/1603.07252</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Durrett, G., Berg-Kirkpatrick, T., &amp; Klein, D. (2016). Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">arXiv:1603.08887 [Cs]</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">. Retrieved from http://arxiv.org/abs/1603.08887</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Hirao, T., Yoshida, Y., Nishino, M., Yasuda, N., &amp; Nagata, M. (2013). Single-Document Summarization as a Tree Knapsack Problem. In </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"> (pp. 1515–1520). Seattle, Washington, USA: Association for Computational Linguistics. Retrieved from http://www.aclweb.org/anthology/D13-1158</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. In S. S. Marie-Francine Moens (Ed.), </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"> (pp. 74–81). Barcelona, Spain: Association for Computational Linguistics.</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Nallapati, R., Zhai, F., &amp; Zhou, B. (2016). SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">arXiv:1611.04230 [Cs]</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">. Retrieved from http://arxiv.org/abs/1611.04230</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">See, A., Liu, P. J., &amp; Manning, C. D. (2017). Get To The Point: Summarization with Pointer-Generator Networks. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">arXiv:1704.04368 [Cs]</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">. Retrieved from http://arxiv.org/abs/1704.04368</span></div></div>







<p class="date">
<a href="https://cse481n.blogspot.com/2018/04/blog-post-3.html">by Ron &amp;amp; Aditya (noreply@blogger.com) at April 11, 2018 06:33 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-9203775015655831448.post-5878905571398539101">
<h4><a href="https://teamoverfit.blogspot.com/2018/04/3-project-proposal.html">#3 Project Proposal</a></h4>
<div class="entry">
<div class="content">
<h2 style="height: 0px;"><span>Team Overfit</span></h2><h3><span><br /></span></h3><h3><span>Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span>Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><span><br /></span><br /><div></div><span><br /></span><br /><div style="text-align: start; text-indent: 0px;"><div style="margin: 0px;"><div><span><b>Blog Post: #3: 04/10/2018</b></span></div><div><span><span><b><br /></b></span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Social Media Predicts Stock Price (StartUp Mode)</span></div><div><span><span><b id="docs-internal-guid-213a19db-b353-3e4c-1df6-5dd289daeb8b" style="font-weight: normal;"><br /></b></span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>There are vast amount of new information related to companies listed on the stock market appears instantly, with immediate impact on stock prices. Our project is for monitoring those text on the social media platform and extract the key information that have impact on the stock prices and predict its future.</span><br /><span><br /></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Background and Project objectives </span></div><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span><a href="https://www.investopedia.com/terms/s/stockmarket.asp">Stock Market</a> refers to the collection of markets and exchanges where the issuing and trading of equities, bonds and other sorts of securities takes place.</span></li><li><span>Social media, such as Twitter, often reflects how people think about a company and therefore can be used as an indicator of the changes of stock price in the near future.</span></li><li><span>Traditionally, analytics use statistical model built on past stock prices and recent news to forecast stock prices. We would like apply Machine Learning and Natural Language Processing models on social media to see if it has enough information for us to make good prediction of future stock price.</span></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"></ul><div><span><span><b style="font-weight: normal;"><br /></b></span></span></div><span><b>Proposed methodologies</b><span style="white-space: pre;"><b><br /></b></span></span><br /><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><b><span>Dataset:</span></b></li><ul><li><span><span style="background-color: white; color: black; vertical-align: baseline; white-space: pre;">Twitter data: </span><span style="background-color: white; color: #1155cc; vertical-align: baseline; white-space: pre;"><a href="https://developer.twitter.com/en/docs">https://developer.twitter.com/en/docs</a></span></span></li></ul></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul><li><span>Preprocess twitter data:</span></li></ul></ul><ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul><li><span>Tokenization</span></li></ul></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul><li><span>Stemming</span></li></ul></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul><li><span>Lemmatization</span></li></ul></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul style="margin-bottom: 0pt; margin-top: 0pt;"></ul></ul><li><span><span style="background-color: white; color: black; vertical-align: baseline; white-space: pre;">Bloomberg financial news dataset: </span><span style="background-color: white; color: #1155cc; vertical-align: baseline; white-space: pre;"><a href="https://github.com/philipperemy/financial-news-dataset">https://github.com/philipperemy/financial-news-dataset</a></span></span></li></ul></ul><span><b>Minimal viable action plan</b></span><br /><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span>Forecast companies’ stock price changes (UP, DOWN, STAY) </span></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><b><span>Model</span></b></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span>N-gram with appropriate smoothing as baseline</span></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span>Use RNN/LSTM/GRU as model</span></li></ul><li><b><span>User Interface</span></b></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span>Command line REPL</span></li></ul></ul><span><b>Stretch goals</b></span><br /><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span>We could implement LSTM/GRU model to extract important information from the text in the preprocess</span></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span>Forecast the approximate future stock price for a company given a future date</span></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span>Auto trader bot that can take streaming tweets from twitter api and update the model prediction </span></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span>Fusion with 8-K reports to elevate the accuracy</span></li></ul><span><b>Evaluation plan</b></span><br /><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span>F-1 score for (UP/DOWN)</span></li><li><span>Loss functions for comparing predictions and expectations.</span></li><li><span>Evaluate on time required to do a prediction.</span></li></ul></div><div style="margin: 0px;"><span><b style="white-space: pre;">Reference</b></span><br /><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span><span style="background-color: white; color: black; vertical-align: baseline; white-space: pre;">On the Importance of Text Analysis for Stock Price Prediction: </span><a href="https://nlp.stanford.edu/pubs/lrec2014-stock.pdf"><span style="background-color: white; color: #1155cc; vertical-align: baseline; white-space: pre;">https://nlp.stanford.edu/pubs/lrec2014-stock.pdf</span></a></span></li><li><span><span style="color: black; vertical-align: baseline; white-space: pre-wrap;">Stock Trend Prediction Using News Sentiment Analysis: </span><span style="color: #1155cc; vertical-align: baseline; white-space: pre-wrap;"><a href="https://arxiv.org/pdf/1607.01958.pdf">https://arxiv.org/pdf/1607.01958.pdf</a></span></span></li></ul><div style="font-family: times;"></div></div></div></div>







<p class="date">
<a href="https://teamoverfit.blogspot.com/2018/04/3-project-proposal.html">by Team Overfit (noreply@blogger.com) at April 11, 2018 06:10 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 10, 2018</h2>

<div class="channelgroup">







<h3><a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a></h3>


<div class="entrygroup" id="http://cse481n-capstone.azurewebsites.net/?p=37">
<h4 lang="en-US"><a href="http://cse481n-capstone.azurewebsites.net/2018/04/10/formal-proposal/">Formal Proposal</a></h4>
<div class="entry">
<div class="content" lang="en-US">
<h3><span style="font-weight: 400;">Motivations:</span></h3>
<p><span style="font-weight: 400;">We want to create novel models for determining if the text is offensive, and why that text is offensive. To do this we want to create a new dataset that makes this task easier. We hope that our dataset and models pave the way for further innovations by others, as well as better trained conversational agents that have a better understanding of what they should or should not say. We’re going to teach them how to watch their language!</span></p>
<p><span style="font-weight: 400;">We would like to correctly classify sentences that keywords matching cannot achieve. For example:</span></p>
<p><b><i>What do you call an adult that has imaginary friends? Religious</i></b></p>
<p><span style="font-weight: 400;">And we would like to tell the reason why the sentence above is bad as well.</span></p>
<h3><span style="font-weight: 400;">Minimal Viable Plan:</span></h3>
<p><span style="font-weight: 400;">By comparing the similarity of content phrases found in r/MeanJokes posts and posts all over Reddit, we hope to create a large, high-quality dataset for training models to detect offensive text. We want to create this dataset and use crowdsourcing to label it. The labels should say if the text was offensive, and if it was then was it an attack against a particular group, what group that was, as well as the reasoning for why the labeler labeled the text this way. </span></p>
<p><span style="font-weight: 400;">While we wait for our data to be labeled, we want to start by creating baseline models on existing datasets, such as Twitter Hate Speech, Wiki Detox, and Stanford Politeness. We think that these datasets are similar enough to begin work on classifiers that don’t make use of deep annotation. After this, we can start work on improving performance on these datasets up until our crowdsourcing completes. We will explore novel models on existing datasets and try to improve their performance. </span></p>
<h3><span style="font-weight: 400;">Stretch Goals:</span></h3>
<p><span style="font-weight: 400;">Once our new Reddit dataset is fully labeled, we want to test the existing models that we made on the other datasets and continue improving them. We also want to use the new data to experiment with Q&amp;A or Deep Annotation models for creating a model that knows why a particularly offensive post is offensive. </span></p>
<p><span style="font-weight: 400;">In case if we can’t receive labeled dataset on time, we will continue to make improvements to novel models on existing datasets.</span></p>
<h3><span style="font-weight: 400;">Evaluation Plan: </span></h3>
<p><span style="font-weight: 400;">Classifier Models: Precision, Recall, F1 score</span></p>
<p><span style="font-weight: 400;">Rationale Models: deeper comparison to crowdsourced label explanations</span></p>
<p><span style="font-weight: 400;">Dataset: Random Sampling + Human Judgement</span></p>
<h3><span style="font-weight: 400;">Existing Work: </span></h3>
<h5><span style="font-weight: 400;">Previous Capstone Project: </span></h5>
<h5><a href="https://michael0x2a.github.io/nlp-capstone/"><span style="font-weight: 400;">Team Inverted Cat</span></a></h5>
<h5><span style="font-weight: 400;">Datasets: </span></h5>
<p><a href="https://github.com/ZeerakW/hatespeech"><span style="font-weight: 400;">Hate Speech Twitter Annotations</span></a><span style="font-weight: 400;"> (Waseem et al. 2016)</span></p>
<p><a href="https://github.com/t-davidson/hate-speech-and-offensive-language"><span style="font-weight: 400;">Hate Speech and Offensive language dataset </span></a><span style="font-weight: 400;"> (Davidson et al. 2017)</span></p>
<p><a href="https://meta.wikimedia.org/wiki/Research:Detox/Data_Release"><span style="font-weight: 400;">Wikipedia Talk Corpus </span></a><span style="font-weight: 400;"> (Wulczyn et al. 2017)</span></p>
<p><a href="http://www.cs.cornell.edu/~cristian//Politeness.html"><span style="font-weight: 400;">Stanford Politeness Corpus</span></a></p>
<p><a href="https://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/"><span style="font-weight: 400;">A list of bad words</span></a></p>
<p><span style="font-weight: 400;">Pre-trained word embeddings: GloVe, Facebook FastText, Google Word2Vec</span></p>
<h5><span style="font-weight: 400;">Papers: </span></h5>
<p><a href="https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06"><span style="font-weight: 400;">Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).</span></a></p>
<p><a href="https://www.semanticscholar.org/paper/Automated-Hate-Speech-Detection-and-the-Problem-of-Davidson-Warmsley/6ccfff0d7a10bf7046fbfd109b301323293b67da"><span style="font-weight: 400;">Davidson, Thomas J et al. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM (2017).</span></a></p>
<p><a href="https://www.semanticscholar.org/paper/Hate-Speech-Detection-with-Comment-Embeddings-Djuric-Zhou/c9948f7213167d65db79b60381d01ea71d438f94"><span style="font-weight: 400;">Djuric, Nemanja et al. “Hate Speech Detection with Comment Embeddings.” </span><i><span style="font-weight: 400;">WWW</span></i><span style="font-weight: 400;">(2015).</span></a></p>
<p><a href="https://www.semanticscholar.org/paper/Using-Convolutional-Neural-Networks-to-Classify-Gamb%C3%A4ck-Sikdar/0dca29b6a5ea2fe2b6373aba9fe0ab829c06fd78"><span style="font-weight: 400;">Gambäck, Björn and Utpal Kumar Sikdar. “Using Convolutional Neural Networks to Classify Hate-Speech.” (2017).</span></a></p>
<p><a href="https://www.semanticscholar.org/paper/Abusive-Language-Detection-in-Online-User-Content-Nobata-Tetreault/e39b586e561b36a3b71fa3d9ee7cb15c35d84203"><span style="font-weight: 400;">Nobata, Chikashi et al. “Abusive Language Detection in Online User Content.” </span><i><span style="font-weight: 400;">WWW</span></i><span style="font-weight: 400;">(2016).</span></a></p>
<p><a href="https://www.semanticscholar.org/paper/Ex-Machina%3A-Personal-Attacks-Seen-at-Scale-Wulczyn-Thain/4a7204431900338877c738c8f56b10a71a52e064"><span style="font-weight: 400;">Wulczyn, Ellery et al. “Ex Machina: Personal Attacks Seen at Scale.” </span><i><span style="font-weight: 400;">WWW</span></i><span style="font-weight: 400;"> (2017).</span></a></p></div>







<p class="date">
<a href="http://cse481n-capstone.azurewebsites.net/2018/04/10/formal-proposal/">by Team Watch Your Language! at April 10, 2018 10:37 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-5600014144802012716.post-8898628104121215850">
<h4><a href="https://nlpcapstonesemparse.blogspot.com/2018/04/blog-3-formal-proposal.html">Blog 3: Formal Proposal</a></h4>
<div class="entry">
<div class="content">
<b style="font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;">Minimal viable action plan </b><br /><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">Implement the model from the java paper mentioned in the previous blog posts. This includes the variable and method camel case encoding, the two step attention, the type constrained decoding, and many other tasks such as preprocessing, evaluation metrics etc.</span></span><br /><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;"><br /></span></span><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>Stretch goals</b></span><br /><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;">This depends on my error analysis on the mvp, but here are a couple ideas.</span><br /><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>1. </b>Incorporate implementation specific encoding. The encoder just uses the method names, but it'd be interesting to also include the method implementations.</span><br /><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;"><b>2. </b>More type constraints on the decoder. Currently if the decoder wants to generate a variable, there's nothing to check that the variable was previously declared.</span></span><br /><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><br /></span><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>Project objectives</b></span><br /><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">Reproduce the strong paper baseline. </span></span><br /><br /><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>Proposed methodologies</b> </span><br /><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">Implement the model from the paper. Potentially experiment with other semantic parsing task architectures and see if they also perform competitively with the baseline, for example seq2seq.</span></span><br /><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;"><br /></span></span><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>Available resources</b></span><br /><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">The dataset and allennlp.</span></span><br /><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b><br /></b></span><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>Evaluation plan</b></span><br /><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;">Test on the test set and measure bleu and exact match metrics.</span><br /><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">Test if this is useful by coming up with a couple of real classes that I've written and see if it generates the method.</span></span><br /><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">Perhaps come up with a new metric, such as a binary executability metric (stretch goal).</span></span><br /><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b><br /></b></span><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>Literature survey</b></span><br /><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">The type constrained architecture was used in a number of recent papers such as "</span><span style="font-size: 14.44px;">A syntactic neural model for parsing natural language to executable code". The dataset is novel in that previous ones haven't used programmatic contexts and have focused on nl2code pairs.</span></span></div>







<p class="date">
<a href="https://nlpcapstonesemparse.blogspot.com/2018/04/blog-3-formal-proposal.html">by nlpcapstone (noreply@blogger.com) at April 10, 2018 03:21 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 06, 2018</h2>

<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=12">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/04/06/warm-up-testing-a-codebase/">Warm Up: Testing a Codebase</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>I installed both the Tensorflow and Pytorch API’s since I’m not sure which framework I will use yet. Then, I downloaded the following repository which implemented Hybrid Code Networks for Dialog State tracking in its respective research paper.</p>
<p>Code base URL: <a href="https://github.com/voicy-ai/DialogStateTracking" rel="nofollow">https://github.com/voicy-ai/DialogStateTracking</a></p>
<p>Research Paper URL: <a href="https://www.semanticscholar.org/paper/Hybrid-Code-Networks%3A-practical-and-efficient-with-Williams-Asadi/0645905d70caf180433145be09c9af266a85c863" rel="nofollow">https://www.semanticscholar.org/paper/Hybrid-Code-Networks%3A-practical-and-efficient-with-Williams-Asadi/0645905d70caf180433145be09c9af266a85c863</a></p>
<p>Their implementation uses Keras (built on Tensorflow) to build the network. The model stores a predetermined set of action templates to execute based on what the user requests. By feeding in features like the previous action taken, a bag of words vector, an entity tracking feature vector, etc. their RNN outputs a softmax distribution over the possible action templates. The action taken is the one with the highest probability. Because the conversation is restricted to a particular domain such as searching for a restaurant, the model performed well when I ran and tested it. The model generally recognized the type of request I was making, but its responses were extremely robotic and towards the end, gave me yes or no questions to answer to narrow down what action it should take. My goal is to generalize the type of information the model can store between time steps to be able to provide responses for requests outside of a restricted domain (like searching for a restaurant in this case).</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/04/06/warm-up-testing-a-codebase/">by ananthgo at April 06, 2018 06:58 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://cse481n.blogspot.com/" title="PrimeapeNLP">Ron Fan, Aditya Saraf <br/> Team PrimeapeNLP</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-3753031463594823927.post-2253300890173394060">
<h4><a href="https://cse481n.blogspot.com/2018/04/blog-post-2.html">Blog Post #2</a></h4>
<div class="entry">
<div class="content">
<div>We’ve mostly settled on working on a single document summarization task. We want to pick a type of document to work on summarizing, although we haven’t decided on one specific category yet. </div><br /> <div>While we narrow down the details of the project, we have been reading a number of papers and other resources to become more familiar with the subject. We have setup PyTorch on our machines, which we are both familiar with, as well as Tensorflow, which we are still playing around with. We’ve found some interesting repositories on GitHub related to SDS that we are trying out: </div> <br /> <div><a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss">https://github.com/tensorflow/models/tree/master/research/textsum</a><br /> <a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss">https://github.com/gregdurrett/berkeley-doc-summarizer</a><br /> <a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss">https://github.com/chakki-works/sumeval</a><br /> <a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss">https://github.com/ceteri/pytextrank</a><br /> <a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss">https://github.com/adamfabish/Reduction</a><br /></div> <br /><div>Not all of these tools use machine learning - many seem to be heuristic-based sentence extractors. Nonetheless, it is interesting to consider their ideas in the context of neural network approaches. </div><br /> <div>One of the reasons we chose to attack this problem is that there is a rich literature to consult; this problem has been worked on in one form or another since 1958 [1]. As one would imagine, this means that there have been many different approaches to this problem, to varying degrees of success. But unlike other problems, where all current approaches are based on deep learning, there is active research into non-neural solutions to SDS. </div><br /> <div>Many researchers have tried to solve SDS with combinatorial optimization, reducing it to the Knapsack problem, the Maximum Coverage problem, or the Budgeted Median problem. For example, the Maximum Coverage problem is: given a number k and a collection S, of m sets, choose less than k sets in S that maximize the number of covered elements. To frame SDS as a Maximum Coverage problem, you break the document into “conceptual units”. Conceptual units are supposed to represent a single concept - for example, “the man bought a book” and the “the man read a book”. But it’s not clear at what granularity these conceptual units should be defined. One easy (but not especially effective) solution is to simply make each word a conceptual unit. Then, the document = S, and each sentence is a set of words inside S. The problem is now to pick k sentences from the document that maximize the word coverage in the document [2]. </div><br /> <div>One example of a recent non-neural approach is from a paper published 5 years ago [3]. The paper solves SDS by reducing it to the so-called Tree Knapsack Problem. We’ve haven’t fully wrapped our heads around the Tree Knapsack problem (it’s actually not that easy to quickly state), but the researchers’ basically involved representing a document as a Rhetorical Structure Theory-based discourse tree (RST-DT) by “select[ing] textual units according to a preference ranking”. The researchers’ first transform the RST-DT into a dependency-based discourse tree (DEP-DT) in order to get a tree that contains textual units on all nodes (RST-DT only have textual units as leaves), and then trim the DEP-DT using the Tree Knapsack problem.  </div><br /> <div>We aim to find a suitable corpus, and implement multiple models directly from these papers as our baseline models. Hopefully, that will give us insight that’ll help us formulate the problem differently. We also want to explore some neural architectures for single document summarization.  </div><br /> <div>We also have to consider whether we want to build an extractive or abstractive text summarization - the former collects a set of sentences or phrases that summarize the document while the latter tries to “learn the internal language representation to generate more human-like summaries, paraphrasing the intent of the original text” [4]. We’re leaning towards an extractive model, although we may try both. </div><br /> <div>[1] = <a target="">https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119004752.ch3</a> <br />[2] = <a target="">http://www.anthology.aclweb.org/E/E09/E09-1089.pdf</a> <br /> [3] = <a target="">https://www.semanticscholar.org/paper/Single-Document-Summarization-as-a-Tree-Knapsack-Hirao-Yoshida/ed0c8a7ab911cdb30b7e95edada3a55c01eb22c5</a><br /> [4] = <a target="">https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/</a></div></div>







<p class="date">
<a href="https://cse481n.blogspot.com/2018/04/blog-post-2.html">by Ron &amp;amp; Aditya (noreply@blogger.com) at April 06, 2018 06:31 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=304">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/06/milestone-2-music-as-a-natural-language-task/">Milestone #2: Music as a Natural Language Task</a></h4>
<div class="entry">
<div class="content" lang="en">
<h3>Framing the problem</h3>
<p>The focus of Natural Language Processing relies on patterns in the structure of language and models that find ways to encode the complexities of these structures. Many forms of music also have large amounts of structure which could potentially be discovered using similar models as a standard natural language.</p>
<p>Music datasets for machine learning purposes have recently become available through projects like MusicNet in 2016 [1]. This music is primarily classical, and provided as both audio and MIDI.</p>
<h3>Project ideas</h3>
<p>For our project we are interested in music with lyrical content – both for the potential to create a creative demo and for the interest of making this a language task. The current direction we are most interested in is the generation of lyrics for a song, given its nonlyrical content. This will be broken up into subtasks depending on the feasible scale of the project. Not all of the following points will necessarily be parts of our project, but we will use them as as starting point as we see the success of our models.</p>
<ul>
<li>Creating a machine learning model for MIDI music</li>
<li>Translating MIDI into specific artists or styles</li>
<li>Creating models for the lyrical content of specific artists or styles of music</li>
<li>Generating lyrics given an artist or style</li>
<li>Seq2seq conversion of MIDI into lyrical content</li>
<li>GANs for either side of the conversion – MIDI encoding or lyrical generating</li>
</ul>
<h3>Using MIDIs in RNNs</h3>
<p>Work by Pakhomov [2] has already used RNNs to create models for lyrics. In his <a href="http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/">blog post</a> he additionally discusses a method for forming any MIDI into piano roll format. This is essentially a matrix where each column represents a different time step, and each row represents a different note. Having a 1 corresponds to that note sounding at that time. The individual time vectors can be used as the inputs to an RNN at each time step to create a model representing the various songs.</p>
<p>One possible data source for our project is karaoke data available from various sources online. If available in large enough quantities this could be extremely convenient because it already contains many pairings of MIDI music to their lyrics.</p>
<h3>Azure</h3>
<p>We intend to use PyTorch to train our models, and have begun setting up an instance on Microsoft Azure.</p>
<h3>Relevant work</h3>
<p>[1] <a href="https://homes.cs.washington.edu/~thickstn/musicnet.html" rel="nofollow">https://homes.cs.washington.edu/~thickstn/musicnet.html</a></p>
<p>[2] <a href="http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/" rel="nofollow">http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/</a></p>
<p>[3] Dong, Hao-Wen. 2017. MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment. <a href="https://arxiv.org/pdf/1709.06298" rel="nofollow">https://arxiv.org/pdf/1709.06298</a></p>
<p>[4] Yu, Lantao. 2016. SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. <a href="https://arxiv.org/abs/1609.05473" rel="nofollow">https://arxiv.org/abs/1609.05473</a></p>
<p>[5] Lee, Sang-gil. 2017. A SeqGAN for Polyphonic Music Generation. <a href="https://arxiv.org/abs/1710.11418" rel="nofollow">https://arxiv.org/abs/1710.11418</a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/06/milestone-2-music-as-a-natural-language-task/">by Nicholas Ruhland at April 06, 2018 06:30 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-9203775015655831448.post-34377626932024049">
<h4><a href="https://teamoverfit.blogspot.com/2018/04/2-milestone-warm-up.html">#2 Milestone: Warm up</a></h4>
<div class="entry">
<div class="content">
<h2 style="height: 0px;"><span>Team Overfit</span></h2><h3><span><br /></span></h3><h3><span>Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span>Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><br /><div></div><br /><div style="text-align: start; text-indent: 0px;"><div><span><b>Blog Post: #2: 04/05/2018</b></span></div><div><span><b><br /></b></span></div><div style="margin: 0px;"></div><br /><ul><li><span>We first installed Pytorch 3.6 and we tried to run small programs on our local machines.</span></li><li><span>We then explored the usage of the RNN and seq2seq APIs, which we are going to use for most of our projects ideas.</span></li><ul><li><span>We looked through the tutorial of RNNs/LSTMs/GRUs from the previous 447 class.</span></li></ul><ul><li><span id="docs-internal-guid-97b5af9d-9943-133b-4f16-5d4414eefd5d"><span><a href="https://colab.research.google.com/drive/11iLtGFDpnIuHj5B0rQDGG5lqq6BQ8FRh">https://colab.research.google.com/drive/11iLtGFDpnIuHj5B0rQDGG5lqq6BQ8FRh</a></span></span></li></ul><li><span><span style="white-space: pre-wrap;">We tried to set up an Azure instance for GPU computation</span></span></li><ul><li><span><span style="white-space: pre-wrap;">We installed cuda support for the Pytorch package and it ran successfully with Tesla K80</span></span></li></ul><li><span><span style="white-space: pre-wrap;">We revisited the Recurrent Neural Networks, Attention and Reading Comprehension projects from the last quarter and experimented with other Pytorch features related to our project.</span></span></li></ul><br /></div></div>







<p class="date">
<a href="https://teamoverfit.blogspot.com/2018/04/2-milestone-warm-up.html">by Team Overfit (noreply@blogger.com) at April 06, 2018 04:48 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2">
<h4><a href="http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2">Technical Details (Blog Post #2)</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph"><span style="color: rgb(0, 0, 0);">For my project I am planning to do some deep learning at the end if I have time and if the results up to that point lead to that track. (I have pytorch installed from NLP so that's nice to have). <br /><br />With that said, I've been working with the Reddit API's and Reddit datadumps to get started on gathering the necessary data for pursuing the Language Accommodation project. I've been trying to figure out if the best approach is to work with the limited requests, the direct json files, or if some of the data dumps will suffice. I hope to have most of that and some basic data visualizations ready in the next couple of days to inform some of the choices I should make regarding the data (i.e. what time period to gather data from, what subreddits to pull from, etc.)</span><br /></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2">April 06, 2018 12:10 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 05, 2018</h2>

<div class="channelgroup">







<h3><a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a></h3>


<div class="entrygroup" id="http://cse481n-capstone.azurewebsites.net/?p=31">
<h4 lang="en-US"><a href="http://cse481n-capstone.azurewebsites.net/2018/04/05/warm-up/">Warm Up!</a></h4>
<div class="entry">
<div class="content" lang="en-US">
<h3><b>Data Collection:</b></h3>
<p><span style="font-weight: 400;">We have already begun the process of collecting data from Reddit for our project, using the Reddit API. We want to train our first neural-net model, which will be able to tell if some text is offensive or not, on a large amount of data.</span></p>
<p><span style="font-weight: 400;">Besides using the Reddit API to get posts from r/MeanJokes, we also use the Reddit submission dataset from </span><a href="https://pushshift.io/"><span style="font-weight: 400;">pushshift.io</span></a><span style="font-weight: 400;"> to get more examples from a wider context. We pre-processed data by filtering out non-text submission and deleted posts. </span></p>
<p><span style="font-weight: 400;">Now that we have the r/meanJokes posts, we want to determine the content phrases of these posts so that we can use them to find similar sentences all over Reddit. We know that the r/meanJokes posts are all offensive, and similar sentences elsewhere in Reddit could give us non-offensive examples. We have found an algorithm for pulling content phrases out of sentences, the </span><span style="font-weight: 400;">Rapid Automatic Keyword Extraction (RAKE) algorithm. We have extracted content phrases from r/meanJokes posts and also want to extract them from the rest of Reddit and the next step is to decide on a way to compare similarity and output the final set of posts we want to train the model on.</span></p>
<h3><b>Deep Learning Tools Set Up:</b></h3>
<p><span style="font-weight: 400;">For our modeling purposes, we are exploring PyTorch and AllenNLP. We learned PyTorch basics and went through Nelson’s tutorial in the undergrad NLP course last quarter. PyTorch would be our weapon of choice if we experiment with novel models.</span></p>
<p><span style="font-weight: 400;">We also installed AllenNLP because it looks like a nice tool to build and evaluate baseline models. We are currently going through AllenNLP’s official tutorial by running some of their existing models and demos.</span></p>
<p><span style="font-weight: 400;">So far, these tools are working as we expected.</span></p>
<p> </p></div>







<p class="date">
<a href="http://cse481n-capstone.azurewebsites.net/2018/04/05/warm-up/">by Team Watch Your Language! at April 05, 2018 05:41 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 04, 2018</h2>

<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=277">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/04/nlp-capstone-post-1-ideation/">NLP Capstone Post #1: Ideation</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>In this post, I’d like to briefly discuss three different ideas I have for my capstone project.</p>
<p>UPDATE (04/05/2018): I am fortunate to be joined by a fellow student, Nicholas Ruhland, for this capstone project.</p>
<h1>A Theoretical Analysis of RNNs (Research Mode):</h1>
<p> A recent <a href="https://arxiv.org/abs/1703.00810">paper of Professor Naftali Tishby</a> provided some useful observations on the behavior of feedforward neural networks, and proposed a promising approach to understanding their performance. Earlier empirical work done in the vision community showed that when a convolutional neural network is trained, layers closer to the input learn lower level features (such as edges and corners) and layers closer to the output learn higher level features (“this part of the image resembles a nose, and this other part resembles an eye”). One might expect similar behavior to occur with general feedforward neural networks: that earlier layers learn lower level features of the input and later levels learn higher level features of the input. The key insight here was to think of each layer of a neural network as a Markov chain, where each layer <img alt="L_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{i}" /> is a (vector-valued) random variable that is conditionally independent of <img alt="L_{j}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{j}" /> for all <img alt="j &lt; i - 1" class="latex" src="https://s0.wp.com/latex.php?latex=j+%3C+i+-+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="j &lt; i - 1" /> given <img alt="L_{i-1}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bi-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{i-1}" />. In this way, information flowing forward in the network can be quantified via notions of entropy from traditional information theory.</p>
<p>The paper contains some empirical work, observing that there are generally two phases to learning artificial neural networks via stochastic gradient descent: the fitting phase, and the compression phase. The fitting phase is the shorter phase, where the model is quickly tuning itself to minimize the empirical loss function. At the end of this phase, we don't necessarily have a model that will generalize to new data. The compression phase is where the model begins to learn the relevant features in the input, with the intuition that there are many irrelevant parts of the input (I don't need to know every atom in an object to identify it). </p>
<p>The goal of this project would be to perform a similar theoretical analysis and empirical work for RNN architectures (whose "natural" Markov chain isn't as simple, as there are cycles) on some traditional NLP task, such as Machine Translation, with the goal of studying the flow of information in an RNN architecture, rather than performing comparably to state-of-the-art Machine Translation models (although this can be a stretch goal).</p>
<p>The relevant steps in this project will likely look like the following:<br />
1. Reading up on the relevant work by Tishby et. al. (and any other theoretical papers on deep learning).<br />
2. Understand basic and traditional RNN architectures.<br />
3. Learning PyTorch.<br />
4. Implementing several of these architectures and testing (for example, to see if learning also comes in two distinct phases: fitting and compression)<br />
5. Using these empirical observations, and information theory to analyze these architectures.<br />
6. Time permitted, play around with new RNN architectures.</p>
<h1>Musical Style Learning from Musical Scores (Research/Start-Up Mode):</h1>
<p> This idea lies somewhat outside traditional NLP in that it tackles the language of music. While the alphabet of a musical score consist chiefly of the 12 musical notes, there is added challenge in that several notes may be played simultaneously, especially if there are several instruments involved or simply the two hands of a pianist. Furthermore, the exact timing of each note played matters, note merely the ordering of the notes.</p>
<p>The idea here is simply to, given the score of a musical piece, represented as a sequence of notes at each time, predict the era (Baroque, Classical, Romantic, etc.) or even, the composer of the piece (Bach, Beethoven, Brahms, etc.) There are several problems to be solved step by step for this project.</p>
<p>1. Data collection from a large library of musical scores (ex: <a href="http://imslp.org/">IMSLP</a>)<br />
2. Data formatting so as to be usable.<br />
3. Model selection.<br />
4. Model implementation (PyTorch).<br />
5. Model testing.</p>
<p>There are also several extensions that can be viewed as stretch goals. For these, the first two can be reused.</p>
<h3>Musical Score Generation:</h3>
<p> Now, we learn how to compose a piece that “sounds” similar to a given composer. This will involve learning from the pieces written by a given input composer, and outputting a new piece. One core challenge here is ensuring that the output is syntactically correct.</p>
<h1>Story Illustration (Start-Up Mode):</h1>
<p> Given a short story and a specific scene (or place in the text), produce an image that is representative of the scene. This project combines aspects of NLP and vision. This project may also explore generative adversarial methods. One well-known challenge here is convergence.</p>
<p>Here are the general steps for this project:<br />
1. Data collection (image captioning dataset can be helpful)<br />
2. Model selection.<br />
3. Model implementation (PyTorch).<br />
4. Model testing.</p>
<p>As an extension, one can also generate several frames to form a short “movie”. Another can be comic book pane generation.</p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/04/nlp-capstone-post-1-ideation/">by Kuikui Liu at April 04, 2018 06:53 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-9203775015655831448.post-3003438141513431489">
<h4><a href="https://teamoverfit.blogspot.com/2018/04/1-initial-project-ideas.html">#1 Initial Project Ideas</a></h4>
<div class="entry">
<div class="content">
<h2 style="height: 0px;"><span style="font-family: Arial, Helvetica, sans-serif;">Team Overfit</span></h2><h3><span style="color: #999999; font-family: Arial, Helvetica, sans-serif;"><br /></span></h3><h3><span style="color: #999999; font-family: Arial, Helvetica, sans-serif;">Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><div><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><b>Blog Post: #1: 04/03/2018</b></span></div><div><span style="color: #444444;"><br /></span></div><div><span style="font-family: Arial, Helvetica, sans-serif;">We are likely going to choose the start-up mode and actually build some cool projects! Here's some of our initial project ideas:</span></div><div><ul><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Twitter Hate Speech Detection</b></span></li><ul><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Dataset:</b> <a href="https://github.com/zeerakw/hatespeech">https://github.com/zeerakw/hatespeech</a></span></li><ul><li><span style="font-family: Arial, Helvetica, sans-serif;">This dataset contains the tweets ID that are labelled as either Raicist, Sexist or Neither Racist or Sexist.</span></li></ul><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Minimal viable plan:</b> We could use LSTM sequence to vector encoding to extract critical features of the tweets. Then, we use RNN and attention mechanism to output the label of the speech and the hate score of tweets in each category.</span></li><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Stretch goals:</b> </span></li><ul><li><span style="font-family: Arial, Helvetica, sans-serif;">We could experiment with different model configurations to improve the performance of the model.</span></li><li><span style="font-family: Arial, Helvetica, sans-serif;">We could build a twitter bot that can collect reports or hateful speech from user, which can be used to train on, so that the model can adapt the model to the latest slangs.</span></li></ul></ul></ul><span style="font-family: Arial, Helvetica, sans-serif;"><br /></span><ul><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Virtual Date chat bot</b></span></li><ul><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Description: </b></span><span id="docs-internal-guid-6febb8b7-8f58-f3ab-f461-d5c0a5077f07"><span style="font-family: Arial; vertical-align: baseline; white-space: pre-wrap;">Use conversations between couples to train a chat bot that user can flirt with when they feel lonely.</span></span></li><li><span><span style="font-family: Arial; vertical-align: baseline; white-space: pre-wrap;"><span id="docs-internal-guid-6febb8b7-8f59-3e54-c748-e595571ddaaa"><span style="font-weight: 700; vertical-align: baseline;">Minimal viable plan:</span><span style="vertical-align: baseline;"> Use neural machine translation and attention mechanism to train a generative chat bot. Some training data can be obtained from romantic movies’ dialogue or from Twitter. The challenge would be how to use NLP to classify whether a conversation is flirt or not.</span></span></span></span></li><li><span><span style="font-family: Arial; vertical-align: baseline; white-space: pre-wrap;"><span><span style="vertical-align: baseline;"><span id="docs-internal-guid-6febb8b7-8f5a-50ce-dc63-78e864ac8b2e"><span style="font-weight: 700; vertical-align: baseline;">Stretch goals:</span><span style="vertical-align: baseline;"> Train the chat bot so that it has consistent personality and have long term memory of chat history.</span></span></span></span></span></span></li></ul></ul><span style="font-family: Arial;"><span style="white-space: pre-wrap;"><br /></span></span><ul><li><span id="docs-internal-guid-6febb8b7-8f5a-cf38-76f0-a21179c20dde"><span style="font-family: Arial; vertical-align: baseline; white-space: pre-wrap;"><b>Detect Violations of Laws (Virtual Lawyer)</b></span></span></li><ul><li><span><span style="font-family: Arial; vertical-align: baseline; white-space: pre-wrap;"><b>Description:</b> The project is to try with different models and learn to detect the laws broke given the description of behaviors or given the sworn testimonies in the court.</span></span></li><li><span><span style="font-family: Arial; vertical-align: baseline; white-space: pre-wrap;"><span id="docs-internal-guid-6febb8b7-8f5b-2fce-e8fd-4e47759e160c"><span style="vertical-align: baseline;"><b>Minimal viable plan:</b> Experiment with LSTM and attention mechanism</span><span style="background-color: white; color: #757575; vertical-align: baseline;">.</span><span style="vertical-align: baseline;"> Given the testimony, detects whether the law is broken.</span></span></span></span></li><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Stretch goals:</b></span></li><ul><li><span style="font-family: Arial, Helvetica, sans-serif;">Given the testimony, detects and return the list of laws have been broken.</span></li><li><span style="font-family: Arial, Helvetica, sans-serif;">Generate texts to offend the testimony.</span></li><li><span style="font-family: Arial, Helvetica, sans-serif;">Generate texts to defend the testimony.</span></li></ul></ul></ul><span style="font-family: Arial, Helvetica, sans-serif;"><br /></span></div></div>







<p class="date">
<a href="https://teamoverfit.blogspot.com/2018/04/1-initial-project-ideas.html">by Team Overfit (noreply@blogger.com) at April 04, 2018 06:36 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=3">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/04/04/the-journey-begins/">Top 3 Project Ideas I’m Excited For</a></h4>
<div class="entry">
<div class="content" lang="en">
<ol>
<li><strong>Visualizing a text-based description:</strong> Train a model to learn a language to image mapping with simple descriptions. The minimum plan would be to feed the model short text descriptions like “white hat” and “black cat” with their corresponding visual outputs. Then, if the text “white cat” is input at test time, the model should output the cat with the same shade of white as the hat. Stretch goals include visualizing multiple objects in the same picture and/or visualizing them in different spatial orientations with respect to each other (e.g. on top of, inside, underneath, next to, etc.).</li>
<li><strong>Generating Multimodal Word Embeddings:</strong> The goal is to create word embeddings that describe a word more holistically from multiple modalities like audio and visual inputs. One possible approach is to concatenate pre-trained word embeddings (e.g. GloVe vector) with additional features based on what context the word is in the present sentence. Then, concatenate this with features generated from a deep fully connected layer of a ConvNet where the input is an image of the actual word (e.g. car). Stretch goals include using these augmented embeddings to enhance performance in applications like sentiment analysis or further augmenting these embeddings with audio features of the word being pronounced (which can help distinguish different meanings of the word).</li>
<li><strong>Dialogue and Information State Tracking:</strong> The goal is to create a model that can either receive contextual information as text or probe the environment with questions and receive an answer as text. This text can either be input into a linear or tree LSTM for entity extraction, coreference resolution, parsing, and/or other algorithms which can extract valuable contextual information. Then, this text can be used to update the current dialogue and/or information state using deep reinforcement learning. The policy then uses the updated state to choose the next action and hopefully keep repeating this until the text is satisfactorily understood. Stretch goals include using this model to answer test questions about the reading material using a machine comprehension model like the ReasoNet architecture.</li>
</ol>
<p> </p>
<p>Git Repo URL: <a href="https://gitlab.cs.washington.edu/ananthgo/cse481n-capstone" rel="nofollow">https://gitlab.cs.washington.edu/ananthgo/cse481n-capstone</a></p>
<p>This project is in research mode with a heavy focus on making improvements in keeping track of the meaning behind lines of input text.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/04/04/the-journey-begins/">by ananthgo at April 04, 2018 05:42 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 03, 2018</h2>

<div class="channelgroup">







<h3><a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-5600014144802012716.post-4534461914269998368">
<h4><a href="https://nlpcapstonesemparse.blogspot.com/2018/04/blog-post-2-warmup.html">Blog Post 2: Warmup</a></h4>
<div class="entry">
<div class="content">
I'm working on my project in my fork of Allennlp. I got the dataset and have written the DatasetReader code to preprocess and index the dataset. I plan to add tests for this, and I need to add further preprocessing code such as splitting variables on camel casing.</div>







<p class="date">
<a href="https://nlpcapstonesemparse.blogspot.com/2018/04/blog-post-2-warmup.html">by nlpcapstone (noreply@blogger.com) at April 03, 2018 09:53 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/inaugural-blog-post">
<h4><a href="http://sarahyu.weebly.com/cse-481n/inaugural-blog-post">Inaugural Blog Post</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">Welcome to the first blog post of <u><em>Jekyll-Hyde</em></u><em> </em><u>(</u>my very cool and somewhat related group-of-1 name).<br />As the name might reveal, I’m interested in using NLP to uncover the duality of language, the ability to simultaneously present both sides of a coin, whether in everyday conversation or more curated prose. Because language matters; so much so that we mend and mold our language to navigate the different social environments and spaces we inhabit, whether for power, survival, or acceptance, and often in the most primitive and subconscious ways. And because in today’s (supposedly) civilized world, the language we employ with another can be a sort of proxy for the relationship we share. I'd like to get at some of these ideas through the NLP capstone and think the next three topics are a potential start. <br /><br />1) Language Accommodation (or my unlikely paper title: <em>Nice Guy by Day, A**hole by Night: Language Accommodation for Self-Presentation in Subreddit Communities)</em><ul><li>​MVP: Scrape Reddit user data, identify language baselines for a given subreddit or capture linguistic differences of a single user across subreddits</li><li>Stretch Goals: Do both (subreddit baselines and user difference) and not only identify a user's language accommodation, but how they fall in line with the communities' baseline (a kind of hive mentality)</li></ul><br />2) Identifying Condescension (another working title: "<em>Well, Actually": Identifying Ambiguities in emails from 'helpful' colleagues</em>)<ul><li>​MVP: Identify an appropriate data source (ideally emails or more personal interactions), manually identify possible ambiguities, train model (maybe one that doesn't require a large dataset) to identify ambiguous spans of a sentence. </li><li>Stretch Goals: Begin identifying entity-entity-relationships with cues from ambiguous interactions or maybe something cooler about context and the different meanings if ambiguous...</li></ul><br />3) Identifying Disrespect (might as well for consistency: <em>Linguistic (dis)R.E.S.P.E.C.T. - Addressing 90% of Comment Sections)</em><ul><li>​MVP: Scrape Youtube comment data, classify comments as hateful/not hateful, train model on classified comments, test and tune</li><li>Stretch Goals: Train a portable model that can work with content from other mediums such as Twitter and Reddit</li></ul><br />I'll be pursuing one of these ideas in <strong>research mode</strong> and you can follow along at:<br />                                      https://github.com/sarahyu17/481n</div>  <div class="wsite-spacer" style="height: 50px;"></div>  <div> 				<form action="http://www.weebly.com/weebly/apps/formSubmit.php" enctype="multipart/form-data" id="form-147197638403517045" method="POST"> 					<div class="wsite-form-container" id="147197638403517045-form-parent" style="margin-top: 10px;"> 						<ul class="formlist" id="147197638403517045-form-list"> 							<h2 class="wsite-content-title">Any Favorite Paper Titles?</h2>  <label class="wsite-form-label wsite-form-fields-required-label"><span class="form-required">*</span> Indicates required field</label><div><div class="wsite-form-field" style="margin: 5px 0px 0px 0px;">   <label class="wsite-form-label" for="input-789590629342031487">Paper Title <span class="form-required">*</span></label>   <div class="wsite-form-radio-container">     <span class="form-radio-container"><input id="radio-0-_u789590629342031487" name="_u789590629342031487" type="radio" value="#1" /><label for="radio-0-_u789590629342031487">#1</label></span><span class="form-radio-container"><input id="radio-1-_u789590629342031487" name="_u789590629342031487" type="radio" value="#2" /><label for="radio-1-_u789590629342031487">#2</label></span><span class="form-radio-container"><input id="radio-2-_u789590629342031487" name="_u789590629342031487" type="radio" value="#3" /><label for="radio-2-_u789590629342031487">#3</label></span>   </div>   <div class="wsite-form-instructions" id="instructions-Paper Title" style="display: none;"></div> </div></div> 						</ul> 					</div> 					<div style="display: none;"> 						<input name="weebly_subject" type="text" /> 					</div> 					<div style="text-align: left; margin-top: 10px; margin-bottom: 10px;"> 						<input name="form_version" type="hidden" value="2" /> 						<input id="weebly-approved" name="weebly_approved" type="hidden" value="approved" /> 						<input name="ucfid" type="hidden" value="147197638403517045" /> 						<input name="recaptcha_token" type="hidden" /> 						<input name="opted_in" type="hidden" value="0" /> 						<input type="submit" /> 						<a class="wsite-button"> 							<span class="wsite-button-inner">vote</span> 						</a> 					</div> 				</form> 				<div class="recaptcha" id="g-recaptcha-147197638403517045"></div> 			  			</div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/inaugural-blog-post">April 03, 2018 07:00 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 02, 2018</h2>

<div class="channelgroup">







<h3><a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a></h3>


<div class="entrygroup" id="http://cse481n-capstone.azurewebsites.net/?p=15">
<h4 lang="en-US"><a href="http://cse481n-capstone.azurewebsites.net/2018/04/02/first-blog-post/">First Blog Post!</a></h4>
<div class="entry">
<div class="content" lang="en-US">
<h3><strong>Team Name: </strong></h3>
<p><span style="font-weight: 400;">Team Watch Your Language!</span></p>
<h3><strong>Three Project Ideas:</strong></h3>
<p><span style="text-decoration: underline;">Offensive Text Recognition</span></p>
<p><span style="font-weight: 400;">Our minimal viable plan is to create two models, one which determines if text is offensive or not, and another which determines if any particular group is targeted by the text, such as a racial, religious, or political grouping. </span></p>
<p><span style="font-weight: 400;">Our stretch goal is to use these models to make a third model which can use the first two outputs as assumptions to then provide human-readable explanations as to why that particular text was labeled the way it was. This way we can determine if text is offensive or not and provide reasons for that labeling. This can be used to assist in teaching conversational agents common sense about what to say.</span></p>
<p><span style="text-decoration: underline;"><span style="font-weight: 400;">Domain-Specific Conversational Agent</span></span></p>
<p><span style="font-weight: 400;">Our minimal viable plan is to create a conversational agent which can provide information and hold a conversation with a well-intentioned user about a particular domain. </span></p>
<p><span style="font-weight: 400;">The stretch goal here would be to just continually make it better at conversing, at least in the particular domain it is trained to be good at talking about.</span></p>
<p><span style="text-decoration: underline;"><span style="font-weight: 400;">Image Description</span></span></p>
<p><span style="font-weight: 400;">Our minimal viable plan is to create a model that can describe a simple image correctly, like generating a sentence describing the spatial relation between a box and a cylinder.</span></p>
<p><span style="font-weight: 400;">For the stretch goal, we would like to improve our model that can describe a unique pattern of an image among 3 (or multiple) other images. For example, if we have 3 images A, B, and C, we would like to come up with a model to generate a sentence that describes image A but not image B or C.</span></p>
<h3><strong>GitLab Repo:</strong></h3>
<p><span style="font-weight: 400;"> </span><a href="https://gitlab.cs.washington.edu/danielby/nlp-capstone"><span style="font-weight: 400;">https://gitlab.cs.washington.edu/danielby/nlp-capstone</span></a></p>
<h3><strong>Mode:</strong></h3>
<p><span style="font-weight: 400;">We will be tackling the Offensive Text Recognition task in</span> <em>research mode</em><span style="font-weight: 400;">!</span></p></div>







<p class="date">
<a href="http://cse481n-capstone.azurewebsites.net/2018/04/02/first-blog-post/">by Team Watch Your Language! at April 02, 2018 06:58 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://cse481n.blogspot.com/" title="PrimeapeNLP">Ron Fan, Aditya Saraf <br/> Team PrimeapeNLP</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-3753031463594823927.post-6307882466820480344">
<h4><a href="https://cse481n.blogspot.com/2018/04/blog-post-1.html">Blog Post #1</a></h4>
<div class="entry">
<div class="content">
<div style="font-family: Verdana; text-align: left;">    <div>Temporary GitHub URL: <a href="https://github.com/rococode/primeapeNLP" target="_blank">https://github.com/rococode/primeapeNLP</a></div>    <div>We plan to be in “research mode” for this capstone.</div>    <div style="font-size: 1.1rem;">Three possible ideas:</div>     <div style="font-size: 1.1rem;">1. New evaluation methods for text generation models</div>    <div>There don’t seem to be many good ways to evaluate the output of machine generated text (such as in the problem of creating hotel reviews). We want our models to generate text that’s indistinguishable from human text, but it’s hard to quantify how similar generated text is to human text, so it’s hard to see when progress is being made. However, it feels like the problem of actually generating human-like text should be harder than the problem of just checking if that text seems human. As a baseline approach, we would like to explore building a regression model for the “humanness” of a piece of unfamiliar text.</div>    <div><strong>        M<span style="font-size: 0.7rem; font-weight: normal;">(inimal)</span>        V<span style="font-size: 0.7rem; font-weight: normal;">(iable)</span>        P<span style="font-size: 0.7rem; font-weight: normal;">(lan)</span>    :</strong></div>    <ol>        <li>Build a reasonably large dataset using a combination of web scraping, existing datasets, and existing generative models</li>        <li>Build a regression model with decent performance on training set</li>        <li>Build a regression model with decent performance on development and test sets</li>    </ol>    <div><strong>Stretch goals:</strong></div>    <ol>        <li>Use our model to score various generative models and compare our model’s rankings for these generative models to commonly agreed-upon rankings by researchers</li>        <li>Integrate model scores into a “generate a lot of possibilities, then search for the best one” approach for text generation</li>    </ol>    <div>We would also explore non-neural methods for evaluating Natural Language Generation (NLG). One idea would be to compare the probability distribution of generated sentences to real sentences. A common problem is that generated text repeats the most probable sentences over and over. Thus, the distribution of sentences is front-loaded. We hypothesize that generated text with a sentence distribution that closely mirrors real text would be more difficult to distinguish from real text. However, we first need to investigate what real world sentence distributions look like. It may be likely that individual sentences are not likely to repeat - in that case, the distribution would be more-or-less uniform across all sentences. We may need to come up with a novel method to categorize similar sentences. We would have to decide whether to use semantic similarity or syntactic similarity.</div>    <div><strong>MVP:</strong></div>    <ol>        <li>Gather a data set - perhaps the data set Ari showed us for hotel reviews.</li>        <li>Examine the existing probability distribution at the sentence-level in the corpus. If the distribution is too uniform, design a method to place similar sentences in the same “bucket” and re-compute the probability distribution.</li>        <li>Design metrics to compute the similarity of two distributions; if a model is “good”, the distribution of the generated sentences will match the distribution of the training corpus.</li>        <li>Use our metric to score generated text models found in the literature.</li>    </ol>    <div><strong>Stretch goals:</strong></div>    <ul>        <li>Use insights from our metrics to improve on current approaches to NLG.</li>    </ul>    <div style="font-size: 1.1rem;">2. Single-document summarization</div>    <div>Single document summarization (SDS) models typically label each sentence in the document as in the summary or not in the summary. The problem then becomes a binary classification problem, and many people train a NN with supervised learning. However, there are other non-neural approaches to this problem that have been tried successfully. One paper shows how SDS can be thought of as a tree based Knapsack problem, which is then solved by a Integer Linear Programming (ILP) solver (see: <a href="https://www.semanticscholar.org/paper/Single-Document-Summarization-as-a-Tree-Knapsack-Hirao-Yoshida/ed0c8a7ab911cdb30b7e95edada3a55c01eb22c5">https://www.semanticscholar.org/paper/Single-Document-Summarization-as-a-Tree-Knapsack-Hirao-Yoshida/ed0c8a7ab911cdb30b7e95edada3a55c01eb22c5</a>). We would like to explore both neural models and non-neural approaches to SDS. </div>    <div><strong>MVP:</strong></div>    <ol>        <li>As a baseline approach, build neural models from current papers that solve SDS. Evaluate the performance using F1 as well as ROUGE metrics (see: <a href="https://en.wikipedia.org/wiki/ROUGE_(metric)">https://en.wikipedia.org/wiki/ROUGE_(metric)</a>).</li>        <li>Build neural models from current papers that focus on non-neural approaches, such as the combinatorial approach described above.</li>        <li>Evaluate neural and non-neural approaches: what do these approaches have in common? What key insights are they leveraging? Are there any generalizations of the problem that can be extracted? Hopefully, this study will allow us to either improve the SOTA neural models or fine tune some non-neural approach.</li>    </ol>    <div><strong>Stretch goal:</strong></div>    <ul>        <li>Use insights from our studies to formulate and solve SDS in a unique way. Evaluate our approach using F1 and ROUGE metrics.</li>    </ul>    <div style="font-size: 1.1rem;">3. Multi-span comprehension</div>    <div>Reading comprehension models generally operate by extracting an answer to a question by outputting a start and end index on the original passage. This is not a very human-like way of answering questions, and makes it impossible to generate good answers to some simple factual questions using common sentence structures. We would like to explore ways of answering questions from passages without being limited to a single span from the passage text.</div>    <div><strong>MVP:</strong></div>    <ol>        <li>Implement a baseline model that performs near state of the art levels on the SQuAD dataset.</li>        <li>Build a dataset that requires information from multiple spans to answer the questions well. We will likely create this dataset by hand.</li>        <li>Build a model that answers questions about a passage by generating multiple spans. We envision designing our model to output a sequence of indices such that every pair of indices corresponds to one part of the answer. This model should perform almost as well as the baseline model on SQuAD, since that’s just a specific case (one span) of a multi-span answer.</li>        <li>Build upon the previous model by using the output spans to generate a formal answer through a generative model, using the output spans and the question sentence as inputs.</li>    </ol>    <div><strong>Stretch goal:</strong></div>    <ul>        <li>Optimize model to actually be effective at answering multispan questions. We expect this to be quite difficult, so while building a functioning model is part of the MVP, actually having it perform comparatively well is a stretch goal.</li>    </ul></div></div>







<p class="date">
<a href="https://cse481n.blogspot.com/2018/04/blog-post-1.html">by Ron &amp;amp; Aditya (noreply@blogger.com) at April 02, 2018 04:32 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>March 30, 2018</h2>

<div class="channelgroup">







<h3><a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-5600014144802012716.post-6768023392170538237">
<h4><a href="https://nlpcapstonesemparse.blogspot.com/2018/03/blog-post-1.html">Blog Post 1: Project Ideas</a></h4>
<div class="entry">
<div class="content">
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span><span style="font-size: 14.6667px; white-space: pre;">I am interested in working in the code generation/semantic parsing space on the research track. My code </span></span><br /><span><span style="font-size: 14.6667px; white-space: pre;">will be in various branches of my fork of allennlp (https://github.com/rajasagashe/allennlp). I will keep you</span></span><br /><span><span style="font-size: 14.6667px; white-space: pre;"> updated on which branch/commits I worked on during each blog post. Also note that project idea 1 has </span></span><br /><span><span style="font-size: 14.6667px; white-space: pre;">the most detail since I have picked it as my project!</span></span></div><h2 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;"><span>Project Idea 1</span></h2><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Minimum Viable Plan:</span><span> Implement the model in the recent UW paper which introduces the task of </span><br /><span>generating the code for a java function from a natural language description. To further aid code </span><br /><span>generation, the class in which the generated function is to reside is provided, i.e. the class variables </span><br /><span>and methods. Thus the encoder encodes the class as well as the utterance and the decoder uses a </span><br /><span>two step attention mechanism and decodes through the java grammar production rules. </span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Stretch Goals:</span><span> Reproduce the state of the art results in the paper. I’m putting this in the stretch goals </span><br /><span>since successfully implementing a neural semantic parser with type constraints is pretty challenging. </span><br /><span>In addition, I hope to experiment with other improvements like encoding the entire class method body</span><br /><span> which wasn’t done.</span></div><h2 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;"><span>Project Idea 2</span></h2><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Minimum Viable Plan: </span><span>Implement this paper: </span><a href="https://arxiv.org/pdf/1704.01696.pdf" style="text-decoration: none;"><span>https://arxiv.org/pdf/1704.01696.pdf</span></a><span>. The model is </span><br /><span>similar to that of the previous idea, but the datasets are for python and ifttt instead.</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Stretch Goals: </span><span>Improve the paper’s result.</span></div><h2 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;"><span>Project Idea 3</span></h2><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Minimum Viable Plan: </span><span>Perform transfer learning across several code generation tasks by using the </span><br /><span>same encoder for them all. This technique would be similar to what was used in the Cove paper </span><br /><a href="https://arxiv.org/pdf/1708.00107.pdf" style="text-decoration: none;"><span>https://arxiv.org/pdf/1708.00107.pdf</span></a><span>. </span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Stretch Goals: </span><span>Improve the individual paper results with this technique.</span></div><div><span><br /></span></div></div>







<p class="date">
<a href="https://nlpcapstonesemparse.blogspot.com/2018/03/blog-post-1.html">by nlpcapstone (noreply@blogger.com) at March 30, 2018 10:41 PM</a>
</p>
</div>
</div>


</div>

</div>


<div class="sidebar">

<h2>Subscriptions</h2>
<ul>
<li>
<a href="" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a class="message" title="http status 503">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a>
</li>
<li>
<a href="https://deeplearningturingtest.wordpress.com/feed/" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a>
</li>
<li>
<a href="" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a class="message" title="http status 503">Belinda Li <br/> Team Sentimentity</a>
</li>
<li>
<a href="http://cse481n-capstone.azurewebsites.net/feed/" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a>
</li>
<li>
<a href="" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a class="message" title="http status 504">Halden Lin <br/> Team undef.</a>
</li>
<li>
<a href="https://mathstoc.wordpress.com/category/nlp-capstone/feed/" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a>
</li>
<li>
<a href="https://teamoverfit.blogspot.com/feeds/posts/default?alt=rss" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a>
</li>
<li>
<a href="https://nlpcapstonesemparse.blogspot.com/feeds/posts/default?alt=rss" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a>
</li>
<li>
<a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://cse481n.blogspot.com/" title="PrimeapeNLP">Ron Fan, Aditya Saraf <br/> Team PrimeapeNLP</a>
</li>
<li>
<a href="" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a class="message" title="http status 503">Ryan Pham <br/> Team NeuralEmpty</a>
</li>
<li>
<a href="http://sarahyu.weebly.com/6/feed" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a>
</li>
<li>
<a href="" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a class="message" title="http status 503">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a>
</li>
<li>
<a href="" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a class="message" title="http status 503">Zichun Liu, Ning Hong, Sujie Zhou <br/> Team The Bugless</a>
</li>
</ul>

<p>
<strong>Last updated:</strong><br>
November 03, 2018 05:31 AM<br>
<em>All times are UTC.</em><br>

<!--
<br>
Powered by:<br>
<a href="http://www.planetplanet.org/"><img src="images/planet.png" width="80" height="15" alt="Planet" border="0"></a>
</p>

<p>
<h2>Planetarium:</h2>
<ul>
<li><a href="http://www.planetapache.org/">Planet Apache</a></li>
<li><a href="http://planet.freedesktop.org/">Planet freedesktop.org</a></li>
<li><a href="http://planet.gnome.org/">Planet GNOME</a></li>
<li><a href="http://planet.debian.net/">Planet Debian</a></li>
<li><a href="http://planet.fedoraproject.org/">Planet Fedora</a></li>
<li><a href="http://planets.sun.com/">Planet Sun</a></li>
<li><a href="http://www.planetplanet.org/">more...</a></li>
</ul>
</p>
!-->
</div>
</body>

</html>
