<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>NLP Capstone Spring 2018</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="planet.css" type="text/css">
<link rel="alternate" href="https://nlpcapstone.github.io/atom.xml" title="" type="application/atom+xml">
</head>

<body>
<h1>NLP Capstone Spring 2018</h1>

<div class="daygroup">
<h2>May 29, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/last-update">
<h4><a href="http://sarahyu.weebly.com/cse-481n/last-update">Last Update</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">The final update for the project! Unfortunately, there is not as much to report on results as usual and not as many fun visualizations. For the 2nd Advanced Model, my plan was to begin on the stretch goals I had initially outlined and train a neural model for Reddit Post classification and Generation. The idea took cue from the Affect-LM paper. Basically it would be similar to this model</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 13.331751602564%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 68.227199377828%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-29-at-9-42-04-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 18.441049019608%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph">which has inputs of the context words, on which to build up the rest of the sentence from; the Affect category which is chosen beforehand to generate the desired output; and an Affect strength to determine the intensity of the affect category defined. <br /><br />My model would be similar to this, but instead look more like the following where we take out the strength factor and choose the mental category  to be fed into the Mental LM.</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 24.666352941176%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 56.902274509804%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/published/screen-shot-2018-05-29-at-10-56-19-pm.png?1527659857" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 18.43137254902%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph">My current model seems to have some issues generating posts containing any language beyond the exact topic of the mental category itself (i.e. using the word depressed for the F30 category), with the model using the cross entropy loss. <br /><br />At this point, the game plan is to fix the bugs and get a working model to test out the post generation. The results will be interesting to see in and of themselves, but I will also compare the language model of the generated posts against the metrics we hav seen throughout the quarter (vennclouds, idp). </div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/last-update">May 29, 2018 05:35 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2" title="Stories by Ryan Pham on Medium">Ryan Pham <br/> Team NeuralEmpty</a></h3>


<div class="entrygroup" id="https://medium.com/p/81ba6be6634">
<h4><a href="https://medium.com/@ryanp97/data-subsets-parent-feeding-and-future-work-81ba6be6634?source=rss-6378d85d3a9b------2">Data Subsets, Parent Feeding, and Future Work</a></h4>
<div class="entry">
<div class="content">
<p>In the last week, I’ve been training more models on different subsets of the data and began working on implementing parent feeding. Though I don’t think I will have parent feeding done in time for the final presentation, it’ll be a nice checkpoint to start at after this capstone finishes.</p><h4>Data Subsets</h4><p>The model trained on all of Michael Goodman’s data last week had an issue with SMATCH (which is why the SMATCH score was omitted in last week’s blogpost). It turns out that there were a couple translation pairs in which the unicode character U+3000 (ideographic/wide space) was used as a token causing it to appear as a surface predicate in some development examples. As a result, SMATCH had issues dealing with these and crashed. Considering how infrequently this surface predicate actually occurred, I decided to invalidate the graphs that contained them during post-processing. This model ‘achieved’ a SMATCH score of 0.54. For reference, when treating the predicates as a bag of words, the model had an F1 score of 0.52.</p><p>Considering the Kyoto Corpus suffers the issue of having many uncommon named entities, I decided to train a model on just the Japanese WordNet corpus. This dataset is significantly smaller with ~105,000 training examples compared to ~325,000 training examples for the combined corpus. I figured this dataset would have significantly fewer named entities and not mis-predict the named abstract predicate so often. This model did, in fact, achieve a better SMATCH score with a F1 of 0.57 and an F1 score of 0.54 when treating the predicates as a bag of words. Notably, the named abstract predicate was mis-predicted less often, though it was still in the top 10 mis-predicted predicates. This resulted in a higher abstract predicate precision, ~0.04 above. Surprisingly, however, the surface predicate precision drop ~0.04. I’m not entirely sure why quite yet, but it may be due to the ratio of number of surface predicates to number of abstract predicates in each dataset.</p><p>I’m currently training a model with all of the data from Michael Goodman and adding all the training examples I had parsed from the Tanaka Corpus. In theory this model should perform slightly better than the model trained with solely Michael Goodman’s dataset, though I won’t be able to tell until late tomorrow considering the time it takes to train a single epoch.</p><h4>Parent Feeding</h4><p>Working with OpenNMT’s codebase has been quite a pain. Though I’ve implemented a short method to calculate the parent indicies of a single graph, I have had lots of trouble figuring out where exactly they should be calculated and how they will be stored. For now I’ve placed it as a step in ShardedTextCorpusIterator. So the pipeline for generating the input to OpenNMT is still the same. The preprocess.py script takes the same inputs and outputs as usual. The only thing that is different now is the saved files will now also contain parent indicies for each example.</p><p>I haven’t been able to figure out how batching will work with this quite yet, so I’m meeting up with Jan later this week to discuss how we should do batching for this. Once Jan clears up how batching works on Wednesday, I should have a good enough understanding to attempt to modify/write a decoder that also uses parent feeding.</p><h4>ELMo Embeddings</h4><p>After discussing with Jan more, I likely would not see significantly improved results with ELMo embeddings unless I was able to scrape more data to train these embeddings with. Since the vocabulary the model is trying to predict are predicates and edges, I would have to generate graphs after scraping more data for both languages. Considering how little time I have left, I decided to leave this for future work.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=81ba6be6634" width="1" /></div>







<p class="date">
<a href="https://medium.com/@ryanp97/data-subsets-parent-feeding-and-future-work-81ba6be6634?source=rss-6378d85d3a9b------2">by Ryan Pham at May 29, 2018 12:20 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 23, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" title="Stories by Halden Lin on Medium">Halden Lin <br/> Team undef.</a></h3>


<div class="entrygroup" id="https://medium.com/p/b7c31ac45ecc">
<h4><a href="https://medium.com/@halden.lin/nlp-capstone-09-any-summary-b7c31ac45ecc?source=rss-2759d54493c0------2">NLP Capstone | 09: Any Summary</a></h4>
<div class="entry">
<div class="content">
<p><em>previous posts: </em><a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5"><em>01</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5"><em>02</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3"><em>03</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7"><em>04</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a"><em>05</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0"><em>06</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-07-formalizing-a2d837ecf66b"><em>07</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-08-human-summaries-33072535817f"><em>08</em></a></p><p>In my last post, I stated a main goal of mine was to visualization <strong>human summaries</strong>. After talking with Prof. Jeff Heer this past week, I’ve developed a more concrete goal for this segment of my project.</p><p>If we are able to develop a method for approximating human ‘attention’ between source and summary, we can use it in the following ways.</p><ol><li><strong>Evaluation tool.</strong> Current evaluation requires reading article, summary, and thinking critically to map between the two in order to determine whether or not the summary is ‘good’.</li><li><strong>Enable cross-model comparison and analysis.</strong> How do different models produce summaries for the same article? Automatic measures, such as Rouge and Meteor, are generally poor indicators of proper quality. Currently, one may read summaries and source text and attempt to qualify proper coverage of key ideas. By introducing a visualization that can be generated from <strong>any</strong> source-summary pair, we can enable more principled analysis.</li><li><strong>Enable model to human comparison and analysis.</strong> This I discussed in the previous post. What do human summaries have that our models are missing? Missing coverage? Missing entities? This visualization tool could answer these questions.</li></ol><p><strong>In general, this tool would allow researchers to gain insights about both human and machine summaries.</strong></p><p>With this in mind, I’ll go into the approaches I’ve been experimenting with in the past week.</p><h4>Hierarchical Similarity</h4><p>Last week, I attempted token-on-token similarity. The results can be seen the gif below. The weight between input and output token <em>x </em>and <em>y</em>, respectively, can be described as so:</p><p><em>a(x, y) = similarity(x, y)</em></p><p>Where similarity is calculated using a standard word embedding API (in this case, <a href="https://spacy.io/">spaCy</a>). The issue with this approach was that context is lost, and so a word will often attend to nearly the entire document with no regard to the ideas coming out of each portion (in summaries, we expect a sentence or phrase to summarize a specific part or few parts of the original document).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*-ganHe0RsisBYzaPKOuHhA.gif" />Token-on-token similarity pays no heed to context — problematic.</figure><p>In attempt to remedy this, I added a factor to each weight that represents the similarity of the tokens’ respective sentences. That is, the weight of a given <em>x, y</em> pair is determined by the similarity of the sentence of <em>x </em>and the sentence of <em>y</em>, multiplied by the similarity of the tokens themselves. To both normalize weights (over output token) and exaggerate salient pairs, I also add a soft-max transformation for each similarity score. The equation below describes this formula.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WE0z9rPYF_Nb_X4Y." /></figure><p>The <em>theta</em> terms here are important in properly exaggerating salient pairs, and so require some tuning.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*QJFNJ0ty3NimB1nBAnTdXg.gif" />Hierarchical Similarity shows some promise, but has a few issues.</figure><p>This approach shows some promise. Context is taken into account, at least at a sentence-by-sentence level. However, there are a few shortcomings that become apparent with more abstractive summaries. In particular:</p><ul><li><strong>Context is better but far from perfect.</strong> Sometimes ideas span multiple sentences, difficult to model. Additionally, repeating words in a sentence get equal ‘attention’ even though one may make more sense from a token-by-token generation standpoint.</li></ul><p>I’ll be exploring this approach further in the next week, but I have concerns about its ability to generalize well, per issues described above.</p><h4>Hidden Markov Model</h4><p>At a high level, we can imagine ‘attention’ as the words and phrases from the source text to that one would draw from to write a portion of a summary. This makes sense: we tend to focus on specific areas of a document at a time when writing summaries. Breaking this into token-by-token time-steps, summary token is <strong>conditioned</strong> on the ‘attention’ vector for that time-step.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/151/1*6wNP-KSn13tGSSwDjmDXxw.png" />Summary tokens are conditioned on attention vectors over the source text.</figure><p>Further, we can reason that attention vectors change from time-step to time-step, dependent on the previous attention vector.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/361/1*krjZPlauErnawaKymE5LAA.png" />Attention vectors are conditioned on each other.</figure><p>This of course is an simplification — the way our minds work is likely far more complex — but it allows us to model the ‘attention’ between source and summary as a Hidden Markov Model (HMM).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/611/1*3wfXRV8pMJZQ74Ux1OGjDA.png" />Source to summary modeled as a Hidden Markov Model.</figure><p>We can then use this model to predict attention vectors at each time-step (e.g. Viterbi, Forward-Backward). This is similar to how HMMs are used to predict part-of-speech tags (where POS tags are conditioned on each other and tokens are conditioned on those tags). Emissions (the edge weight going from distribution to summary token) can be defined by token similarity, but there are still a challenges here.</p><ol><li>How to define transition probabilities?</li><li>Treat attention states as distributions or single tokens (e.g. argmax in vector)?</li></ol><p>I’ll need to consider this approach further to see if I can work out these kinks.</p><h4>POS Tags</h4><p>I’ve also been slowly improving the visualization tool itself. I’ll briefly describe my progress on this front.</p><p>Using <a href="https://www.nltk.org/">NLTK</a>, I was able to part-of-speech tag machine-generated summaries. At the top right of the visualization, users are presented a panel of the POS Tags used by the <a href="https://catalog.ldc.upenn.edu/ldc99t42">Penn Tree Bank</a>, which NLTK sources from. Non-present tags are greyed-out.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*i87AaOJBtCzk4h_G5b5z9A.gif" />Users can highlight tokens to view the corresponding tag, or mouse over tags to highlight all corresponding tokens.</figure><p>This should allow more in-depth analysis of the attention vectors produced by the machine. Eventually I’d like to work towards highlighting named entities in the source / summary to allow users to identify present / missing ideas centered on important entities.</p><h4>Upcoming Work</h4><ol><li>Continue working on visualizing source-summary alignment.</li><li>Continue improving visualization.</li></ol><p>I have lots, lots, lots to do. Until next time!</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b7c31ac45ecc" width="1" /></div>







<p class="date">
<a href="https://medium.com/@halden.lin/nlp-capstone-09-any-summary-b7c31ac45ecc?source=rss-2759d54493c0------2">by Halden Lin at May 23, 2018 06:43 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=27">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/05/23/advanced-model-attempt-2-part-2/">Advanced Model Attempt 2 (Part 2)</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>This week I further tuned my hyperparameters and increased the exploration rate of my model so the slot-filling Q-values were fixed for all my questions. This made sure that my slot filling accuracy is perfect (when there’s no dropout). After tweaking the model, I tested and recorded the number of games it won using both the database it trained on as well as another random database of 100 people.</p>
<p>Training Database: Model won 90/100 games.</p>
<p>Validation Database: Model won 87/100 games.</p>
<p>As expected, the validation accuracy is not too far behind the training accuracy. While the NLU component of the model is just as accurate for both databases, the order of questions is more tailored towards the training database so the sequence of questions might be a bit inefficient for the other database meaning some additional games must have been lost on time. The reference paper I based this model on was trained using speech utterances and that model got about a 92% win rate (although they didn’t specify whether the database they evaluated on was different from the one they trained on). Therefore, my model is not quite as well trained as theirs but it’s still not too far behind in accuracy.</p>
<p>Now that I have my minimal action plan working, this week I’ll work on my final report and presentation as well as try to make this model work for a more generic dialogue scenario as a possible demo.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/05/23/advanced-model-attempt-2-part-2/">by ananthgo at May 23, 2018 06:34 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@be.li.nda?source=rss-fad49d942bf3------2" title="Stories by Belinda Zou Li on Medium">Belinda Li <br/> Team Sentimentity</a></h3>


<div class="entrygroup" id="https://medium.com/p/db16f27c255d">
<h4><a href="https://medium.com/@be.li.nda/nlp-capstone-blog-9-additions-to-advanced-model-db16f27c255d?source=rss-fad49d942bf3------2">NLP Capstone Blog #9: Additions to Advanced Model</a></h4>
<div class="entry">
<div class="content">
<p>This week, I took into account my error analysis from last week and made some modifications to my model to try and improve F1 scores.</p><h3>Modification to Architecture</h3><p>I modified the architecture of my model a little to better aggregate across mentions. Instead of taking the mean across all mentions for each entity as I had done previously, I paired up all mentions of the holder/target entity, concatenated them, and then aggregated each pair through an attentive sum.</p><p>More specifically, holder/target aggregation is computed as follows:</p><p>Let [<em>h_0, h_1, …, h_n</em>] be the encoded holder mentions and [<em>t_0, t_1, …, t_m</em>] be the encoded target mentions.</p><p>I took all pairwise combinations of the mentions and concatenated them, creating (<em>n</em> x <em>m</em>) pairs in total:</p><p>[<em>h_0, t_0</em>], [<em>h_1, t_0</em>], …, [<em>h_n, t_0</em>]</p><p>[<em>h_0, t_1</em>], [<em>h_1, t_1</em>], …, [<em>h_n, t_1</em>]</p><p>…</p><p>[<em>h_0, t_m</em>], [<em>h_1, t_m</em>], …, [<em>h_n, t_m</em>]</p><p>I then passed each concatenated pair through a linear layer to compute attention</p><p><em>α_ij = </em><strong>w</strong><em>_α </em>*<em> </em>[<em>h_i, t_j</em>]<em> + b_α</em></p><p><em>a_ij</em> = softmax(<em>α_ij</em>)</p><p>…and computed the final representation <em>x </em>of the holder/target pair through an attentive sum:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/199/1*kq036bF6zKqsTBN1OKL5uQ.png" /></figure><p>Afterwards, I append embeddings for any features <em>Φ(x)</em><strong> </strong>to <em>x</em>, and pass it through a final linear layer to compute the final scores.</p><p><em>v = </em>[<em>x</em>, <em>Φ_1(x)</em>,<em> Φ_2(x)</em>, <em>Φ_3(x)</em>, <em>Φ_4(x)</em>, <em>Φ_5(x)</em>]</p><p>scores = <strong>w</strong><em>_α </em>*<em> v + b_α</em></p><p>The aggregation mechanism is still suboptimal, however, given that I’m basically doing the equivalent of just summing up weighted copies of the encoded holder and target entities — more experimentation is necessary on this point. Moreover, I’d like to take into account the proximity of the holder and target mentions into the attention mechanism — giving more attention to entity pairs that are closer or co-occur in a sentence.</p><h3>Addition of Features</h3><p>The second thing I did this week was add features to my model in accordance with my error analysis from last week. 50-dimensional embeddings for each features are appended to the entity pair representation right before the last step of the model, where the entire vector is then passed through a linear layer to compute the final scores.</p><p>I took into account 5 features total:</p><ol><li><strong>Co-occurrence feature</strong>: a feature for the number of sentences in which the holder/target entity co-occurs</li><li><strong>Holder mention frequency feature</strong>: a feature for the number of times the holder entity is mentioned in the document</li><li><strong>Target mention frequency feature</strong>: a feature for the number of times the target entity is mentioned in the document</li><li><strong>Holder mention rank feature</strong>: a feature for the rank of the holder entity, relative to all other entities, and its number of mentions in the document</li><li><strong>Target mention rank feature</strong>: a feature for the rank of the target entity, relative to all other entities, and its number of mentions in the document</li></ol><p>As my ablation studies show, all of the features are helpful in improving the score:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/722/1*qllhecRTNgittY0GTyBQmQ.png" />Ablation studies, where I removed the one of the features each time and ran the model on the dev data.</figure><h3>Final Plan</h3><ol><li>Train separate classifiers for co-occurring and non-co-occurring entities</li><li>Experiment with aggregation functions</li><li>Encode restraints in the loss function (if time suffices)</li><li>Write the final paper and get a demo up and running</li></ol><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=db16f27c255d" width="1" /></div>







<p class="date">
<a href="https://medium.com/@be.li.nda/nlp-capstone-blog-9-additions-to-advanced-model-db16f27c255d?source=rss-fad49d942bf3------2">by Belinda Zou Li at May 23, 2018 05:53 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/ef93c2149aa2">
<h4><a href="https://medium.com/nlp-capstone-blog/advanced-model-update-from-definition-extraction-to-entity-discovery-ef93c2149aa2?source=rss----9ba3897b6688---4">Advanced Model Update: From Definition Extraction to Entity Discovery</a></h4>
<div class="entry">
<div class="content">
<p>Over the past few weeks, developing a dataset to test our model and flesh out this novel task has proven to be a difficult task in itself. Here, we discuss what worked, what didn’t work, and how the development of our dataset has influenced our perspective of the task, and ultimately, what we will now expect out of our advanced model.</p><p>To recap, we began making preliminary version of our dataset using ROUGE, cosine similarity, and skip-bigrams. In particular, given a definition-document pair, we aimed to extract sentences from the document that were most conducive to describing and re-creating the definition. From there, our model can compute latent representations of the term, sentences, and document as a whole, in order to learn how to extract these sentences we’ve chosen.</p><h3>The Heuristics that Failed</h3><p>Unfortunately, we can’t all be winners.</p><h4><strong>Cosine Similarity</strong></h4><p>We used <a href="https://spacy.io/usage/vectors-similarity">spaCy’s</a> implementation of cosine similarity using context vectors. Admittedly, cosine similarity does a great job of ruling out sentences that have nothing to do with the current definition when extracting. However, there was much too little variation in the scores that sentence-definition pairs received. Often, they would range from 0.80 to 0.94, and tended to cluster around 0.83–0.87 and 0.90 to 0.93. Not only are so many sentence-definition pairs scoring so highly, it becomes a very fine line between what we should keep and what we shouldn’t.</p><p>We attempted to be extremely strict and only keep pairs that scored 0.94; but this often led to many great pairs being ruled out. We speculate that medical language in general tends to cluster together with respect to the rest of the vocabulary in which spaCy’s word vectors were trained. We also speculate that being a bag-of-words method in defining similarity, much of the richness in context and order that makes differences and similarity obvious at a glance are washed away. Despite the method clearly being able to separate contrived sentence-definition pairs, in the landscape of our data, it fails to draw the line the way we’d like it to.</p><h4>Skip-bigrams</h4><p>This heuristic in particular was troublesome in that, many UMLS definitions were 1–2 sentences long, while others were several paragraphs. So the idea of using the number of overlapping skip-bigrams between a sentence-definition pair will severely punish shorter glosses. Because of this, longer definitions having little relevance to a sentence may likely still match to it.</p><h3>The Heursitics that Worked</h3><p>After several attempts at tuning the above heuristics, we decided to look for more. The following heuristics are how our final dataset will be constructed.</p><h4>Google’s Top 10,000 Words</h4><p>There’s currently a <a href="https://github.com/first20hours/google-10000-english">repository</a> containing the top 1000 and top 10000 words according to n-gram frequency analysis of Google’s Trillion Word Corpus. In particular we are using the <strong>no swears </strong>list.</p><p>Given the roughly 800,000 glosses that UMLS provides us, we shave this down to roughly 165,000 by removing all definitions that contain an synonym that is also contained within the Google no-swears top 10k list. This drastically reduces our search space when creating examples, and ultimately we are okay with it since common words are trivial to define.</p><h4>First 15%</h4><p>Given a definition-document pair, only attempt extraction if at least one of the aliases (synonyms) that the definition defines occurs within the first 15% of the sentences.</p><h4>Word Embeddings</h4><p>Following the first two heuristics, given definition-document pairs that make it through these filters we then extract <strong>all</strong> sentences containing <strong>any </strong>of the alias for the document.</p><p>We then calculate a similarity score between each sentence and the gold standard definition of the term. We do so by using pre-trained word vectors, namely Glove vectors, to better represent sentences. Each sentence is represented as the average of all its word vectors and similarity is defined as the Euclidean distance between the gold standard vector and the sentence vector. Given these distances, we sort them and choose the smallest 5 sentences if there are that many. We believe that through this, we are using better representations of sentences as opposed to the heuristics we tried previously. Although we did consider training our own set of word vectors (the large size of the Semantic Scholar corpus would allow us to do this), we felt that given the time constraints, Glove vectors were sufficient for now.</p><p>We then filter out the document to include only mentions of the entity that we are trying to extract (or its aliases). This approach is made possible by UMLS pairing all definitions with all of the aliases that it defines.</p><p>After choosing the sentences for each term-document pair, we then incorporate aliases when creating the final training examples. Recall that each training example includes a term, a gold standard definition, sentences within the document, and the target vector. In order to encourage the model to associate synonyms with each other, we can swap out the terms and its aliases in the target sentences, randomly inserting an alias or the term in places that another alias or term might be. This will not only give us a way to produce more training examples, it will also help the model understand the contexts of similar words, which might help it discover entities.</p><h3>Reframing the Problem to Entity Discovery</h3><p>Originally, our task was to generate definitions of entities consistent with our corpus. We then reframed the task as an extractive process.</p><p>The dataset described above however, will allow us to solve a task that could be described as a ‘superclass’ of definition extraction, which essentially aims to extract all sentences <strong>relevant</strong> to an entity as opposed to only sentences that help <strong>define </strong>it. We call it ‘Entity Discovery’, a term coined by AI2 when they originally proposed this type of task during the early stages of the capstone. Given our ranking scheme, we will still tend towards selecting sentences conducive to definitions, but we’re not quite confident enough that every sentence our heuristics will choose resemble a definition or add to one.</p><p>Rather, we now see the potential of our model (which, given this dataset, does not have to change at all!). This new dataset will allow us to train a model to learn latent representations of queries and map them to latent representations of sentences. Note that often times, chemical and medical terms have numerous aliases that take different but systematic forms. Given a reasonably trained model on such data, it should theoretically generalize to novel terms and learn what synonyms would look like and the contexts in which they would appear, ones that have not been added to KBs yet, aiding researchers and medical students to learn about ill-defined terms that have synonyms and reference that have not yet been fully documented.</p><p>We call this a ‘superclass’ of definition extraction presumably because, if we were successful at extracting all sentences pertinent to an entity, that definition extraction would simply take a subset of these sentences.</p><h3>In Conclusion</h3><p>It has taken some time and experimentation to find our footing in creating this dataset, but we’ve found it. The scripts are running, the data looks reasonable, and we are excited to finally see how our model will perform.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ef93c2149aa2" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/advanced-model-update-from-definition-extraction-to-entity-discovery-ef93c2149aa2">Advanced Model Update: From Definition Extraction to Entity Discovery</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/advanced-model-update-from-definition-extraction-to-entity-discovery-ef93c2149aa2?source=rss----9ba3897b6688---4">by Tam Dang at May 23, 2018 05:34 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2" title="Stories by Ryan Pham on Medium">Ryan Pham <br/> Team NeuralEmpty</a></h3>


<div class="entrygroup" id="https://medium.com/p/ff44ae10d41e">
<h4><a href="https://medium.com/@ryanp97/incorporating-more-data-ff44ae10d41e?source=rss-6378d85d3a9b------2">Incorporating more Data</a></h4>
<div class="entry">
<div class="content">
<p>Based on the results of last week’s hyper-parameter tuning, I wanted to incorporate more data to see if the issue was with the dataset or not. So this week I worked on adding the Kyoto corpus as well as Japanese WordNet (parallel corpus) definitions and examples into my dataset.</p><p>I began parsing the graphs from the Kyoto Corpus similar to how I did with the Tanaka Corpus earlier this week until Michael Goodman, the linguistics grad student I have also been working with gave me access to his preprocessed version. Michael had split up the data in many tiny chunks for each corpus such that each subdirectory was a subset of the actual data. Also the data was stored in a different format than I was expecting, but it was easily converted to the Penman format using the mrs-to-penman script mentioned in earlier blog posts.</p><p>In total, this allowed me to triple the size of my dataset from ~124,000 examples to ~325,000 examples (the result of combining Tanaka, Kyoto, and WordNet corpora from Michael). Although this is not as significant as I was hoping, it adds a lot of variety to the types of sentences and graphs that the model has been trained on up until this point.</p><p>The Tanaka Corpus is very casual in nature since it was essentially crowd-sourced by a teacher asking his students to translate sentences for him. As a result, the sentences are usually in casual speech form, some examples are from songs, some of the translations include mistakes, etc. The Kyoto Corpus, on the other hand, was created from manual translation of Wikipedia articles with the purpose of the data being used for travel brochures and similar tasks in mind. As a result, a translation pair from the Kyoto Corpus is likely to contain named entities that are not often found in the other translation pairs in the dataset, which causes errors mentioned later in this post. The Japanese WordNet examples are the definitions of words and examples of the words being used in sentences. This also seems like it may contribute to the issues mentioned later.</p><p>Something to note is that the preprocessed data I got from Michael contained a different version of the Tanaka Corpus than the one I am currently using. It seems to have been segmented differently and/or seems that it might be a non-current version since it was shipped with the Jacy grammar. So the Tanaka Corpus that came with Michael’s data accounts for ~1,000/325,000 examples which is significantly fewer parsed graphs than I was able to obtain.</p><p>In the coming week, I want to experiment with training a model with the Michael’s dataset combined with my current dataset as well as different subsets of the data (i.e. just the Kyoto Corpus or just the Japanese WordNet corpus).</p><h4>Retraining the Baseline</h4><p>After I preprocessed the data from Michael, I chose the best model from my previous attempts (though they were all pretty similar in performance), which happened to be my initial baseline, and trained it on Michael’s combined dataset. I was optimistically hoping for improved performance in terms of predicate precision and recall, but the model performed much worse.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*IhuwDSj7OrdW4n_Z9bivNA.png" /></figure><p>Using the same concept as last week, I calculated the most commonly mis-predicted/overly-predicted predicates:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XGtwfRJBrQcPyptwMBth5Q.png" /></figure><p>Just like with the previous models and dataset, the model still has issues mis-predicting abstract predicates and grammar concepts like particles. Something new to the list is the named abstract predicate. This was not even in the top 20 mis-predicted predicates in any of the previous models, but it shot up to the top of the charts with this new dataset. Like I mentioned earlier, the cause for this can most likely be attributed to the Kyoto Corpus and Japanese WordNet translation pairs containing many more named entities compared to the Tanaka Corpus.</p><p>Something else that I noticed is that this new dataset resulted in many more predictions having large length differences. In the baseline model, ~2,200/12,000 translation pairs differed by 5 or more predicates. With this new dataset, ~4,400/13,000 translation pairs differed by 5 or more predicates. One possible reason for this is the Kyoto Corpus — the Kyoto Corpus has translation pairs for both titles and summaries. The length difference between these is usually quite large since summaries are just longer by nature and carry much more semantic meaning.</p><h4>Future Work</h4><p>Like I mentioned earlier, I want to continue experimenting with different subsets of the data for training the model. I mentioned last week that I wanted to try implementing parent feeding into the decoder to try and force the model to really learn the semantic meanings of the non-terminals. I didn’t have time to do that for this blog post, but it is something that is a possibility for the next blogpost and/or the final presentation. Something else that I plan to try is ELMo embeddings as suggested by Yejin. OpenNMT currently does not have ELMo embedding support, as far as I am aware, so I may switch back over to AllenNLP to explore this addition to the model.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ff44ae10d41e" width="1" /></div>







<p class="date">
<a href="https://medium.com/@ryanp97/incorporating-more-data-ff44ae10d41e?source=rss-6378d85d3a9b------2">by Ryan Pham at May 23, 2018 04:48 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a></h3>


<div class="entrygroup" id="http://cse481n-capstone.azurewebsites.net/?p=114">
<h4 lang="en-US"><a href="http://cse481n-capstone.azurewebsites.net/2018/05/22/adversarial-data-collection-pilot-reddit-comment-storage-design-advanced-attempt-ii-continue/">Adversarial Data Collection Pilot &amp; Reddit Comment Storage Design (Advanced Attempt II Continue)</a></h4>
<div class="entry">
<div class="content" lang="en-US">
<h3>Adversarial Data Collection</h3>
<h4>Motivation for adversarial Data collection</h4>
<p><span style="font-weight: 400;">As we had shown previously, our model is very fixated with certain keywords when deciding that something is hate speech. For example, consider this sentence from a support subreddit:</span></p>
<p>“My wife was raped My wife tonight was raped, she doesn’t want to go to the police.”</p>
<p><span style="font-weight: 400;">Our best model considers this to be hate with 89% certainty. In particular it thinks the use of “raped” and “wife” (as opposed to “husband”) demonstrates hate speech. We think this has a lot to do with hateful speech online being commonly sexist against women while there isn’t as much obvious sexism against men. </span></p>
<p><span style="font-weight: 400;">However, we still want it to learn more sophisticated patterns. We decided that the best way to do this would be to find examples that we know are not hateful but that use the same keywords as the sentences the model deems hateful. This would add more uncertainty to the dataset, allowing us to train a more complex model. </span></p>
<p><span style="font-weight: 400;">Since we do not have any labeled data to choose from, we had to pick some sort of unlabeled data that we know in advance is going to almost always be not hateful. For this purpose, we decided to use news article headlines. Thus, these headlines are our adversarial data. They have the keywords that our model thinks are hateful, but they are almost guaranteed to not be hateful at all. </span></p>
<p><span style="font-weight: 400;">We considered two choices for collecting news article headlines. We could either use Reddit again, or use the Bing News search API through Azure. With Reddit, we would have to filter out everything except for a whitelist of news congregation subreddits and then search through the titles of the posts there for our keywords. In this type of subreddit, it’s enforced by moderators that the title of the post be the same as the headline of the article being linked. Because of our existing data collection experience, we would not need to learn anything new to go this route. With Bing, we would have to get access to their Search API resource in Azure and then pass it our keywords and manipulate the results. We ended up going with Bing because we decided that, as a search engine, Bing would be better at deciding the most relevant headlines for our keywords and thus provide us with better data for less effort.</span></p>
<h4>Pilot pipeline</h4>
<p><span style="font-weight: 400;">First, we want to extract ngrams from reddit posts that are labeled as hate by our current best model. These ngrams would later be ranked using document frequencies, and the most frequent ngrams would be used to search for news titles through Bing Search API.</span></p>
<p><span style="font-weight: 400;">We used the model with gru seq2vec encoder, 50d glove twitter embeddings, and ELMo, trained on combined twitter dataset to make predictions on 18k collected reddit posts (removing all posts with less than 4 tokens after preprocessing). We then extracted all posts labeled “hate” by this model. A total number of 3410 posts were labeled “hate”. Then we extracted all 1-3 ngrams after removing stop words and punctuations but keeping the special character apostrophe (‘) in place because there are words like can’t, won’t, don’t, etc. The output file contains multiple lines of lists. Each list consists of all 1-3 grams of a posts classified as “hate”, and each line is for each post.</span></p>
<p><span style="font-weight: 400;">After we generated a list of ngrams for each post, we need to process it to get ngrams that our model feels really hateful. Therefore, we decided to find the most frequent ngrams that appeared in all posts that are labeled as hate.</span></p>
<p><span style="font-weight: 400;">First, we need to do some data preprocessing.</span></p>
<ol>
<li style="font-weight: 400;"><span style="font-weight: 400;">We turned all words into lowercase and removed all non-alphabetic characters (including numbers) except punctionations within word like “can’t”. </span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Then we split each ngram into words, for each word, we used NLTK toolkit to tag it, and lemmatized back to the stem form. </span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Lastly, we decided to remove all unigrams after previous steps, because we found unigrams less descriptive comparing to bigram or trigrams.</span></li>
</ol>
<p><span style="font-weight: 400;">Then we compute the document frequency, and come up with a list of top 100 ngrams for us to explore on search engine.</span></p>
<p><span style="font-weight: 400;">For example, our ngrams looks like [“year old”, “what’s difference”, “year ago”, “old girl”, “year old girl”, “walk bar” ….]. We are aware that words like “year old”, “year old girl” looks really similar, and we would like to combine them, however, because this requires a lot more engineering and this is a pilot, we think it would be worth doing as a future work.</span></p>
<p><span style="font-weight: 400;">Once we have the ngrams we want to use as keywords for finding headlines, we send them through Bing’s news search feature. This required some experimenting to get right because if you just give Bing the keywords it will find articles that have them in the body, or sometimes just on the same website as the article. We had to make sure to use the “intitle:” restriction for each keyword of the ngram in question. As well, we found that Bing will silently filter out keywords that it thinks are offensive. For example, if you just search for articles with “intitle: jew” it removes that keyword and does an empty news search instead, which just returns the most recent articles published. This led to us doing a second round of filtering on top of the Bing results to make sure the sentences we give to the model actually have the keywords we want to test it against. As well, because our Azure tier doesn’t include Bing search, we had to use a 7-day free trial. For future work on this project we would have to change our Azure tier to keep using Bing, or we would have to switch to using Reddit for article headlines.</span></p>
<p><span style="font-weight: 400;">In the end we took the top 100 ngrams from the hate predicted posts and got up to 20 articles for each one. This resulted in almost 900 article headlines. However, when we ran these through our model we only got 73 of them labeled hate. This was an unexpected result, because we were expecting these headlines to confuse the model. On further investigation, we realized that because of Bing’s built in silent filtering, we were much less likely to get headlines that had these potentially controversial phrases in them. This happens even with safe-search turned off. If we had known about this when we were first brainstorming, we would have probably gone with using Reddit to get headlines instead.</span></p>
<p><span style="font-weight: 400;">Here are a few of the headlines that made it through Bing’s silent filter that the model did think were hate speech:</span></p>
<ul>
<li><em>Teen scarred for life after masked thug hurled acid over her in racially-motivated attack because she dated a black guy</em></li>
<li><em>I had to brutally murder the black gay guy because he hit on me</em></li>
<li><em>Starbucks drops the Jewish group Anti-Defamation League from its racial bias training after activists criticized their support of Israel and their failure to endorse Black …</em></li>
<li><em>Neo-Nazi who beat a black man with a 2-by-4 in Charlottesville pleads guilty</em></li>
<li><em>Kansas Cops Detain Black Man Because Of Vegetation On His Windshield</em></li>
<li><em>Can a Black Person Truly Love Black People if They Date Outside the Race?</em></li>
<li><em>Community responds after woman calls police on black people barbecuing</em></li>
<li><em>University survey asking if students want to know whether ‘black people hate America’ draws ire</em></li>
<li><em>Community responds after woman calls police on black people barbecuing</em></li>
<li><em>Oakland Residents Throw “BBQing While Black” Party After White Woman Called Police on Black Men for Grilling</em></li>
<li><em>3 are arrested in the stabbing of black man that officials call a hate crime</em></li>
<li><em>WATCH: Racist campers call black man a ‘n*gger’ 30 times after he asks them to leave his street</em></li>
<li><em>Understanding why you don’t call a black man a boy</em></li>
<li><em>Woman can’t get DirecTV to cancel service</em></li>
<li><em>Why you can’t get ‘Chelsea Dagger’ out of your head</em></li>
<li><em>Madonna: I Can’t Get Taylor Swift’s Songs Out of My Head</em></li>
<li><em>Spieth can’t seem to get anything right at Sawgrass</em></li>
<li><em>‘I hate her, can’t stand the b’: Daniel Heazlewood jailed for 11 years for killing his mother</em></li>
<li><em>You Can’t Tell Kids to ‘Just Say No’ to Legal Weed</em></li>
</ul>
<p><span style="font-weight: 400;">It’s clear that these were chosen based on keyword matching. We hope that including them in the training dataset in the future could make the model better, but we might have to iterate on this process using Reddit for headlines instead if we want more and better adversarial data.</span></p>
<h4>future work</h4>
<p><span style="font-weight: 400;">As mentioned above, currently we are using the most common non-trivial ngrams in all posts that are classified as hateful. However, we talked about another interesting way: found ngrams that have greatest ratio of df in predicted hate posts to df in predicted none posts. By doing so, we would be more sure about these ngrams’ contribution to the hatefulness of the post. This is a data processing step we can do in a future iteration to increase the adversarial impact of the data, since right now it is actually mostly clear to the model that these headlines should be labeled none.</span></p>
<p><span style="font-weight: 400;">Also, from our chosen ngrams, we found a lot of words that looks really similar. For example, we may have “year old” and “year old girl” appear at the same time, we can use other techniques to get rid one of them, like stemming and lemmatization . However, it might be computational challenging because of the large amount of ngrams we have. We think this is definitely a direction to look into.</span></p>
<p><span style="font-weight: 400;">Currently we are still working with unlabeled data, so we just used everything the model predicted as hate. Once our initial set of reddit data is labeled we could use that to make better decisions about what ngrams to use, such as by leaving out ngrams from true positives and true negatives from the start, and just dealing with the examples the model gets wrong. </span></p>
<p><span style="font-weight: 400;">Finally, we would like to try out other ways to collect headlines in the future, especially if there is no way around Bing’s silent filter. Using Reddit news subreddits like described up above is a possible alternative.</span></p>
<h3>Context extraction for Reddit Comment Design</h3>
<p><span style="font-weight: 400;">Here we present a high-level idea about how to store Reddit comment objects in DBMS for future context retrieval.  This is a substantial project on its own, therefore the implementation might not happen this quarter. We hope our exploration on this subject would pave the path for future work. </span></p>
<p><span style="font-weight: 400;">Last week, we presented an interesting paper, Anyone Can Become a Troll:</span><span style="font-weight: 400;"><br />
</span><span style="font-weight: 400;">Causes of Trolling Behavior in Online Discussions by Cheng et. al. To recap, the paper concludes that Negative Mood and Negative Discussion Context are the two causes of trolling behavior online. Moreover, the paper states that when training and evaluating a logistic regression classifier, “features relating to discussion context are most informative” (Cheng et. al.). </span></p>
<p><span style="font-weight: 400;">This gives us an idea that could potentially improve our model performance on reddit comments. Since a reddit post/discussion format is very much like CNN.com’s comment section where the post takes the role of an article, and all subsequent comments either spawn directly from the post or from previous comments, we believe the context of comments would also be a very useful feature for us to use when we use reddit comment data in the future (note: we are collecting reddit posts at this moment, but the data collection pipeline could easily be ported to reddit comments). </span></p>
<p><span style="font-weight: 400;">A reddit comment JSON object looks like this in the comment dumps we get from pushshift.io: </span></p>
<pre><span style="font-weight: 400;">{
    'author': 'LysergicOracle',</span>

<span style="font-weight: 400;">    'author_flair_css_class': None,</span>

<span style="font-weight: 400;">    'author_flair_text': None,</span>

<span style="font-weight: 400;">    'body': '&lt;3',</span>

<span style="font-weight: 400;">    'can_gild': True,</span>

<span style="font-weight: 400;">    'controversiality': 0,</span>

<span style="font-weight: 400;">    'created_utc': 1512086400,</span>

<span style="font-weight: 400;">    'distinguished': None,</span>

<span style="font-weight: 400;">    'edited': False,</span>

<span style="font-weight: 400;">    'gilded': 0,</span>

<span style="font-weight: 400;">    'id': 'dql1dzn',</span>

<span style="font-weight: 400;">    'is_submitter': False,</span>

<span style="font-weight: 400;">    'link_id': 't3_7go27t',</span>

<span style="font-weight: 400;">    'parent_id': 't1_dql0d4o',</span>

<span style="font-weight: 400;">    'permalink': '/r/freefolk/comments/7go27t/jonerys_first_fight_306_ac_colorized/dql1dzn/',</span>

<span style="font-weight: 400;">    'retrieved_on': 1514212661,</span>

<span style="font-weight: 400;">    'score': 2,</span>

<span style="font-weight: 400;">    'stickied': False,</span>

<span style="font-weight: 400;">    'subreddit': 'freefolk',</span>

<span style="font-weight: 400;">    'subreddit_id': 't5_37tpy',</span>

<span style="font-weight: 400;">    'subreddit_type': 'public',</span>
<span style="font-weight: 400;">}</span></pre>
<p><span style="font-weight: 400;">Here we get a comment from subreddit freefolks and the comment’s content is ‘&lt;3’. “link_id” is the id of the post under which this comment is posted, and “parent_id” is the id of the comment or post under which this comment is posted. In the case where the current comment is a top-level comment (directly posted under the post, with no parent comment), “link_id” and “parent_id” would be the same. We can also use “created_utc” to figure out which comment was created first if they are siblings, thus knowing which comment could have had an influence on the other.  </span></p>
<p><span style="font-weight: 400;">To recreate the discussion context of a comment, we are thinking of two kinds of queries: 1) queries that find all “ancestor comments” of the current comment; 2) queries that finds all “sibling comments” that were posted at an earlier timestamp than the current comment. Then we can incorporate the level of negativity of these comments as features when learning or making predictions of the current comment.</span></p>
<p><span style="font-weight: 400;">To store these comment objects in a DBMS for fast retrieval and context construction, we did some research on which DBMS to use. There are many DBMS systems out there that support JSON datatype, but the two we mainly looked at were MongoDB and PostgreSQL. Both of them are open source projects. MongoDB supports native JSON storage and has its own set of query commands. PostgreSQL was initially designed to be a SQL database, but it has supported JSON datatype for a few years. While both were valid choices, PostgreSQL supports SQL like query language on JSON data. It also supports recursive queries, which is perfect for the purpose of context tree construction. Therefore, we decided PostgreSQL is a better choice for Reddit comment storage.</span></p>
<p><span style="font-weight: 400;">For future work, we would spin up a PostgreSQL server and write a client-side package for comment context retrieval.</span></p>
<h3>Advanced Model Attempt Update</h3>
<p><span style="font-weight: 400;">Last week, we implemented an attention LSTM model and we reported that there was a bug in pytorch. However, after doing some checking, we realized that there was a dimension error on our side.</span></p>
<p><span style="font-weight: 400;">Now we have complete statistics for our attention LSTM/GRU model:</span></p>
<p><span style="font-weight: 400;">All models are trained and evaluated on twitter Waseem dataset.</span></p>
<table>
<tbody>
<tr>
<td>50d</td>
<td>LSTM w/o ELMo</td>
<td>LSTM w/ ELMo</td>
<td>GRU w/o ELMo</td>
<td>GRU w/ ELMo</td>
</tr>
<tr>
<td>F1</td>
<td>0.7907</td>
<td>0.7879</td>
<td>0.7823</td>
<td>0.7770</td>
</tr>
<tr>
<td>Precision</td>
<td>0.7961</td>
<td>0.8018</td>
<td>0.7781</td>
<td>0.7851</td>
</tr>
<tr>
<td>Recall</td>
<td>0.7862</td>
<td>0.7784</td>
<td>0.7875</td>
<td>0.7708</td>
</tr>
<tr>
<td>Accuracy</td>
<td>0.8194</td>
<td>0.8207</td>
<td>0.8056</td>
<td>0.8091</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>100d</td>
<td>LSTM w/o ELMo</td>
<td>LSTM w/ ELMo</td>
<td>GRU w/o ELMo</td>
<td>GRU w/ ELMo</td>
</tr>
<tr>
<td>F1</td>
<td>0.7907</td>
<td>0.7864</td>
<td>0.7913</td>
<td>0.7931</td>
</tr>
<tr>
<td>Precision</td>
<td>0.8076</td>
<td>0.7873</td>
<td>0.7931</td>
<td>0.7999</td>
</tr>
<tr>
<td>Recall</td>
<td>0.7798</td>
<td>0.7854</td>
<td>0.7895</td>
<td>0.7876</td>
</tr>
<tr>
<td>Accuracy</td>
<td>0.8242</td>
<td>0.8132</td>
<td>0.8180</td>
<td>0.8221</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>200d</td>
<td>LSTM w/o ELMo</td>
<td>LSTM w/ ELMo</td>
<td>GRU w/o ELMo</td>
<td>GRU w/ ELMo</td>
</tr>
<tr>
<td>F1</td>
<td>0.7944</td>
<td>0.7745</td>
<td>0.7948</td>
<td>0.7725</td>
</tr>
<tr>
<td>Precision</td>
<td>0.7902</td>
<td>0.7870</td>
<td>0.7874</td>
<td>0.8001</td>
</tr>
<tr>
<td>Recall</td>
<td>0.7995</td>
<td>0.7659</td>
<td>0.8074</td>
<td>0.7580</td>
</tr>
<tr>
<td>Accuracy</td>
<td>0.8166</td>
<td>0.8091</td>
<td>0.8132</td>
<td>0.8132</td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;">From our results, we see a very consistent performance (~0.79 F1 score) when attention is used. And it seems ELMo, in this case, does not introduce any help. </span></p>
<p><span style="font-weight: 400;">The reason might be attention with ELMo need much more data in order to have a performance improvement. We will train on the combined dataset in the coming days and update our statistics.</span></p>
<h3>Work Cited</h3>
<p><a href="https://files.clr3.com/papers/2017_anyone.pdf"><span style="font-weight: 400;">Cheng, Justin et al. “Anyone Can Become a Troll: Causes of Trolling Behavior in Online Discussions.” CSCW : proceedings of the Conference on Computer-Supported Cooperative Work. Conference on Computer-Supported Cooperative Work 2017 (2017): 1217-1230.</span></a></p></div>







<p class="date">
<a href="http://cse481n-capstone.azurewebsites.net/2018/05/22/adversarial-data-collection-pilot-reddit-comment-storage-design-advanced-attempt-ii-continue/">by Team Watch Your Language! at May 23, 2018 02:11 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 22, 2018</h2>

<div class="channelgroup">







<h3><a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-5600014144802012716.post-7945337673285975235">
<h4><a href="https://nlpcapstonesemparse.blogspot.com/2018/05/blog-9.html">Blog 9</a></h4>
<div class="entry">
<div class="content">
<div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Decided to edit code prototypes. The motivation comes from a high baseline score which uses this </span><br /><span>approach. Namely, when generating a method, if the closest method by maximum comment Jaccard </span><br /><span>distance is picked, a bleu score of .34 is achieved, almost 15 points higher than the actual model! </span><br /><span>Though note em is 0 for this baseline since it's picking a different method.</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span><br /></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>I’ve run some experiments with the code prototypes. None seem to be working that well but I have </span><br /><span>some ideas as to why.</span></div><b id="docs-internal-guid-4c49591c-89e4-bc06-8efe-ea78d3b9b1e1" style="font-weight: normal;"><br /></b><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Since the dataset is slightly altered the baseline is at .336 bleu and .128 em.</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>The prototype model which concats encoded prototype and utterance is at .331</span><br /><span> bleu and .119 em, meaning it’s not using the prototype information and is the same </span><br /><span>as the baseline. This is evident after examining the model outputs.</span></div><div><span><br /></span></div><div><span><span style="font-size: 14.6667px; white-space: pre-wrap;">The model needs to be designed better in order to figure out what parts of the prototype to copy. This is what I'm currently figuring out.</span></span></div></div>







<p class="date">
<a href="https://nlpcapstonesemparse.blogspot.com/2018/05/blog-9.html">by nlpcapstone (noreply@blogger.com) at May 22, 2018 11:38 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 17, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a></h3>


<div class="entrygroup" id="https://medium.com/p/29b207b75065">
<h4><a href="https://medium.com/@viterbi.or.not/advanced-model-2-part-1-29b207b75065?source=rss-c522ef075bb3------2">Advanced Model #2, Part 1</a></h4>
<div class="entry">
<div class="content">
<p>This week, we started working on our second advanced model attempt! The chatlog data is fully integrated this time, though we are still working on making our model effective across all types of text conversation. In addition to finishing construction of our proposed pipeline, we’ve also upgraded almost all of the previously existing components (pre-processor, feature vectorizer, model) to make the first big steps towards completing our final advanced model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*FIza3SY9EreLeCf1." />Pipeline for integrating different datasets with a common feature set into our model, now completed!</figure><h4><strong>Adding the Chatlog Dataset — For Real This Time!</strong></h4><p>Achieving one of our major goals for the advanced model, we have finished integrating chatlog data into our model! This involved a considerable amount more effort than we anticipated because the chat data ended up being much more noisy and varied than the email data, leading to a significant portion of our time being spent re-formatting and parsing the original data files to get it into the same input structure as the email data.</p><p>Even in our initial testing, the impact from adding chatlog data has been noticeable. There are two major benefits that we have identified for the importance of incorporating this second data source. The first, and perhaps most obvious, is that it expands the domain in which our model is able to operate — because conversation data in the modern age increasingly happens over both email and chat, it increases the helpfulness of our model to be able to produce summaries for chatlogs as well.</p><p>However, the other benefit is that the chatlog dataset has considerably more data points available than the email dataset, perhaps due to the nature of chat as a noisy, casual medium. A single email thread consists of about 80 sentences on average, and we only have 32 email threads to train with when we are using an 80% / 20% split in k-fold cross validation. That means the email data has only about 2500 sentences of training data.</p><p>By contrast, the chatlog dataset is considerably larger. Again using 80% of the data as training data, we have 1118 threads of about 1200 sentences each, meaning the chatlog data has about 1,341,600 sentences or approximately 500 times the data points that were previously available. Through the use of this added data, our goal is ultimately to improve the performance of our model on a smaller dataset, such as email, solely by virtue of the massively increased amount of training data that is available by combining different data types.</p><p>Of course, to make this arrangement work, it is necessary to use a universal feature set that can apply to any type of data. That way, the model is trained on the same set of features regardless of how the data was originally structured, and it has universal applicability for any future datasets that might be added. One of the more unique aspects of our project is this combination of different text conversation mediums and the utility of our features across all conversation types.</p><h4><strong>Text Segmentation</strong></h4><p>An interesting challenge we encountered while incorporating both email and chat data was finding analogous parts and features between the two formats. Email threads were easily separated into emails, but chatlogs being one long, unstructured flow of conversation made it more difficult to process. To help with this problem, we added preprocessing of chatlogs into Longest Contiguous Messages (LCM). To capture LCM, we concatenated subsequent messages of the same user and identified a boundary when the user changes or when a period is encountered.</p><p>The effect of this incorporation was to create email-like “chunks” of the conversation, which we were then able to use for certain document-dependent features like TF-ISF that rely on having messages segmented into individual documents.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gW-GdW9aTe7v05OL." />A before (left) and after (right) of applying LCM on a chat excerpt, taken from Sood et. al.</figure><h4><strong>Input Structure</strong></h4><p>To improve the context our features are able to operate with, we changed the input format of our data to have one more level of granularity. In our previous iterations of the model, our input structure was a list of threads, each thread being a list of sentences. Our new input format has one more nested layer — each thread is now a list of ‘chunks’ and each chunk is a list of sentences. For emails, a chunk is naturally a single email in the thread. For chatlogs, a chunk is determined by LCM text segmentation, with each chunk consisting of the contiguous messages by a single user.</p><p>This changes the sentence position feature to mean the sentence position within a chunk rather than within the whole thread, which we believe is more relevant to the sentence’s importance. This also allows us to add a “position from end of the chunk” feature which is helpful for eliminating ending lines (like sign-offs or parting words) and other features that might depend on chunk granularity context.</p><h4><strong>Results of Cross-Training</strong></h4><p>In order to examine the efficacy of our multi-dataset training approach, we ran some experiments to look at the ROUGE scores that are produced. Our first attempt was to use the Naive Bayes model in order to try this cross-training approach, as it had generally been the most successful in prior experiments. Unfortunately, we discovered that the Naive Bayes model performed extremely poorly in many combinations — after training on the chat data and evaluating on email data, for example, it would cause the vast majority of the email summaries to be blank after determining that every single sentence sentence should be classified as not being included in the summary. However, we were able to use the Decision Tree model instead to produce promising results. We are still attempting to determine the cause of this discrepancy, although one possibility might be that the Naive Bayes model ascribed too much significance to a certain feature that is significantly different between chat and email, such as sentence length or some quirk of proper punctuation.</p><p>The following table shows our results while training using the chat and evaluating using the email data, with self-trained email data provided as a comparison. In order to determine the effect of data size on the ROUGE score, we experimented with varying sizes of chat examples:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pXyyq6x_CVDaNGxx3DpsSw.png" />Results of various cross-training configurations using email and chat data</figure><p>As can be seen from the above table, from a ROUGE score standpoint, the effect of training on more data is considerably more important than the effect of training on the same type of data as is being used for evaluation. Having 12,000 sentences upon which to train causes the model’s ROUGE scores to shoot up significantly higher than having 2,500 sentences, regardless of the type of data.</p><p>Another thing we have been experimenting with is training on both types of data before evaluating on a single type of data. In this case, it would mean training on both email and chat before evaluating on either email or chat data. The theoretical advantage of this approach would be that it maximizes the amount of available data, and if there are significant differences between the types of data, it would be sure to incorporate the type of data that the model will be evaluated on.</p><p>However, in practical use, it seems as though training on multiple data sources will not contribute significantly to the success of our model because of the fact that the chatlog dataset so completely eclipses the amount of data available in the email dataset. Therefore, from a numerical perspective, training on both types of data would add only a 0.2% increase in data from training on just the chatlog data. In addition, it stands to reason that having such a numerical disadvantage means any significant benefit from training on the same type of data that the model is being evaluated on would be negligible. We are continuing to develop this capability, but for the time being it seems like a low priority.</p><h4><strong>Topic Segmentation</strong></h4><p>In addition to segmenting by the author of contiguous chat messages, we also wanted to add a new feature for our model that incorporates the changing of topics throughout a message. The idea behind this feature is to use the changing structure of text to determine the boundaries between different topics through an existing algorithm for topic detection called TextTiling (More detail can be found in the <a href="http://www.aclweb.org/anthology/J97-1003">paper describing TextTiling</a>). We used an implementation of TextTiling found within NLTK.</p><p>While the paper we used for our baseline model mentioned using TextTiling, they only used the algorithm as a preliminary attempt to segment the chatlog into different chunks. We found that approach to be ineffective, likely because the chunks in an email thread (emails) are separated by author, while TextTiling topics cover multiple authors but cover a single topic, and therefore the two are largely incomparable. To support our concept of a universal feature set, we instead used TextTiling as a separate feature, offering context for sentences.</p><p>Currently, our models are capable of using TextTiling in order to determine the position of a sentence within a topic and incorporate the relative position within the topic as a feature. Unfortunately, we have only been able to successfully run TextTiling for tiny datasets so far due to errors that manifest for noisier data, and as a result we are not ready to report results using it.</p><h4><strong>Compression Ratio</strong></h4><p>One of the major problems we noted in the summaries our model was generating was that while they scored well in ROUGE metrics, the model seemed to place importance on too many sentences and the summaries themselves often ended up too long to be practical. Indeed, summaries for some threads would be composed of over 50% of the “actual” contents of the thread (the parts that were kept after preprocessing), making our summaries close to general reproductions of the original text.</p><p>As a solution, we experimented with using a regression model instead of a classifier, giving each sentence a score rather than a binary 1 or 0 (include in the summary or not). With a regression model, we gain the ability to control the compression ratio by changing the threshold of score we accept to include in the summary.</p><h4><strong>Results of Compression Ratio</strong></h4><p>With these updates to our model, we experimented with tuning regression the running using the new possible training and validation configurations to get the following results.</p><p>We first conducted an experiment with the threshold for our regression model to determine the optimal threshold value:</p><pre>python main.py bc3/full --type email --model regression_br --threshold &lt;t value&gt;</pre><p>For those following along at home, the above command is what we used to generate the values for the following table.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cNtKD45MlqM822FrIv3R1w.png" />Results of experimenting with the threshold value using the regression model</figure><p>Based on the results, we chose a threshold of 0.3 to continue with. The following table compares the regression model with the Naive Bayes classifier using the full email dataset:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dixp1JhnxIxOVdM3DZ97_A.png" />Comparison between our best regression model and classifier model</figure><p>In general, the “best” regression model did not have an enormous impact on the ROUGE scores as compared to the best results from the classification models (Naive Bayes), increasing some ROUGE metrics slightly and decreasing others. However, because the regression model allows configuration of the number of sentences that are output by changing the threshold hyperparameter, it adds an additional capability to the model whereby a user can select an optimal “size” of summary.</p><p>We developed this feature in response to the problem of “reproducing” the original text in our summaries, and by human standards it seems to be a big improvement. Consider the following email thread summary, produced using our Naive Bayes classifier model:</p><pre>Chris Lilley, Brian Stell and others have been discussing the rash of irate, &amp;quot;get me off this list&amp;quot; mesages the listserv has received, lately.</pre><pre>Well, folks: YOU CAN'T UNSUBSCRIBE FROM THIS LIST!</pre><pre>I've tried for 2 months to get off this list, I've followed the rules, I've tried variations of the theme, looking for some hidden code--all to no avail.</pre><pre>So, the last resort of those who have tried everything else is to post to the list they want to be rid of.</pre><pre>PLEASE GET ME OFF THIS LIST@!%$#$/-\%</pre><pre>Well, that explains a lot!</pre><pre>I've been trying for awhile too, and I can't seem to get off.</pre><pre>Please remain calm.</pre><pre>Our automated list manager works very well.</pre><pre>Sometimes there are problems due to:</pre><pre>- you being subscribed under another name/address --</pre><pre>this was the case for the first among the two recent messages.</pre><pre>Well, after receiving a message that informed me of this, I responded with an unsubscribe e-mail with my unaliased e-mail address, and today received an automated response informing me that your software could not find my name on your list.</pre><pre>I am very calm.</pre><pre>Thanks for the helpful info, but I just received a message saying that I have been removed from the list.</pre><pre>Because I posted my difficulties to the list.</pre><pre>Sorry I had to burden you all with my problems, but as you can see, it worked.</pre><pre>Sam Berlow</pre><pre>UNSUBSCRIBE.SIGROLLY</pre><p>By contrast, here is the summary for the same email thread, but using a Bayesian Ridge regression model and incorporating a threshold of 0.55 in order to produce a much smaller summary:</p><pre>Chris Lilley, Brian Stell and others have been discussing the rash of irate, “get me off this list” mesages the listserv has received, lately.</pre><pre>I've tried for 2 months to get off this list, I've followed the rules, I've tried variations of the theme, looking for some hidden code--all to no avail.</pre><pre>Well, after receiving a message that informed me of this, I responded with an unsubscribe e-mail with my unaliased e-mail address, and today received an automated response informing me that your software could not find my name on your list.</pre><pre>Thanks for the helpful info, but I just received a message saying that I have been removed from the list.</pre><p>The difference is quite pronounced, with almost half the sentences in the former being removed when a regression model and higher threshold is used. However, from a human perspective, the second summary is nearly just as informative, and in fact there are several extraneous sentences in the first summary that are dropped. Because the ROUGE scores are roughly the same in both cases, but the second summary is somewhat better from a human viewpoint, we would consider the use of a compression ratio to be a success in improving our model.</p><h4><strong>Next Steps</strong></h4><p>So far we’ve trained on single data sources (email or chat) before evaluation since we weren’t quite able to get training on both sources at once working yet in this part 1 attempt. We aim to fix this next week and be able to train on multiple data sources and fine tune the cross-training.</p><p>We think there is a lot of potential in features based on topic segmentation, semantic meaning, and the conversation-specific aspect of the authorship of a sentence. So, we also aim to finish our implementation of TextTiling-based feature, a feature that incorporates Rhetorical Structure Theory (RST) output, and feature that includes information from tracking the contributions of different participants throughout the conversation.</p><p>After completing these improvements, we would like to take a crack at our stretch goal of generating abstractive summaries and make a demo of our summarizer on a chat interface!</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=29b207b75065" width="1" /></div>







<p class="date">
<a href="https://medium.com/@viterbi.or.not/advanced-model-2-part-1-29b207b75065?source=rss-c522ef075bb3------2">by Viterbi Or Not To Be at May 17, 2018 06:56 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=339">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/05/17/nlp-capstone-post-8-training-challenges/">NLP Capstone Post #8: Training challenges</a></h4>
<div class="entry">
<div class="content" lang="en">
<h1><span style="font-weight: 400;">Challenges with training TSL model</span></h1>
<p><span style="font-weight: 400;">In our previous post we proposed a three-model system that would allow us to take advantage of a larger corpus of higher quality lyrics data for the production of lyrics. We also finally tackle the alignment task with a simple approach of determining whether a lyric token should be produced at each timestep. This seems sensible since we have begun dividing the MIDIs into pianorolls with a constant frequency.</span></p>
<p><span style="font-weight: 400;">Unfortunately, even after several bugs bashed, we’ve been still unable to produce even sensible timings. We find the RNN collapses to repeatedly generating 0 (for no lyric event), even though a randomly initialized RNN will repeatedly generate 1 (and perform better with respect to classification accuracy).</span></p>
<p> </p>
<h1><span style="font-weight: 400;">Future direction</span></h1>
<p><span style="font-weight: 400;">If we are able to produce something reasonable from our existing architecture, we would like to move on to a second model that structures the problem as machine translation. We have decided to focus on the paper Attention is All You Need by Vaswani et al. [1] for our presentation in two weeks. The structure of our problem is straightforward to apply to translation as converting pianoroll format into english sentences. Incorporating attention has shown promising results in the literature, though that is no guarantee that our noisy dataset would be able to take advantage of this proposed architecture.</span></p>
<p> </p>
<h1><span style="font-weight: 400;">References</span></h1>
<p><span style="font-weight: 400;">[1] </span><a href="https://arxiv.org/abs/1706.03762"><span style="font-weight: 400;">https://arxiv.org/abs/1706.03762</span></a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/05/17/nlp-capstone-post-8-training-challenges/">by Nicholas Ruhland at May 17, 2018 05:55 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 16, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-2">
<h4><a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-2">Advanced Model Attempt #2</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">Wow, can't believe we're already in week 8! <br /><br />This past week, I've worked on shifting my project from not having a neural component, to well, having one. With inspiration from the work of Ghosh et al. in <em>Affect-LM: A Neural Language Model for Customizable Affective Text Generation, </em>I've extended my project (and am encroaching on stretch goal territory) to include a neural language model for reddit post generation***. The idea would be to train a model on reddit posts out of five categories - F30 (mood [affective] disorders), F40<font color="#515151"> (Anxiety, dissociative, stress-related, somatoform and other nonpsychotic mental disorders), X71 (Intentional Self Harm), F10 (addiction categories), and neurotypical advice - and use that model for post generation, based on the specified target type, i.e. generate a F30 reddit post. <br /><br /></font><br /></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-2">May 16, 2018 11:19 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-9203775015655831448.post-3973266691169482239">
<h4><a href="https://teamoverfit.blogspot.com/2018/05/8-milestone-advanced-model-attempt-2.html">#8 Milestone: Advanced model attempt #2</a></h4>
<div class="entry">
<div class="content">
<h2 style="height: 0px;"><span>Team Overfit</span></h2><h3><span><br /></span></h3><h3><span>Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span>Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><div style="text-align: start; text-indent: 0px;"><div style="margin: 0px;"><div><span><b>Blog Post: #8: 05/15/2018</b></span></div><div><span><span><b><br /></b></span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Social Media Predicts Stock Price (StartUp Mode)</span><br /><span><br /></span><span>This week, we weren't able to make our model make better prediction on stock price movement.</span><br /><h3><span>We have tried:</span></h3><ul><li><span>Improve initialization weights and bias</span></li><ul><li><span>Increase standard deviation so that the initial price prediction varies more preventing the model to be stuck at local minima</span></li><li><span>Decrease bias so that the initial prediction are closer to the expected, and therefore, converge faster</span></li></ul><li><span>Tune hyperparameters of LSTM and try out different settings of data</span></li><ul><li><span>Number of time steps of RNN</span></li><li><span>Size of dictionary</span></li><ul><li><span>Remove vocab that have high document frequency</span></li></ul><li><span>Number of LSTM hidden units</span></li><li><span>Number of RNN layers</span></li><li><span>Learning rate</span></li><li><span>Dropout layer</span></li></ul><li><span>Visualize and investigate our prediction/training to have better understanding of our model</span></li><ul><li><span>Find out the top k words that have most weight for a given day</span></li><ul><li><span>Most of them do not seem to relate to stock price movement</span></li></ul><li><span>Compare the prediction/target graph for each epochs to see if the predictions are improving/convergin</span></li></ul></ul><h3><span>In progress and next steps:</span></h3><div><ul><li><span>Find good news data and write preprocessor</span></li><li><span>Research on better featurizations</span></li><li><span>Research on better model</span></li><li><span>Add learning rate decay to our training</span></li><li><span>Investigate the correlation between sentiment analysis of Tweets and stock price movement</span></li></ul></div></div></div></div></div>







<p class="date">
<a href="https://teamoverfit.blogspot.com/2018/05/8-milestone-advanced-model-attempt-2.html">by Team Overfit (noreply@blogger.com) at May 16, 2018 06:37 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/ca5a7f69db85">
<h4><a href="https://medium.com/nlp-capstone-blog/advanced-model-2-ca5a7f69db85?source=rss----9ba3897b6688---4">Advanced Model # 2</a></h4>
<div class="entry">
<div class="content">
<p>As explained in our previous blog post, our current challenge is involves constructing the dataset in an efficient and effective manner. This blog post will detail the progress made in the past week in relation to data construction and the challenges we have faced. We will also briefly highlight our plan for the next week.</p><h4>Training</h4><p>We’ve attempted training on a small subset of the data in the format the model should expect, and ran into an issue with batching.</p><p>As of now, we backpropagate once per batch. But at this point, latent representations of sentences and the document have already been computed. Meaning, the RNNs have already encoded every word in the document before parameters are updated. This, in conjunction with the series of affines for each sentence will likely produce a computation graph that we won’t have enough memory to backprop on.</p><p>We’ve now switched to backpropagation once per sentence, which will hopefully lead to faster learning.</p><h4>New Heuristics</h4><p>Last week we described a method of BIO tagging sentences that involved using ROUGE. While this method might produce good tags, we found that ROUGE was incredibly slow to run. Since we run ROUGE once for every sentence in every document, we chose to develop a difference heuristic that worked like ROUGE but was much faster. This led us to experimenting with two new approaches, which are detailed below.</p><h4>Skip-bigrams</h4><p>When learning about ROUGE, we learned of a variety of ROUGE called ROUGE-SU. Here, SU stands for Skip Bigrams and Unigrams. Skip Bigrams refer to bigrams which are formed as any subsequent pair of words in the sentence. In other words, it’s every bigram possible in a sentence such that the bigram follows sentence order. For our first approach, we decided to use the same greedy algorithm described from previous blog posts, except we try to maximize the skip bigram overlap between the reference skip bigrams and the set of sentences we choose to extract. We implemented this functionality from scratch.</p><h4>Cosine Similarity</h4><p>Cosine similarity is defined as a measure of similarity between two vectors. Essentially, it is a way of determining the cosine of the angle between two sequences of text in Euclidean space. A value tending toward 1 means that the two pieces of text are more similar, while smaller values mean there is less correlation. In NLP, this metric is used as a bag-of-words comparison, combining the words of both sequences into a master set of words, and computing the cosine similarity between each sequence’s respective frequency vector whose dimensionality is equal to the size of the set. In other words, it is the cosine of the angle between their tf-idf vectors.</p><p>Currently, we’re using spaCy’s implementation of cosine similarity.</p><h4>Example Data</h4><p>When we are sufficiently strict (ex. enforcing a skip-bigram intersection of at least 10 with a cosine similarity of at least 0.94) then we can get promising matches:</p><p>PAPER: Mechanisms of NO/cGMP-Dependent Vasorelaxation TERMs: {‘omega-Nitro-L-Arginine, N’, ‘NO2Arg’, ‘N omega Nitro L Arginine’, ‘L-NNA’, ‘Nitroarginine [Chemical/Ingredient]’, ‘NG-Nitro-L-Arginine’, ‘omega-Nitroarginine’, ‘NG-nitro-L-arginine’, ‘N(omega)-Nitroarginine’, ‘NOLA’, ‘N omega-Nitro-L-Arginine’, ‘N OMEGA NITROARGININE L’, ‘omega Nitroarginine’, ‘NOARG’, ‘NG-Nitroarginine’, ‘N(G)-Nitroarginine’, ‘NG NITROARGININE L’, ‘NG Nitro L Arginine’, ‘Nitroarginine’, ‘NG Nitroarginine’}<br />TERM FOUND: True<br />100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 145/145 [00:01&lt;00:00, 103.03it/s]<br />REFERENCE: An amino acid derivative and nitric oxide synthase (NOS) inhibitor with potential antineoplastic and antiangiogenic activities. Upon administration, NG-nitro-L-arginine inhibits the enzyme nitric oxide synthase, thereby preventing the formation of nitric oxide (NO). By preventing NO generation, the vasodilatory effects of NO are abrogated leading to vasoconstriction, reduction in vascular permeability and an inhibition of angiogenesis. As blood flow to tumors is restricted, this may result in an inhibition of tumor cell proliferation. NO plays an important role in tumor blood flow and stimulation of angiogenesis, tumor progression, survival, migration and invasiveness.<br />CHOSEN: [‘NO coordinates the blood-flow distribution between arterioles and the microvasculature by regulating the diameter of small arteries.7 The importance of NO and cGMP for the regulation of vascular tone and blood pressure has been recently strengthened by the observation that mice deficient in eNOS, ANP, the ANP receptor guanylyl cyclase A, or cGKI develop hypertension.2–6,17’]<br />MAX SKIPGRAM MATCHES: 38<br />MAX COSINE SIMILARITY: 0.9430605549205237</p><p>The ‘CHOSEN’ array contains a sentence that was extracted with our heuristics. Like the reference, it mentions nitric oxide, blood flow, and regulation of vascularity. The goal is to train model on instances such as these and were able to extract sentences of such relevance.</p><p>We should also note that definitions define multiple terms (in other words, terms may have multiple synonyms). The extracted sentence ‘NO coordinates the blood-flow distribution between arterioles …’ itself contains none of the entities explicitly, and yet seems to align well and be indicative of it’s definition. Robustness in recognizing a given term and its synonyms, along with being able to extract sentences about that term without the term actually being in it is extremely desirable for us. A model that is able to recognize sentences about a particular technical term without the term being present would be an especially helpful research tool.</p><p>Unfortunately, the heuristic is not perfect and can be lead astray:<br /> “title”: “On the influence of various physicochemical properties of the CNTs based implantable devices on the fibroblasts’ reaction in vitro”<br /> },<br /> “e_gold”: “ A record of something that is being done, has been done, can be done, or is intended or requested to be done. Examples: The kinds of acts that are common in health care are (1) a clinical observation, (2) an assessment of health condition (such as problems and diagnoses), (3) healthcare goals, (4) treatment services (such as medication, surgery, physical and psychological therapy), (5) assisting, monitoring or attending, (6) training and education services to patients and their next of kin, (7) and notary services (such as advanced directives or living will), (8) editing and maintaining documents, and many others. Discussion and Rationale: Acts are the pivot of the RIM; all domain information and processes are represented primarily in Acts. Any profession or business, including healthcare, is primarily constituted of intentional and occasionally non-intentional actions, performed and recorded by responsible actors. An Act-instance is a record of such an action. Acts connect to Entities in their Roles through Participations and connect to other Acts through ActRelationships. Participations are the authors, performers and other responsible parties as well as subjects and beneficiaries (which includes tools and material used in the performance of the act, which are also subjects). The moodCode distinguishes between Acts that are meant as factual records, vs. records of intended or ordered services, and the other modalities in which act can appear. One of the Participations that all acts have (at least implicitly) is a primary author, who is responsible of the Act and who \”owns\” the act. Responsibility for the act means responsibility for what is being stated in the Act and as what it is stated. Ownership of the act is assumed in the sense of who may operationally modify the same act. Ownership and responsibility of the Act is not the same as ownership or responsibility of what the Act-object refers to in the real world. The same real world activity can be described by two people, each being the author of their Act, describing the same real world activity. Yet one can be a witness while the other can be a principal performer. The performer has responsibilities for the physical actions; the witness only has responsibility for making a true statement to the best of his or her ability. The two Act-instances may even disagree, but because each is properly attributed to its author, such disagreements can exist side by side and left to arbitration by a recipient of these Act-instances. In this sense, an Act-instance represents a \”statement\” according to Rector and Nowlan (1991) [Foundations for an electronic medical record. Methods Inf Med. 30.] Rector and Nowlan have emphasized the importance of understanding the medical record not as a collection of facts, but \”a faithful record of what clinicians have heard, seen, thought, and done.\” Rector and Nowlan go on saying that \”the other requirements for a medical record, e.g., that it be attributable and permanent, follow naturally from this view.\” Indeed the Act class is this attributable statement, and the rules of updating acts (discussed in the state-transition model, see Act.statusCode) versus generating new Act-instances are designed according to this principle of permanent attributable statements. Rector and Nolan focus on the electronic medical record as a collection of statements, while attributed statements, these are still mostly factual statements. However, the Act class goes beyond this limitation to attributed factual statements, representing what is known as \”speech-acts\” in linguistics and philosophy. The notion of speech-act includes that there is pragmatic meaning in language utterances, aside from just factual statements; and that these utterances interact with the real world to change the state of affairs, even directly cause physical activities to happen. For example, an order is a speech act that (provided it is issued adequately) will cause the ordered action to be physically performed. The speech act theory has culminated in the seminal work by Austin (1962) [How to do things with words. Oxford University Press]. An activity in the real world may progress from defined, through planned and ordered to executed, which is represented as the mood of the Act. Even though one might think of a single activity as progressing from planned to executed, this progression is reflected by multiple Act-instances, each having one and only one mood that will not change along the Act-instance life cycle. This is because the attribution and content of speech acts along this progression of an activity may be different, and it is often critical that a permanent and faithful record be maintained of this progression. The specification of orders or promises or plans must not be overwritten by the specification of what was actually done, so as to allow comparing actions with their earlier specifications. Act-instances that describe this progression of the same real world activity are linked through the ActRelationships (of the relationship category \”sequel\”). Act as statements or speech-acts are the only representation of real world facts or processes in the HL7 RIM. The truth about the real world is constructed through a combination (and arbitration) of such attributed statements only, and there is no class in the RIM whose objects represent \”objective state of affairs\” or \”real processes\” independent from attributed statements. As such, there is no distinction between an activity and its documentation. Every Act includes both to varying degrees. For example, a factual statement made about recent (but past) activities, authored (and signed) by the performer of such activities, is commonly known as a procedure report or original documentation (e.g., surgical procedure report, clinic note etc.). Conversely, a status update on an activity that is presently in progress, authored by the performer (or a close observer) is considered to capture that activity (and is later superceded by a full procedure report). However, both status update and procedure report are acts of the same kind, only distinguished by mood and state (see statusCode) and completeness of the information. “,<br /> “entity”: “act”,<br /> “extracted”: [<br /> “Since their discovery in 1952, carbon nanotubes (CNTs) have been attracting increasing attention in being applied in various areas of materials science due to their outstanding mechanical properties, high chemical and thermal stability and, in some cases, very good conductivity via an electron transfer.”,<br /> “Thus, at that time point, differences in fibroblasts’ proliferation rate may have been governed by different chemical composition of the samples and an increased amount of COOH species in the CNT_ox [28].”<br /> ],</p><p>Note that since the script selected this example, that the cosine similarity score between the extracted sentences and the reference were above 0.93. Not only is the cosine similarity too generous as a heuristic, the fact that the reference is so large means that it will almost always overlap with more than enough skip-bigrams to reach our skip-bigram threshold.</p><p>Examples like these and others are concerning, but the hope is that helpful examples like the NO example outnumber the noise that make it past our heuristics.</p><h4>Going Forward</h4><p>Our goals for the next week include fine tuning our data collection thresholds to maximize the quality of our dataset, training the model, and hopefully producing results. As we mentioned in our previous blog post, the model is ready for training, but the real challenge might be with the way we produce our dataset. In the upcoming week, we expect to experiment with new heuristics for sentence similarity and tweak the existing heuristics in order to produce the best dataset.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ca5a7f69db85" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/advanced-model-2-ca5a7f69db85">Advanced Model # 2</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/advanced-model-2-ca5a7f69db85?source=rss----9ba3897b6688---4">by Karishma Mandyam at May 16, 2018 05:39 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 15, 2018</h2>

<div class="channelgroup">







<h3><a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-5600014144802012716.post-6025423166932572092">
<h4><a href="https://nlpcapstonesemparse.blogspot.com/2018/05/blog-8.html">Blog 8</a></h4>
<div class="entry">
<div class="content">
I'm focusing now on combining coding patterns from the new class at test time with the patterns learned from the training set. This means both the code and javadocs for all the other methods are now being utilized as well, as opposed to the previously where just the method name is used. This is really interesting, both from code and language understanding, but also very challenging. This week I've been doing analysis on this approach, and preparing the dataset for this modified task through filtering heuristics.<br /><br />I ran an experiment to empirically test whether using the method implementations will help. For each method whose code your supposed to generate, I picked the class method which maximized the bleu and em score. This provides a soft upper bound. This naive baseline gets a bleu of .42(2 times the state of the art!) and an em of 0.06. When I randomly picked another implementation(soft lower bound) I got bleu of .13 which is half the state of the art.<br /><br />Additionally, to learn these coding patterns, the documentation and code needs to be of high quality hence I've been filtering the data to reduce the noise. Here are the criteria, if any are true the class is removed. This so far has cut down the number of classes from around 2 million to a hundred thousand. There's more work still to be done here.<br /><br /><ul><li>A method uses identifiers not present in the class and occurring in the training set under 7 times.</li><li>Use of too many integer literals</li><li>Too few method javadocs.</li></ul><br /><br /></div>







<p class="date">
<a href="https://nlpcapstonesemparse.blogspot.com/2018/05/blog-8.html">by nlpcapstone (noreply@blogger.com) at May 15, 2018 06:08 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2" title="Stories by Ryan Pham on Medium">Ryan Pham <br/> Team NeuralEmpty</a></h3>


<div class="entrygroup" id="https://medium.com/p/5c8a38264cae">
<h4><a href="https://medium.com/@ryanp97/error-analysis-and-the-transformer-architecture-5c8a38264cae?source=rss-6378d85d3a9b------2">Error Analysis and the Transformer Architecture</a></h4>
<div class="entry">
<div class="content">
<p>In the past week, I’ve been tuning hyper-parameters for the LSTM based models, exploring the Transformer architecture, and looking more in-depth at the predictions the model is making for error analysis.</p><h4>Swapping LSTM with Transformer</h4><p>As suggested by Nelson, I trained a model using the Transformer architecture from “<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a>” (Vaswani et al.) to explore and see if different architecture would help improve the performance of the LSTM based models.</p><p>I used the same hyper-parameters that was used to achieve the results listed in the paper. Unfortunately, the model showed a significant decrease in performance compared to the LSTM models. The main cause of this was due to ill-formed graphs. Unlike the LSTM models, the transformer was unable to learn the general requirements for a well-formed graph even after training for 50 epochs.</p><p>Every graph starts with an opening paren, (, which is the only place this token will occur (other opening parentheses are always attached to edge labels). The LSTM based model was able to correctly identify that this token only occurs at the beginning of the sequence while the transformer was unable to make this connection. Another issue that the transformer ran into was predicting a set of features after an edge label without any predicate, something that the LSTM based models also did not run into.</p><p>I believe that one of the causes of these issues is the fact that the Transformer was trained using batches of tokens rather than batches of sentences. However, even after training the transformer on batches of graphs, it still experienced issues of predicting edges without a predicate. Even more unfortunately, neither model was able to increase the SMATCH score or the surface/abstract predicate scores by any significant number.</p><pre>SMATCH (F1):<br />    Transformer (Tokens) - 0.60<br />    Transformer (Graphs) - 0.63</pre><pre>Surface Predicate F1:<br />    Transformer (Tokens) - 0.52<br />    Transformer (Graphs) - 0.54</pre><pre>Abstract Predicate F1:<br />    Transformer (Tokens) - 0.71<br />    Transformer (Graphs) - 0.73</pre><h4>Error Analysis</h4><p>This week I also trained some different models with larger hidden sizes, but unfortunately still did not see much improvement (all of them had similar performances as all the previously listed and tested models).</p><p>With the baseline and other models that did not have a coverage attention mechanism, there were around 2,200~2,300 predictions that differed in length by at least 15 tokens (around 5 predicates less than the gold-label graph). We hoped that adding a coverage mechanism would alleviate this issue, but the model with coverage actually resulted in more predictions having a large length difference. After looking into the parsed graph data more carefully, there doesn’t seem to be a clear correlation between the length of the graph in English and the graph length in Japanese. Furthermore, attending to the English predicates may not be the correct thing to do since there are pieces of grammar that may not be able to be directly related to some predicate in the input.</p><p>I also calculated which predicates were commonly mis-predicted as well as ones that were not predicted when they should have been. Surprisingly There was quite a big overlap between these two sets. Below is a small subset of the predicates for the baseline model trained with features:</p><pre>Abstract Predicates:<br />    def_q<br />    udef_q<br />    pron<br />    cop_id<br />    nominalization</pre><pre>Surface Predicates:<br />    _wa_d<br />    _ni_p<br />    _no_p<br />    _koto_n_nom<br />    _sono_q</pre><p>Something interesting to note are the surface predicates. The first three listed are called particles in the Japanese grammar. The first predicate, _wa_d, serves multiple purposes. は can be used to be a topic marker as well as being used to show contrast between subjects. The second predicate, _ni_p, also has multiple purposes with more varied usage. に can be used to mark time, destination, place, etc. So it seems that the models have trouble predicting predicates which have widely varied usage. Despite having a fairly high SMATCH score, it seems that the model is not quite learning the semantic structure of the Japanese graphs.</p><p>There are a couple reasons for this. Just like I mentioned in early posts, the sequence based models is likely not the best method of performing this semantic transfer and something like a TreeLSTM is more likely to be able to accurately capture the tree structure and the semantic meaning in the tree structure.</p><p>The second reason may be because of inconsistencies in the data. The Tanaka corpus is known to have a lot of casual speech, and despite taking some precautionary steps to avoid this (by using a modified version of Jacy to account for casual speech and slang), the data may still be too noisy for the number of examples in the training set that we have. To work around this, I can look into incorporating the Tanaka corpus as mentioned in one of my earlier posts. Michael Goodman was nice enough to link me the <a href="https://github.com/goodmami/xmt/blob/master/scripts/data-preparation/kyoto-wiki.sh">script</a> he used to grab the sentence pairs from the Kyoto corpus, so this is a reasonable goal for the up-coming week. Additionally, Michael suggested looking into Japanese WordNet for additional data on top of the Kyoto Corpus.</p><h4>Plans for the Next Week</h4><p>As mentioned above, I’ll be looking into incorporating more data from both the Kyoto Corpus and WordNet. Another thing I plan to look into is parent feeding which acts as a nice middle ground between the LSTM based models I’m currently using and a TreeLSTM. The idea behind this attention mechanism is to feed the previous decoder time step from the parent node into the current decoder time step. This allows the model to make more direct connections when it comes to long range dependencies.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5c8a38264cae" width="1" /></div>







<p class="date">
<a href="https://medium.com/@ryanp97/error-analysis-and-the-transformer-architecture-5c8a38264cae?source=rss-6378d85d3a9b------2">by Ryan Pham at May 15, 2018 06:58 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 10, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/2ac19f7510f9">
<h4><a href="https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-continued-2ac19f7510f9?source=rss----9ba3897b6688---4">Advanced Model Attempt #1 (Continued)</a></h4>
<div class="entry">
<div class="content">
<p>This week we continued work on the advanced version of the model which incorporates the SummaRuNNer architecture from (Nallapati et al.) and adds in a character level RNN for parsing UMLS terms. The models are complete but we ran into several challenges along the way, namely constructing our final training dataset from Semantic Scholar and UMLS terms. Without the dataset, we have not been able to train our model. In this blog post, we dissect our current challenges and our plans looking forward.</p><h4>Building the Model</h4><p>The original SummaRuNNer model fit our requirements pretty well. However, we ran into several challenges when implementing the model. First, there were no existing implementations of the architecture, so we had to implement the model from scratch. Furthermore, the time we spent building the model detracted from time that we could have spent working on gathering the data. The model used by Nallapati et al. also did not condition on a particular term in the paper. Our approach requires that we somehow incorporate the term so we spent additional time figuring out a character level RNN which encodes the term and includes it in the many affine transformations described in the SummaRuNNer paper.</p><p>Moreover, we had to consider how to optimize the model when it came to large amounts of data. One of the approaches we worked on was batching, essentially evaluating many sentences at once. This allowed us to speed up training by a significant amount. Finally, we spent time integrating our model into the existing architecture. Ultimately, the model was difficult, but completed.</p><h4>Collecting the Data</h4><p>There are no current datasets that we can use to train our model. To quickly recap the requirements for the data, each training example must comprise of one document, one entity or technical term, and a target representing the ideal summary of the document. In order to build this dataset, we had to individually collect each of these aspects and combine them.</p><p>In order to gather technical terms, we used the UMLS dataset, which contains over 150,000 medical terms. Obtaining the license to download UMLS and the actual process of downloading the data through the UMLS specialized data downloader took several days. Parsing the data was fairly straightforward however.</p><p>In order to gather documents, we are using the AI2 Semantic Scholar dataset, which contains over 7 million research papers. While the downloading process for Semantic Scholar was incredibly slow, we realized that we couldn’t simply download all the documents because some of them were not medical papers. Handling Computer Science papers becomes an issue because our entities are medical terms and we do not expect a computer science paper to have any relation to medical terms.</p><p>This brings us to the process of combining Semantic Scholar documents with UMLS terms. We use a distant supervision method which essentially applies a greedy approach to extract a group of sentences from each document with the highest ROUGE score while using the UMLS definitions as reference summaries. This is precisely where we are currently struggling. Computing ROUGE takes a very long time, considering the fact that we compute ROUGE as many times as there are sentences in each document. Though we filter out document-term pairs based on whether the term appears in the document, it seems that the ROUGE metric may not yield the best target sentences for us because our reference summaries tend to be fairly short while our documents tend to be fairly long. Currently, we have all the scripts running for this data collection process but aim to develop a better heuristic to collect data.</p><h4>Next Steps</h4><p>At this point, we have not been able to test our data because we are still building the dataset. In order to speed up this process, our immediate goal will be to develop a faster and more accurate heuristic to gather target sentences from each document. We will also explore filtering out the Semantic Scholar papers to only retain medical papers, which are more likely to correlate to the terms in UMLS. Once we build our dataset, we can test our completed models, tune hyper-parameters, and potentially utilize attention mechanisms while constructing the document level representation used by SummaRuNNer.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2ac19f7510f9" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-continued-2ac19f7510f9">Advanced Model Attempt #1 (Continued)</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-continued-2ac19f7510f9?source=rss----9ba3897b6688---4">by Karishma Mandyam at May 10, 2018 06:40 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 09, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a></h3>


<div class="entrygroup" id="https://medium.com/p/d7a06e892cdd">
<h4><a href="https://medium.com/@viterbi.or.not/advanced-model-1-part-2-d7a06e892cdd?source=rss-c522ef075bb3------2">Advanced Model #1, Part 2</a></h4>
<div class="entry">
<div class="content">
<p>Welcome back to Advanced Model #1 and thanks for sticking with us! This week, we’ve gotten some promising results in our first advanced model attempt and made a lot of progress with the incorporation of a second data source, although there is still work to be done toward fully integrating the chatlog data into a single model. In this post, we’ll go over what we’ve done since last week and give some numbers as well as example summaries from this model, then start laying out what we would like to achieve for Advanced Model #2.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*AsUzi76WPSKaMji6." />Our project roadmap and where we are at now, highlighted in orange</figure><h4><strong>Preprocessing</strong></h4><p>We decided to use preprocessing instead of features for some of the attributes of our data after discovering that our feature sets failed to focus solely on the conversation in some data types and instead gave false importance to format-specific patterns such as quoted text in emails. We eventually settled on the following distinction:</p><ul><li>Attributes of the data that existed at the conversation-level should be features: that is, anything universally applicable to textual conversations in general was implemented as a feature for the model</li><li>Attributes of the data at the data source-level should be preprocessed. This allowed us to share features across different data types (for example, using tf-idf across both emails and chatlogs) while minimizing the number of “dead” features (for example, simply assigning a constant value to a feature for detecting email signatures when vectorizing the chatlog data).</li></ul><p>The preprocessing we decided to do falls under the following categories:</p><p><strong>Identifying format-specific content:</strong> Quirks of the data format can sometimes present problems for our model, for example, we noticed separator lines composed of all dashes or symbols as well as email signatures (left example below) were making it into the summaries.</p><p><strong>Removing quoted email text:</strong> The email dataset contains quoted text of previous emails in the thread when there is an email reply, causing duplication of sentences in the summaries because the model seems to want to include the important sentence as much as possible. An example of a generated summary with the problem is in the right example below.</p><p>After initially parsing the data, we do this preprocessing step to identify such sentences using simple regular expression rules and remove them before computing feature vectors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vUpBdbFjg5Exfd-8." />An email signature (left) and quoted text (right) in generated summaries before adding preprocessing</figure><h4><strong>Adding the Chatlog Dataset — New Pipeline</strong></h4><p>One of our major goals for the advanced model was to incorporate the chatlog dataset we identified early on in the project. Of course, having the capability to automatically generate summaries for chat conversations expands the usefulness and possible applications of our model: although email threads are incredibly common in today’s world, chat communication is also experiencing an incredible rise in popularity. Furthermore, as chat communications tend toward a large number of participants and a model of very frequent responses, our analysis is that chat data summaries have the potential to be even more useful than their email counterparts.</p><p>However, our goal with incorporating the chatlog data is ultimately to combine it with the email data in order to be able to train our model on both data types at once. To do this, we saw the need to create a common list of features that can be applied to any data input, allowing the model to train on both sources of data indiscriminately. As the Sood et. al. paper suggests, training on both sets of data has the potential to improve the resulting summaries of the model for each type of data separately, although the paper in question does not specify their results beyond the notion that they were promising in a preliminary examination.</p><p>The end goal for our system is illustrated below:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YodqfIjCDfX5xJLfNG_JFQ.png" />Pipeline for integrating different datasets with a common feature set into our model</figure><p>The blended colors represent the notion of common features, achieved through separate pre-processing and vectorizing of each data source but used in a single shared model.</p><p>To achieve this, we needed to do a major restructuring of our codebase, and it is unfortunately the case that a huge portion of our time this week was spent improving old code to make it more modular and compatible with these new requirements. What was once a single script to parse, process, vectorize, and train on a single dataset had to be broken apart into different modules for each of those tasks that could implement a common interface. In addition, with the growing number of options and tasks that had to be supported, we discovered it was necessary to entirely separate the concerns of processing text and handling configuration: to that end, we created a new, much more robust command-line interface and broke all other components into individual directories with as much code factoring as possible.</p><p>As a result of our design process and refactor, we currently have almost all of the infrastructure necessary for incorporating the chatlog data and training our model on all of the available data before evaluating on a single source. As we mentioned above, however, there were several challenges with parsing the chatlog data, namely the messy nature of the chats and the number of possible edge cases present in such a large dataset. Therefore, we are still in the process of finishing the parser component, but once it is complete we should be able to use both types of data in Advanced Model #2 and report on the impact that it makes. See our progress so far with this pipeline structure on our <a href="https://github.com/viterbi-or-not-to-be/viterbi-or-not-to-be/tree/master/conversation">GitHub repository</a>!</p><h4><strong>Results</strong></h4><p>Through the addition of the preprocessing step, we have greatly improved our ROUGE scores and become competitive with the advanced model in the Sood et. al. paper. Here are the results of our advanced models, using all of the features we have discussed so far and with the preprocessing step:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iDsZaXOhuHshbaFWXFlPvQ.png" />Results of the three advanced model types</figure><p>These results are significantly better than our baseline, and we believe our model is generally pretty good at picking up on which sentences are important to keep! To more directly see the impact of preprocessing, we compared the results of the Naive Bayes model with and without preprocessing:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*BBrr-Z2ilFTbK8xi-t2SPg.png" />Comparison between our advanced model with preprocessing and without preprocessing</figure><p>The model without preprocessing does decently, but the preprocessing definitely contributes significantly to our advanced model performance. To show our progress since the baseline model, this compares the results of the Naive Bayes baseline models with those of our current Naive Bayes model:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*M_zctk4dPmVbmVHeHmRYYQ.png" />Comparison between our advanced model and our baselines</figure><p>Both ROUGE-1 and ROUGE-2 have stepped up by several percentage points, but the most significant increase is seen in the ROUGE-L improvement. Finally, our model compared with the model in the Sood et. al. paper:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ty7sqP2swRfdf1UGw7wb_g.png" />Comparison between our advanced model and the Sood et. al. model</figure><p>As you can see, our ROUGE-L score is slightly lower but our ROUGE-1 and ROUGE-2 scores are higher! We’ve previously discussed the drawbacks of ROUGE―there is no definitively better type of ROUGE metric and ROUGE in general is not necessarily a reliable indicator of how good the generated summaries really are, so we are wary about quantifying our model’s performance in comparison to the Sood et. al. model’s performance based just on ROUGE scores. However, we haven’t been able to find any examples of the summaries generated by their model so this is all we have to go off of for now.</p><h4><strong>Example Summary and Analysis</strong></h4><p>Because ROUGE provides relatively unreliable metrics for evaluation of our model’s summaries, we have also turned to human evaluation. One of the summaries that our model was able to generate based off an input email thread, using all of the preprocessing and extra features that we have built, is reproduced below in its original condition except for display-motivated truncation:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BMVwYccbvULdQcwp." />Comparison between human annotated reference summary (left) and summary generated by our advanced model (right)</figure><p>This summary is fairly representative of the summaries that are being generated, and compared to the results we saw from our baseline models it is clearly doing better in terms of human evaluation. The sentences highlighted in green are those that are shared between the reference annotation and our system’s generated summary — interestingly, all but one of the sentences in the reference summary were successfully captured by our system. Therefore, reading through the summary generated by our system gives a strong impression of the topics that were discussed in the thread, and seemingly gives a human reader all the information that is needed.</p><p>However, there are certainly aspects of the current summaries produced by the model that are lacking. In the summary given above, all 6 sentences that were truncated for display purposes at the ellipsis and the four additional non-green sentences displayed were not included in the reference summary. This summary comes from a thread with only 31 sentences after preprocessing, meaning that while our model successfully captured all of the sentences marked as important by a human, it did so at the cost of incorporating an enormous percentage of unnecessary extra text that serves to make the summary less of a “summary” while adding relatively little content. To address this, we will need to figure out how to reward the model for producing more concise summaries, or to incorporate a “compression factor” that allows producing a summary with the k most relevant sentences for some value of the hyperparameter k. Another issue is highlighted in red above — while we added preprocessing to remove email signatures based on a set of heuristics, to avoid deleting important content we made the preprocessor act conservatively when uncertain, and as a result some signatures like in the example above made their way into the summaries. Although other features would ideally prevent these signatures from appearing in many cases, we will need to improve our preprocessing heuristics in order to target these kinds of medium-specific text examples before moving on to incorporate additional types of data.</p><h4><strong>Next Steps</strong></h4><p>To entirely finish Advanced Model #1, we would like to complete the process of incorporating chatlog data and then transition into work on Advanced Model #2 by continuing to explore and add conversation-specific features. We also want to address the excessive lengthiness problem we found in the generated summaries by doing something to limit the number of sentences the model is allowed to select for the extractive summary.</p><p>Beyond these immediate steps, we would love to be able to take a stab at abstractive summarization! It’s a really challenging but rewarding conversation summarization problem that we will need to put some more thought into to attempt. We’ve preliminarily determined that we can make an attempt using the extractive output from our current model along with some other contextual metadata from the conversation data (for example, authors of the chosen sentences), to get a start on an abstractive summary. In general, we think this will require more modeling of entity relationships and text generation.</p><p>Overall, we’re excited with what our model has been able to do so far and optimistic about what we’ll be able to achieve with Advanced Model #2, so stay tuned!</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d7a06e892cdd" width="1" /></div>







<p class="date">
<a href="https://medium.com/@viterbi.or.not/advanced-model-1-part-2-d7a06e892cdd?source=rss-c522ef075bb3------2">by Viterbi Or Not To Be at May 09, 2018 06:33 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 03, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/d01e84c5e1da">
<h4><a href="https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-neural-based-definition-extraction-d01e84c5e1da?source=rss----9ba3897b6688---4">Advanced Model Attempt #1: Neural-Based Definition Extraction</a></h4>
<div class="entry">
<div class="content">
<p>We last left off on the idea of using an FSA with a restricted vocabulary; restricted in the sense that we extract sentences coupled with a neural language model to assure semantic quality while allowing a generative RNN model a reasonable amount of improvisation to produce abstractive definitions.</p><p>Here, we discuss our approach for the extractive component of this model, and consider it our first attempt at an advanced model for the task.</p><h3>Introducing Extractive Summarization</h3><p>Recall that extractive summarization is the idea of reducing text down to a subset of its sentences that still preserves its semantic integrity. In particular, we intend to build on the work of a successful nerual-based extractive summarizer and tailor it to solve our task.</p><p><a href="https://arxiv.org/abs/1611.04230">SummaRuNNer</a> is an RNN-based extractive summarization algorithm developed by Nallapati et al. that encodes documents from the word level up to and across the sentence level before making inference. Essentially, the model is a binary classifier on sentences within a document on whether it should be included in a summary. Its decisions are conditioned on</p><ul><li>Averaged-pooled word-level hidden states of the sentence</li><li>Average-pooled sentence-level hidden states of the document</li><li>An abstract representation of the summary built so far (average-pooling of the word-level pooled hidden states of sentences selected thus far)</li></ul><p>After which, there are several affine transformations conducive to selecting and filtering sentences:</p><ul><li>Content: affine on the abstract sentence representation that measures semantic richness</li><li>Salience: bilinear affine on the abstract sentence representation and the document representation to measure cohesiveness</li><li>Novelty: bilinear affine on the abstract sentence representation and the running summary representation to address redundancy</li><li>Absolute and Relative Positioning: two separate affines on the embedded index of the sentence to allow how far we are into the document to influence inference</li></ul><p>As of now, we have built from scratch our own <a href="https://github.com/NLP-Capstone-Project/machine-dictionary/blob/development-tam/machine_dictionary_rc/models/SummaRuNNer.py">unofficial implementation of this model</a> with inspiration from another <a href="https://github.com/hpzhao/SummaRuNNer">unofficial implementation</a> and is capable of summarizing documents the way we’ve formatted them. What’s left is for us to tailor this model to fit the task.</p><h3>A Slight Twist on an Established Task</h3><p>As of now, the model summarizes documents. We’d like it so that it instead zeroes in on query terms we give it given a research paper, to intelligently extract only sentences from that paper conducive to defining that term.</p><p>Our approach for augmenting SummaRuNNer to be a definition extractor involves</p><ul><li>Encoding the query term with a character-level RNN and using its concatenated hidden states as its representation</li><li>Introducing this new query-term abstract representation when constructing the document representation through a bilinear affine</li><li>Further introducing this query term by converting many of the non-bilinear affines (content, positioning, and possibly new ones for the task) to further condition inference on the query term.</li></ul><p>Essentially, the sentences we extract from the document are being conditioned on the term we’re trying to define. Encoding technical terms using a character level RNN allows similar technical terms to have similar hidden representations. For example, if we see the term “Chronic Lymphocytic Leukemia” in the training data and encounter “Chronic Myelogenous Leukemia” in the testing data, we would have more of an idea of how to approach this new term because of its character level similarities to the term we have already seen during training time. This might help us break down more complicated novel technical terms at testing time.</p><p>Experiments have yet to be conducted on the effectiveness of this approach but will be discussed later in <strong>Advanced Model Attempt #1 (cont.):</strong> another post later in this series discussing the results of the groundwork we’ve laid out here.</p><h3>Training Methods</h3><h4>Collection Training Data with UMLS and ROUGE:</h4><p>Recall that SummaRuNNer is a model that aims to extract the sentences in a document that summarize it best. It does so by training on examples that teach the model which sentences to extract from the document.</p><p>SummaRuNNeR uses a <em>distant supervision</em> method that relies on ROUGE in order to produce training examples for the model. This portion of the architecture, which we refer to as the “extractor”, extracts the sentences out of each document which maximize the ROUGE score when compared against the gold standard definition for the term in question. The extractor in a summarization context can use a greedy approach as follows:</p><ul><li>Look at each sentence in the document one at a time and consider appending it to the extracted sentences that we have already chosen.</li><li>Calculate the ROUGE score of the old extracted sentences + this new sentence in comparison to the gold standard summarization for the document.</li><li>If the ROUGE score increases from the previous ROUGE score, keep the new sentence.</li><li>Otherwise, we don’t keep the new sentence and move on.</li></ul><p>Although this method may not produce the most optimal and compact set of sentences that are relevant, this approach will be faster and is reasonable. The output of the extractor for each document is a tensor whose length is the number of sentences in the document, and is 0 if the sentence is tagged with O or 1 if the sentence is tagged with I.</p><p>To tailor this style of data collection to our task however, we optimize on ROUGE with respect to an entity’s gold-standard definition instead of a gold-standard summarization of the document. We collect entity-definition pairs through <a href="https://www.nlm.nih.gov/research/umls/">UMLS</a> and creating training examples of the form</p><ul><li>Entity (the technical term to define)</li><li>Gold-standard definition for the entity</li><li>The target sentence IO tags found via distant supervision with ROUGE on sentences of a research paper with the gold-standard definition being the reference</li><li>A Semantic Scholar research paper in which the sentences came from (provides the sentences in which to perform inference)</li></ul><p>With this data, we can train the definition extraction model discussed earlier; we train using these &lt;entity, IO-tagged sentences, publication&gt; examples to learn a tagger that can extract sentences most relevant to a term given a publication.</p><p>While this may result in an unnecessarily large number of training data points, we can also consider pruning this dataset later on if we have irrelevant entities for a particular document. For example, if we were trying to find a training example that used the entity “dental cavity” for a document that was about blood cancers, we might not want to keep this training example because there wouldn’t be much of a correlation between the two. In order to do this, we can introduce a ROUGE threshold, where we only keep the training example if the ROUGE score of the sentences extracted by the tagger are above a particular threshold. This might be an optimization for the future.</p><p>Our previous approach was unsupervised and it relied only on the semantic scholar dataset to produce definitions. Our current approach is an extension of SummaRuNNer which requires gold standard definitions for entities that we’d like to define in each paper. We chose to focus on medical terms, and one of the most complete datasets for medical terms and their definitions happens to be the UMLS dataset. This dataset contains a <em>Metathesaurus</em> which contains, amongst many other pieces of data, medical terms and their definitions. The technical terms in the dataset serve as references for ROUGE in the tagging phase above.</p><h4>In summary</h4><p>Training is fairly straightforward; loss between predicted and target sentences is computed with log loss (each sentence in a document is IO-tagged where sentences labeled with <em>I </em>are to be included in the definition). Essentially, the definition extractor, much like SummaRuNNer, is trained as a sentence tagger.</p><h4>Attention as a Stretch Goal</h4><p>The first part of our basic SummaRuNNer-based model uses a document representation to predict tags for sentences in a document. The current document representation is constructed by averaging the hidden states from words in each sentence and averaging the hidden states from each sentence in the document. However, we believe that simply averaging the sentences may not be the best approach to constructing the latent document representation. One of our stretch goals for us to optimize the model will be to attend to the most important parts of sentences in each document. We can do this using the method proposed in Hierarchical Attention Networks for Document Classification (Yang et. al 2016).</p><p>This approach introduces a word level context vector and a sentence level context vector which allow us to calculate attention coefficients on the fly for every word in each sentence and every sentence in the document. In this manner, we can take a weighted sum of the hidden states in the sentences and will hopefully produce better document representations overall. The word level and sentence level context vectors can be initialized randomly and learned throughout training.</p><h4>Conclusion</h4><p>We are very excited to have found a supervised approach to this task per the advice of AI2 researchers. It’s a straightforward approach with measurable loss and clearer metrics.</p><p>We also hope to have enough time before the capstone is over to introduce attention!</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d01e84c5e1da" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-neural-based-definition-extraction-d01e84c5e1da">Advanced Model Attempt #1: Neural-Based Definition Extraction</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-neural-based-definition-extraction-d01e84c5e1da?source=rss----9ba3897b6688---4">by Tam Dang at May 03, 2018 03:32 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 02, 2018</h2>

<div class="channelgroup">







<h3><a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a></h3>


<div class="entrygroup" id="http://cse481n-capstone.azurewebsites.net/?p=64">
<h4 lang="en-US"><a href="http://cse481n-capstone.azurewebsites.net/2018/05/01/advanced-attempt-i/">Advanced Attempt I</a></h4>
<div class="entry">
<div class="content" lang="en-US">
<h3><b>Data Collection: </b></h3>
<p><span style="font-weight: 400;">We have made a lot of progress increasing our data quality since the last blog post. We have fine-tuned our filtering parameters and experimented with a few different definitions for set similarity. On top of Jaccard Index, we tried Dice Index and Cosine Similarity. We found that depending on the threshold, these different methods gave very similar results, but Dice Index seemed to provide the highest quality sentences while being more tolerant of long sentences (unlike Jaccard, which favored short sentences). Although it’s very picky, we’re certain that due to the huge amount of raw Reddit data we have we can still get a dataset big enough to train our complex neural nets.</span></p>
<p><span style="font-weight: 400;">As an example of our improvement, consider this example from the last post:</span></p>
<pre><span style="font-weight: 400;">MeanJokes Post: “Don’t be offended but Fuck you”
</span>Similar Post: “fuck Foligno”
Similar Post: “fuck narek”
Similar Post: “fuck”
Similar Post: “Fuck me?”
Similar Post: “Fuck me”
Similar Post: “fuck me”
Similar Post: “Fuck it”
Similar Post: “Fuck”
Similar Post: “Who the fuck are you?”</pre>
<p><span style="font-weight: 400;">Now our output would look like:</span></p>
<pre><span style="font-weight: 400;">MeanJokes Post: Don't be offended but Fuck you
</span>Similar Post: why the fuck does he have to talk in a screaming voice
Similar Post 171137: "Officer, I have no idea what in the fuck you're talking about.
Similar Post 92163: Or maybe you just fuck me in public for all too see.
Similar Post 18052: "you know, I'm finally happy". UGH, fuck off.
Similar Post 2567: So reddit, that's my fuck up. Any advice if any of you are in HR?
Similar Post 160778: Now I'm questining what numbers are real and what was put down to fuck with me and what's serious.
Similar Post 210956: And when i ask him about it, he cusses me out (tells me to fuck off) and i just die/break down internally.</pre>
<p><span style="font-weight: 400;">As well, since Dice is so picky and because the MeanJokes set tends to have a very particular structure to all its posts, we are also adding in some other obviously offensive posts to use for our set similarity step. We’re using a hate speech lexicon developed by Tom Davidson (linked below) to extract hateful posts from the general Reddit set. We will concatenate this with the MeanJokes set before running Set Similarity against all of the posts again, hopefully giving us a wider range of language structure for our dataset. </span></p>
<p><span style="font-weight: 400;">Our final improvement was discovering that a handful of subreddits contribute a majority of the noise in our data. This noise is mostly of two varieties: 1. Personal ads for intimate encounters and 2. Trading requests, for both physical and virtual items. A handful of these subreddits are very activate and are surprisingly a large chunk of Reddit’s posts, although none of them ever get nearly enough upvotes to be noticed by the average user. </span></p>
<p><span style="font-weight: 400;">So in order to combat having a lot of posts of this sort in our dataset:</span></p>
<p><img alt="" class="alignnone wp-image-65" height="144" src="http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/Picture1-300x110.png" width="393" /></p>
<p><span style="font-weight: 400;">We have put together a blacklist of subs and filtered them out of the posts we consider for set similarity.</span></p>
<table style="height: 620px;" width="414">
<tbody>
<tr>
<td><span style="font-weight: 400;">100k posts</span></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Top 10 Black List</span></td>
<td></td>
<td><span style="font-weight: 400;">Top 10 White List</span></td>
<td></td>
</tr>
<tr>
<td><span style="font-weight: 400;">RocketLeagueExchange’</span></td>
<td><span style="font-weight: 400;">1860</span></td>
<td><span style="font-weight: 400;">AskReddit’</span></td>
<td><span style="font-weight: 400;">5978</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">A5XHE’</span></td>
<td><span style="font-weight: 400;">1373</span></td>
<td><span style="font-weight: 400;">Showerthoughts’</span></td>
<td><span style="font-weight: 400;">1709</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dirtykikpals’</span></td>
<td><span style="font-weight: 400;">1128</span></td>
<td><span style="font-weight: 400;">The_Donald’</span></td>
<td><span style="font-weight: 400;">850</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dirtypenpals’</span></td>
<td><span style="font-weight: 400;">870</span></td>
<td><span style="font-weight: 400;">teenagers’</span></td>
<td><span style="font-weight: 400;">720</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">DirtySnapchat’</span></td>
<td><span style="font-weight: 400;">792</span></td>
<td><span style="font-weight: 400;">GlobalOffensiveTrade’</span></td>
<td><span style="font-weight: 400;">681</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">dirtyr4r’</span></td>
<td><span style="font-weight: 400;">438</span></td>
<td><span style="font-weight: 400;">Bitcoin’</span></td>
<td><span style="font-weight: 400;">651</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">AppNana’</span></td>
<td><span style="font-weight: 400;">372</span></td>
<td><span style="font-weight: 400;">relationships’</span></td>
<td><span style="font-weight: 400;">586</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Roleplaykik’</span></td>
<td><span style="font-weight: 400;">368</span></td>
<td><span style="font-weight: 400;">FIFA’</span></td>
<td><span style="font-weight: 400;">558</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">buildapc’</span></td>
<td><span style="font-weight: 400;">364</span></td>
<td><span style="font-weight: 400;">explainlikeimfive’</span></td>
<td><span style="font-weight: 400;">500</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">AgeplayPenPals’</span></td>
<td><span style="font-weight: 400;">329</span></td>
<td><span style="font-weight: 400;">Fireteams’</span></td>
<td><span style="font-weight: 400;">469</span></td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;"><br />
Running the filter on 100k posts, we can see that most of the most common subreddits that remained are conversational in nature, while those that were removed would not make very useful sentences.</span></p>
<p> </p>
<h3><b>Advanced Model Attempt: </b></h3>
<h4><span style="font-weight: 400;">Combining Datasets: </span></h4>
<p><span style="font-weight: 400;">In our last blog post, we mentioned our concern about the small size of </span><span style="font-weight: 400;">Waseem’s twitter dataset</span><span style="font-weight: 400;">. This week, we combined that dataset with another twitter hate speech dataset made by Thomas Davidson. The Davidson dataset contains 24,802 labeled tweets. Each tweet is coded by at least 3 CrowdFlower users. Each row contains 5 columns:</span></p>
<table style="height: 336px;" width="584">
<tbody>
<tr>
<td>count</td>
<td>number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).</td>
</tr>
<tr>
<td>hate_speech</td>
<td>number of CF users who judged the tweet to be hate speech.</td>
</tr>
<tr>
<td>offensive_language</td>
<td>number of CF users who judged the tweet to be offensive.</td>
</tr>
<tr>
<td>neither</td>
<td>number of CF users who judged the tweet to be neither offensive nor non-offensive.</td>
</tr>
<tr>
<td>class</td>
<td>class label for majority of CF users. 0 – hate speech 1 – offensive language 2 – neither</td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;">Davidson et. al. used the following definition for hate speech: language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. According to the paper, </span><span style="font-weight: 400;">“only 5% of tweets were coded by the majority of coders”</span><span style="font-weight: 400;">. If we directly combine Waseem Data with this we might get a even more skewed class distribution (31% ‘hate’, 69% ‘none’). Therefore, we decided to change the class labels of Davidson a little bit: if all CF users unanimously coded a tweet hate_speech or offensive_language, the tweet would be labeled ‘hate’; otherwise, the tweet would be labeled ‘none’. The modified Davidson dataset has a class distribution of 76% ‘hate’ and 24% ‘none’. Then we combined these two datasets (removed duplicate tweets if there are any). The new combined dataset has 40,509 tweets and a class distribution of 59% ‘hate’ and 41% ‘none. The combined dataset is much larger than the altered Waseem dataset (~15k tweets) and the labels are more balanced. </span></p>
<p><span style="font-weight: 400;">We do have the concern whether this more these more generously labeled ‘hate’ tweets are noisy. However, because the Waseem dataset is also more generous to ‘none’ labels (as long as the tweet is neither racist or sexist), we believe they would have some counter effect on each other. After all, data noise is very unlikely to be completely removed. </span></p>
<h4><span style="font-weight: 400;">Preprocessing: </span></h4>
<p><span style="font-weight: 400;">Since the model we tried requires each sentence to have at least 4 tokens, we decided to ignore sentences with less than 4 tokens after pre-processing.</span></p>
<h4><span style="font-weight: 400;">The effectiveness of Combined Dataset:</span></h4>
<p><span style="font-weight: 400;">To illustrate the effectiveness of the combined dataset, we chose the best NN model set up from baseline II to train on Waseem dataset and combined dataset separately and evaluated the two trained models on Waseem dev data. We decided not to evaluate on test data yet because we don’t want any leaked info from test.</span></p>
<p><span style="font-weight: 400;">Set up — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer GRU</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Trained on Waseem Dataset, epoch chosen: 13</span></li>
</ul>
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Waseem-Dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy </span></td>
<td><span style="font-weight: 400;">0.8235</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.8022</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.7876</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.7940</span></td>
</tr>
</tbody>
</table>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Trained on Combined Dataset, epoch chosen: 16</span></li>
</ul>
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Waseem-Dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy </span></td>
<td><span style="font-weight: 400;">0.8152</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.7979</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.7672</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.7788</span></td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;">Although the two models had the same setup, the one trained on the combined dataset got performance close to the one trained on the original Waseem dataset despite the fact that we now have really different class distributions in the two datasets. </span></p>
<h4><span style="font-weight: 400;">Retrain Some Baseline Models on Combined Dataset:</span></h4>
<p><span style="font-weight: 400;">Here we retained some baseline models with different set ups on the combined dataset and evaluated them on both Waseem dev data and combined dev data.</span></p>
<ul>
<li>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Model1 — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer GRU, epoch chosen: 16</span><br />
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Combined-dev</span></td>
<td><span style="font-weight: 400;">Waseem-dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.8665</span></td>
<td><span style="font-weight: 400;">0.8152</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.8614</span></td>
<td><span style="font-weight: 400;">0.7979</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.8628</span></td>
<td><span style="font-weight: 400;">0.7672</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.8621</span></td>
<td><span style="color: #ff0000;">0.7788</span></td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Model2 — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer LSTM, epoch chosen: 19</span><br />
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Combined-dev</span></td>
<td><span style="font-weight: 400;">Waseem-dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.8625</span></td>
<td><span style="font-weight: 400;">0.8091</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.8577</span></td>
<td><span style="font-weight: 400;">0.7875</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.8578</span></td>
<td><span style="font-weight: 400;">0.7648</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.8578</span></td>
<td><span style="color: #ff0000;">0.7739</span></td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;"> Model3 — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer BiGRU, epoch chosen: 16</span></p></li>
</ul>
</li>
</ul>
<ul>
<li>
<ul>
<li style="font-weight: 400;">
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Combined-dev</span></td>
<td><span style="font-weight: 400;">Waseem-dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.8618</span></td>
<td><span style="font-weight: 400;">0.8104</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.8567</span></td>
<td><span style="font-weight: 400;">0.7916</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.8575</span></td>
<td><span style="font-weight: 400;">0.7620</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.8571</span></td>
<td><span style="color: #ff0000;">0.7732</span></td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;">Model4 — embedding: 100 dimensional glove twitter embeddings, encoder: 1 layer GRU, epoch chosen: 11</span></p></li>
</ul>
</li>
</ul>
<ul>
<li>
<ul>
<li style="font-weight: 400;">
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Combined-dev</span></td>
<td><span style="font-weight: 400;">Waseem-dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.8651</span></td>
<td><span style="font-weight: 400;">0.8194</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.8605</span></td>
<td><span style="font-weight: 400;">0.7971</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.8603</span></td>
<td><span style="font-weight: 400;">0.7834</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.8604</span></td>
<td><span style="color: #ff0000;">0.7894</span></td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
</ul>
<h4><span style="font-weight: 400;">Model:</span></h4>
<p><span style="font-weight: 400;">Our first Advanced model will be a CNN model.</span></p>
<h5><span style="font-weight: 400;">The intuition of choosing this model:</span></h5>
<p><span style="font-weight: 400;">CNN provides us a convenient way to extract the most important information within the given fragment of a sentence through filters and max pooling. We found it might be a worth trying model on our task.</span></p>
<p><span style="font-weight: 400;">Our model looks like:</span></p>
<p><img alt="" class="alignnone wp-image-67" height="158" src="http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/Picture2-300x116.png" width="409" /></p>
<pre><span style="font-weight: 400;">Image credit:</span><span style="font-weight: 400;">Gambäck, B., &amp; Sikdar, U.K. (2017). Using Convolutional Neural Networks to Classify Hate-Speech.</span></pre>
<ul>
<li>
<ul>
<li>
<ol>
<li style="font-weight: 400;"><span style="font-weight: 400;">Preprocess all words and encode them using pretrained glove embeddings.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Feed result into a convolution neural network, taking 2, 3 and 4-grams into consideration. Output dimension is 28, 26 for English alphabets, 1 for digits and 1 for all other symbols. </span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Feed result into a 2-layer feed-forward neural net, with dimension (28, 2)  and dropout (0.3, 0.3)</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Softmax on the result and pick the major class</span></li>
</ol>
</li>
</ul>
</li>
</ul>
<p><span style="font-weight: 400;">With not much tuning, here’s what our best model looks like:</span></p>
<ul>
<li>
<ul>
<li>
<ul>
<li><span style="font-weight: 400;">200 dimension embedding, filters=100, trained on Waseem twitter dataset</span></li>
</ul>
</li>
</ul>
</li>
</ul>
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">Waseem dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400;">0.79169</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400;">0.78274</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400;">0.80086</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400;">0.82142</span></td>
</tr>
</tbody>
</table>
<ul>
<li>
<ul>
<li>
<ul>
<li><span style="font-weight: 400;">200 dimension embedding, filters=100, trained on the combined dataset</span></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><span style="color: #ff0000;">*revised, due to an imperfection in the combined dataset, there was a mistake in numbers</span></p>
<table>
<tbody>
<tr>
<td></td>
<td><span style="font-weight: 400;">combined dev</span></td>
<td><span style="font-weight: 400;">Waseem dev</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">F1</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.85983</span></td>
<td><span style="color: #ff0000;">0.77451</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Recall</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.86029</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.76156</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Precision</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.85937</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.78791</span></td>
</tr>
<tr>
<td><span style="font-weight: 400;">Accuracy</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.86437</span></td>
<td><span style="font-weight: 400; color: #ff0000;">0.80837</span></td>
</tr>
</tbody>
</table>
<p><del><span style="font-weight: 400;">With no doubt, our combined dataset provides a huge boost on performance on original Waseem twitter dataset.</span></del></p>
<p><span style="color: #ff0000;">From the above results, it seems CNN hasn’t show an improvement on our job. We think doing more hyper parameter tuning should give us some improvement. Furthermore, we would like to incorporate Elmo to see if that will help us our not.</span></p>
<p><del><span style="font-weight: 400;">However, we do have some concern about our models: almost all of the best models we have with a large number of filters have their best epoch generally to be the first few epochs. We are a little bit concerned about that since that may be a sign of overfitting.</span></del></p>
<p><span style="color: #ff0000;">Since our result hasn’t show any improvement, we think it’s more appropriate to do error analysis once we gain some improvement.</span></p>
<p><del><span style="font-weight: 400;">Therefore, before we start to do any error analysis, we would like to do a little bit more hyperparameters since we haven’t really try different drop out rate or other output dimension values other than the one specified in the paper we referenced.</span></del></p>
<h3><span style="font-weight: 400;">Next Step:</span></h3>
<p><span style="font-weight: 400;">First, we will dig deeper on the model we have right now. We will first play with its parameters and then conduct error analysis on it.</span></p>
<p><span style="font-weight: 400;">As suggested in previous blog post feedback, we would like to try Elmo and see how much can we improve with it. Furthermore, we would like to try things like character level embedding as well as another very interesting model which combines CNN with GRU to make prediction.</span></p>
<h3><span style="font-weight: 400;">Work Cited:</span></h3>
<p><a href="https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06">Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).</a></p>
<p><a href="https://www.semanticscholar.org/paper/Automated-Hate-Speech-Detection-and-the-Problem-of-Davidson-Warmsley/6ccfff0d7a10bf7046fbfd109b301323293b67da">Davidson, Thomas J et al. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM (2017).</a></p>
<p><a href="https://www.semanticscholar.org/paper/Using-Convolutional-Neural-Networks-to-Classify-Gamb%C3%A4ck-Sikdar/0dca29b6a5ea2fe2b6373aba9fe0ab829c06fd78">Gambäck, Björn and Utpal Kumar Sikdar. “Using Convolutional Neural Networks to Classify Hate-Speech.” (2017).</a></p>
<p> </p></div>







<p class="date">
<a href="http://cse481n-capstone.azurewebsites.net/2018/05/01/advanced-attempt-i/">by Team Watch Your Language! at May 02, 2018 06:17 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 01, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1">
<h4><a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1">Advanced Model Attempt #1 (Act 1)</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">The first part of implementing my advanced model attempt was to work on implementing the IDP Algorithm presented in  Monroe, B. L., Colaresi, M. P., &amp; Quinn, K. M. (2008). <em>Fightin' words: Lexical feature selection and evaluation for identifying the content of political conflict. </em><font size="2">Political Analysis, 16(4), 372-403. </font><br /><br />In doing so, I was able to find the weighted log odds ratio of each word present in both ND and NT posts, ultimately showing which type of subreddit each word was 'more affiliated' with. The findings were as one might expect, especially with my previous baselines and were in line with the results from those. As seen below we see some familiar words within the ND (I, you, <strong>she</strong>​) and NT (http) - so sorry for the ugly terminal output, but I need to find a prettier CSV presentation:</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 50%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-02-at-12-42-36-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 50%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-02-at-12-43-09-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph">This was a good first step, and will need some more work hashing out some final implementation details, but my next step in making this an actual advanced model, is to now utilize some of that reddit data that I've been harvesting for the past week or so. With that, we have a lot more data and might need to make some changes on the subreddit subsets depending on how the data has developed (changes in sentence length and number of total number of posts in each subreddit). Off to more data!!!</div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1">May 01, 2018 07:00 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 25, 2018</h2>

<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=323">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/25/nlp-capstone-post-5-a-new-hope/">NLP Capstone Post #5: A New Hope</a></h4>
<div class="entry">
<div class="content" lang="en">
<h1><span style="font-weight: 400;">Dataset Improvements</span></h1>
<p><i><span style="font-weight: 400;">Last time, on Music NLP.</span></i><span style="font-weight: 400;"> We ran into many midi data parsing issues. Since then, we have discovered a new dataset called the Lakh MIDI Dataset (</span><a href="http://colinraffel.com/projects/lmd/"><span style="font-weight: 400;">http://colinraffel.com/projects/lmd/</span></a><span style="font-weight: 400;">) that comes with reasonably well-formed midi files. Using the “Clean MIDI Subset”, we found thousands of midi files with their associated song names and songwriters. From these midi files, we extracted all with nonempty “lyric” fields when parsed via the pretty_midi package (which, incidentally, is also developed by Colin Raffel). After this step, we were left with ~1200 midi files that contain lyrics.</span></p>
<p><span style="font-weight: 400;">We currently clean the lyrical content by removing all unusual symbols and setting all characters to lowercase. We leave all lyrical tokens as is, which typically means syllable. Due to the inconsistent quality of the MIDI annotations, many songs are tokenized instead to characters, words, or even sentences. We will explore other methods for processing data if this is not sufficient for our results.</span></p>
<p><span style="font-weight: 400;">It is unfortunate we did not find this dataset sooner, because most of our challenges up to this point have been dealing with the poor quality of the gathered data.</span></p>
<h1><span style="font-weight: 400;">Alignment</span></h1>
<p><span style="font-weight: 400;">For our task of producing karaoke style output, there are two main tasks we have to solve. The first task is the generation of plausible lyrics, and the second is to align the lyrics to the proper time along the musical data. The alignment task has been studied extensively, but specifically aligning lyrical content to MIDI has not been covered in literature we have found. The most common alignment task is lyrics to audio data, as opposed to MIDI. The other common task is to align audio data to the notes defined in a MIDI file. In [1], they show a method that takes a MIDI file with annotated lyrics and uses this to align the lyrics to the raw audio. Unfortunately this is not our task, because we are trying to generate the annotated MIDI.</span></p>
<p><span style="font-weight: 400;">This week, we have decided to ignore the alignment task and focus primarily on making a reasonable lyrical model. We will return to alignment next week.</span></p>
<p><span style="font-weight: 400;">The next step was to align the lyrics with pianoroll. Fortunately, well-formed midi data parsed into PrettyMIDI objects come with a “get_piano_roll” function that takes as input a list of “times” which correspond to where in time pretty_midi will attempt to sample the music. As each syllable in the lyrics comes with a start time for when the singer enunciates it, we can pass in these start times to produce pianoroll that is aligned (up to small error) with the lyrics.</span></p>
<p><span style="font-weight: 400;">For some implementation reasons that are difficult to explain in English, it is possible for “get_piano_roll” to produce NaN entries, which we have replaced with zeros. Due to this and the potential for other such problems, we have forked the pretty_midi package and will be able to modify the code for our needs. For example, as pointed out in [2], “in a given MIDI file there is no reliable way of determining which instrument is a transcription of the vocals in a song”. As such, there are many choices for how to do alignment; pretty_midi has implemented just one. It is an interesting task to see how different alignment methods help or hurt our models.</span></p>
<h1><span style="font-weight: 400;">Lyric prediction</span></h1>
<p><span style="font-weight: 400;">Now that we have aligned pianoroll to lyrics data, we can begin engineering the model. Last time, we used an LSTM to generate lyrics given starting characters. Here, we will again use LSTMs, but instead, work at the syllable level and take as input the pianoroll of a song. As each column of a pianoroll is a time slice, each input vector to the LSTM is a single time slice. Each time slice is a 128-dimensional vector, with each entry representing the activation of an instrument; there are 128 midi recognized “instruments”.</span></p>
<p> </p>
<p><span style="font-weight: 400;">All that is left is to play with the architecture. </span></p>
<p><img alt="RNN model" class="alignnone size-full wp-image-322" src="https://mathstoc.files.wordpress.com/2018/04/rnn-model1.png?w=676" /></p>
<p><span style="font-weight: 400;">At the moment, our pipeline looks like what is shown in the diagram. At each iteration, we take a song, extract the lyrics and the corresponding pianoroll data. We then feed each time slice of the pianoroll data through an encoder unit, then through an LSTM unit, then through a decoder unit, and finally through a softmax to produce the prediction. Our loss is the negative log-likelihood (negative logarithm of the RNN softmax probability of the true syllable).</span></p>
<p><span style="font-weight: 400;">We will compare our final model to this baseline with respect to the loss on a held-out validation set. We will also experiment with loss functions other than cross entropy to see how it affects the actual lyrical output.</span></p>
<h1><span style="font-weight: 400;">Model results</span></h1>
<p><span style="font-weight: 400;">We have so far only trained our model for a single iteration over the training set. For an empirical evaluation on the current model quality, we ran a single MIDI through the input and computed the argmax word for each output. This produced a result in which every predicted lyric was an empty message, which is the most common string in the training set. We will explore methods to handle this class imbalance as our next task.</span></p>
<h1>References</h1>
<p><span style="font-weight: 400;">[1] Müller, Meinard &amp; Kurth, Frank &amp; Damm, David &amp; Fremerey, Christian &amp; Clausen, Michael. (2007). Lyrics-Based Audio Retrieval and Multimodal Navigation in Music Collections. 4675. 112-123. 10.1007/978-3-540-74851-9_10.</span><br />
<span style="font-weight: 400;">[2] </span><span style="font-weight: 400;">Raffel, Colin and Daniel P. W. Ellis. “Extracting Ground-Truth Information from MIDI Files: A MIDIfesto.” </span><i><span style="font-weight: 400;">ISMIR</span></i><span style="font-weight: 400;"> (2016). </span><span style="font-weight: 400;"> </span></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/25/nlp-capstone-post-5-a-new-hope/">by Nicholas Ruhland at April 25, 2018 04:44 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 11, 2018</h2>

<div class="channelgroup">







<h3><a href="https://cse481n.blogspot.com/" title="PrimeapeNLP">Ron Fan, Aditya Saraf <br/> Team PrimeapeNLP</a></h3>


<div class="entrygroup" id="tag:blogger.com,1999:blog-3753031463594823927.post-4531878816260312232">
<h4><a href="https://cse481n.blogspot.com/2018/04/blog-post-3.html">Blog Post #3</a></h4>
<div class="entry">
<div class="content">
<h1 dir="ltr" id="docs-internal-guid-ea0c9d97-b369-9237-6f13-3675807d7a60" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Project Objectives</span></h1><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Single document summarization (SDS) is one of the remaining challenging problems in natural language processing. Novel methods are presented frequently in new papers, but they often do not include specific code allowing for reproducibility and are evaluated on specific datasets that make comparisons between models meaningless and difficult.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">There are many approaches to SDS, but they can be broadly divided into combinatorial approaches and neural approaches. Neural approaches build a neural architecture, such as a seq2seq/encoder-decoder model or single sequence RNNs. Combinatorial approaches will either try to frame the problem as an optimization problem, and then use an ILP solver, or frame the problem as a classic NP-hard problem, like Knapsack or Maximum Coverage. We want to explore both approaches, and compare their performance on the same dataset.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">SDS is comprised of two tasks: extractive summarization and abstractive summarization. Extractive summarization compiles a summary by selecting sentences from the document’s text while abstractive summarization generates text for the summary (sentences that may not have been present in the document’s text). While abstractive summarization might have more intuitive appeal, our project will focus on extractive summarization to enable meaningful comparisons between neural and combinatorial approaches (combinatorial approaches often must be extractive).</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">In this project, we plan to implement at least one neural and one combinatorial model for extractive single document summarization. We hope to establish some meaningful ways to compare the differences between selections made by the different types of models. Our primary goal is to better understand the strengths and weaknesses of neural and combinatorial models for single document summarization - a particular important aspect of SDS given the general roughness of existing evaluation metrics. We will gauge our progress based on reaching acceptable performance on commonly-used evaluation metrics when we implement models.</span></div><h1 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Methodology</span></h1><h3 dir="ltr" style="line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Minimal Viable Action Plan</span></h3><ol style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Build our data set using existing data. Specifically, convert data better suited for training abstractive summarization models into data that can be used for extractive summarization..</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Implement a simple combinatorial model (for example, we can do a simple maximum coverage problem, where we set up the “universe” to be the vocabulary of the document, and treat the sentences as sets of words).</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Implement a simple neural model (just treat the problem as a generic binary classification problem).</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Train models on identical data sets and do a baseline comparison -- how well does a simple neural model do vs. a simple combinatorial model? This doesn’t tell us much about the relative strengths of the two approaches (we can’t quantify “simple”), but with some error analysis, we might be able to see what sentences neural models are misidentifying vs. what sentences combinatorial models are misidentifying.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Build at least one state-of-the-art combinatorial model (adapting from a recent paper). We have two candidate papers: Hirao et al.’s Tree Knapsack approach and Durrett et al.’s Compression/Anaphoricity</span></div></li></ol><br /><h3 dir="ltr" style="line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Stretch Goals</span></h3><ol style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Design our own model that is a combination of the strong points of the combinatorial and neural models. Ideally, our model would be as good as or better than the existing models we implemented on the quantitative and qualitative metrics we use.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Alternatively, we can use ideas from one domain to improve an aspect of a SOTA model in the other domain. For example, we might learn that neural models are great at dealing with named entities, and so incorporate a neural layer in a combinatorial model (perhaps by allowing the output of the neural layer to determine the weights of named entities).</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Design a common system for comparing performance of extractive summarization models. Rather than a differentiable evaluation metric, we think it may be useful to choose a set of “tough” documents to summarize and bundle them together with specific reasons for their difficulty, so that researchers may more easily identify weaknesses in models they are working on.</span></div></li></ol><h1 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Available Resources</span></h1><h3 dir="ltr" style="line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Dataset/Evaluation</span></h3><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">For this problem, we will be using the DailyMail/CNN dataset. From our initial research, this seems to be the standard dataset for both document summarization as well as basic reading comprehension. The dataset has 400,000 articles, and includes both the full text of the article as well as bullet point “highlights”. For reading comprehension, an important word is omitted from the highlights and the machine is asked to fill in the blank. For text summarization, the bullet points are considered the “gold standard” summaries -- machine generated summaries are evaluated against the bullet points, typically</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"> </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">using ROUGE metrics</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"> </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">(Lin, 2004). While this works fine for abstractive summarization, this training corpus is not annotated enough for extractive summarization. More specifically, extractive summarization requires sentence level binary annotations, to indicate whether each sentence does or doesn’t belong in the summary. So we need to first convert the bullet points into more fine grained annotations.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">We’ve looked at two papers which briefly touched on this. Nallapati et. al. used a greedy approach, where they added one sentence at a time to the extractive summary while seeking to maximize the Rouge score with respect to the abstractive summary (the bullet points). They also tried to use an RNN decoder in combination with the abstractive summaries to train the extractive model without using sentence-level annotations. However, this approach was slightly less successful than estimating sentence-level annotations. Cheng and Lapata used a different approach - they created a “rule-based system that determines whether a given sentence matches a highlight...The rules take into account the position of the sentence in the document, the unigram and bigram overlap between document sentences and highlights, [and] the number of entities appearing in the highlight and in the document sentence”. It’s not 100% clear what rules the authors used, but according to Nallapati et. al., the rule-based approach found a better “ground-truth” than the greedy approach.</span></div><h3 dir="ltr" style="line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">GitHub Repositories</span></h3><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Some researchers publish the code they used in their paper on GitHub. We can use repos for quick comparisons or to see how they design their code.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><a href="https://github.com/abisee/pointer-generator" style="text-decoration: none;"><span>https://github.com/abisee/pointer-generator</span></a><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">: This repo is for See et al.’s Pointer-Generator neural model.</span></div><br /><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><a href="https://github.com/cheng6076/NeuralSum" style="text-decoration: none;"><span>https://github.com/cheng6076/NeuralSum</span></a><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">: This repo is for Cheng and Lapata’s neural model, that combines a sentence level RNN with a word level CNN.</span></div><h1 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Related Work and References</span></h1><br /><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Cheng, J., &amp; Lapata, M. (2016). Neural Summarization by Extracting Sentences and Words. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">arXiv:1603.07252 [Cs]</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">. Retrieved from http://arxiv.org/abs/1603.07252</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Durrett, G., Berg-Kirkpatrick, T., &amp; Klein, D. (2016). Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">arXiv:1603.08887 [Cs]</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">. Retrieved from http://arxiv.org/abs/1603.08887</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Hirao, T., Yoshida, Y., Nishino, M., Yasuda, N., &amp; Nagata, M. (2013). Single-Document Summarization as a Tree Knapsack Problem. In </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"> (pp. 1515–1520). Seattle, Washington, USA: Association for Computational Linguistics. Retrieved from http://www.aclweb.org/anthology/D13-1158</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. In S. S. Marie-Francine Moens (Ed.), </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"> (pp. 74–81). Barcelona, Spain: Association for Computational Linguistics.</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Nallapati, R., Zhai, F., &amp; Zhou, B. (2016). SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">arXiv:1611.04230 [Cs]</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">. Retrieved from http://arxiv.org/abs/1611.04230</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">See, A., Liu, P. J., &amp; Manning, C. D. (2017). Get To The Point: Summarization with Pointer-Generator Networks. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">arXiv:1704.04368 [Cs]</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">. Retrieved from http://arxiv.org/abs/1704.04368</span></div></div>







<p class="date">
<a href="https://cse481n.blogspot.com/2018/04/blog-post-3.html">by Ron &amp;amp; Aditya (noreply@blogger.com) at April 11, 2018 06:33 AM</a>
</p>
</div>
</div>


</div>

</div>


<div class="sidebar">

<h2>Subscriptions</h2>
<ul>
<li>
<a href="https://medium.com/feed/@viterbi.or.not" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a>
</li>
<li>
<a href="https://deeplearningturingtest.wordpress.com/feed/" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a>
</li>
<li>
<a href="https://medium.com/feed/@be.li.nda" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://medium.com/@be.li.nda?source=rss-fad49d942bf3------2" title="Stories by Belinda Zou Li on Medium">Belinda Li <br/> Team Sentimentity</a>
</li>
<li>
<a href="http://cse481n-capstone.azurewebsites.net/feed/" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="http://cse481n-capstone.azurewebsites.net" title="Team Watch Your Language!">Boyan Li, Dennis Orzikh, Lanhao Wu <br/> Team Watch Your Language!</a>
</li>
<li>
<a href="https://medium.com/feed/@halden.lin" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" title="Stories by Halden Lin on Medium">Halden Lin <br/> Team undef.</a>
</li>
<li>
<a href="https://mathstoc.wordpress.com/category/nlp-capstone/feed/" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a>
</li>
<li>
<a href="https://teamoverfit.blogspot.com/feeds/posts/default?alt=rss" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a>
</li>
<li>
<a href="https://nlpcapstonesemparse.blogspot.com/feeds/posts/default?alt=rss" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://nlpcapstonesemparse.blogspot.com/" title="NlpCapstone">Rajas Agashe <br/> Team Han Flying Solo</a>
</li>
<li>
<a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://cse481n.blogspot.com/" title="PrimeapeNLP">Ron Fan, Aditya Saraf <br/> Team PrimeapeNLP</a>
</li>
<li>
<a href="https://medium.com/feed/@ryanp97" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2" title="Stories by Ryan Pham on Medium">Ryan Pham <br/> Team NeuralEmpty</a>
</li>
<li>
<a href="http://sarahyu.weebly.com/6/feed" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a>
</li>
<li>
<a href="https://medium.com/feed/nlp-capstone-blog" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a>
</li>
<li>
<a href="https://medium.com/feed/@hongnin1" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://medium.com/@hongnin1?source=rss-c450eb982161------2" title="Stories by Ning Hong on Medium">Zichun Liu, Ning Hong, Sujie Zhou <br/> Team The Bugless</a>
</li>
</ul>

<p>
<strong>Last updated:</strong><br>
August 14, 2018 10:34 PM<br>
<em>All times are UTC.</em><br>

<!--
<br>
Powered by:<br>
<a href="http://www.planetplanet.org/"><img src="images/planet.png" width="80" height="15" alt="Planet" border="0"></a>
</p>

<p>
<h2>Planetarium:</h2>
<ul>
<li><a href="http://www.planetapache.org/">Planet Apache</a></li>
<li><a href="http://planet.freedesktop.org/">Planet freedesktop.org</a></li>
<li><a href="http://planet.gnome.org/">Planet GNOME</a></li>
<li><a href="http://planet.debian.net/">Planet Debian</a></li>
<li><a href="http://planet.fedoraproject.org/">Planet Fedora</a></li>
<li><a href="http://planets.sun.com/">Planet Sun</a></li>
<li><a href="http://www.planetplanet.org/">more...</a></li>
</ul>
</p>
!-->
</div>
</body>

</html>
