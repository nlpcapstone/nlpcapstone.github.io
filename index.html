<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>NLP Capstone Spring 2018</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="planet.css" type="text/css">
<link rel="alternate" href="https://nlpcapstone.github.io/atom.xml" title="" type="application/atom+xml">
</head>

<body>
<h1>NLP Capstone Spring 2018</h1>

<div class="daygroup">
<h2>May 29, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/last-update">
<h4><a href="http://sarahyu.weebly.com/cse-481n/last-update">Last Update</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">The final update for the project! Unfortunately, there is not as much to report on results as usual and not as many fun visualizations. For the 2nd Advanced Model, my plan was to begin on the stretch goals I had initially outlined and train a neural model for Reddit Post classification and Generation. The idea took cue from the Affect-LM paper. Basically it would be similar to this model</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 13.331751602564%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 68.227199377828%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-29-at-9-42-04-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 18.441049019608%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph">which has inputs of the context words, on which to build up the rest of the sentence from; the Affect category which is chosen beforehand to generate the desired output; and an Affect strength to determine the intensity of the affect category defined. <br /><br />My model would be similar to this, but instead look more like the following where we take out the strength factor and choose the mental category  to be fed into the Mental LM.</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 24.666352941176%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 56.902274509804%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/published/screen-shot-2018-05-29-at-10-56-19-pm.png?1527659857" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 18.43137254902%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph">My current model seems to have some issues generating posts containing any language beyond the exact topic of the mental category itself (i.e. using the word depressed for the F30 category), with the model using the cross entropy loss. <br /><br />At this point, the game plan is to fix the bugs and get a working model to test out the post generation. The results will be interesting to see in and of themselves, but I will also compare the language model of the generated posts against the metrics we hav seen throughout the quarter (vennclouds, idp). </div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/last-update">May 29, 2018 05:35 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 23, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a></h3>


<div class="entrygroup" id="https://medium.com/p/d4957f6e802">
<h4><a href="https://medium.com/@viterbi.or.not/advanced-model-2-part-2-d4957f6e802?source=rss-c522ef075bb3------2">Advanced Model #2, Part 2</a></h4>
<div class="entry">
<div class="content">
<p>Now that we’ve finished our minimum viable product, for Part 2 we’re focusing on further analysis and improvement of our advanced model, working on a demo to showcase our results, and drawing up a final action plan to wrap up the project!</p><h4><strong>Demo</strong></h4><p>One of the big things that we have been working on in the past week is a demo to illustrate a possible use case for our model. Our goal for the demo is ultimately to provide an interface in which users can simulate having a conversation or opening a long conversation and subsequently being able to access a useful summary of the topics and ideas that were discussed.</p><p>While coming up with our concept for the demo, we had to first make a choice about the input format that would be presented to users. After much discussion, we decided that the interface would have the most impact if it could operate on “live” data — that is, rather than simply uploading an existing conversation, if users could interact with a summary in realtime during a conversation it would help show exactly how our model works. Furthermore, although we set out to explore this project mainly for its technical merit and to understand both the universal features of language and the techniques used in summarization, we believe that our findings would be most directly useful as an augmentation for an application that already handles conversations. That way, a user has the opportunity to get immediate assistance after falling behind in a group conversation, and to explore their records of conversations via summaries rather than perusing the entire conversation.</p><p>All of these requirements, from the realtime interaction to the use case as an augmentation to an existing app, pointed us toward using chat data for the demo. We ultimately decided to create a simplistic chat interface that would allow users to stage a conversation and generate summaries directly from that — our idea being that generating the summary, chatting more in the conversation, and then seeing how the summary changes would be the most effective feedback loop for allowing users to understand what our model is doing.</p><p>As it currently exists, our demo is extremely bare-bones and serves as a proof of concept more than anything else. Users can chat in realtime, and summaries can be requested using an API call to pull the summary from a pre-trained model in our backend. We are currently working on improving the interface to be more usable, and on hosting the demo so that people can give it a try in more situations than just locally from within our repository.</p><h4><strong>Plans for Demo Expansion: Summary Visualization</strong></h4><p>In order to make the summary more usable, we want to “weave” it into the conversation so that it can serve as an augmentation rather than a totally separate feature. After all, because our model currently produces only extractive summaries, it makes sense to tie the exact sentences chosen for extraction back to the conversation. Right now, we are working on making the summary interactive in the sense that a user can click the sentences in the summary in one pane and jump to those points in the conversation, using the summary almost as a table of contents. We believe this design showcases our summary effectively and uses it as a convenient entry point for the user while still allowing them to get more detailed information if needed.</p><p>Another possibility would be to allow the user to adjust the compression ratio used in the summary, to specify if they want the summary to be only 10% of the conversation or 50%. This has the possibility of making it more user-friendly because they can create a summary for a specific use case, such as an extremely high-level summary being more useful for someone’s records as a simple reminder of a conversation they have already read, while a more in-depth summary might serve better in the use case of catching up on a conversation that has had many messages since the user last visited. However, there is also a downside to this option — by allowing users to specify a hard and fast percentage, our model loses the ability to dynamically adapt the length of the summary based on what is important. In other words, if there are n sentences available in the summary and n+1 nearly equally important sentences to fully capture the thread’s meaning, enforcing the n sentences would cause the summary to lose value while using an absolute threshold would make the summary longer if there is truly more content to display.</p><h4><strong>Plans for Expansion: Feature Visualization</strong></h4><p>One additional thing we have been exploring is the option for users to visualize the factors that lead to a sentence being selected for the summary. This would be a more technical option than those described above, because it would expose the actual features our model uses. To make this work, we would need to come up with a reliable way to calculate the “basis” for each sentence’s classification, which is a somewhat nebulous concept and therefore something we are still discussing. In general, it is possible to determine the weight that our model places on each of the features, but we would need to also understand the boundary used for each feature in order to provide useful information about a specific sentence. For example, consider a feature like the intensity of the sentiment score for a given sentence: we might be able to determine that our model places a higher-than-average weight on this feature, but we would need information about whether a higher or lower sentiment score is more likely to lead to classification as part of the summary in order to display to the user whether or not that had led to the specific sentence’s labelling. In addition, the high number of features we have in our feature set threatens to overwhelm any interface we include them in.</p><p>If individual features for sentences cannot be applied, another approach we have considered is displaying information such as the topic segmentation that has been applied or which sentences have been modified based on our preprocessing. Doing so would allow the user of our demo to easily understand how our model responds to certain types of data, which would also allow us to potentially debug our model as we make improvements over the next few weeks.</p><h4><strong>Process of Chat Integration</strong></h4><p>Because a large part of our overall project plan is to incorporate both chatlog and email data, a lot of our effort over the past few weeks has been in standardizing our feature set and project pipeline to be able to support chat data. While a big portion of that has been focused on parsing and preprocessing in order to be able to train and evaluate on chat data, this past week we have been running tests to understand the differences between the different types of data and the limitations of using the enormous chatlog dataset.</p><p>In particular, we noticed that despite having so much data available, the chatlog dataset was effectively unusable as a training set when using a Naive Bayes model. Although that model had previously been most effective for almost all of our feature sets, we noticed that when applied to the chatlog data it had the distinct tendency to produce summaries in which 0 (or very, very few) sentences were selected. This problem did not show up using Decision Trees, which had decent results based on both ROUGE scores and human evaluation.</p><p>Although we are still not entirely sure about the cause of the issue with Naive Bayes, one possible explanation is that the number of sentences included in the reference summary for each training thread is so small that the model ended up having the best chance of classifying a sentence correctly by simply discluding every sentence. It is possible that the structural differences between the Naive Bayes and Decision Tree models caused the latter model to create trees of such a depth that they were able to learn the difference between included and excluded sentences even with so few examples. We definitely have more analysis to do in the coming weeks to understand this disparity better, but more than anything else it alerted us to the fact that the vastness of the chatlog data is a double-edged sword — although it provides more data on which to train, it also brings a new problem of extremely sparse reference summaries.</p><h4><strong>Fixing using Regression SVM</strong></h4><p>Normally, using Decision Trees over Naive Bayes would not be a major setback, because although Naive Bayes performed slightly better for most applications in the email dataset they were relatively comparable. However, with the introduction of the compression ratio that we discussed in last week’s blog post, we found that additional work was necessary to use the compression ratio with the chatlog data. Because the Naive Bayes regression implementation we had been using never labelled sentences as being part of the summary during our testing, we eventually decided to incorporate SKLearn’s SVM Regression model in order to provide regression functionality for the chatlog data and enable us to manually tune the compression ratio.</p><h4><strong>Participant Tracking</strong></h4><p>In an effort to capture more conversation-specific aspects of the data, we added a feature to take into account the author of a sentence when vectorizing. The author frequency feature is calculated by taking the count of contributions from the sentence’s author to the thread and normalizing it by the total number of sentences. This measure represents the amount of weight a participant has in a conversation which can translate to indicating how important the contributions of specific participants are.</p><h4><strong>Discourse Tracking</strong></h4><p>To track conversation flow, we also added a feature to give more conversational context to the current sentence. The previous TF-ISF feature is the TF-ISF score of the chunk that the sentence is in reply to, giving it more information about the conversation up to the current point. If a sentence is in reply to an important chunk, it may indicate that the sentence should also be included in the summary.</p><h4><strong>Plans for Abstractive Summarization</strong></h4><p>To tackle our stretch goal of generating abstractive summaries, we would most likely take the approach of using the extracted sentences along with corresponding metadata as a starting point as we have mentioned before. More specifically, we would like to capture the authors of each sentence so we can create sentences that do a better job of describing the conversation (for example, “Person A said &lt;extracted content&gt;. Person B then replied…” and so on).</p><p>We have also thought about various strategies to generate more natural text from the extracted sentences. These include training and using a language model on the abstractive summary annotations that some of our datasets have, looking at techniques for sentence compression, and applying tense correction.</p><p>Sentence compression would help us progress beyond needing to use direct quotes from the corpus in our summary by being able to get a short and sweet version of the sentence that still retains its main meaning and purpose. This could mean, for example, reducing a sentence like “I would really like to have a delicious cup of coffee” to simply “I want coffee.”</p><p>Applying tense correction would partner with the idea of including who said what in the abstractive summary. For example, if Person A says “I want coffee” adding the author attribution would result in a sentence like “Person A said I want coffee.” To make this a more natural sentence, we can correct the first person “I” to a third person “they” — resulting in “Person A said they want coffee” which would read better in an abstractive summary.</p><h4><strong>Plans for Semantic Features</strong></h4><p>Something we have noticed about our current feature set is that it does not include any features based off of semantic properties of the sentences. Being able to determine the purpose of a sentence within a chunk or thread would be really helpful information for deciding its importance to the conversation. For example, sentences in an email can be broken down into categories like ‘Greeting,’ ‘Background / Context,’ ‘Problem,’ ‘Elaboration,’ ‘Solution,’ and ‘Sign-off.’ Some of the more obvious uses of these categories would be inferring that Greeting and Sign-off are not meaningful content in the conversation and should not be included in a summary.</p><p>To do this, we would like to look more into incorporating Rhetorical Structure Theory (RST), particularly in reference to the relations between sentences in a chunk, to identify the category of a sentence. We can also approach this feature by looking for certain cue words (perhaps words like “because,” “if,” “since,” “so”) to categorize the sentence.</p><p>Overall, we’ve had a lot of fun (and learned a lot!) working on this model, and we hope you enjoyed reading about it too!</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d4957f6e802" width="1" /></div>







<p class="date">
<a href="https://medium.com/@viterbi.or.not/advanced-model-2-part-2-d4957f6e802?source=rss-c522ef075bb3------2">by Viterbi Or Not To Be at May 23, 2018 06:58 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" title="Stories by Halden Lin on Medium">Halden Lin <br/> Team undef.</a></h3>


<div class="entrygroup" id="https://medium.com/p/b7c31ac45ecc">
<h4><a href="https://medium.com/@halden.lin/nlp-capstone-09-any-summary-b7c31ac45ecc?source=rss-2759d54493c0------2">NLP Capstone | 09: Any Summary</a></h4>
<div class="entry">
<div class="content">
<p><em>previous posts: </em><a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5"><em>01</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5"><em>02</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3"><em>03</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7"><em>04</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a"><em>05</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0"><em>06</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-07-formalizing-a2d837ecf66b"><em>07</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-08-human-summaries-33072535817f"><em>08</em></a></p><p>In my last post, I stated a main goal of mine was to visualization <strong>human summaries</strong>. After talking with Prof. Jeff Heer this past week, I’ve developed a more concrete goal for this segment of my project.</p><p>If we are able to develop a method for approximating human ‘attention’ between source and summary, we can use it in the following ways.</p><ol><li><strong>Evaluation tool.</strong> Current evaluation requires reading article, summary, and thinking critically to map between the two in order to determine whether or not the summary is ‘good’.</li><li><strong>Enable cross-model comparison and analysis.</strong> How do different models produce summaries for the same article? Automatic measures, such as Rouge and Meteor, are generally poor indicators of proper quality. Currently, one may read summaries and source text and attempt to qualify proper coverage of key ideas. By introducing a visualization that can be generated from <strong>any</strong> source-summary pair, we can enable more principled analysis.</li><li><strong>Enable model to human comparison and analysis.</strong> This I discussed in the previous post. What do human summaries have that our models are missing? Missing coverage? Missing entities? This visualization tool could answer these questions.</li></ol><p><strong>In general, this tool would allow researchers to gain insights about both human and machine summaries.</strong></p><p>With this in mind, I’ll go into the approaches I’ve been experimenting with in the past week.</p><h4>Hierarchical Similarity</h4><p>Last week, I attempted token-on-token similarity. The results can be seen the gif below. The weight between input and output token <em>x </em>and <em>y</em>, respectively, can be described as so:</p><p><em>a(x, y) = similarity(x, y)</em></p><p>Where similarity is calculated using a standard word embedding API (in this case, <a href="https://spacy.io/">spaCy</a>). The issue with this approach was that context is lost, and so a word will often attend to nearly the entire document with no regard to the ideas coming out of each portion (in summaries, we expect a sentence or phrase to summarize a specific part or few parts of the original document).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*-ganHe0RsisBYzaPKOuHhA.gif" />Token-on-token similarity pays no heed to context — problematic.</figure><p>In attempt to remedy this, I added a factor to each weight that represents the similarity of the tokens’ respective sentences. That is, the weight of a given <em>x, y</em> pair is determined by the similarity of the sentence of <em>x </em>and the sentence of <em>y</em>, multiplied by the similarity of the tokens themselves. To both normalize weights (over output token) and exaggerate salient pairs, I also add a soft-max transformation for each similarity score. The equation below describes this formula.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WE0z9rPYF_Nb_X4Y." /></figure><p>The <em>theta</em> terms here are important in properly exaggerating salient pairs, and so require some tuning.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*QJFNJ0ty3NimB1nBAnTdXg.gif" />Hierarchical Similarity shows some promise, but has a few issues.</figure><p>This approach shows some promise. Context is taken into account, at least at a sentence-by-sentence level. However, there are a few shortcomings that become apparent with more abstractive summaries. In particular:</p><ul><li><strong>Context is better but far from perfect.</strong> Sometimes ideas span multiple sentences, difficult to model. Additionally, repeating words in a sentence get equal ‘attention’ even though one may make more sense from a token-by-token generation standpoint.</li></ul><p>I’ll be exploring this approach further in the next week, but I have concerns about its ability to generalize well, per issues described above.</p><h4>Hidden Markov Model</h4><p>At a high level, we can imagine ‘attention’ as the words and phrases from the source text to that one would draw from to write a portion of a summary. This makes sense: we tend to focus on specific areas of a document at a time when writing summaries. Breaking this into token-by-token time-steps, summary token is <strong>conditioned</strong> on the ‘attention’ vector for that time-step.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/151/1*6wNP-KSn13tGSSwDjmDXxw.png" />Summary tokens are conditioned on attention vectors over the source text.</figure><p>Further, we can reason that attention vectors change from time-step to time-step, dependent on the previous attention vector.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/361/1*krjZPlauErnawaKymE5LAA.png" />Attention vectors are conditioned on each other.</figure><p>This of course is an simplification — the way our minds work is likely far more complex — but it allows us to model the ‘attention’ between source and summary as a Hidden Markov Model (HMM).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/611/1*3wfXRV8pMJZQ74Ux1OGjDA.png" />Source to summary modeled as a Hidden Markov Model.</figure><p>We can then use this model to predict attention vectors at each time-step (e.g. Viterbi, Forward-Backward). This is similar to how HMMs are used to predict part-of-speech tags (where POS tags are conditioned on each other and tokens are conditioned on those tags). Emissions (the edge weight going from distribution to summary token) can be defined by token similarity, but there are still a challenges here.</p><ol><li>How to define transition probabilities?</li><li>Treat attention states as distributions or single tokens (e.g. argmax in vector)?</li></ol><p>I’ll need to consider this approach further to see if I can work out these kinks.</p><h4>POS Tags</h4><p>I’ve also been slowly improving the visualization tool itself. I’ll briefly describe my progress on this front.</p><p>Using <a href="https://www.nltk.org/">NLTK</a>, I was able to part-of-speech tag machine-generated summaries. At the top right of the visualization, users are presented a panel of the POS Tags used by the <a href="https://catalog.ldc.upenn.edu/ldc99t42">Penn Tree Bank</a>, which NLTK sources from. Non-present tags are greyed-out.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*i87AaOJBtCzk4h_G5b5z9A.gif" />Users can highlight tokens to view the corresponding tag, or mouse over tags to highlight all corresponding tokens.</figure><p>This should allow more in-depth analysis of the attention vectors produced by the machine. Eventually I’d like to work towards highlighting named entities in the source / summary to allow users to identify present / missing ideas centered on important entities.</p><h4>Upcoming Work</h4><ol><li>Continue working on visualizing source-summary alignment.</li><li>Continue improving visualization.</li></ol><p>I have lots, lots, lots to do. Until next time!</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b7c31ac45ecc" width="1" /></div>







<p class="date">
<a href="https://medium.com/@halden.lin/nlp-capstone-09-any-summary-b7c31ac45ecc?source=rss-2759d54493c0------2">by Halden Lin at May 23, 2018 06:43 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=27">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/05/23/advanced-model-attempt-2-part-2/">Advanced Model Attempt 2 (Part 2)</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>This week I further tuned my hyperparameters and increased the exploration rate of my model so the slot-filling Q-values were fixed for all my questions. This made sure that my slot filling accuracy is perfect (when there’s no dropout). After tweaking the model, I tested and recorded the number of games it won using both the database it trained on as well as another random database of 100 people.</p>
<p>Training Database: Model won 90/100 games.</p>
<p>Validation Database: Model won 87/100 games.</p>
<p>As expected, the validation accuracy is not too far behind the training accuracy. While the NLU component of the model is just as accurate for both databases, the order of questions is more tailored towards the training database so the sequence of questions might be a bit inefficient for the other database meaning some additional games must have been lost on time. The reference paper I based this model on was trained using speech utterances and that model got about a 92% win rate (although they didn’t specify whether the database they evaluated on was different from the one they trained on). Therefore, my model is not quite as well trained as theirs but it’s still not too far behind in accuracy.</p>
<p>Now that I have my minimal action plan working, this week I’ll work on my final report and presentation as well as try to make this model work for a more generic dialogue scenario as a possible demo.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/05/23/advanced-model-attempt-2-part-2/">by ananthgo at May 23, 2018 06:34 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/ef93c2149aa2">
<h4><a href="https://medium.com/nlp-capstone-blog/advanced-model-update-from-definition-extraction-to-entity-discovery-ef93c2149aa2?source=rss----9ba3897b6688---4">Advanced Model Update: From Definition Extraction to Entity Discovery</a></h4>
<div class="entry">
<div class="content">
<p>Over the past few weeks, developing a dataset to test our model and flesh out this novel task has proven to be a difficult task in itself. Here, we discuss what worked, what didn’t work, and how the development of our dataset has influenced our perspective of the task, and ultimately, what we will now expect out of our advanced model.</p><p>To recap, we began making preliminary version of our dataset using ROUGE, cosine similarity, and skip-bigrams. In particular, given a definition-document pair, we aimed to extract sentences from the document that were most conducive to describing and re-creating the definition. From there, our model can compute latent representations of the term, sentences, and document as a whole, in order to learn how to extract these sentences we’ve chosen.</p><h3>The Heuristics that Failed</h3><p>Unfortunately, we can’t all be winners.</p><h4><strong>Cosine Similarity</strong></h4><p>We used <a href="https://spacy.io/usage/vectors-similarity">spaCy’s</a> implementation of cosine similarity using context vectors. Admittedly, cosine similarity does a great job of ruling out sentences that have nothing to do with the current definition when extracting. However, there was much too little variation in the scores that sentence-definition pairs received. Often, they would range from 0.80 to 0.94, and tended to cluster around 0.83–0.87 and 0.90 to 0.93. Not only are so many sentence-definition pairs scoring so highly, it becomes a very fine line between what we should keep and what we shouldn’t.</p><p>We attempted to be extremely strict and only keep pairs that scored 0.94; but this often led to many great pairs being ruled out. We speculate that medical language in general tends to cluster together with respect to the rest of the vocabulary in which spaCy’s word vectors were trained. We also speculate that being a bag-of-words method in defining similarity, much of the richness in context and order that makes differences and similarity obvious at a glance are washed away. Despite the method clearly being able to separate contrived sentence-definition pairs, in the landscape of our data, it fails to draw the line the way we’d like it to.</p><h4>Skip-bigrams</h4><p>This heuristic in particular was troublesome in that, many UMLS definitions were 1–2 sentences long, while others were several paragraphs. So the idea of using the number of overlapping skip-bigrams between a sentence-definition pair will severely punish shorter glosses. Because of this, longer definitions having little relevance to a sentence may likely still match to it.</p><h3>The Heursitics that Worked</h3><p>After several attempts at tuning the above heuristics, we decided to look for more. The following heuristics are how our final dataset will be constructed.</p><h4>Google’s Top 10,000 Words</h4><p>There’s currently a <a href="https://github.com/first20hours/google-10000-english">repository</a> containing the top 1000 and top 10000 words according to n-gram frequency analysis of Google’s Trillion Word Corpus. In particular we are using the <strong>no swears </strong>list.</p><p>Given the roughly 800,000 glosses that UMLS provides us, we shave this down to roughly 165,000 by removing all definitions that contain an synonym that is also contained within the Google no-swears top 10k list. This drastically reduces our search space when creating examples, and ultimately we are okay with it since common words are trivial to define.</p><h4>First 15%</h4><p>Given a definition-document pair, only attempt extraction if at least one of the aliases (synonyms) that the definition defines occurs within the first 15% of the sentences.</p><h4>Word Embeddings</h4><p>Following the first two heuristics, given definition-document pairs that make it through these filters we then extract <strong>all</strong> sentences containing <strong>any </strong>of the alias for the document.</p><p>We then calculate a similarity score between each sentence and the gold standard definition of the term. We do so by using pre-trained word vectors, namely Glove vectors, to better represent sentences. Each sentence is represented as the average of all its word vectors and similarity is defined as the Euclidean distance between the gold standard vector and the sentence vector. Given these distances, we sort them and choose the smallest 5 sentences if there are that many. We believe that through this, we are using better representations of sentences as opposed to the heuristics we tried previously. Although we did consider training our own set of word vectors (the large size of the Semantic Scholar corpus would allow us to do this), we felt that given the time constraints, Glove vectors were sufficient for now.</p><p>We then filter out the document to include only mentions of the entity that we are trying to extract (or its aliases). This approach is made possible by UMLS pairing all definitions with all of the aliases that it defines.</p><p>After choosing the sentences for each term-document pair, we then incorporate aliases when creating the final training examples. Recall that each training example includes a term, a gold standard definition, sentences within the document, and the target vector. In order to encourage the model to associate synonyms with each other, we can swap out the terms and its aliases in the target sentences, randomly inserting an alias or the term in places that another alias or term might be. This will not only give us a way to produce more training examples, it will also help the model understand the contexts of similar words, which might help it discover entities.</p><h3>Reframing the Problem to Entity Discovery</h3><p>Originally, our task was to generate definitions of entities consistent with our corpus. We then reframed the task as an extractive process.</p><p>The dataset described above however, will allow us to solve a task that could be described as a ‘superclass’ of definition extraction, which essentially aims to extract all sentences <strong>relevant</strong> to an entity as opposed to only sentences that help <strong>define </strong>it. We call it ‘Entity Discovery’, a term coined by AI2 when they originally proposed this type of task during the early stages of the capstone. Given our ranking scheme, we will still tend towards selecting sentences conducive to definitions, but we’re not quite confident enough that every sentence our heuristics will choose resemble a definition or add to one.</p><p>Rather, we now see the potential of our model (which, given this dataset, does not have to change at all!). This new dataset will allow us to train a model to learn latent representations of queries and map them to latent representations of sentences. Note that often times, chemical and medical terms have numerous aliases that take different but systematic forms. Given a reasonably trained model on such data, it should theoretically generalize to novel terms and learn what synonyms would look like and the contexts in which they would appear, ones that have not been added to KBs yet, aiding researchers and medical students to learn about ill-defined terms that have synonyms and reference that have not yet been fully documented.</p><p>We call this a ‘superclass’ of definition extraction presumably because, if we were successful at extracting all sentences pertinent to an entity, that definition extraction would simply take a subset of these sentences.</p><h3>In Conclusion</h3><p>It has taken some time and experimentation to find our footing in creating this dataset, but we’ve found it. The scripts are running, the data looks reasonable, and we are excited to finally see how our model will perform.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ef93c2149aa2" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/advanced-model-update-from-definition-extraction-to-entity-discovery-ef93c2149aa2">Advanced Model Update: From Definition Extraction to Entity Discovery</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/advanced-model-update-from-definition-extraction-to-entity-discovery-ef93c2149aa2?source=rss----9ba3897b6688---4">by Tam Dang at May 23, 2018 05:34 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 17, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a></h3>


<div class="entrygroup" id="https://medium.com/p/29b207b75065">
<h4><a href="https://medium.com/@viterbi.or.not/advanced-model-2-part-1-29b207b75065?source=rss-c522ef075bb3------2">Advanced Model #2, Part 1</a></h4>
<div class="entry">
<div class="content">
<p>This week, we started working on our second advanced model attempt! The chatlog data is fully integrated this time, though we are still working on making our model effective across all types of text conversation. In addition to finishing construction of our proposed pipeline, we’ve also upgraded almost all of the previously existing components (pre-processor, feature vectorizer, model) to make the first big steps towards completing our final advanced model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*FIza3SY9EreLeCf1." />Pipeline for integrating different datasets with a common feature set into our model, now completed!</figure><h4><strong>Adding the Chatlog Dataset — For Real This Time!</strong></h4><p>Achieving one of our major goals for the advanced model, we have finished integrating chatlog data into our model! This involved a considerable amount more effort than we anticipated because the chat data ended up being much more noisy and varied than the email data, leading to a significant portion of our time being spent re-formatting and parsing the original data files to get it into the same input structure as the email data.</p><p>Even in our initial testing, the impact from adding chatlog data has been noticeable. There are two major benefits that we have identified for the importance of incorporating this second data source. The first, and perhaps most obvious, is that it expands the domain in which our model is able to operate — because conversation data in the modern age increasingly happens over both email and chat, it increases the helpfulness of our model to be able to produce summaries for chatlogs as well.</p><p>However, the other benefit is that the chatlog dataset has considerably more data points available than the email dataset, perhaps due to the nature of chat as a noisy, casual medium. A single email thread consists of about 80 sentences on average, and we only have 32 email threads to train with when we are using an 80% / 20% split in k-fold cross validation. That means the email data has only about 2500 sentences of training data.</p><p>By contrast, the chatlog dataset is considerably larger. Again using 80% of the data as training data, we have 1118 threads of about 1200 sentences each, meaning the chatlog data has about 1,341,600 sentences or approximately 500 times the data points that were previously available. Through the use of this added data, our goal is ultimately to improve the performance of our model on a smaller dataset, such as email, solely by virtue of the massively increased amount of training data that is available by combining different data types.</p><p>Of course, to make this arrangement work, it is necessary to use a universal feature set that can apply to any type of data. That way, the model is trained on the same set of features regardless of how the data was originally structured, and it has universal applicability for any future datasets that might be added. One of the more unique aspects of our project is this combination of different text conversation mediums and the utility of our features across all conversation types.</p><h4><strong>Text Segmentation</strong></h4><p>An interesting challenge we encountered while incorporating both email and chat data was finding analogous parts and features between the two formats. Email threads were easily separated into emails, but chatlogs being one long, unstructured flow of conversation made it more difficult to process. To help with this problem, we added preprocessing of chatlogs into Longest Contiguous Messages (LCM). To capture LCM, we concatenated subsequent messages of the same user and identified a boundary when the user changes or when a period is encountered.</p><p>The effect of this incorporation was to create email-like “chunks” of the conversation, which we were then able to use for certain document-dependent features like TF-ISF that rely on having messages segmented into individual documents.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gW-GdW9aTe7v05OL." />A before (left) and after (right) of applying LCM on a chat excerpt, taken from Sood et. al.</figure><h4><strong>Input Structure</strong></h4><p>To improve the context our features are able to operate with, we changed the input format of our data to have one more level of granularity. In our previous iterations of the model, our input structure was a list of threads, each thread being a list of sentences. Our new input format has one more nested layer — each thread is now a list of ‘chunks’ and each chunk is a list of sentences. For emails, a chunk is naturally a single email in the thread. For chatlogs, a chunk is determined by LCM text segmentation, with each chunk consisting of the contiguous messages by a single user.</p><p>This changes the sentence position feature to mean the sentence position within a chunk rather than within the whole thread, which we believe is more relevant to the sentence’s importance. This also allows us to add a “position from end of the chunk” feature which is helpful for eliminating ending lines (like sign-offs or parting words) and other features that might depend on chunk granularity context.</p><h4><strong>Results of Cross-Training</strong></h4><p>In order to examine the efficacy of our multi-dataset training approach, we ran some experiments to look at the ROUGE scores that are produced. Our first attempt was to use the Naive Bayes model in order to try this cross-training approach, as it had generally been the most successful in prior experiments. Unfortunately, we discovered that the Naive Bayes model performed extremely poorly in many combinations — after training on the chat data and evaluating on email data, for example, it would cause the vast majority of the email summaries to be blank after determining that every single sentence sentence should be classified as not being included in the summary. However, we were able to use the Decision Tree model instead to produce promising results. We are still attempting to determine the cause of this discrepancy, although one possibility might be that the Naive Bayes model ascribed too much significance to a certain feature that is significantly different between chat and email, such as sentence length or some quirk of proper punctuation.</p><p>The following table shows our results while training using the chat and evaluating using the email data, with self-trained email data provided as a comparison. In order to determine the effect of data size on the ROUGE score, we experimented with varying sizes of chat examples:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pXyyq6x_CVDaNGxx3DpsSw.png" />Results of various cross-training configurations using email and chat data</figure><p>As can be seen from the above table, from a ROUGE score standpoint, the effect of training on more data is considerably more important than the effect of training on the same type of data as is being used for evaluation. Having 12,000 sentences upon which to train causes the model’s ROUGE scores to shoot up significantly higher than having 2,500 sentences, regardless of the type of data.</p><p>Another thing we have been experimenting with is training on both types of data before evaluating on a single type of data. In this case, it would mean training on both email and chat before evaluating on either email or chat data. The theoretical advantage of this approach would be that it maximizes the amount of available data, and if there are significant differences between the types of data, it would be sure to incorporate the type of data that the model will be evaluated on.</p><p>However, in practical use, it seems as though training on multiple data sources will not contribute significantly to the success of our model because of the fact that the chatlog dataset so completely eclipses the amount of data available in the email dataset. Therefore, from a numerical perspective, training on both types of data would add only a 0.2% increase in data from training on just the chatlog data. In addition, it stands to reason that having such a numerical disadvantage means any significant benefit from training on the same type of data that the model is being evaluated on would be negligible. We are continuing to develop this capability, but for the time being it seems like a low priority.</p><h4><strong>Topic Segmentation</strong></h4><p>In addition to segmenting by the author of contiguous chat messages, we also wanted to add a new feature for our model that incorporates the changing of topics throughout a message. The idea behind this feature is to use the changing structure of text to determine the boundaries between different topics through an existing algorithm for topic detection called TextTiling (More detail can be found in the <a href="http://www.aclweb.org/anthology/J97-1003">paper describing TextTiling</a>). We used an implementation of TextTiling found within NLTK.</p><p>While the paper we used for our baseline model mentioned using TextTiling, they only used the algorithm as a preliminary attempt to segment the chatlog into different chunks. We found that approach to be ineffective, likely because the chunks in an email thread (emails) are separated by author, while TextTiling topics cover multiple authors but cover a single topic, and therefore the two are largely incomparable. To support our concept of a universal feature set, we instead used TextTiling as a separate feature, offering context for sentences.</p><p>Currently, our models are capable of using TextTiling in order to determine the position of a sentence within a topic and incorporate the relative position within the topic as a feature. Unfortunately, we have only been able to successfully run TextTiling for tiny datasets so far due to errors that manifest for noisier data, and as a result we are not ready to report results using it.</p><h4><strong>Compression Ratio</strong></h4><p>One of the major problems we noted in the summaries our model was generating was that while they scored well in ROUGE metrics, the model seemed to place importance on too many sentences and the summaries themselves often ended up too long to be practical. Indeed, summaries for some threads would be composed of over 50% of the “actual” contents of the thread (the parts that were kept after preprocessing), making our summaries close to general reproductions of the original text.</p><p>As a solution, we experimented with using a regression model instead of a classifier, giving each sentence a score rather than a binary 1 or 0 (include in the summary or not). With a regression model, we gain the ability to control the compression ratio by changing the threshold of score we accept to include in the summary.</p><h4><strong>Results of Compression Ratio</strong></h4><p>With these updates to our model, we experimented with tuning regression the running using the new possible training and validation configurations to get the following results.</p><p>We first conducted an experiment with the threshold for our regression model to determine the optimal threshold value:</p><pre>python main.py bc3/full --type email --model regression_br --threshold &lt;t value&gt;</pre><p>For those following along at home, the above command is what we used to generate the values for the following table.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cNtKD45MlqM822FrIv3R1w.png" />Results of experimenting with the threshold value using the regression model</figure><p>Based on the results, we chose a threshold of 0.3 to continue with. The following table compares the regression model with the Naive Bayes classifier using the full email dataset:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dixp1JhnxIxOVdM3DZ97_A.png" />Comparison between our best regression model and classifier model</figure><p>In general, the “best” regression model did not have an enormous impact on the ROUGE scores as compared to the best results from the classification models (Naive Bayes), increasing some ROUGE metrics slightly and decreasing others. However, because the regression model allows configuration of the number of sentences that are output by changing the threshold hyperparameter, it adds an additional capability to the model whereby a user can select an optimal “size” of summary.</p><p>We developed this feature in response to the problem of “reproducing” the original text in our summaries, and by human standards it seems to be a big improvement. Consider the following email thread summary, produced using our Naive Bayes classifier model:</p><pre>Chris Lilley, Brian Stell and others have been discussing the rash of irate, &amp;quot;get me off this list&amp;quot; mesages the listserv has received, lately.</pre><pre>Well, folks: YOU CAN'T UNSUBSCRIBE FROM THIS LIST!</pre><pre>I've tried for 2 months to get off this list, I've followed the rules, I've tried variations of the theme, looking for some hidden code--all to no avail.</pre><pre>So, the last resort of those who have tried everything else is to post to the list they want to be rid of.</pre><pre>PLEASE GET ME OFF THIS LIST@!%$#$/-\%</pre><pre>Well, that explains a lot!</pre><pre>I've been trying for awhile too, and I can't seem to get off.</pre><pre>Please remain calm.</pre><pre>Our automated list manager works very well.</pre><pre>Sometimes there are problems due to:</pre><pre>- you being subscribed under another name/address --</pre><pre>this was the case for the first among the two recent messages.</pre><pre>Well, after receiving a message that informed me of this, I responded with an unsubscribe e-mail with my unaliased e-mail address, and today received an automated response informing me that your software could not find my name on your list.</pre><pre>I am very calm.</pre><pre>Thanks for the helpful info, but I just received a message saying that I have been removed from the list.</pre><pre>Because I posted my difficulties to the list.</pre><pre>Sorry I had to burden you all with my problems, but as you can see, it worked.</pre><pre>Sam Berlow</pre><pre>UNSUBSCRIBE.SIGROLLY</pre><p>By contrast, here is the summary for the same email thread, but using a Bayesian Ridge regression model and incorporating a threshold of 0.55 in order to produce a much smaller summary:</p><pre>Chris Lilley, Brian Stell and others have been discussing the rash of irate, “get me off this list” mesages the listserv has received, lately.</pre><pre>I've tried for 2 months to get off this list, I've followed the rules, I've tried variations of the theme, looking for some hidden code--all to no avail.</pre><pre>Well, after receiving a message that informed me of this, I responded with an unsubscribe e-mail with my unaliased e-mail address, and today received an automated response informing me that your software could not find my name on your list.</pre><pre>Thanks for the helpful info, but I just received a message saying that I have been removed from the list.</pre><p>The difference is quite pronounced, with almost half the sentences in the former being removed when a regression model and higher threshold is used. However, from a human perspective, the second summary is nearly just as informative, and in fact there are several extraneous sentences in the first summary that are dropped. Because the ROUGE scores are roughly the same in both cases, but the second summary is somewhat better from a human viewpoint, we would consider the use of a compression ratio to be a success in improving our model.</p><h4><strong>Next Steps</strong></h4><p>So far we’ve trained on single data sources (email or chat) before evaluation since we weren’t quite able to get training on both sources at once working yet in this part 1 attempt. We aim to fix this next week and be able to train on multiple data sources and fine tune the cross-training.</p><p>We think there is a lot of potential in features based on topic segmentation, semantic meaning, and the conversation-specific aspect of the authorship of a sentence. So, we also aim to finish our implementation of TextTiling-based feature, a feature that incorporates Rhetorical Structure Theory (RST) output, and feature that includes information from tracking the contributions of different participants throughout the conversation.</p><p>After completing these improvements, we would like to take a crack at our stretch goal of generating abstractive summaries and make a demo of our summarizer on a chat interface!</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=29b207b75065" width="1" /></div>







<p class="date">
<a href="https://medium.com/@viterbi.or.not/advanced-model-2-part-1-29b207b75065?source=rss-c522ef075bb3------2">by Viterbi Or Not To Be at May 17, 2018 06:56 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=339">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/05/17/nlp-capstone-post-8-training-challenges/">NLP Capstone Post #8: Training challenges</a></h4>
<div class="entry">
<div class="content" lang="en">
<h1><span style="font-weight: 400;">Challenges with training TSL model</span></h1>
<p><span style="font-weight: 400;">In our previous post we proposed a three-model system that would allow us to take advantage of a larger corpus of higher quality lyrics data for the production of lyrics. We also finally tackle the alignment task with a simple approach of determining whether a lyric token should be produced at each timestep. This seems sensible since we have begun dividing the MIDIs into pianorolls with a constant frequency.</span></p>
<p><span style="font-weight: 400;">Unfortunately, even after several bugs bashed, we’ve been still unable to produce even sensible timings. We find the RNN collapses to repeatedly generating 0 (for no lyric event), even though a randomly initialized RNN will repeatedly generate 1 (and perform better with respect to classification accuracy).</span></p>
<p> </p>
<h1><span style="font-weight: 400;">Future direction</span></h1>
<p><span style="font-weight: 400;">If we are able to produce something reasonable from our existing architecture, we would like to move on to a second model that structures the problem as machine translation. We have decided to focus on the paper Attention is All You Need by Vaswani et al. [1] for our presentation in two weeks. The structure of our problem is straightforward to apply to translation as converting pianoroll format into english sentences. Incorporating attention has shown promising results in the literature, though that is no guarantee that our noisy dataset would be able to take advantage of this proposed architecture.</span></p>
<p> </p>
<h1><span style="font-weight: 400;">References</span></h1>
<p><span style="font-weight: 400;">[1] </span><a href="https://arxiv.org/abs/1706.03762"><span style="font-weight: 400;">https://arxiv.org/abs/1706.03762</span></a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/05/17/nlp-capstone-post-8-training-challenges/">by Nicholas Ruhland at May 17, 2018 05:55 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 16, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-2">
<h4><a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-2">Advanced Model Attempt #2</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">Wow, can't believe we're already in week 8! <br /><br />This past week, I've worked on shifting my project from not having a neural component, to well, having one. With inspiration from the work of Ghosh et al. in <em>Affect-LM: A Neural Language Model for Customizable Affective Text Generation, </em>I've extended my project (and am encroaching on stretch goal territory) to include a neural language model for reddit post generation***. The idea would be to train a model on reddit posts out of five categories - F30 (mood [affective] disorders), F40<font color="#515151"> (Anxiety, dissociative, stress-related, somatoform and other nonpsychotic mental disorders), X71 (Intentional Self Harm), F10 (addiction categories), and neurotypical advice - and use that model for post generation, based on the specified target type, i.e. generate a F30 reddit post. <br /><br /></font><br /></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-2">May 16, 2018 11:19 PM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" title="Stories by Halden Lin on Medium">Halden Lin <br/> Team undef.</a></h3>


<div class="entrygroup" id="https://medium.com/p/33072535817f">
<h4><a href="https://medium.com/@halden.lin/nlp-capstone-08-human-summaries-33072535817f?source=rss-2759d54493c0------2">NLP Capstone | 08: Human Summaries</a></h4>
<div class="entry">
<div class="content">
<p><em>previous posts: </em><a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5"><em>01</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5"><em>02</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3"><em>03</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7"><em>04</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a"><em>05</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0"><em>06</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-07-formalizing-a2d837ecf66b"><em>07</em></a></p><p>I’ll keep this blogpost short — my current undertakings are in-progress and it might be a week or more before they are realized. To preface:</p><ol><li>Visualizing human summaries</li><li>Improving the visualization.</li></ol><h3>Visualizing Human Summaries</h3><p>When I was presenting my project update last week, Prof. Choi asked a very interesting question. What if we could use this visualization tool to not only understand how the model is generating summaries, but also how human summaries are produced and how the two compare?</p><p>I’ll briefly explain the thought behind this.</p><p>Visualizations provide a mapping from raw data (in this case, attention weights and input / output tokens) to visual encodings. These visual encodings are valuable in that they allow us as humans to perceive the data in a meaningful way. For example, the attention visualizer I am working on allows us to identify overall patterns in the attention of the model.</p><p>How do we interpret human written summaries, i.e. their relation to the source text? Perhaps we scan the document and attempt to match paragraphs or sentences to sentences in the summary. This can be compared to the ‘attention’ our minds use to generate the summary. If we can visualize this mapping, perhaps in a more refined and detailed manner (i.e. token by token) then we should be able to compare the human summaries with the machine generated summaries, right? And then one may be able to identify what the human summaries have that the machine summaries do not, or visa versa. The hope is that if we can enable this kind of comparison, researchers may be better equipped to improve their models by using these insights.</p><p>So far, I’ve experimented with two methods for generating this ‘attention’ from human summary to source text.</p><h4>Word Similarity</h4><p>The first approach that sprung to mind was to use word similarity as a proxy for ‘attention’. To do this, I used <a href="https://nlp.stanford.edu/projects/glove/">pre-trained GloVe embeddings</a> and the <a href="https://radimrehurek.com/gensim/">Gensim API</a> to calculate word similarities between each input token / output token pair. <a href="https://spacy.io/">SpaCy</a> was used to tokenize the sequences. The result is a matrix of weights, similar to attention distribution, albeit not normalized per output token (with attention, for a given output token the aggregate over all input tokens is 1). As the example below shows, this method falls flat, as output tokens are matched to input tokens regardless of context. This means it makes little sense to compare these weights to attention.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*D6pMsnZjlNiz1XJyN5xcBQ.png" />The word ‘the’ is matched with nearly every token in the input sequence, likely a result of its extreme commonness and proximity to most words in embedding space.</figure><h4>Summarization Model</h4><p>The second method I considered was one suggested by Ari. Here, we use the same model used to generate the machine summaries. The difference is that at each decoder time-step, instead of feeding in the previous <strong>predicted</strong> token, we feed in the previous <strong>actual</strong> token. This means that instead of having a decoding pipeline that looks like this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*N-til0kZzQeAVI3zkoDKVg.jpeg" /></figure><p>Where y-hats represent predicted output tokens, we have one that looks like this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*H7nMlelojnzNkfthS2ljAg.jpeg" /></figure><p>Where y (no hat) represents true output token (the token from the human summary). This is a process similar to the one taken during training of generative encoder-decoder models.</p><p>By grabbing attention distributions just as we had with the machine summaries, we hope to get an approximation of the attention distributions for the human summaries — we are essentially feeding the model the answer. However, there is a catch, and an important one. Because attention weights are used to create a context vector that is then fed into the <strong>next</strong> decoder unit to predict the <strong>next </strong>word, we run into an issue when the next word is predicted incorrectly. Turns out, this happens often under the model being used (from <a href="https://github.com/abisee/pointer-generator">See et al.</a>). This actually makes sense, as the model has been shown to produce largely <strong>extractive </strong>summaries, and so one would expect the model to, at each time-step, attempt to produce the next word in the source text that follows the word fed to it. With <strong>abstractive</strong> summaries, this is not often the optimal choice. This results in attention weights that make little sense, as seen below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*c5KuVp0qzjkcQxNAJSfYLw.png" />According to this, ‘manager’ was the focal point of attention when producing the word ‘were’. It would seem that the model intended to predict ‘manager’, and so when we map this attention weight to the human token, we run into an issue.</figure><p>Here’s the problem. The model is trained to use attention weights to <strong>generate </strong>an output token. What we want is the opposite. We want the attention <strong>given</strong> an output token — use the output token to generate the attention weights. This poses a significant challenge.</p><h4>So How Else?</h4><p>So neither of these methods seem to produce anything meaningful. I’m not ready to give up though — this is an intriguing problem. In the next week I’ll be brainstorming other methods. One that might have some traction is to use a few heuristics to approximate ‘attention’ using word similarities in conjunction with context. For example, by imposing a penalty on the weight if the context of the output token is dissimilar to the word ‘attended’ to in the source text. Much more work to be done here.</p><h3>Improving the visualization</h3><p>This section will be short. There are a few problems I looked into in the past week.</p><ol><li>Implement divided edge bundling, as produced by <a href="http://vis.stanford.edu/files/2011-DividedEdgeBundling-InfoVis.pdf">Selassie et al. (2011)</a>. I described this briefly in my previous blog post. The obstacle here is that there is not available d3 implementation of the algorithm. In fact, the only implementation I could find available was <a href="https://github.com/kakearney/divedgebundle-pkg">one for Matlab</a>, produced by <a href="http://kellyakearney.net/">Kelly Kearny (University of Washington)</a>. This might prove more difficult than the remaining time in this quarter allows, but I’ve started the process anyways and will see where it takes me.</li><li>Highlighting extraction. That is, making it apparent in the visualization when the model is simply copying. I’ve been playing around with things such as color to encode this, but haven’t settled on anything I like.</li></ol><p>More work to come!</p><h4>References</h4><ol><li><a href="https://arxiv.org/pdf/1704.04368.pdf">See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” <em>ACL</em> (2017).</a></li><li><a href="http://vis.stanford.edu/files/2011-DividedEdgeBundling-InfoVis.pdf">Selassie, David et al. “Divided Edge Bundling for Directional Network Data.” <em>IEEE Transactions on Visualization and Computer Graphics</em> 17 (2011): 2354–2363.</a></li></ol><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=33072535817f" width="1" /></div>







<p class="date">
<a href="https://medium.com/@halden.lin/nlp-capstone-08-human-summaries-33072535817f?source=rss-2759d54493c0------2">by Halden Lin at May 16, 2018 06:58 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=25">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/05/16/advanced-model-attempt-2-part-1/">Advanced Model Attempt 2 (Part 1)</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>This week I was able to greatly improve upon the results from last week. Last week my average reward got to about -23 at best, but this week by tweaking my reward function I was able to greatly speed up convergence and bump my model up to about +70 reward on average. I made my reward function more strict about interpreting the user response correctly by giving a +6 reward for correct interpretation, -6 reward for the opposite interpretation and 0 reward for interpreting as unknown (model is unsure).</p>
<p>One way I tried to improve over last week was reducing my CNN architecture to simplify the state representation, but surprisingly this had the opposite effect of underfitting on the user responses and misinterpreting them more. In fact, I later found out that my reward function was the main reason my model didn’t converge and in the end I actually ended up increasing the number of filters in my CNN architecture.</p>
<p>At this point, my model is able to win between 3 and 4 out of 5 games on average because there is 1 out of my 31 questions for which it learned the wrong Q values, probably due to insufficient exposure to the right answer, so I’ll increase the amount of exploration the model takes until these values are straightened out and the model wins 5 out of 5 games. Then I’ll evaluate the model performance on a separate database of 100 different people. While I expect the end result to be the same (winning 100% of games), I expect the model to take longer to win on average because the order of questions it asks to eliminate people is tailored towards the training database.</p>
<p>My goals for this week are reporting more exact quantitative results of model performance in terms of games won and reward gained, both on the training database and a validation database of 100 different people. Furthermore, I’ll experiment with a slightly more complicated dialogue scenario but I probably won’t go too deep considering I only have about a week left.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/05/16/advanced-model-attempt-2-part-1/">by ananthgo at May 16, 2018 06:55 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/ca5a7f69db85">
<h4><a href="https://medium.com/nlp-capstone-blog/advanced-model-2-ca5a7f69db85?source=rss----9ba3897b6688---4">Advanced Model # 2</a></h4>
<div class="entry">
<div class="content">
<p>As explained in our previous blog post, our current challenge is involves constructing the dataset in an efficient and effective manner. This blog post will detail the progress made in the past week in relation to data construction and the challenges we have faced. We will also briefly highlight our plan for the next week.</p><h4>Training</h4><p>We’ve attempted training on a small subset of the data in the format the model should expect, and ran into an issue with batching.</p><p>As of now, we backpropagate once per batch. But at this point, latent representations of sentences and the document have already been computed. Meaning, the RNNs have already encoded every word in the document before parameters are updated. This, in conjunction with the series of affines for each sentence will likely produce a computation graph that we won’t have enough memory to backprop on.</p><p>We’ve now switched to backpropagation once per sentence, which will hopefully lead to faster learning.</p><h4>New Heuristics</h4><p>Last week we described a method of BIO tagging sentences that involved using ROUGE. While this method might produce good tags, we found that ROUGE was incredibly slow to run. Since we run ROUGE once for every sentence in every document, we chose to develop a difference heuristic that worked like ROUGE but was much faster. This led us to experimenting with two new approaches, which are detailed below.</p><h4>Skip-bigrams</h4><p>When learning about ROUGE, we learned of a variety of ROUGE called ROUGE-SU. Here, SU stands for Skip Bigrams and Unigrams. Skip Bigrams refer to bigrams which are formed as any subsequent pair of words in the sentence. In other words, it’s every bigram possible in a sentence such that the bigram follows sentence order. For our first approach, we decided to use the same greedy algorithm described from previous blog posts, except we try to maximize the skip bigram overlap between the reference skip bigrams and the set of sentences we choose to extract. We implemented this functionality from scratch.</p><h4>Cosine Similarity</h4><p>Cosine similarity is defined as a measure of similarity between two vectors. Essentially, it is a way of determining the cosine of the angle between two sequences of text in Euclidean space. A value tending toward 1 means that the two pieces of text are more similar, while smaller values mean there is less correlation. In NLP, this metric is used as a bag-of-words comparison, combining the words of both sequences into a master set of words, and computing the cosine similarity between each sequence’s respective frequency vector whose dimensionality is equal to the size of the set. In other words, it is the cosine of the angle between their tf-idf vectors.</p><p>Currently, we’re using spaCy’s implementation of cosine similarity.</p><h4>Example Data</h4><p>When we are sufficiently strict (ex. enforcing a skip-bigram intersection of at least 10 with a cosine similarity of at least 0.94) then we can get promising matches:</p><p>PAPER: Mechanisms of NO/cGMP-Dependent Vasorelaxation TERMs: {‘omega-Nitro-L-Arginine, N’, ‘NO2Arg’, ‘N omega Nitro L Arginine’, ‘L-NNA’, ‘Nitroarginine [Chemical/Ingredient]’, ‘NG-Nitro-L-Arginine’, ‘omega-Nitroarginine’, ‘NG-nitro-L-arginine’, ‘N(omega)-Nitroarginine’, ‘NOLA’, ‘N omega-Nitro-L-Arginine’, ‘N OMEGA NITROARGININE L’, ‘omega Nitroarginine’, ‘NOARG’, ‘NG-Nitroarginine’, ‘N(G)-Nitroarginine’, ‘NG NITROARGININE L’, ‘NG Nitro L Arginine’, ‘Nitroarginine’, ‘NG Nitroarginine’}<br />TERM FOUND: True<br />100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 145/145 [00:01&lt;00:00, 103.03it/s]<br />REFERENCE: An amino acid derivative and nitric oxide synthase (NOS) inhibitor with potential antineoplastic and antiangiogenic activities. Upon administration, NG-nitro-L-arginine inhibits the enzyme nitric oxide synthase, thereby preventing the formation of nitric oxide (NO). By preventing NO generation, the vasodilatory effects of NO are abrogated leading to vasoconstriction, reduction in vascular permeability and an inhibition of angiogenesis. As blood flow to tumors is restricted, this may result in an inhibition of tumor cell proliferation. NO plays an important role in tumor blood flow and stimulation of angiogenesis, tumor progression, survival, migration and invasiveness.<br />CHOSEN: [‘NO coordinates the blood-flow distribution between arterioles and the microvasculature by regulating the diameter of small arteries.7 The importance of NO and cGMP for the regulation of vascular tone and blood pressure has been recently strengthened by the observation that mice deficient in eNOS, ANP, the ANP receptor guanylyl cyclase A, or cGKI develop hypertension.2–6,17’]<br />MAX SKIPGRAM MATCHES: 38<br />MAX COSINE SIMILARITY: 0.9430605549205237</p><p>The ‘CHOSEN’ array contains a sentence that was extracted with our heuristics. Like the reference, it mentions nitric oxide, blood flow, and regulation of vascularity. The goal is to train model on instances such as these and were able to extract sentences of such relevance.</p><p>We should also note that definitions define multiple terms (in other words, terms may have multiple synonyms). The extracted sentence ‘NO coordinates the blood-flow distribution between arterioles …’ itself contains none of the entities explicitly, and yet seems to align well and be indicative of it’s definition. Robustness in recognizing a given term and its synonyms, along with being able to extract sentences about that term without the term actually being in it is extremely desirable for us. A model that is able to recognize sentences about a particular technical term without the term being present would be an especially helpful research tool.</p><p>Unfortunately, the heuristic is not perfect and can be lead astray:<br /> “title”: “On the influence of various physicochemical properties of the CNTs based implantable devices on the fibroblasts’ reaction in vitro”<br /> },<br /> “e_gold”: “ A record of something that is being done, has been done, can be done, or is intended or requested to be done. Examples: The kinds of acts that are common in health care are (1) a clinical observation, (2) an assessment of health condition (such as problems and diagnoses), (3) healthcare goals, (4) treatment services (such as medication, surgery, physical and psychological therapy), (5) assisting, monitoring or attending, (6) training and education services to patients and their next of kin, (7) and notary services (such as advanced directives or living will), (8) editing and maintaining documents, and many others. Discussion and Rationale: Acts are the pivot of the RIM; all domain information and processes are represented primarily in Acts. Any profession or business, including healthcare, is primarily constituted of intentional and occasionally non-intentional actions, performed and recorded by responsible actors. An Act-instance is a record of such an action. Acts connect to Entities in their Roles through Participations and connect to other Acts through ActRelationships. Participations are the authors, performers and other responsible parties as well as subjects and beneficiaries (which includes tools and material used in the performance of the act, which are also subjects). The moodCode distinguishes between Acts that are meant as factual records, vs. records of intended or ordered services, and the other modalities in which act can appear. One of the Participations that all acts have (at least implicitly) is a primary author, who is responsible of the Act and who \”owns\” the act. Responsibility for the act means responsibility for what is being stated in the Act and as what it is stated. Ownership of the act is assumed in the sense of who may operationally modify the same act. Ownership and responsibility of the Act is not the same as ownership or responsibility of what the Act-object refers to in the real world. The same real world activity can be described by two people, each being the author of their Act, describing the same real world activity. Yet one can be a witness while the other can be a principal performer. The performer has responsibilities for the physical actions; the witness only has responsibility for making a true statement to the best of his or her ability. The two Act-instances may even disagree, but because each is properly attributed to its author, such disagreements can exist side by side and left to arbitration by a recipient of these Act-instances. In this sense, an Act-instance represents a \”statement\” according to Rector and Nowlan (1991) [Foundations for an electronic medical record. Methods Inf Med. 30.] Rector and Nowlan have emphasized the importance of understanding the medical record not as a collection of facts, but \”a faithful record of what clinicians have heard, seen, thought, and done.\” Rector and Nowlan go on saying that \”the other requirements for a medical record, e.g., that it be attributable and permanent, follow naturally from this view.\” Indeed the Act class is this attributable statement, and the rules of updating acts (discussed in the state-transition model, see Act.statusCode) versus generating new Act-instances are designed according to this principle of permanent attributable statements. Rector and Nolan focus on the electronic medical record as a collection of statements, while attributed statements, these are still mostly factual statements. However, the Act class goes beyond this limitation to attributed factual statements, representing what is known as \”speech-acts\” in linguistics and philosophy. The notion of speech-act includes that there is pragmatic meaning in language utterances, aside from just factual statements; and that these utterances interact with the real world to change the state of affairs, even directly cause physical activities to happen. For example, an order is a speech act that (provided it is issued adequately) will cause the ordered action to be physically performed. The speech act theory has culminated in the seminal work by Austin (1962) [How to do things with words. Oxford University Press]. An activity in the real world may progress from defined, through planned and ordered to executed, which is represented as the mood of the Act. Even though one might think of a single activity as progressing from planned to executed, this progression is reflected by multiple Act-instances, each having one and only one mood that will not change along the Act-instance life cycle. This is because the attribution and content of speech acts along this progression of an activity may be different, and it is often critical that a permanent and faithful record be maintained of this progression. The specification of orders or promises or plans must not be overwritten by the specification of what was actually done, so as to allow comparing actions with their earlier specifications. Act-instances that describe this progression of the same real world activity are linked through the ActRelationships (of the relationship category \”sequel\”). Act as statements or speech-acts are the only representation of real world facts or processes in the HL7 RIM. The truth about the real world is constructed through a combination (and arbitration) of such attributed statements only, and there is no class in the RIM whose objects represent \”objective state of affairs\” or \”real processes\” independent from attributed statements. As such, there is no distinction between an activity and its documentation. Every Act includes both to varying degrees. For example, a factual statement made about recent (but past) activities, authored (and signed) by the performer of such activities, is commonly known as a procedure report or original documentation (e.g., surgical procedure report, clinic note etc.). Conversely, a status update on an activity that is presently in progress, authored by the performer (or a close observer) is considered to capture that activity (and is later superceded by a full procedure report). However, both status update and procedure report are acts of the same kind, only distinguished by mood and state (see statusCode) and completeness of the information. “,<br /> “entity”: “act”,<br /> “extracted”: [<br /> “Since their discovery in 1952, carbon nanotubes (CNTs) have been attracting increasing attention in being applied in various areas of materials science due to their outstanding mechanical properties, high chemical and thermal stability and, in some cases, very good conductivity via an electron transfer.”,<br /> “Thus, at that time point, differences in fibroblasts’ proliferation rate may have been governed by different chemical composition of the samples and an increased amount of COOH species in the CNT_ox [28].”<br /> ],</p><p>Note that since the script selected this example, that the cosine similarity score between the extracted sentences and the reference were above 0.93. Not only is the cosine similarity too generous as a heuristic, the fact that the reference is so large means that it will almost always overlap with more than enough skip-bigrams to reach our skip-bigram threshold.</p><p>Examples like these and others are concerning, but the hope is that helpful examples like the NO example outnumber the noise that make it past our heuristics.</p><h4>Going Forward</h4><p>Our goals for the next week include fine tuning our data collection thresholds to maximize the quality of our dataset, training the model, and hopefully producing results. As we mentioned in our previous blog post, the model is ready for training, but the real challenge might be with the way we produce our dataset. In the upcoming week, we expect to experiment with new heuristics for sentence similarity and tweak the existing heuristics in order to produce the best dataset.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ca5a7f69db85" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/advanced-model-2-ca5a7f69db85">Advanced Model # 2</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/advanced-model-2-ca5a7f69db85?source=rss----9ba3897b6688---4">by Karishma Mandyam at May 16, 2018 05:39 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 10, 2018</h2>

<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=335">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/05/10/nlp-capstone-post-7-tsl-pipeline/">NLP Capstone Post #7: TSL Pipeline</a></h4>
<div class="entry">
<div class="content" lang="en">
<p> </p>
<h2><span style="font-weight: 400;">Modeling issues</span></h2>
<p><span style="font-weight: 400;">As seen in our results last week, the RNN architecture we have been training has not been able to produce any coherent series of tokens based on the music data provided in the clean Lakh dataset. To analyze the poor results of this model, we have considered various features of the quality of the data. To simplify the issue of timing the lyric tokens, this model attempts to predict a lyric token at every timestep. Between each token we have summed all the musical data, producing a piano roll that looks approximately like the following image.</span></p>
<p><img alt="Screen Shot 2018-05-09 at 4.31.53 PM" class="  wp-image-331 aligncenter" height="303" src="https://mathstoc.files.wordpress.com/2018/05/screen-shot-2018-05-09-at-4-31-53-pm.png?w=501&amp;h=303" width="501" /></p>
<p><span style="font-weight: 400;">In the event that two lyrics occur at exactly the same time step, we end up with a gap in the notes, here highlighted in red.</span></p>
<p><img alt="Screen Shot 2018-05-09 at 4.31.53 PM" class="  wp-image-334 aligncenter" height="302" src="https://mathstoc.files.wordpress.com/2018/05/screen-shot-2018-05-09-at-4-31-53-pm1.png?w=500&amp;h=302" width="500" /></p>
<p><span style="font-weight: 400;">At first we expected this problem to occur in only a small number of cases, but it is often the result of the newline character appearing in a message simultaneously with the first lyric of the next sentence. This processing poses several problems to the task of learning the lyrical content based on the structure of the music. First, the large number of musical gaps may be confounding the model due to the large variety in lyrics that will be seen at those time steps. Additionally, we lose all information about the song timing since all regions without lyrics are compressed into a single time step. In theory, gaps in lyrics could hint to the model that the next section should start a new verse or chorus.</span></p>
<h2><span style="font-weight: 400;">The TSL Pipeline</span></h2>
<p><span style="font-weight: 400;">As suggested in the previous blogpost, we would like to be able to augment the results of the musical model with a higher quality lyrical dataset. The Kaggle lyrics dataset has shown promising results in previous blogposts at the quality of the lyric sequences it has been able to produce.</span></p>
<p><span style="font-weight: 400;">The TSL Pipeline is a combination of three models: Timing, Seed, and Lyrics. The architecture may look something like the following diagram:</span></p>
<p><img alt="Training" class="  wp-image-333 aligncenter" height="221" src="https://mathstoc.files.wordpress.com/2018/05/training.png?w=531&amp;h=221" width="531" /></p>
<p><span style="font-weight: 400;">During training, each pianoroll will be separated into data representing the timing, notes and lyrics. These get passed into respective models to learn timing and “seed” information. Additional lyrics information from the Kaggle dataset is used to train a lyrical model.</span></p>
<p><img alt="Evaluation" class="  wp-image-332 aligncenter" height="220" src="https://mathstoc.files.wordpress.com/2018/05/evaluation.png?w=538&amp;h=220" width="538" /></p>
<p><span style="font-weight: 400;">At evaluation time, the lyrics from the original pianoroll are not passed into the Seed model. Instead, the Seed model attempts to predict some seed based on the musical content, and will pass its result into the lyrics model. The combination of these lyrics and timing information constitute the complete description of our karaoke output.</span></p>
<h2><span style="font-weight: 400;">Timing Model</span></h2>
<p><span style="font-weight: 400;">In all previous posts we ignored the issue of lyrics timing in the interest of creating a reasonable lyrical model. Our current timing model is similar to our previous model attempt, but the data is generated differently. Instead of computing a pianoroll sample at each lyrical timestep, we us a constant sampling frequency of 10 timesteps per second. We then annotate each timestep with a 1 or 0 based on if a lyric was annotated at that step. The model will then attempt to predict for each step of a given pianoroll the probability there should be a lyric at that time.</span></p>
<h2><span style="font-weight: 400;">Seed Model</span></h2>
<p><span style="font-weight: 400;">The seed model will be a simplified version of the poorly performing model from before. Instead of predicting all lyrics, it will attempt to predict a small subset of the initial lyrics. This would also allow us to create a dataset with more training examples by splitting each song into smaller samples.</span></p>
<h2><span style="font-weight: 400;">Lyrics Model</span></h2>
<p><span style="font-weight: 400;">The lyrics model will be similar to the one described in the second blog post, which is a character level RNN for generating lyrics. This will take the first few words predicted by the seed model and generate the remainder of the lyrics. Since it’s trained on the large Kaggle dataset the quality seems to be much higher than what our MIDI training has produced.</span></p>
<h2>Results</h2>
<p>As of this blog post, we are still testing various hyperparameters and waiting for models to converge.  Additional results will follow once we can examine the various output.</p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/05/10/nlp-capstone-post-7-tsl-pipeline/">by Nicholas Ruhland at May 10, 2018 06:56 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/2ac19f7510f9">
<h4><a href="https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-continued-2ac19f7510f9?source=rss----9ba3897b6688---4">Advanced Model Attempt #1 (Continued)</a></h4>
<div class="entry">
<div class="content">
<p>This week we continued work on the advanced version of the model which incorporates the SummaRuNNer architecture from (Nallapati et al.) and adds in a character level RNN for parsing UMLS terms. The models are complete but we ran into several challenges along the way, namely constructing our final training dataset from Semantic Scholar and UMLS terms. Without the dataset, we have not been able to train our model. In this blog post, we dissect our current challenges and our plans looking forward.</p><h4>Building the Model</h4><p>The original SummaRuNNer model fit our requirements pretty well. However, we ran into several challenges when implementing the model. First, there were no existing implementations of the architecture, so we had to implement the model from scratch. Furthermore, the time we spent building the model detracted from time that we could have spent working on gathering the data. The model used by Nallapati et al. also did not condition on a particular term in the paper. Our approach requires that we somehow incorporate the term so we spent additional time figuring out a character level RNN which encodes the term and includes it in the many affine transformations described in the SummaRuNNer paper.</p><p>Moreover, we had to consider how to optimize the model when it came to large amounts of data. One of the approaches we worked on was batching, essentially evaluating many sentences at once. This allowed us to speed up training by a significant amount. Finally, we spent time integrating our model into the existing architecture. Ultimately, the model was difficult, but completed.</p><h4>Collecting the Data</h4><p>There are no current datasets that we can use to train our model. To quickly recap the requirements for the data, each training example must comprise of one document, one entity or technical term, and a target representing the ideal summary of the document. In order to build this dataset, we had to individually collect each of these aspects and combine them.</p><p>In order to gather technical terms, we used the UMLS dataset, which contains over 150,000 medical terms. Obtaining the license to download UMLS and the actual process of downloading the data through the UMLS specialized data downloader took several days. Parsing the data was fairly straightforward however.</p><p>In order to gather documents, we are using the AI2 Semantic Scholar dataset, which contains over 7 million research papers. While the downloading process for Semantic Scholar was incredibly slow, we realized that we couldn’t simply download all the documents because some of them were not medical papers. Handling Computer Science papers becomes an issue because our entities are medical terms and we do not expect a computer science paper to have any relation to medical terms.</p><p>This brings us to the process of combining Semantic Scholar documents with UMLS terms. We use a distant supervision method which essentially applies a greedy approach to extract a group of sentences from each document with the highest ROUGE score while using the UMLS definitions as reference summaries. This is precisely where we are currently struggling. Computing ROUGE takes a very long time, considering the fact that we compute ROUGE as many times as there are sentences in each document. Though we filter out document-term pairs based on whether the term appears in the document, it seems that the ROUGE metric may not yield the best target sentences for us because our reference summaries tend to be fairly short while our documents tend to be fairly long. Currently, we have all the scripts running for this data collection process but aim to develop a better heuristic to collect data.</p><h4>Next Steps</h4><p>At this point, we have not been able to test our data because we are still building the dataset. In order to speed up this process, our immediate goal will be to develop a faster and more accurate heuristic to gather target sentences from each document. We will also explore filtering out the Semantic Scholar papers to only retain medical papers, which are more likely to correlate to the terms in UMLS. Once we build our dataset, we can test our completed models, tune hyper-parameters, and potentially utilize attention mechanisms while constructing the document level representation used by SummaRuNNer.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2ac19f7510f9" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-continued-2ac19f7510f9">Advanced Model Attempt #1 (Continued)</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-continued-2ac19f7510f9?source=rss----9ba3897b6688---4">by Karishma Mandyam at May 10, 2018 06:40 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 09, 2018</h2>

<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=23">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/05/09/advanced-model-attempt-1-part-2/">Advanced Model Attempt 1 (Part 2)</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>This week I completely finished the model end-to-end and programmed the rules of the dialogue/game. The input to the model is created as follows: the user response is broken up into individual words and converted into a matrix of word embeddings. That matrix is run through a CNN before being fed into the LSTM and finally the policy networks to compute the Q values for user actions (questions to the user) and hypothesis actions (updates to the database). The model I created closely follows the one in this paper: <a href="https://arxiv.org/abs/1606.02560" rel="nofollow">https://arxiv.org/abs/1606.02560</a></p>
<p>I was able to run the simulation on my computer and debug a few issues with the model before starting the actual training session. Furthermore, I moved the model to Azure, set up the SQL database on the remote computer, and am currently running the simulation on tensorflow-gpu.</p>
<p>The initial results are not too great but somewhat promising. The model assigns +30 reward for winning, -30 for losing, -5 for bad guesses and +1 if it correctly understands the user response and performs the right action on the database. My model started out with an average of -30 reward for an entire game (it almost never took the same action that the user said and then abruptly lost) and after about 100,000 time steps that average has decreased to -23. This tells me the model is eventually learning how to understand the user responses but the convergence is still a lot slower than I expected.</p>
<p>As a side note, while it seems silly to try classifying user responses into buckets like ‘Yes’, ‘No’, ‘Unknown’ instead of training a separate classifier, the goal is to build a model that can be generalized to more complex dialogue types where there are more nuanced actions that heavily depend on dialogue state, not just the current user response.</p>
<p>The main difference between my model and the model in the paper (barring any bugs I am unaware of) is how the user response is represented. In both models, the model tries to combine NLU and dialog state tracking into one end-to-end model and the user response sentence representation is crucial in determining the dialogue state at any given time step. The paper creates a very short bag of bigrams feature vector (of about 30 features) from the sentence whereas I create glove vectors (embedding dimension = 50) for each word and run them through a CNN to get an output of 250-300 features.</p>
<p>The paper made it sound like they got the model to converge and play the game well after about 120000 time steps. To play the game well, the model needs to correctly understand what action should be done on the database based on the user response. My guess is their model converges much faster and can learn to make the right action much faster than mine because of their simpler state representation. Once the model learns how to correctly interpret the user response, it becomes much easier to learn how to win from any given dialogue state. This allows the model to generalize to people it hasn’t seen because the model only learns what actions to take based on the dialogue state, not on the attributes of any given person.</p>
<p>My goal for next week is to try tuning my model to converge faster. My main plan will be to shorten and simplify the state representation to be more like the paper’s (although it will still be different since the paper used utterance embeddings for speech, not word embeddings). If the model somehow works well after tuning, I will look into trying to train the model on more complex user responses where the set of actions to take are more qualitative than yes and no.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/05/09/advanced-model-attempt-1-part-2/">by ananthgo at May 09, 2018 06:58 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" title="Stories by Halden Lin on Medium">Halden Lin <br/> Team undef.</a></h3>


<div class="entrygroup" id="https://medium.com/p/a2d837ecf66b">
<h4><a href="https://medium.com/@halden.lin/nlp-capstone-07-formalizing-a2d837ecf66b?source=rss-2759d54493c0------2">NLP Capstone | 07: Formalizing</a></h4>
<div class="entry">
<div class="content">
<p><em>previous posts: </em><a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5"><em>01</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5"><em>02</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3"><em>03</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7"><em>04</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a"><em>05</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0"><em>06</em></a></p><p>A couple of developments since I last posted:</p><ol><li>I’m now more formally receiving guidance from Kanit (Ham) Wongsuphasawat and Tongshuang (Sherry) Wu of the Interactive Data Lab. Special thanks to them for helping me thus far!</li><li><a href="https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0">Last week</a> I was uncertain as to the future direction of this project. After much deliberation and several conversations, I’ve decided to remain on the tool-based approach originally conceived. There are a couple reasons for this. First, the user study I proposed has, at least from an NLP perspective, limited novelty. What’s more, learning from the study by modifying the underlying model would require time that would likely fall outside of the quarter. Second, carrying out this study would involve significant logistical work (again, a time constraint). Finally, in beginning to formalize this visualization tool, I’ve become more excited in its potential as a useful part of a researcher’s debugging pipeline. In any case, any model modifications I may make as part of the user study would difficult without a similar tool.</li><li>As mentioned, I’ve been formalizing this tool in a React application, iterating on the exploration I’ve done with prototypes in the weeks previous. The rest of this post will describe my work here.</li></ol><h4>Starting with Text</h4><p>First things first: text brushing is critical in understanding the attention each output token, or a series of output tokens, pays to the source text. In the gif below, the left side holds the source text (along with a mini-map to prevent the need of scrolling to understand the overall distribution of attention), while the right holds the summary. Selection and brushing have been implemented, as in the prototypes of last week, albeit cleaned up.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/999/1*LKOEMB1cwvp2qXRy58_-4w.gif" />Cleaned up selection and brushing, in addition to a minimap for long input sequences.</figure><h4><strong>Challenges with Text (Future Work):</strong></h4><ol><li>Sentence / paragraph level structure is lost via tokenizing. This is an artifact of the tokenization process performed prior to feeding text into models.</li><li>Lowercase and always-on spacing between tokens makes text difficult to read. For now, I’ve been dealing this with a few hand-coded rules. For example, removing spaces before punctuation, and capitalizing the first word after end of sentence punctuation.</li></ol><p>I’ll need to keep brainstorming to find methods for addressing these issues.</p><h4>How do we enable identification of patterns?</h4><p>While selection and brushing over the text is valuable in allowing users to understand attention for specific words or phrases, it falls short in enabling big-picture identification of patterns. Without brushing over each token and / or sentence (and memorizing coverage along the way), the closest users may get is the aggregate view (when nothing is selected) in which <strong>what</strong> is being attended to is apparent, but not <strong>how.</strong> That is to say, it is not apparent which tokens / phrases in the summary attend to which tokens / phrases in the source text. In particular, below is a growing list of goals for this visualization.</p><ol><li><strong>Enable identification of coverage</strong>. For words / phrases / sentences, where is the attention being paid, and by what?</li><li><strong>Enable identification of missing coverage</strong>. What is being unattended to that should be?</li><li><strong>Enable identification of extraction vs abstraction</strong>. Where is copying occuring? Where is true abstraction occuring?</li></ol><p>If this tool can accomplish these items, I believe it will be a good start in proving its value.</p><p>With this in mind, we need some sort of visualization to accompany the two blocks of text. As previously mentioned, heat-maps may not be the best solution.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Kd4lr3ZZmOZ6ffMdp82PJg.gif" />Interactive heat-map, as prototyped previously.</figure><p>Lag is apparent (likely a result of the large number of elements drawn), and even ignoring this, the tiles become extremely small as the input / output sequences grow, making it difficult to pick out even high attention weights. More visual weight is needed for significant attention weights, which is difficult to accomplish as x and y space is already taken by the input / output token position.</p><p><strong>So where else can we look?</strong></p><h4>Flowmaps</h4><p>Looking back at my project proposal, I saw this visualization made by Rikters et al. (2017).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*uP_-gGiPkv6YMFYz." />An example of a flow-map from machine translation [Rikters et al. 2017].</figure><p>I thought this might be worth exploring, so I attempted to create something similar.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SxZUaHFYssV5JtXo." />A flowmap displaying all edges is problematic. This doesn’t scale either.</figure><p>Unsurprisingly, there are far too many edges to display without significant overlap and occlusion. Summarization rears its head again as a challenge with its large input sequences.</p><p><strong>How do we remedy this?</strong></p><p>One observation is that high attention weights are fairly sparse (as evident by the aggregate on the source text above). What if we filtered out these insignificant weights? A naive approach is to take the top <em>k </em>percent of weights and display only those.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*I4jiiM98R88ZfMKz." />A flowmap showing only the top 1% of weights.</figure><p>The above example is displaying only the top 1% of attention weights. Edges have both their width and opacity scaled by their weight within this 1% domain. Significant (wide) weights are clearly identifiable. Thinner lines, faint (or nearly invisible) lines can also be seen, indicating that the much more significant weights have been preserved. Four distinct ‘rays’ can be seen, seemingly corresponding to the four sentence of the summary.</p><p>Selection and brushing seemed like intuitive follow-ups for interaction to enable more detailed / accurate pattern identification.</p><p><strong>Selection</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*U8_42zGX24nN8iB7Wo-zPw.gif" />Selection over the flowmap, both over the output nodes as well as the output text.</figure><p>Selection allows users to orient themselves in the flowmap, picking out which edges correspond to which input / output tokens.</p><p><strong>Brushing</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*1t7BRJX9g2k80B67IFQ4lw.gif" />Brushing over the flowmap allows for windowed pattern identification.</figure><p>Brushing enables accurate pattern identification. In the example above, brushing over the distinct rays allows us to see the almost entirely extractive nature of the summary — there is a clear 1:1 mapping from input to output that is implied by the clean structure of the rays, and confirmed upon inspecting the corresponding input / output tokens for these rays.</p><h4><strong>Challenges with Flowmaps (Future Work):</strong></h4><ol><li><strong>Selecting a top k% is not ideal — it does not generalize well.</strong> In the pathological case, where attention is evenly distributed for all tokens, we lose a lot of valuable information. A potential band-aid to this is to allow users to select the percentage of weights displayed, but this may be dangerous as high percentages can crash the browser. Perhaps a more elegant solution would be to perform clustering on the weights. I’ll need to do more research here.</li><li><strong>Overlapping paths can make reading the flowmap difficult.</strong> The example shown above is fairly clean, direction flows, for the most part, in a single direction. You could imagine, however that if a summary is extremely abstractive, pulling from all over the source, there might be significant overlap in edges, decreasing legibility. A potential solution to this is edge bundling, where edges going in similar directions are pulled together to preserve pattern recognition.</li></ol><p>A good starting point for exploration here is the paper <a href="http://vis.stanford.edu/files/2011-DividedEdgeBundling-InfoVis.pdf"><em>Divided Edge Bundling for Directional Network Data</em></a> by Selassie, Heller, &amp; Heer (2011). In this, the authors describe a method for bundling, divided edge bundling, that holds characteristics I believe are important for my own visualization.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Ma6OlxvATqGNHbyu." />Different techniques applied to a network of GitHub contributions along the west coast of the United States [Selassie, Heller, &amp; Heer 2011].</figure><p>In the coming weeks I hope to dive into these techniques and explore their impact on the flowmap I’ve developed thus far.</p><h4>In Summary</h4><p>Still a lot of work to do! Here are my goals for the next week.</p><ol><li>Improve flowmap visualization.</li><li>Find examples that cover the problem space (e.g. low coverage, abstraction, extraction).</li><li>Keep brainstorming.</li><li>Optimize code.</li></ol><p>Things are getting interesting!</p><h4>References</h4><ol><li><a href="https://ufal.mff.cuni.cz/pbml/109/art-rikters-fishel-bojar.pdf">Rikters, Matīss, Mark Fishel, and Ondřej Bojar. “Visualizing neural machine translation attention and confidence.” <em>The Prague Bulletin of Mathematical Linguistics</em> 109.1 (2017): 39–50.</a></li><li><a href="http://vis.stanford.edu/files/2011-DividedEdgeBundling-InfoVis.pdf">Selassie, David et al. “Divided Edge Bundling for Directional Network Data.” <em>IEEE Transactions on Visualization and Computer Graphics</em> 17 (2011): 2354–2363.</a></li></ol><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a2d837ecf66b" width="1" /></div>







<p class="date">
<a href="https://medium.com/@halden.lin/nlp-capstone-07-formalizing-a2d837ecf66b?source=rss-2759d54493c0------2">by Halden Lin at May 09, 2018 06:48 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a></h3>


<div class="entrygroup" id="https://medium.com/p/d7a06e892cdd">
<h4><a href="https://medium.com/@viterbi.or.not/advanced-model-1-part-2-d7a06e892cdd?source=rss-c522ef075bb3------2">Advanced Model #1, Part 2</a></h4>
<div class="entry">
<div class="content">
<p>Welcome back to Advanced Model #1 and thanks for sticking with us! This week, we’ve gotten some promising results in our first advanced model attempt and made a lot of progress with the incorporation of a second data source, although there is still work to be done toward fully integrating the chatlog data into a single model. In this post, we’ll go over what we’ve done since last week and give some numbers as well as example summaries from this model, then start laying out what we would like to achieve for Advanced Model #2.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*AsUzi76WPSKaMji6." />Our project roadmap and where we are at now, highlighted in orange</figure><h4><strong>Preprocessing</strong></h4><p>We decided to use preprocessing instead of features for some of the attributes of our data after discovering that our feature sets failed to focus solely on the conversation in some data types and instead gave false importance to format-specific patterns such as quoted text in emails. We eventually settled on the following distinction:</p><ul><li>Attributes of the data that existed at the conversation-level should be features: that is, anything universally applicable to textual conversations in general was implemented as a feature for the model</li><li>Attributes of the data at the data source-level should be preprocessed. This allowed us to share features across different data types (for example, using tf-idf across both emails and chatlogs) while minimizing the number of “dead” features (for example, simply assigning a constant value to a feature for detecting email signatures when vectorizing the chatlog data).</li></ul><p>The preprocessing we decided to do falls under the following categories:</p><p><strong>Identifying format-specific content:</strong> Quirks of the data format can sometimes present problems for our model, for example, we noticed separator lines composed of all dashes or symbols as well as email signatures (left example below) were making it into the summaries.</p><p><strong>Removing quoted email text:</strong> The email dataset contains quoted text of previous emails in the thread when there is an email reply, causing duplication of sentences in the summaries because the model seems to want to include the important sentence as much as possible. An example of a generated summary with the problem is in the right example below.</p><p>After initially parsing the data, we do this preprocessing step to identify such sentences using simple regular expression rules and remove them before computing feature vectors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vUpBdbFjg5Exfd-8." />An email signature (left) and quoted text (right) in generated summaries before adding preprocessing</figure><h4><strong>Adding the Chatlog Dataset — New Pipeline</strong></h4><p>One of our major goals for the advanced model was to incorporate the chatlog dataset we identified early on in the project. Of course, having the capability to automatically generate summaries for chat conversations expands the usefulness and possible applications of our model: although email threads are incredibly common in today’s world, chat communication is also experiencing an incredible rise in popularity. Furthermore, as chat communications tend toward a large number of participants and a model of very frequent responses, our analysis is that chat data summaries have the potential to be even more useful than their email counterparts.</p><p>However, our goal with incorporating the chatlog data is ultimately to combine it with the email data in order to be able to train our model on both data types at once. To do this, we saw the need to create a common list of features that can be applied to any data input, allowing the model to train on both sources of data indiscriminately. As the Sood et. al. paper suggests, training on both sets of data has the potential to improve the resulting summaries of the model for each type of data separately, although the paper in question does not specify their results beyond the notion that they were promising in a preliminary examination.</p><p>The end goal for our system is illustrated below:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YodqfIjCDfX5xJLfNG_JFQ.png" />Pipeline for integrating different datasets with a common feature set into our model</figure><p>The blended colors represent the notion of common features, achieved through separate pre-processing and vectorizing of each data source but used in a single shared model.</p><p>To achieve this, we needed to do a major restructuring of our codebase, and it is unfortunately the case that a huge portion of our time this week was spent improving old code to make it more modular and compatible with these new requirements. What was once a single script to parse, process, vectorize, and train on a single dataset had to be broken apart into different modules for each of those tasks that could implement a common interface. In addition, with the growing number of options and tasks that had to be supported, we discovered it was necessary to entirely separate the concerns of processing text and handling configuration: to that end, we created a new, much more robust command-line interface and broke all other components into individual directories with as much code factoring as possible.</p><p>As a result of our design process and refactor, we currently have almost all of the infrastructure necessary for incorporating the chatlog data and training our model on all of the available data before evaluating on a single source. As we mentioned above, however, there were several challenges with parsing the chatlog data, namely the messy nature of the chats and the number of possible edge cases present in such a large dataset. Therefore, we are still in the process of finishing the parser component, but once it is complete we should be able to use both types of data in Advanced Model #2 and report on the impact that it makes. See our progress so far with this pipeline structure on our <a href="https://github.com/viterbi-or-not-to-be/viterbi-or-not-to-be/tree/master/conversation">GitHub repository</a>!</p><h4><strong>Results</strong></h4><p>Through the addition of the preprocessing step, we have greatly improved our ROUGE scores and become competitive with the advanced model in the Sood et. al. paper. Here are the results of our advanced models, using all of the features we have discussed so far and with the preprocessing step:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iDsZaXOhuHshbaFWXFlPvQ.png" />Results of the three advanced model types</figure><p>These results are significantly better than our baseline, and we believe our model is generally pretty good at picking up on which sentences are important to keep! To more directly see the impact of preprocessing, we compared the results of the Naive Bayes model with and without preprocessing:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*BBrr-Z2ilFTbK8xi-t2SPg.png" />Comparison between our advanced model with preprocessing and without preprocessing</figure><p>The model without preprocessing does decently, but the preprocessing definitely contributes significantly to our advanced model performance. To show our progress since the baseline model, this compares the results of the Naive Bayes baseline models with those of our current Naive Bayes model:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*M_zctk4dPmVbmVHeHmRYYQ.png" />Comparison between our advanced model and our baselines</figure><p>Both ROUGE-1 and ROUGE-2 have stepped up by several percentage points, but the most significant increase is seen in the ROUGE-L improvement. Finally, our model compared with the model in the Sood et. al. paper:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ty7sqP2swRfdf1UGw7wb_g.png" />Comparison between our advanced model and the Sood et. al. model</figure><p>As you can see, our ROUGE-L score is slightly lower but our ROUGE-1 and ROUGE-2 scores are higher! We’ve previously discussed the drawbacks of ROUGE―there is no definitively better type of ROUGE metric and ROUGE in general is not necessarily a reliable indicator of how good the generated summaries really are, so we are wary about quantifying our model’s performance in comparison to the Sood et. al. model’s performance based just on ROUGE scores. However, we haven’t been able to find any examples of the summaries generated by their model so this is all we have to go off of for now.</p><h4><strong>Example Summary and Analysis</strong></h4><p>Because ROUGE provides relatively unreliable metrics for evaluation of our model’s summaries, we have also turned to human evaluation. One of the summaries that our model was able to generate based off an input email thread, using all of the preprocessing and extra features that we have built, is reproduced below in its original condition except for display-motivated truncation:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BMVwYccbvULdQcwp." />Comparison between human annotated reference summary (left) and summary generated by our advanced model (right)</figure><p>This summary is fairly representative of the summaries that are being generated, and compared to the results we saw from our baseline models it is clearly doing better in terms of human evaluation. The sentences highlighted in green are those that are shared between the reference annotation and our system’s generated summary — interestingly, all but one of the sentences in the reference summary were successfully captured by our system. Therefore, reading through the summary generated by our system gives a strong impression of the topics that were discussed in the thread, and seemingly gives a human reader all the information that is needed.</p><p>However, there are certainly aspects of the current summaries produced by the model that are lacking. In the summary given above, all 6 sentences that were truncated for display purposes at the ellipsis and the four additional non-green sentences displayed were not included in the reference summary. This summary comes from a thread with only 31 sentences after preprocessing, meaning that while our model successfully captured all of the sentences marked as important by a human, it did so at the cost of incorporating an enormous percentage of unnecessary extra text that serves to make the summary less of a “summary” while adding relatively little content. To address this, we will need to figure out how to reward the model for producing more concise summaries, or to incorporate a “compression factor” that allows producing a summary with the k most relevant sentences for some value of the hyperparameter k. Another issue is highlighted in red above — while we added preprocessing to remove email signatures based on a set of heuristics, to avoid deleting important content we made the preprocessor act conservatively when uncertain, and as a result some signatures like in the example above made their way into the summaries. Although other features would ideally prevent these signatures from appearing in many cases, we will need to improve our preprocessing heuristics in order to target these kinds of medium-specific text examples before moving on to incorporate additional types of data.</p><h4><strong>Next Steps</strong></h4><p>To entirely finish Advanced Model #1, we would like to complete the process of incorporating chatlog data and then transition into work on Advanced Model #2 by continuing to explore and add conversation-specific features. We also want to address the excessive lengthiness problem we found in the generated summaries by doing something to limit the number of sentences the model is allowed to select for the extractive summary.</p><p>Beyond these immediate steps, we would love to be able to take a stab at abstractive summarization! It’s a really challenging but rewarding conversation summarization problem that we will need to put some more thought into to attempt. We’ve preliminarily determined that we can make an attempt using the extractive output from our current model along with some other contextual metadata from the conversation data (for example, authors of the chosen sentences), to get a start on an abstractive summary. In general, we think this will require more modeling of entity relationships and text generation.</p><p>Overall, we’re excited with what our model has been able to do so far and optimistic about what we’ll be able to achieve with Advanced Model #2, so stay tuned!</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d7a06e892cdd" width="1" /></div>







<p class="date">
<a href="https://medium.com/@viterbi.or.not/advanced-model-1-part-2-d7a06e892cdd?source=rss-c522ef075bb3------2">by Viterbi Or Not To Be at May 09, 2018 06:33 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 08, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1-part-21">
<h4><a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1-part-21">Advanced Model Attempt #1 (Act 2 Scene 1)</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">This week, I had to do a little backtracking and restructuring, but all for the better! <ul><li>More Data! I have had a couple jobs still running that has allowed me to have more reddit data to work with overall, which is especially good news for the next advanced model</li><li>Subreddit ReOrg - A large portion of this week has been on identifying different subsets for the neurodivergent and neurotypical groups. In conversation with Maarten, we discussed that the subreddits in the current state are a bit too broad to address the hypotheses and may not produce a focused language model that makes sense. Currently, the neurodivergent group is a broad strokes grouping of all types of differences. With that in mind, I spent some time researching, following some Reddit rabbit holes, and learning about ICD 10 <font color="#515151">(International Statistical Classification of Diseases and Related Health Problems (ICD), a medical classification list by the World Health Organization (WHO)) to classify the subreddits and the mental illnesses they speak to. Among these topics, we specifically wanted to look at the the two 'Blocks' - Mood (affective) disorders and Neurotic, Stress-Related and Somatoform disorders. This would be defined as the new 28 subreddits constituting the neurodivergent set, and the 22 neurotypical subreddits are those that are general advice, community support, or discussions on mental health, broadly speaking. These were chosen from the initial ND set because while some of them discussed mental health and neurodiversity, they were open to more than just users dealing with mental health related issues. They had similar types of posts where people are more open and discussing in detail their personal lives, a kind of venting space. And choosing from this group rather than to look at more general reddit, was a decision made so that I could find the nuances rather than the more general difference seen in discussing psychology and not. Despite the significant reduction of subreddits (from 126 to 28 ND and 100 to 22 NT), the data collection I mentioned above has resulted in 5x the original vocabulary size and we now have a combined 1 million words in the newest set (being added to as we speak)</font></li></ul><br />With that, I was able to run the previous vennclouds and idp models (and Act 2 Scene 2, before next week will hopefully include the Connotation Frames model results). For the vennclouds, I think we see some similarity from before of the "my/me/I" terms speaking in first person, but the NT posts has more discussion of a second person or a more general reference to an other or friends. It might also be useful to find a list of prepositional phrases to remove and to find something more interesting in the venn diagram middle portion. </div>  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-09-at-2-32-31-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>  <div class="paragraph"><br />I also ran the IDP model from before and found the following top categories:<br /><strong>ND</strong><br /><em>I/<span>my/</span></em><span><em>me/myself</em> - the personal discussion makes sense, but I'm a bit curious as to why it would be more prevalent than in the other discussion forums that are also predominantly venting spaces focused on the self seeking the community. </span><br /><em>anxiety/depression/OCD</em> - these are the subreddit categories we basically chose<br /><em>feel/feeling </em>- I was surprised to see a significantly higher mention of feeling in the ND categories despite the other subreddits still being communities discussing mental illness along with the larger public in discussing general problems <br /><br /><strong>NT</strong><br /><em>you / his/he/him / girls/she/her - </em>This is a bit surprising given that I had made a bit of a fuss around the female mentions being significantly higher in the ND categories. I'm interested to see what this shift might mean, but even more so, I think the discussion of another is an interesting contrast between the two that might be better suited for the connotation frames results that I hope to do in the next couple of days. <br /><em>https</em> - links are back! But more interestingly, while they were prevalent in the original NT group, which we hypothesized were due to the higher proportion of anecdotal advice in the ND categories, it seems that even in regards to discussions around mental illness, people bring in outside links and whatnot to advise, or uplift other users. <br /><br />Next Steps: As I mentioned above, I'd like to run this same new and larger dataset with the connotation frames model. Beyond that, I'd like to spend the next two weeks working on either implementing a SAGE log-linear model to describe these language models, or a deep learning model. If possible, I hope to also get the graphical model going <br /><br /></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1-part-21">May 08, 2018 07:00 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 03, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a></h3>


<div class="entrygroup" id="https://medium.com/p/4626fe37cadb">
<h4><a href="https://medium.com/@viterbi.or.not/advanced-model-1-part-1-4626fe37cadb?source=rss-c522ef075bb3------2">Advanced Model #1, Part 1</a></h4>
<div class="entry">
<div class="content">
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_Pn2CYQZkoIzZYLwVPV2MA.png" /></figure><p>Work on our advanced model is underway! To recap, we aim to upgrade our baseline model by including other forms of conversational data (chatlogs and meeting transcripts in addition to the email data we have been using) into our training and adding features that target characteristics specific to conversational text.</p><h4>Extensibility and Other Datasets</h4><p>To achieve this, we first worked on restructuring our baseline code to be extensible to allow addition of inputs from other datasets and to allow different methods of feature extraction based on the dataset. We’ve started splitting our baseline code into smaller parts (parser, feature extractor, model) and creating a structure for the new code segments and datasets. We’ve also parsed one additional data type (GNUe chatlogs) and begun the process of integrating the data into the structure. Unfortunately, we have not yet been able to produce summaries using this dataset — while we have the parser working, several of our features and evaluation scripts do not translate easily to the new formatting and we are still working on refactoring our codebase to handle it. As we complete our first advanced model, bringing these new data sources into the fold will be a top priority so that we can begin to compare the differences between the dataset and work toward our stretch goal of training on both datasets in order to improve performance on each.</p><h4>Preprocessing</h4><p>Looking at the generated summaries from our baseline model, we found a few mistakes that could be easily avoided using automated preprocessing. The summaries often included quoted replies, email signatures, and other insignificant lines that could be eliminated by looking at simple characteristics of the sentence (for example, a line or sentence ending in a comma usually indicates that it is a greeting or closing clause). We previously added a feature based on the sentence starting with a ‘&gt;’ character to eliminate sentences that were quotes from previous emails in the thread, which improved performance. We are now moving this to the preprocessing step. The preprocessing will occur during after parsing the dataset, but before feature extraction, and because it is highly specific to the types of data we are parsing it will have to be implemented separately for both email and chat data.</p><p>Another addition we have been looking into has been detection of email signatures. While we have not yet created a very successful system for this, we have some ideas on how to proceed as we finish our first advanced model. We have already tried adding a feature for a sentence’s proximity to the end of an email, but unfortunately it was not very successful in removing signatures. Therefore, we are hoping that preprocessing will fit the task better — for example, we might examine the email for the last contiguous block of non-empty lines, and remove them under the assumption that the email signature will always be at the end and will never be useful in a summary. But this comes with its own challenges, as some emails in the dataset (and in general use) do not have a signature, and thus the last contiguous block might be content or even the entire email in some cases. One idea we have to combat this would be application of a language model to the sentences in a contiguous chunk. If a language model (such as an n-gram model) were trained on the English language and exhibited a high perplexity when looking at a given sentence, it might be safe to say that it is not prose and is instead a collection of names, emails, or organization names as one might expect to see in a signature. Be sure to check back in our Part 2 blog post after we have implemented this preprocessing and reported on the results!</p><h4>Conversation-Specific Features</h4><p>We started our foray into conversation-specific features with the those listed in the complete features set (beyond the basic set in our baseline) in the Sood et. al. paper, before expanding upon this set with our own contributions that target email data specifically. So far, we’ve implemented the following features based on the paper —</p><p><strong>Is Question:</strong> Important issues and concerns are often expressed in the form of questions. To take advantage of this idea, this feature represents whether or not (1 or 0) the sentence is a question. When training using this feature, we found that</p><p><strong>Sentiment Score: </strong>Based on the idea that strong sentiments and opinions are important to a conversation, this feature captures the sentiment of a sentence. The score is determined by taking the difference between the positive and negative and score for each word in the sentence, then adding up the scores of all the words in the sentence and normalizing by the number of words in the sentence.</p><p>In addition to the features in the Sood et. al. paper, we came up with some more feature ideas based on our error analysis of the baseline model —</p><p><strong>Numbers:</strong> Even with the ‘Special Terms’ feature, our baseline model often missed lines containing numbers or statistics that we would want to preserve in a summary because they were short or eliminated by the importance of other features. In order to address this shortcoming, this feature uses a simple regular expression mechanism to determine the occurrences of numeric data in a sentence.</p><p><strong>URLs:</strong> The baseline model also often missed URLs, which are typically important information and usually contained in the summary annotations of the dataset. This feature would address this problem by using a regular expression to determine how many URLs are contained in the sentence.</p><h4>Results</h4><p>At first, we tested the features that were described in the Sood et. al. paper, evaluating them on the Naive Bayes model that we had found to be most generally successful in our previous baseline experimentation. As corroborated by the paper’s findings, we discovered that these features were somewhat impactful in improving the performance of our model, at least as far as ROUGE scores were concerned:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*X9KXUGGowXEptmVn3kv-jg.png" />Results of the model with the conversation-specific features from Sood et. al. compared with the baseline</figure><p>Noticeably, the addition of these conversation features was strictly better for ROUGE evaluation. Both the question feature and sentiment feature were roughly equivalent, although we noticed that when we subjected the summaries to human evaluation it was easier to notice an improvement from the question feature. Many of the important sentences in the reference summaries seem to have been questions, perhaps because asking a question is such a common way to transition to a new topic in an email thread. As a result, we found that this feature helped populate our summaries with many of the questions from the reference summaries, establishing clear boundaries. Even if some of the sentences in our summaries were less impactful, having the questions made a big difference for a human reader because it was possible to get the general idea of the thread from the questions alone. As an example, this is one of our generated summaries, compared with a reference:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ONLBMtMpzebsdPUa1AzWBQ.png" />Comparison of reference summary (left) to our generated summary (right) using the Is Question feature</figure><p>It is noticeable that the questions add a significant amount of relevance to our summary, although there is still work to be done as far as capturing the shorter, choppier, but still more important non-question sentences.</p><p>However, because another significant goal of our advanced model is to explore other conversation-specific features not addressed in the research paper, we did some additional exploration on our own to find features that could take our model’s performance beyond the results of Sood et. al., even if only for email data. So far, we have found two that improved our ROUGE metrics while also seeming to make more relevant summaries by detecting numbers and detecting URLs in a given sentence. Our results are as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xiga8kKb3O5cYHYP2sDbTQ.png" />Results of the model with our conversation-specific features compared with the baseline</figure><p>Using the numbers feature in particular, we found that our model was able to outperform the baseline considerably, although upon human examination we had trouble determining exactly why this had happened as the summaries did not capture that many numbers. However, as far as human examination went, we discovered that the URL feature made a clear impact, adding a considerable number of important URLs to our summaries that had originally been passed over by the model even though human annotators had marked them as being especially important.</p><p>Finally, we tried all of these new features together to see how they compared to the baseline:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*l7tr_T6H5jnqWRbSf7NeGA.png" />Results of the model with the all of the additional features discussed compared with the baseline</figure><p>Ultimately, while these features were relatively few in number, their use of conversation-specific data clearly led to a great deal of performance increase for our model.</p><h4>Next Steps</h4><p>For Part 2 of our advanced model, we would like to finish restructuring and incorporating the other datasets into our model input. Because one of our stretch goals is to be able to find a way to incorporate multiple types of data into a single model, we are hoping to do this in a way that allows us to compute the same (or at least similar) features for each type of data.</p><p>Another goal is to implement the preprocessing steps described above as a part of dataset parsing. Significantly, there are some advanced preprocessing techniques described by the Sood et. al. paper that deal with chat data specifically, so we hope to incorporate those once the chat data is working correctly. Finally, we want to continue adding features — although it is certainly unlikely that all of the features we try will be as successful as the ones described above, we hope that this project will culminate in a thorough examination of the impacts that various features can make, so we will continue to explore in this regard as we finish up our first advanced model.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4626fe37cadb" width="1" /></div>







<p class="date">
<a href="https://medium.com/@viterbi.or.not/advanced-model-1-part-1-4626fe37cadb?source=rss-c522ef075bb3------2">by Viterbi Or Not To Be at May 03, 2018 06:56 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/d01e84c5e1da">
<h4><a href="https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-neural-based-definition-extraction-d01e84c5e1da?source=rss----9ba3897b6688---4">Advanced Model Attempt #1: Neural-Based Definition Extraction</a></h4>
<div class="entry">
<div class="content">
<p>We last left off on the idea of using an FSA with a restricted vocabulary; restricted in the sense that we extract sentences coupled with a neural language model to assure semantic quality while allowing a generative RNN model a reasonable amount of improvisation to produce abstractive definitions.</p><p>Here, we discuss our approach for the extractive component of this model, and consider it our first attempt at an advanced model for the task.</p><h3>Introducing Extractive Summarization</h3><p>Recall that extractive summarization is the idea of reducing text down to a subset of its sentences that still preserves its semantic integrity. In particular, we intend to build on the work of a successful nerual-based extractive summarizer and tailor it to solve our task.</p><p><a href="https://arxiv.org/abs/1611.04230">SummaRuNNer</a> is an RNN-based extractive summarization algorithm developed by Nallapati et al. that encodes documents from the word level up to and across the sentence level before making inference. Essentially, the model is a binary classifier on sentences within a document on whether it should be included in a summary. Its decisions are conditioned on</p><ul><li>Averaged-pooled word-level hidden states of the sentence</li><li>Average-pooled sentence-level hidden states of the document</li><li>An abstract representation of the summary built so far (average-pooling of the word-level pooled hidden states of sentences selected thus far)</li></ul><p>After which, there are several affine transformations conducive to selecting and filtering sentences:</p><ul><li>Content: affine on the abstract sentence representation that measures semantic richness</li><li>Salience: bilinear affine on the abstract sentence representation and the document representation to measure cohesiveness</li><li>Novelty: bilinear affine on the abstract sentence representation and the running summary representation to address redundancy</li><li>Absolute and Relative Positioning: two separate affines on the embedded index of the sentence to allow how far we are into the document to influence inference</li></ul><p>As of now, we have built from scratch our own <a href="https://github.com/NLP-Capstone-Project/machine-dictionary/blob/development-tam/machine_dictionary_rc/models/SummaRuNNer.py">unofficial implementation of this model</a> with inspiration from another <a href="https://github.com/hpzhao/SummaRuNNer">unofficial implementation</a> and is capable of summarizing documents the way we’ve formatted them. What’s left is for us to tailor this model to fit the task.</p><h3>A Slight Twist on an Established Task</h3><p>As of now, the model summarizes documents. We’d like it so that it instead zeroes in on query terms we give it given a research paper, to intelligently extract only sentences from that paper conducive to defining that term.</p><p>Our approach for augmenting SummaRuNNer to be a definition extractor involves</p><ul><li>Encoding the query term with a character-level RNN and using its concatenated hidden states as its representation</li><li>Introducing this new query-term abstract representation when constructing the document representation through a bilinear affine</li><li>Further introducing this query term by converting many of the non-bilinear affines (content, positioning, and possibly new ones for the task) to further condition inference on the query term.</li></ul><p>Essentially, the sentences we extract from the document are being conditioned on the term we’re trying to define. Encoding technical terms using a character level RNN allows similar technical terms to have similar hidden representations. For example, if we see the term “Chronic Lymphocytic Leukemia” in the training data and encounter “Chronic Myelogenous Leukemia” in the testing data, we would have more of an idea of how to approach this new term because of its character level similarities to the term we have already seen during training time. This might help us break down more complicated novel technical terms at testing time.</p><p>Experiments have yet to be conducted on the effectiveness of this approach but will be discussed later in <strong>Advanced Model Attempt #1 (cont.):</strong> another post later in this series discussing the results of the groundwork we’ve laid out here.</p><h3>Training Methods</h3><h4>Collection Training Data with UMLS and ROUGE:</h4><p>Recall that SummaRuNNer is a model that aims to extract the sentences in a document that summarize it best. It does so by training on examples that teach the model which sentences to extract from the document.</p><p>SummaRuNNeR uses a <em>distant supervision</em> method that relies on ROUGE in order to produce training examples for the model. This portion of the architecture, which we refer to as the “extractor”, extracts the sentences out of each document which maximize the ROUGE score when compared against the gold standard definition for the term in question. The extractor in a summarization context can use a greedy approach as follows:</p><ul><li>Look at each sentence in the document one at a time and consider appending it to the extracted sentences that we have already chosen.</li><li>Calculate the ROUGE score of the old extracted sentences + this new sentence in comparison to the gold standard summarization for the document.</li><li>If the ROUGE score increases from the previous ROUGE score, keep the new sentence.</li><li>Otherwise, we don’t keep the new sentence and move on.</li></ul><p>Although this method may not produce the most optimal and compact set of sentences that are relevant, this approach will be faster and is reasonable. The output of the extractor for each document is a tensor whose length is the number of sentences in the document, and is 0 if the sentence is tagged with O or 1 if the sentence is tagged with I.</p><p>To tailor this style of data collection to our task however, we optimize on ROUGE with respect to an entity’s gold-standard definition instead of a gold-standard summarization of the document. We collect entity-definition pairs through <a href="https://www.nlm.nih.gov/research/umls/">UMLS</a> and creating training examples of the form</p><ul><li>Entity (the technical term to define)</li><li>Gold-standard definition for the entity</li><li>The target sentence IO tags found via distant supervision with ROUGE on sentences of a research paper with the gold-standard definition being the reference</li><li>A Semantic Scholar research paper in which the sentences came from (provides the sentences in which to perform inference)</li></ul><p>With this data, we can train the definition extraction model discussed earlier; we train using these &lt;entity, IO-tagged sentences, publication&gt; examples to learn a tagger that can extract sentences most relevant to a term given a publication.</p><p>While this may result in an unnecessarily large number of training data points, we can also consider pruning this dataset later on if we have irrelevant entities for a particular document. For example, if we were trying to find a training example that used the entity “dental cavity” for a document that was about blood cancers, we might not want to keep this training example because there wouldn’t be much of a correlation between the two. In order to do this, we can introduce a ROUGE threshold, where we only keep the training example if the ROUGE score of the sentences extracted by the tagger are above a particular threshold. This might be an optimization for the future.</p><p>Our previous approach was unsupervised and it relied only on the semantic scholar dataset to produce definitions. Our current approach is an extension of SummaRuNNer which requires gold standard definitions for entities that we’d like to define in each paper. We chose to focus on medical terms, and one of the most complete datasets for medical terms and their definitions happens to be the UMLS dataset. This dataset contains a <em>Metathesaurus</em> which contains, amongst many other pieces of data, medical terms and their definitions. The technical terms in the dataset serve as references for ROUGE in the tagging phase above.</p><h4>In summary</h4><p>Training is fairly straightforward; loss between predicted and target sentences is computed with log loss (each sentence in a document is IO-tagged where sentences labeled with <em>I </em>are to be included in the definition). Essentially, the definition extractor, much like SummaRuNNer, is trained as a sentence tagger.</p><h4>Attention as a Stretch Goal</h4><p>The first part of our basic SummaRuNNer-based model uses a document representation to predict tags for sentences in a document. The current document representation is constructed by averaging the hidden states from words in each sentence and averaging the hidden states from each sentence in the document. However, we believe that simply averaging the sentences may not be the best approach to constructing the latent document representation. One of our stretch goals for us to optimize the model will be to attend to the most important parts of sentences in each document. We can do this using the method proposed in Hierarchical Attention Networks for Document Classification (Yang et. al 2016).</p><p>This approach introduces a word level context vector and a sentence level context vector which allow us to calculate attention coefficients on the fly for every word in each sentence and every sentence in the document. In this manner, we can take a weighted sum of the hidden states in the sentences and will hopefully produce better document representations overall. The word level and sentence level context vectors can be initialized randomly and learned throughout training.</p><h4>Conclusion</h4><p>We are very excited to have found a supervised approach to this task per the advice of AI2 researchers. It’s a straightforward approach with measurable loss and clearer metrics.</p><p>We also hope to have enough time before the capstone is over to introduce attention!</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d01e84c5e1da" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-neural-based-definition-extraction-d01e84c5e1da">Advanced Model Attempt #1: Neural-Based Definition Extraction</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-neural-based-definition-extraction-d01e84c5e1da?source=rss----9ba3897b6688---4">by Tam Dang at May 03, 2018 03:32 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 02, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" title="Stories by Halden Lin on Medium">Halden Lin <br/> Team undef.</a></h3>


<div class="entrygroup" id="https://medium.com/p/6f773ae418d0">
<h4><a href="https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0?source=rss-2759d54493c0------2">NLP Capstone | 06: Uncertainty</a></h4>
<div class="entry">
<div class="content">
<p>previous posts: <a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5">01</a> <a href="https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5">02</a> <a href="https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3">03</a> <a href="https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7">04</a> <a href="https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a">05</a></p><p>I’ve begun to realize I may not be getting as much out of the project I chose I had hoped. My initial motivation for my project was a hope of expanding my knowledge and developing insights on the NLP front by leveraging the familiarity of Visualization. While I am certainly learning a lot by reading papers on Attention and Neural Networks as a whole (especially through my in-class paper presentation), I feel the work I am doing in building a tool for visualizing and debugging attention models may not be providing me the space to explore NLP that I had hoped for. While the tool will certainly <strong>enable </strong>exploration, my concern is that this exploration will not occur until after the tool is completed at the end of the quarter.</p><p>The good news is that there have been two recent developments that, while increasing my uncertainty, offer potential for greater depth in exploration along the NLP front.</p><h4>1. Potential Pivot</h4><p>I voiced these concerns with Prof. Choi this past week and was given a good amount of valuable advice. Per her suggestion, the beginning of my last cycle began with three tasks.</p><ol><li>Read <a href="https://dl.acm.org/citation.cfm?id=2470718"><em>The Efficacy of Human Post-editing for Language Translation</em></a><em>, </em>authored by<em> </em>Spence Green, Jeff Heer, and Christopher Manning. This paper is unique in that it presents the value of Visualization and HCI within Natural Language Processing, but not as a window into a model. Rather, the authors explore a specific task integral to the Language Translation pipeline and present suggestions for future work in improving Language Translation.</li><li>Do in-depth human error-analysis of existing summarization models. I used examples from See et al.’s paper <em>Get To The Point: Summarization with Pointer-Generator Networks </em>(2017).This was helpful gaining a better intuition as to the problem space and the challenges currently posed by machine summarization.</li><li>Think about how summarization as a task, whether that be the development of models, the model’s task itself, or end-user tasks that use the model, can be re-framed in order to leverage Visualization. This was especially time consuming, as it was difficult for me, but it helped immensely in taking a step back to understand the purpose of these models. This, in turn, helped me understand how my work can fit into this purpose.</li></ol><p>The next few days consisted largely of brainstorming pivots for my project. The most promising direction that came out of these sessions is very briefly outlined below.</p><p><strong>Assisted Cognitive Document Abstraction</strong></p><p>Machine-generated document summaries, even the state-of-the-art, are infrequently used in practice because their summaries are quite poor. Perhaps we can leverage existing models to, rather than produce definite summaries which may be error-prone or difficult to understand, create visualizations over the source text in order to assist humans in comprehension and abstraction.</p><p>For example, instead of treating attention weights as input for an output of text, as we do in summarization models, we can view them as output for human interpretation. Aggregate attention distributions (in summarization) highlight areas of the input text that are salient for the summary produced. Note that this is potentially more valuable than highlighting extractive summaries in the text because attention could potentially point towards different areas of the text that relate to a summary sequence. In this way, generation of summaries becomes a proxy task for creating salient highlights for text. We could then use this as a starting point from which ‘related’ sections in an article may be highlighted for users upon interaction (e.g. mousing over an attended-to sequence).</p><p>The hope is that these visualizations will increase the speed (over no summarization) or accuracy (over machine summarization) at which readers can abstract / understand key ideas in a document.</p><p>Most excitingly, with this re-framing of the task for these models, from sequence output to highlighting, perhaps the models can be modified by adding or removing constraints and mechanisms in order to improve performance for this new task.</p><p>Upon presenting this idea (in longer form) to Prof. Choi, I was encouraged to (1) think more about weaknesses of removing summaries altogether and (2) push for more novelty in the approach — is there any meaningful insight about attention models or summarization as a task that can be gleamed from this pivot, and if not, how can I work towards that. While I do not yet have answers to these concerns, the next development may result in a few.</p><h4>2. Related work, here at the Allen School</h4><p>It was just recently brought to my attention that a Tongshuang (Sherry) Wu, a PhD student in the Interactive Data Lab (in which I am currently working), is also working on visualizations for understanding attention models in NLP. As a part of her project, she and a few of her peers have developed a preliminary visualization tool for an attentive QA model (on the SQuAD dataset).She and my mentor, Kanit (Ham) Wongsuphasawat (whom I have been bouncing ideas off recently), have kindly offered to meet and discuss her work and insights on the problem space. Perhaps collaboration is a possibility — this is exciting! In any case, I suspect talking with Sherry and Ham will provide me the insight and guidance to make a decision on the direction of my project.</p><h4><strong>Future Work</strong></h4><p>I hope to make another blog post in the coming few days as I iron out a future direction. Before this, however, future work is unclear. Until next time!</p><h4>In the meanwhile (supplementary material)</h4><p>I’ve also been playing around with my visualization prototypes, even as I am uncertain as to whether or not they will be relevant to my project after this week. Here’s what I’ve discovered and implemented in that time.</p><ol><li>Interactive heat-maps likely won’t work.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_bVKvDPn6jBADKwFU9VG3g.gif" />Interactive heat-maps result in a large degree of lag between input and visual update. This is likely due to the extremely large size of the attention matrix in summarization (24,000 individual squares in the heat-map).</figure><p>This is unfortunate, but browser limitations are limitations that must be worked around.</p><p>2. Selection over output text.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/999/1*KfMQd6YnDzdH9dZOyjVEIw.gif" />Mousing over words in the summary results in a view of the attention distribution over the article for that decoder time-step.</figure><p>This is similar to the interactive visualizations presented by See et al. in their <a href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html">blogpost</a>.</p><p>3. Brushing over output text.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*nwGIF3VgNDeuFxRml8gGzg.gif" />Brushing over the summary results in an aggregate attention distribution (i.e. coverage) over the article for the selected decoder time-steps.</figure><p>This is an interaction technique I have yet to see in work involving attention analysis, so this is exciting! It looks to be somewhat useful in identifying sections of input text that are salient to an <strong>idea</strong> rather than a <strong>single word</strong> in the output text.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6f773ae418d0" width="1" /></div>







<p class="date">
<a href="https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0?source=rss-2759d54493c0------2">by Halden Lin at May 02, 2018 06:59 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=21">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/05/02/advanced-model-attempt-1-part-1/">Advanced Model Attempt 1 (Part 1)</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>This week I created and populated my SQL database with the birth year, birth place, industry, gender, profession, and continent information corresponding to 100 random famous people spanning across all time periods. Furthermore, I created a little over 30 questions that the model can ask as well as the corresponding SQL queries for each question. During each game for the simulation, the user will randomly pick a person for the model to guess and the model picks from a list of these questions to ask. Then, the query corresponding to this question is used to extract the truth value of the question from the database (Yes, No, Unknown). This answer is used as the response to simulate a real person giving that answer through user input. Now that the code has been written to interact with the database, the model can now fully create the observation at any point, which is the input vector to the DRQN. Next, I will hardcode the sample rewards as well as the rules of the game (maximum 20 questions, rewards for winning/losing/wrong guess, terminating game, etc.). This week I will be focusing on getting the simulation to run end-to-end, use tensorflow-gpu, and do hyperparameter tuning.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/05/02/advanced-model-attempt-1-part-1/">by ananthgo at May 02, 2018 06:38 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 01, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1">
<h4><a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1">Advanced Model Attempt #1 (Act 1)</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">The first part of implementing my advanced model attempt was to work on implementing the IDP Algorithm presented in  Monroe, B. L., Colaresi, M. P., &amp; Quinn, K. M. (2008). <em>Fightin' words: Lexical feature selection and evaluation for identifying the content of political conflict. </em><font size="2">Political Analysis, 16(4), 372-403. </font><br /><br />In doing so, I was able to find the weighted log odds ratio of each word present in both ND and NT posts, ultimately showing which type of subreddit each word was 'more affiliated' with. The findings were as one might expect, especially with my previous baselines and were in line with the results from those. As seen below we see some familiar words within the ND (I, you, <strong>she</strong>​) and NT (http) - so sorry for the ugly terminal output, but I need to find a prettier CSV presentation:</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 50%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-02-at-12-42-36-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 50%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-02-at-12-43-09-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph">This was a good first step, and will need some more work hashing out some final implementation details, but my next step in making this an actual advanced model, is to now utilize some of that reddit data that I've been harvesting for the past week or so. With that, we have a lot more data and might need to make some changes on the subreddit subsets depending on how the data has developed (changes in sentence length and number of total number of posts in each subreddit). Off to more data!!!</div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1">May 01, 2018 07:00 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 26, 2018</h2>

<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=18">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/04/26/strawman-baseline-2-same-drqn-model-with-a-different-policy/">Strawman/Baseline 2: Same DRQN Model with a Different Policy</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>I decided to narrow the scope of my problem by changing the types of questions asked to the user to be only yes/no questions. This simplifies interpreting the user response into a classification task. More specifically, the questions will be about figuring out the attributes of any entities brought up in the text.  Therefore, I will use a relational database to store the accumulated entity-attribute relationships instead of a semantic network since it will be easier to extract quantifiable features by performing specific queries. In addition to asking the user questions, I decided to involve database queries in the training process. This way the model can query the database about any inferences or guesses it has about the attributes of an entity, and the database can return a list of entities that fit the hypothesis. Then, the model is given a reward based on how much it was able to narrow down the list of entities. This allows the model to get frequent reward signals from the database to speed up training.</p>
<p>Because of this change, my model architecture has also changed. The DRQN will now take as inputs the action from the previous time step (one-hot vector), the user response (word embeddings passed through CNN), and the database response (number of entities the previous query narrowed it down to). In addition, the outputs to the LSTM at each time step will feed into A+1 policy networks where A = number of attributes. The first A policy networks are needed because the model needs to learn how to guess each attribute independently. The last policy network determines which question the model will ask the user. So far I have implemented most of this architecture but still need to add in the policy networks and debug.</p>
<p>Finally, I will implement a question simulator to randomly pick an entity and have the model guess what it is, similar to 20 questions. During this simulation, rewards will be automatically given every time the model queries the database and whenever the game ends (win or loss). Furthermore, a small penalty is given for a wrong guess. By implementing this simulator, the need for user input to give rewards is eliminated and this should completely automate and greatly speed up training.</p>
<p>My goal for next week is to finish implementing and debugging the model to start this training simulation.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/04/26/strawman-baseline-2-same-drqn-model-with-a-different-policy/">by ananthgo at April 26, 2018 06:58 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 25, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" title="Stories by Halden Lin on Medium">Halden Lin <br/> Team undef.</a></h3>


<div class="entrygroup" id="https://medium.com/p/306dca636d3a">
<h4><a href="https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a?source=rss-2759d54493c0------2">NLP Capstone | 05: Experimenting</a></h4>
<div class="entry">
<div class="content">
<p><em>previous posts: </em><a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5"><em>01</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5\"><em>02</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3"><em>03</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7"><em>04</em></a></p><p>Hi! Here’s what I’ve been up to in the past week.</p><h4>Progress on the TensorBoard Plugin</h4><p>Real data collection and the backend are functioning!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*l6M8uMcswVoaEtL0_HGt0w.png" />Architecture of the Attention Plugin.</figure><p>At this point last week, I had implemented the nodes in green above. These were the operators / functions required to produce Summary protobufs that are in turn saved to disk.</p><p>This week, I completed a number of tasks to produce a bare-bones functioning plugin (sans visualizations).</p><p>First, I modified the source code for <a href="https://github.com/abisee/pointer-generator">See et al.’s (2017) attentional models</a> to use the Attention Plugin API to save input text, output text, and attention distributions during evaluation.</p><p>Next, I implemented the Attention Plugin’s back-end, which is used to fulfill requests made by the front-end. This service currently offers two services:</p><ol><li>tags This route returns all tags associated for each run in the log. This should include 3 tags for each run: one for each of the input, output, and attention tensors.</li><li>attention This route returns a list values associated with the given tag (including time and step stamps). This can be used by the front-end to acquire each of the input, output, and attention lists (converted from tensors) by passing the corresponding tag (retrieved using the tags route).</li></ol><p>Finally, as a proof of concept, I modified the front-end provided in the <a href="https://github.com/tensorflow/tensorboard-plugin-example">TensorBoard Plugin Example</a> to consume this back-end, showing it is able to retrieve summaries. Now we just need some visualizations to consume the data!</p><h4>Visualization Prototyping Begins</h4><p>While data collection and back-end development has been wrapping up, I’ve begun to prototype static visualizations for the plugin. To do this, I used data produced by <a href="https://github.com/abisee/pointer-generator">See et al.’s (2017) pre-trained attentional models</a> (produced only at decode time without the Attention Plugin). Through this process, I hope to gain two things in particular.</p><ol><li>A idea of what will/won’t work as visualizations for summarization tasks.</li><li><strong>A better understanding of the behavior of attentive models</strong>, and through that a better idea of how static and/or interactive visualizations can further interpretability and understanding.</li></ol><p>The first idea I decided to pursue was that of a <strong>condensed heat-map</strong>. You may recall the conventional heat-map used for attention visualizations described in <a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5">my first blog post</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/612/1*_sq2Vy_Py7hEXp2tWBBXxg.png" />Rikters et al. (2017). A heat-map with relatively large cells, allowing for display of text along the axes.</figure><p>The issues I noted with this visualization pattern are as follows:</p><ol><li>It is difficult to fit the words (as seen above) on the x-axis, harming readability.</li><li>This does not scale well with large input or output (e.g. summarization)</li><li>We do not read single-tokens at a time (i.e. y-axis), and input and output are generally not in this format either.</li></ol><p>To address the point (2), scalability, I decided to try producing a heat-map with no text labels, and thus each cell could be as small as a single pixel.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WOlVYgmeZ0DmutTZkDXn2g.png" />A prototype of a condensed heat-map, where x-axis represents input and y-axis represents output.</figure><p>The color scale is a discrete scale, where each step is determined by the quantiles of the weight distribution. The x-axis represents the input text, and the y-axis represents the output text, with each cell representing the amount of attention paid for that pair (output paid to input). The good news here is that the attention distribution is relatively easy to understand at a quick glance. The downside is that cells that are not part of a larger trend (you may notice a lone red spot near the top of the heat-map, approximately a quarter of the way through the x-axis) are harder to make out, as the cells are so small. Further, the distribution is contextless — we don’t know the structure of the input text or what words these high weights are associated with. In the example above, we understand that the model focused primarily on the beginning of the article, but we can’t tell whether that is good or bad without seeing the text.</p><p>To remedy this, I decided to also display the input text, with the input text highlighted according to the maximum of the weights it received. This also solves concerns (1) and (3) for the conventional heat-map.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dJXS9s_391dACPuUwAgPmA.png" />The input text corresponding to the heat-map above, where each token is highlighted according to its max attention weight received.</figure><p>By putting these two together (along with the output text for reference), we can gain a better understanding of how the model arrived at its summary. A viewer can now map the attention distribution shown in the heat-map to text in the input sequence by looking for patches of similar color intensity.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dMqdcl01Za5U4nYrXpye6g.png" />A prototype static visualization including both heat-map and highlighted text.</figure><p>To get a better sense of how this visualization pattern would play out, I built a light web-page that allows users to cycle through different input / output examples. The gif below shows several of these.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*_ACE44hvUSrwfE3F04gslg.gif" />The described visualization pattern over several input / output sequences.</figure><p>More exploration (inside and outside of this pattern) will need to occur, but this seems promising!</p><h4>What’s Next</h4><p>Lots to get done this next week. Here’s what’s in my plan:</p><ol><li>Continue working on data collection and cleaning up the TensorBoard plug-in. Move beyond the proof-of-concept front-end and show that meaningful visualizations (perhaps extremely basic ones) can be generated using the plugin back-end as a data source.</li><li>Read more into the model provided by See et al. (2017), as well as related work, to gain a better understanding of the architecture and function/behavior of attention. A closer study of the works cited in my <a href="https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5">second blog post</a> will be a good starting point. The better I understand this mechanism the more equipped I’ll be to create meaningful visualizations.</li><li>Continue prototyping static visualizations, move on to interactive visualizations. Acquire feedback from peers for both.</li></ol><p>Thanks for reading!</p><h4>Works Cited</h4><ul><li><a href="https://arxiv.org/pdf/1704.04368.pdf">See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” <em>ACL</em> (2017).</a></li><li><a href="https://ufal.mff.cuni.cz/pbml/109/art-rikters-fishel-bojar.pdf">Rikters, Matīss, Mark Fishel, and Ondřej Bojar. “Visualizing neural machine translation attention and confidence.” <em>The Prague Bulletin of Mathematical Linguistics</em> 109.1 (2017): 39–50.</a></li></ul><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=306dca636d3a" width="1" /></div>







<p class="date">
<a href="https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a?source=rss-2759d54493c0------2">by Halden Lin at April 25, 2018 06:48 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a></h3>


<div class="entrygroup" id="https://medium.com/p/a21b51cdd27c">
<h4><a href="https://medium.com/@viterbi.or.not/baseline-model-2-a21b51cdd27c?source=rss-c522ef075bb3------2">Baseline Model #2</a></h4>
<div class="entry">
<div class="content">
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*M0h3Wof_sgtxGwgnT2IncQ.png" />Bassline = Baseline?</figure><p>This week, we completed the baseline portion of our project (see the roadmap below!) by finishing the implementation of our various baseline models and evaluating their performance with the goal of providing context for the rest of the project. In last week’s blog post, we described the process of replicating the findings of a related research paper using a single model, and incorporating only features that apply to text summarization in general. This week, we expanded upon that start by adding an additional simple, single-feature baseline and by evaluating our implementation using other models as well! Finally, we built upon our codebase from last week with various bugfixes and feature additions, such as implementing k-fold cross validation for more reliable metrics.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*K0NWdjTpiYezkyqh34fdow.png" />Roadmap of our project — At this point, we’ve completed the first big block, highlighted in orange!</figure><h4>Simple Baseline</h4><p>The simplest baseline we put together used only the sentence length feature, a very basic but often reliable measure of sentence importance, with a Naive Bayes model.</p><h4>More Realistic Baselines</h4><p>The more realistic and comprehensive baseline models that we implemented use the full “basic feature set” described in our <a href="https://medium.com/@viterbi.or.not/baseline-model-1-a6690114c441">last blog post</a> with Naive Bayes, Decision Tree, and Multilayer Perceptron models. The comprehensive Naive Bayes baseline is the same as the baseline model described in our previous post — the main progress on this week’s baselines is the addition of a simple feature to catch email lines that are quoted replies (a problem we found in a lot of the summaries generated by our models) as well as experimentation with the different types of models.</p><h4>Results and Evaluation</h4><p>As expected, the simple Naive Bayes baseline performed pretty poorly. The models using all of the features in the basic set all performed similarly well, with slight fluctuations in which ROUGE metric they did better in (seen in the table below). Overall, we found that our Naive Bayes model performed competitively with the other models in ROUGE-L and the best in ROUGE-1 and ROUGE-2, though most importantly, we thought the summaries generated by Naive Bayes were the most satisfactory when actually reading them.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Hv_d7fQj-ATlUEaV79Moiw.png" />Table comparing the performances of the various baseline models we implemented, the best performing in each ROUGE metric highlighted in orange</figure><p>Our best baseline model does better than the corresponding model in <a href="http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf">Summarizing Online Conversations: A Machine Learning Approach</a> in the ROUGE-1 and ROUGE-2 metrics<strong>, </strong>but does significantly worse in the ROUGE-L metric. However, it is not clear which of the ROUGE metrics is more “important” to score well in, and it is hard to do a complete comparison between our model and theirs without a sample of the summaries generated by their model (which is not provided in the paper).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sFxZT2d-C_jKMnFxTWUHuA.png" />Table comparing the performance of the our best baseline model with the corresponding model in <a href="http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf"><strong>Summarizing Online Conversations: A Machine Learning Approach</strong></a>, the best performing in each ROUGE metric highlighted in orange</figure><p>In an effort to better understand what these automated metrics are measuring, here are descriptions of what ROUGE-L and ROUGE-N (ROUGE-1 and ROUGE-2) measure —</p><p><strong>ROUGE-L: </strong>Based on Longest Common Subsequence statistics, identifies longest co-occurring in sequence n-grams.</p><p><strong>ROUGE-1: </strong>Unigram overlap between system and reference summaries.</p><p><strong>ROUGE-2: </strong>Bigram overlap between system and reference summaries.</p><p>ROUGE-L and ROUGE-N clearly measure summary quality very differently, and rather than increase together, the metrics vary greatly and sometimes change inversely. While performing a mini ablation study with our baselines, we noticed that removing certain features increased ROUGE-L but cause large drops in both ROUGE-1 and ROUGE-2, as well as generated less satisfactory summaries (judged by us reading the generated summaries).</p><p>Because ROUGE does not necessarily reliably measure the quality of a conversation summary, human monitoring of generated summaries and error analysis are crucial to this project.</p><h4>Error Analysis</h4><p>So, although we used the ROUGE metrics for our model in order to compare it to our baseline research paper, we put an emphasis on human evaluation due to the inability of ROUGE to capture all the elements of successful summarization. Namely, although ROUGE is capable of determining whether the words and subsequences used in a summary match the human-annotated reference, it cannot capture critical aspects of the text such as its coverage of the source document’s most important points or whether it makes logical sense when read.</p><p>Upon reading through the summaries produced by our model, we discovered an interesting mix of results. Several sentences that appeared key to establishing the email thread’s topic were included in the summary by the baseline, indicating that its features for topic identification and term similarity were contributing in a positive manner. However, although it is expected that any extractive summary will produce grammatically imperfect results, there were noticeably major issues with the formation of our baseline model’s summaries.</p><p>An excerpt of one fairly representative summary produced by our model appears below, unedited except for truncation and formatting for clarity. This is a summary of an email chain concerning accessible technology from the BC3 corpus:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JGjecxwGpwd-rxwnf_t_Cw.png" />Comparison with reference summary</figure><p>As one can see, only the first two sentences (green) of this particular summary match the human-annotated summary; the rest (red) diverge and never again are the same sentences shared. This trend is seen across most of the summaries produced by our baseline model, where early sentences tend to be shared — one hypothesis is that the first email in a thread has clear significance in establishing that thread’s topic, while subsequent emails are less certain, leading to divergence between the model and human understanding.</p><p>Beyond the matching of sentences, there are certain aspects of the summary that, through a human evaluation, can be identified as clearly not belonging in the summary:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ni7uIemzWgCsW4Yvh0jlwA.png" />Email-specific formatting leads to summarization mistakes</figure><p>In this example, there are sequences of text that are directly repeated (orange). This occurs due to emails having quoted text from previous emails in their message bodies, and as the general text summarization features attribute the same likelihood of being relevant to the summary to both versions of the text based solely on content (and not, for instance, sentence position), it makes sense that both would be included. Doing so seems not to vastly impact the ROUGE score, but provides another example of the importance of human evaluation because it seems reasonable that no human would prefer to have those repetitions in a summary. Another example of the baseline model’s errors can be found in the email signature present in the summary (blue), which a human reader would clearly not find meaningful for the summary.</p><p>Both of these mistakes can be attributed to the fact that our baseline model uses general features for text summarization, and does not factor in conversation-specific features such as recognizing an email signature as being irrelevant to the topics being discussed. Once we begin work on our minimum viable product, we expect it to perform much better in this category, because we will have the chance to add these features.</p><p>Finally, the following examples show cases in which our model makes the opposite mistake from above, failing to capture portions of the original conversation that are important to the summary through human eyes. As expected, both of the common cases of failure we identified seem to stem from the fact that the missed content is formatted in a different way than normal text.</p><p>For the same conversation as above, the following shows just the URLs that were included in a human-annotated summary. Highlighted in green are the URLs which our baseline model also included in its generated summary:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*P9C4OsXf9w3KK4ishqa9Tg.png" />Almost all important URLs are not captured by the baseline model</figure><p>As one can see, the baseline model had a 10% success rate in identifying URLs from the email thread that the human annotator deemed as important. In a thread that primarily dealt with identifying resources from the internet, having these URLs in a summary would be highly desirable, so we consider this to be a major failing of the baseline model. Because URLs are much different from typical text, using general text summarization features likely led to this absence because there are no features that can ascribe importance to URLs based purely off their general format.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Wfd7cl-8KZ36YvAWXG4wHw.png" />Structured data, such as ordered lists, are not captured</figure><p>As another example, the following is a comparison between our baseline model and a human-annotated reference summary for a different conversation that includes discussion of poll results:</p><p>Although the baseline model does relatively well in identifying sentences related to the discussion of the poll results, its summary is noticeably lacking the results themselves. We hypothesize that the general text summarization features are insufficient to detect numerical data or items that are presented in a list, such as the one above, and therefore because the actual text within each poll result item is very generic the baseline model was unable to successfully select them for the summary.</p><p>Ultimately, these few examples are not enough to capture the exact failings of our baseline model, but by analyzing them in conjunction with the other summaries generated for our validation set, we were able to get an impression of the types of improvements that will be needed. In our next step of including conversation-specific features, we plan to make our top priority addressing structured data unique to emails by creating features to target email summaries, quoted text (and repeated text in general), and certain entities such as URLs or lists.</p><h4>Steps Towards an Advanced Model</h4><p>Keeping the weaknesses of our baseline in mind, our next steps towards creating a more advanced conversation summarization model (and approaching our Minimum Viable Product!) include the following:</p><p><strong>Incorporate conversation-specific features</strong></p><ul><li>We would like to explore using topic segmentation to preprocess conversation data and potentially improve model performance.</li><li>We would also like to add features that incorporate detection of<strong> structured data</strong> (URLs, Lists, Numbers), <strong>sentiment scores</strong> of sentences, and <strong>discourse markers</strong> (defining the purpose of a sentence in the text, for example, identifying a sentence as an email signature).</li></ul><p><strong>Train the models on other datasets</strong></p><ul><li>We will need preprocess the other datasets and restructure some of our code to handle input of the other datasets.</li><li>We anticipate some challenges based on the differences of the other datasets — the chatlog data will require much more preprocessing and meeting transcripts will likely have different vocabulary (i.e. no URLs, fewer abbreviations) which may impact our feature engineering process.</li></ul><p>Check back next week to see our initial work on the most exciting part — the advanced model!</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a21b51cdd27c" width="1" /></div>







<p class="date">
<a href="https://medium.com/@viterbi.or.not/baseline-model-2-a21b51cdd27c?source=rss-c522ef075bb3------2">by Viterbi Or Not To Be at April 25, 2018 06:29 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/79ed786e5c74">
<h4><a href="https://medium.com/nlp-capstone-blog/baseline-ii-and-updates-79ed786e5c74?source=rss----9ba3897b6688---4">Baseline II and Updates</a></h4>
<div class="entry">
<div class="content">
<p>This week, our focus was to improve the original baseline model with an approach more tailored to the task of generating definitions. There were a few key challenges that the original baseline approach did not address. This included generating grammatically sound English sentences, and incorporating keywords. Over the past few weeks, we also explored text generation techniques used in poetry (Ghazvininejad et. al). Our new approach is inspired by the techniques used in this paper and aims to address the two major problems with our first baseline.</p><h4>Revised Approach</h4><p>One of the biggest issues with the first baseline models, which were neural language models, was that the sentence outputs were not coherent or grammatical. Ghazvininejad et. al addressed a similar structural issue by creating a large Finite State Machine of all possible paths one could take while generating a sonnet. Each path is grounded in the filtered vocabulary developed in earlier steps and technically would have produced a structurally sound sonnet. Though all of these paths were not great, the FSM provided a foundation for generating the best sonnet. We use a similar approach, where we create an FSM of all possible paths through the training corpus. For example, if we encountered the following sentence in the corpus, it’s corresponding FSM would look like this:</p><blockquote>Osteoporosis is a bone disease.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uWBb5Vc4EerUU9hEvzHDuQ.png" />Example FSM</figure><p>In this manner, we construct an FSM for the entire corpus.</p><h4>Extraction</h4><p>Another challenge of the original baseline was that it did not focus on the topic we provided to start with. Even though we provided the seed word, it tended to stray off topic very quickly. In order to fix this, we chose to use some techniques from extractive summarization and the keyword identification process from the poetry generation paper. To start, we have to identify some common words associated with the term we’re defining. In Generating Topical Poetry, the authors use distance between word vectors as a metric for similarity, which we also do in our model. This allows us to determine the top few keywords associated with the term.</p><p>Now, we can proceed to look through the corpus and extract all sentences that contain a keyword or the term itself. This allows us to capture the context in which a term is mentioned. In the future, we can capture more sentences than this, especially if we use a window larger than one sentence for capturing context. Once we have our extracted sentences, we can proceed to modify the FSM.</p><p>Our modified FSM will only contain words that are included in the extracted sentences and a set of pre-selected connective words required to generate grammatical sentences. We hope that this will narrow down the search space in our decoding phase.</p><h4>Definition Generation via Beam Search</h4><p>The FSM mentioned previously needs a scoring mechanism with which to extract likely paths. We plan on using an RNN language model (LSTM, GRU, etc.) to decode the FSM and produce the generated definition one sentence at a time. Although it is asking a bit much to trust the neural language model to produce coherent, semantically rich sentences, we trust that the amount of structure we’re introducing before inference nudges the model to connect the dots in the most sensible way.</p><p>An issue we’re left with is the actual beam search itself. Beam search is a useful alternative to exhaustive search for fixed sized sequences in that we only continue paths that are one of the top K likely paths at every step. The difficulty lies in the fact that beam search is best used for deriving most likely paths of a fixed length. When we’re at the stage of generating definitions, we’ll have to figure out how exactly sentence length will be enforced or relaxed. It’ll be especially difficult to enforce grammar, in particular, how to terminate sentences. In the most cases this can be mitigated by generating the sentences backwards and appending punctuation.</p><p>Difficulties also lie in what seed to use per generated sentence. It may make sense for semantically relevant terms to appear at the end of each sentence but that isn’t necessarily how all of these words are used in practice. Regardless, an FSA using a restricted vocabulary from extracted sentences coupled with a neural language model, we believe, will be the best of both worlds. We gain assurance in semantic quality uses aspects of extractive summarization and structure we introduce while allowing the generation a reasonable amount of improvisation.</p><h4>BIO-Tagging Approach for Sentence Extraction</h4><p>For what could be a part of the FSM-style definition generation, or even a standalone definition extractor, we plan on labeling sentences throughout the corpus using BIO tags. This approach proposed by AI2 researchers involves the use of distant supervision to label sentences conducive to definition structure and semantics by picking sentences that meet a ROUGE threshold w.r.t to gold standard definitions. We would then collect triples of terms, their gold standard definitions, and their BIO-tagged sentences. We could then train a sequence tagger to recognize what sentences in a paper are conducive to definitions and which ones aren’t.</p><p>Possible sources of gold standards to use for ROUGE are include WordNet, which has a large breadth of glosses but each gloss tends to be very short. We are also exploring the idea of using UMLS which would provide technical medical terms along the lines of what we’d like the model to be able to define, and another data set composed of NELL and Freebase which can be found <a href="http://rtw.ml.cmu.edu/wk/WebSets/glossFinding_wsdm_2015_online/index.html">here</a>. With “Automatic Gloss Finding for a Knowledge Base using Ontological Constraints”, Dalvi et al. set out to simplify KBs the same way we are, and they were kind enough to make this dataset of ~500k glosses available to the research community for continuing this work.</p><p>A supervised aspect of this project has been lacking until now, and we believe that incorporating this sequence-tagging or other intelligent forms of extracting rich, definition-like sentences will mean the language model and beam search won’t have to work as hard. The added assurance of a restricted search space to only what is relevant is better both for inference and training.</p><h4>Progress</h4><p>So far, the most difficult part of our project has been determining a more advanced approach to start with. Though the initial baseline model was easy to come up with, this model took several days to design. Most of the work we accomplished over the past few weeks involved talking with Waleed Ammar from AI2 and reading several research papers in order to define the architecture we have proposed above. As such, we have not made enough progress on this approach to evaluate it thoroughly. In this section, we list a breakdown of all the tasks we have.</p><ul><li>The infrastructure and interface for constructing the FSM is complete</li><li>The system used to determine keywords given a term is in progress. We are currently debating whether or not to use pre-trained word vectors and are working on finishing the code</li><li>The extraction phase of the model (after retrieving the keywords) is not complete as it relies on the keywords. However, this part should be fairly straightforward and should be complete by the end of the week</li><li>Beam search decoding is in progress and the functionality to find the next beam is complete but the infrastructure for deciding when to terminate the search is in progress.</li><li>Although the RNN for beam search through the FSM is already written (we can use the same RNN from the original baseline), it needs to be trained on a corpus, preferably the Semantic Scholar corpus</li></ul><p>Another challenge we are currently facing involves organizing the data. In our original baseline model, the loss would steadily decrease as we trained on a single document but would then suddenly spike upward when we switched to a new document. The AI2 Semantic Scholar dataset consists of many different types of research papers including Computer Science, Medical, and various other domains. As such, the language in each domain differs drastically, so organizing the papers into linguistically similar groups has remained a challenge. Currently, the API provides no such tools for categorizing the papers.</p><h4>Conclusion</h4><p>Overall, we hope that this approach is a step closer to defining an architecture specific to the definition generation task. That said, there are several ways to improve the individual pieces of this architecture. We can change the hyper parameters of models in every phase, change the way they are trained, and introduce new concepts such as sequence tagging to improve the quality of the text generated by the model. This current architecture gives us a baseline on which we can continually improve.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=79ed786e5c74" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/baseline-ii-and-updates-79ed786e5c74">Baseline II and Updates</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/baseline-ii-and-updates-79ed786e5c74?source=rss----9ba3897b6688---4">by Karishma Mandyam at April 25, 2018 05:43 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=323">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/25/nlp-capstone-post-5-a-new-hope/">NLP Capstone Post #5: A New Hope</a></h4>
<div class="entry">
<div class="content" lang="en">
<h1><span style="font-weight: 400;">Dataset Improvements</span></h1>
<p><i><span style="font-weight: 400;">Last time, on Music NLP.</span></i><span style="font-weight: 400;"> We ran into many midi data parsing issues. Since then, we have discovered a new dataset called the Lakh MIDI Dataset (</span><a href="http://colinraffel.com/projects/lmd/"><span style="font-weight: 400;">http://colinraffel.com/projects/lmd/</span></a><span style="font-weight: 400;">) that comes with reasonably well-formed midi files. Using the “Clean MIDI Subset”, we found thousands of midi files with their associated song names and songwriters. From these midi files, we extracted all with nonempty “lyric” fields when parsed via the pretty_midi package (which, incidentally, is also developed by Colin Raffel). After this step, we were left with ~1200 midi files that contain lyrics.</span></p>
<p><span style="font-weight: 400;">We currently clean the lyrical content by removing all unusual symbols and setting all characters to lowercase. We leave all lyrical tokens as is, which typically means syllable. Due to the inconsistent quality of the MIDI annotations, many songs are tokenized instead to characters, words, or even sentences. We will explore other methods for processing data if this is not sufficient for our results.</span></p>
<p><span style="font-weight: 400;">It is unfortunate we did not find this dataset sooner, because most of our challenges up to this point have been dealing with the poor quality of the gathered data.</span></p>
<h1><span style="font-weight: 400;">Alignment</span></h1>
<p><span style="font-weight: 400;">For our task of producing karaoke style output, there are two main tasks we have to solve. The first task is the generation of plausible lyrics, and the second is to align the lyrics to the proper time along the musical data. The alignment task has been studied extensively, but specifically aligning lyrical content to MIDI has not been covered in literature we have found. The most common alignment task is lyrics to audio data, as opposed to MIDI. The other common task is to align audio data to the notes defined in a MIDI file. In [1], they show a method that takes a MIDI file with annotated lyrics and uses this to align the lyrics to the raw audio. Unfortunately this is not our task, because we are trying to generate the annotated MIDI.</span></p>
<p><span style="font-weight: 400;">This week, we have decided to ignore the alignment task and focus primarily on making a reasonable lyrical model. We will return to alignment next week.</span></p>
<p><span style="font-weight: 400;">The next step was to align the lyrics with pianoroll. Fortunately, well-formed midi data parsed into PrettyMIDI objects come with a “get_piano_roll” function that takes as input a list of “times” which correspond to where in time pretty_midi will attempt to sample the music. As each syllable in the lyrics comes with a start time for when the singer enunciates it, we can pass in these start times to produce pianoroll that is aligned (up to small error) with the lyrics.</span></p>
<p><span style="font-weight: 400;">For some implementation reasons that are difficult to explain in English, it is possible for “get_piano_roll” to produce NaN entries, which we have replaced with zeros. Due to this and the potential for other such problems, we have forked the pretty_midi package and will be able to modify the code for our needs. For example, as pointed out in [2], “in a given MIDI file there is no reliable way of determining which instrument is a transcription of the vocals in a song”. As such, there are many choices for how to do alignment; pretty_midi has implemented just one. It is an interesting task to see how different alignment methods help or hurt our models.</span></p>
<h1><span style="font-weight: 400;">Lyric prediction</span></h1>
<p><span style="font-weight: 400;">Now that we have aligned pianoroll to lyrics data, we can begin engineering the model. Last time, we used an LSTM to generate lyrics given starting characters. Here, we will again use LSTMs, but instead, work at the syllable level and take as input the pianoroll of a song. As each column of a pianoroll is a time slice, each input vector to the LSTM is a single time slice. Each time slice is a 128-dimensional vector, with each entry representing the activation of an instrument; there are 128 midi recognized “instruments”.</span></p>
<p> </p>
<p><span style="font-weight: 400;">All that is left is to play with the architecture. </span></p>
<p><img alt="RNN model" class="alignnone size-full wp-image-322" src="https://mathstoc.files.wordpress.com/2018/04/rnn-model1.png?w=676" /></p>
<p><span style="font-weight: 400;">At the moment, our pipeline looks like what is shown in the diagram. At each iteration, we take a song, extract the lyrics and the corresponding pianoroll data. We then feed each time slice of the pianoroll data through an encoder unit, then through an LSTM unit, then through a decoder unit, and finally through a softmax to produce the prediction. Our loss is the negative log-likelihood (negative logarithm of the RNN softmax probability of the true syllable).</span></p>
<p><span style="font-weight: 400;">We will compare our final model to this baseline with respect to the loss on a held-out validation set. We will also experiment with loss functions other than cross entropy to see how it affects the actual lyrical output.</span></p>
<h1><span style="font-weight: 400;">Model results</span></h1>
<p><span style="font-weight: 400;">We have so far only trained our model for a single iteration over the training set. For an empirical evaluation on the current model quality, we ran a single MIDI through the input and computed the argmax word for each output. This produced a result in which every predicted lyric was an empty message, which is the most common string in the training set. We will explore methods to handle this class imbalance as our next task.</span></p>
<h1>References</h1>
<p><span style="font-weight: 400;">[1] Müller, Meinard &amp; Kurth, Frank &amp; Damm, David &amp; Fremerey, Christian &amp; Clausen, Michael. (2007). Lyrics-Based Audio Retrieval and Multimodal Navigation in Music Collections. 4675. 112-123. 10.1007/978-3-540-74851-9_10.</span><br />
<span style="font-weight: 400;">[2] </span><span style="font-weight: 400;">Raffel, Colin and Daniel P. W. Ellis. “Extracting Ground-Truth Information from MIDI Files: A MIDIfesto.” </span><i><span style="font-weight: 400;">ISMIR</span></i><span style="font-weight: 400;"> (2016). </span><span style="font-weight: 400;"> </span></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/25/nlp-capstone-post-5-a-new-hope/">by Nicholas Ruhland at April 25, 2018 04:44 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 24, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/actual-strawman-update">
<h4><a href="http://sarahyu.weebly.com/cse-481n/actual-strawman-update">Actual Strawman Update</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph"><strong>Real Data and Results Have Been Seen! </strong><br />As I mentioned in my last post, I was struggling with accessing the data, but I've since solved my problems and got to learn some cool tools along the way (like apparently you can read a compressed file without decompressing??? wild). I've also spent a large part of the week learning and fighting with SqlAlchemy, PyMySQL, MySQL, and UTF-8 issues. With the interaction of all of these, I was able to read (most of the) Reddit posts of January 2017 (thanks to Jason Baumgartner publishing these dumps on pushshift.io, I will donate when I have an income) which amounted to 80 million posts, find the users we are interested in, find neurotypical subreddits these users post to, and then get posts of our two (neurotypical and neurodivergent) subreddit subsets. <br /><br />Side Note: I'm going to start referencing the Neurodivergent set as ND, and Neurotypical as NT, trying to save some typing<br /><br /><strong>Baseline #1 (kind of an update of the Strawman #1):</strong><ul><li><span>Glen Coppersmith and Erin Kelly (2014). <strong><em>Dynamic Wordclouds and Vennclouds for Exploratory Data Analysis. </em></strong></span><span><font size="2">Association for Computational Linguistics Workshop on Interactive Language Learning and Visualization</font></span></li></ul> With thanks to Coppersmith and Kelly, I was able to make a Term Frequency Venncloud as seen below that show in black the most frequent terms found in both subreddit subsets, and then separated into the most frequent terms in neurodivergent subreddits and neurotypical subreddits in blue and red, respectively. </div>  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0px; margin-right: 0px; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-04-26-at-1-48-22-pm_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;">Naurodivergent vs. Neurotypical Subreddit Venncloud</div> </div></div>  <div class="paragraph"><br />As we can see, the middle of the venncloud is pretty uninteresting, but here are some notable points:<ul><li>Personalization: Frequency of you/me, your/my words which indicate some sense of relationship and more personalization between ND posters. This contrasts the frequency of they, indicating some discussion of an other, in the NT subreddits.</li><li>"is": I interpreted the frequency of this word in the NT subreddits as a more definitive and declarative way of speech, rather than other words such as "think", "feel" and "maybe" (in the ND subreddits) which signal more hesitation. This is a point touched on and described as dogma in Fast &amp; Horvitz which is one of the papers I discussed in a previous post.</li><li>"www", "imagesofnetwork" :  This is something I cold probably fix; the way I pre-process the data scrubs and separates the links into separate words. At the end of the day though, this shows that there are significantly more links in NT subreddits. My thought is that the lack of such in the ND subreddits might mean more anecdotal and personal interactions than when compared to ND subreddits</li><li>Moral Adjectives: Some of the ND frequently used terms are what I am going to call Morale Adjectives (let me know if there's an actual term for this); here I mean, we see words like "good", "right", "bad", which are often used to describe habits or behavior.</li><li>SURPRISE GENDER DIFF: As you can see, 'she' is one of the most frequent ND words, whereas 'he' is  one of the most frequently used NT words. Some thoughts: 1) doesn't show anything, there are some partner subreddits and may just show that the predominantly male reddit user base talks about different genders in the two, but they themselves may not be a different gender distribution or 2) could show different gender engagement in the different subsets.</li></ul><br /><strong>Baseline #2: Connotation Frames</strong><ul><li><span style="color: rgb(0, 0, 0);">Hannah Rashkin, Sameer Singh, Yejin Choi. 2016. <strong><em>Connotation Frames: A Data-Driven Investigation.</em></strong><font size="2"> In Proceedings of ACL 2016</font></span></li><li><span style="color: rgb(0, 0, 0);">Maarten Sap, Marcella Cindy Prasettio, Ari Holtzman, Hannah Rashkin, &amp; Yejin Choi. 2017. <em><strong>Connotation Frames of Power and Agency in Modern Films.</strong></em> <font size="2">sched. to appear EMNLP 2017 short papers. </font></span></li></ul> With help from Maarten Sap, another model I explored was the Connotation Frames formalism, to look at the verbs used in both our NT and ND subreddits and the sentiments these provide between agent and subject. However, we found no significant differences between the two (output below).</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 50%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-04-26-at-9-45-10-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 50%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/nt-verbs_1_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph"><br /><strong>Baseline #3: LIWC2015</strong><br />Finally, I used LIWC2015 to count and classify the psychological meanings and categories for both NT and ND subreddits. This serves as another type of language model to define these two 'languages' and offers us another metric on which to find similarities and differences. ​<br /><br />Just for some clarification, the way that this model works is by having 73 categories (more information available <a href="https://liwc.wpengine.com/compare-dictionaries/" target="_blank">here</a>), anywhere from topics - PRONOUN, HEALTH, BIO - to grammar - VERB, ARTICLE - and gives the percentage of the language that each category accounts for in that 'language'. In our case, we see the distribution of categories in NT subreddits compared to ND subreddits. <br /><br /><span>My hypotheses were:</span><ul><li>[You, Heshe, Pronoun, Health, Feel, They] categories would be significantly higher in ND </li><li>[Anger, Power, Swear] categories would be significantly lower in ND than in NT</li></ul><br />After getting the results, I report the top 10 categories with the largest % difference between the two. <ul><li>HEALTH(3.34x), <span>INGEST(2.69x), BIO(2.13x)</span><br /><ul><li>These categories are topic specific (ingestion related to drug subreddits and bio on biological processes) and align with what we expect in mental health topic subreddits</li></ul></li><li>FEEL(2.04x), SAD(1.88x), ANX - anxiety (<span>2.86x)</span><br /><ul><li><span>​Also make sense for the support communities within the ND group, potentially, topical for "anxiety" as a temporary and consistent feeling</span><br /></li></ul></li><li>FAMILY(2.07x), HOME (2.05x)<br /><ul><li>This was a bit surprising, I believe appeals to the family and home tend are prominent in support groups as well as the "partners of" subreddits we have in the ND group</li></ul></li><li>I (1.73x)<ul><li>There seems to also be a lot of personal discussion, which we expect in subreddits that are meant to discuss personal problems</li></ul></li><li>FEMALE (1.81x)<br /><ul><li>ITS HERE AGAIN WHY AND HOW</li></ul></li></ul>​<br />That's all for baseline models, here's to my first attempt at the more advanced model this week...</div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/actual-strawman-update">April 24, 2018 07:00 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 20, 2018</h2>

<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=16">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/04/20/strawman-baseline-1-deep-recurrent-q-network/">Strawman/Baseline 1: Deep Recurrent Q Network</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>For this week, I decided to first develop the RL model for asking questions. I chose to try a Deep Recurrent Q Network (DRQN) first (and the policy gradient method later) using the following repository as a starting point:</p>
<p><a href="https://github.com/awjuliani/DeepRL-Agents/blob/master/Deep-Recurrent-Q-Network.ipynb" rel="nofollow">https://github.com/awjuliani/DeepRL-Agents/blob/master/Deep-Recurrent-Q-Network.ipynb</a></p>
<p>This repository implements the DRQN for games by using LSTM cells to encode sequential information as successive frames are passed in for each time step. These frames are then passed through a CNN, then the LSTM cell, and then output the Q values. In the code I replaced where the input image (frame) is passed into the DRQN with the 2D word embedding matrix (GloVe vector for each word in sentence) and passed it straight into the CNN. I also gave a default reward of 0 to the model and managed to run the model without errors.</p>
<p>My goal for next week is to change the reward allocation to be user input, hardcode all the question templates, and begin training the model.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/04/20/strawman-baseline-1-deep-recurrent-q-network/">by ananthgo at April 20, 2018 06:47 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 19, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" title="Stories by Halden Lin on Medium">Halden Lin <br/> Team undef.</a></h3>


<div class="entrygroup" id="https://medium.com/p/be87c31976b7">
<h4><a href="https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7?source=rss-2759d54493c0------2">NLP Capstone | 04: First Steps</a></h4>
<div class="entry">
<div class="content">
<p><em>previous posts: </em><a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5"><em>01</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5"><em>02</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3"><em>03</em></a></p><p>Nearing 4 weeks in — I’ve finally got a foothold in the development process. Over this past week I’ve been looking through TensorBoard and TensorFlow source-code and documentation in an attempt to develop a foundation for plugin development.</p><p><strong>Notable Resources:</strong></p><ul><li><a href="https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard">TensorBoard Documentation</a></li><li><a href="https://github.com/tensorflow/tensorboard">TensorBoard Source Code</a> and <a href="https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins">Existing Plugins</a></li><li><a href="https://github.com/tensorflow/tensorboard-plugin-example/blob/master/README.md">Developing a TensorBoard Plugin</a> (a simple example)</li></ul><h4>Completed Milestones:</h4><ol><li>Understand the structure of a TensorBoard Plugin, specifically how the architecture of the Attention Plugin should look like.</li><li>Design and write the data fetching layer.</li></ol><h4>Plugin Architecture</h4><p>In TensorFlow, data from iterations of training / evaluation is stored as a set of <strong>summaries</strong>. These can take the form of any tensor, including text, image, scalars, or time series. These are written to disk as the computation graph is executed. Each ‘summary’ takes the form of a <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/summary.proto">Summary protocol buffer</a>, which holds, in addition to the data stored, critical identifying information (tags and metadata). A plugin can then read summaries associated with particular tags and sessions from disk to serve to the TensorBoard front-end via a plugin back-end, where a visualization is rendered.</p><p>Following <a href="https://github.com/tensorflow/tensorboard-plugin-example/blob/master/README.md"><em>Developing a TensorBoard plugin</em></a><em>, </em>the Attention Plugin with have three primary components:</p><ol><li>Data API layer. This is what allows users to capture relevant summaries from within their models.</li><li>Plugin backend, which serves said summaries.</li><li>Frontend, where the visualizations are displayed.</li></ol><p>Unlike commonly used plugins such as the <a href="https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/scalar]">scalar</a> and <a href="https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/histogram">histogram</a> plugins, the Attention Plugin consumes 3 distinct values: the input text, the decoded output text, and the attention matrix that correlates the two. Initially, I attempted to store these all in a single Summary protobuf, where the first two rows of the encapsulating matrix would contain the text, and the rest would contain the attention weights. This results in a mixing of string and float types in a single Tensor, which is not valid (to my knowledge) in TensorFlow. I then realized I could store these separately, each in their own summary, and retrieve them via an identifying name. The resulting architecture is shown in the diagram below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QK7USkxsSC3u6F0LKbKWXQ.png" />The architecture of the Attention Plugin.</figure><p>I decided to make the output text an optional summary, as models don’t necessarily need to decode (via Viterbi, Beam Search, or otherwise) an output sequence while training. The input text and attention matrix are still valuable, as summary statistics (e.g. coverage, important words, etc) can be gleamed without the decoded text.</p><h4>Data API Layer</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0m1-fllpNYMP666FhR0nbA.png" />Data API Layer highlighted in green.</figure><p>As the TensorBoard authors suggest, the data API layer, defined in attention_summary.py, provides two methods for creating Summary protobufs, which can then be written to disk via a FileWriter. The first is via a TensorFlow op, which can be thought of as a node in the computation graph, that produces a Summary when the graph is executed. The second is by directly creating the protobuf, which allows for data to be saved outside the execution of a TensorFlow session. I’ve implemented both of these. There is a separate method for each of the three datum used by the plugin (input text, output text, attention matrix), and each of the three summary datum are tagged differently (e.g. name/attention_input_summary, name/attention_output_summary) in order to allow for distinguishable retrieval later.</p><p>An example of the usage of both methods can be found in attention_demo.py.</p><h4>Next Steps</h4><p>I suspect the work completed this week was the biggest hurdle in terms of time:code ratio. I would not be surprised if I had to revisit the work done here in order to clean things up or fix small bugs. However, with this understanding and architecture nailed down, I expect implementation of the rest of the plugin will come at a faster pace. With that said, three tasks stand as immediate goals for the next week.</p><ol><li>Modify my forked repository of the summarization model created by <a href="https://arxiv.org/pdf/1704.04368.pdf">See et al.</a> (original found <a href="https://github.com/abisee/pointer-generator">here</a>) to produce and save the appropriate summaries for the Attention Plugin. I’ve already started looking into this, and expect to have to fiddle around with the training / evaluation scheme in order to grab an appropriate amount of data.</li><li>Implement the backend of the Attention Plugin.</li><li>Begin prototyping visualizations (pen &amp; paper) and acquire preliminary feedback.</li></ol><p>Until next time.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=be87c31976b7" width="1" /></div>







<p class="date">
<a href="https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7?source=rss-2759d54493c0------2">by Halden Lin at April 19, 2018 06:33 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 18, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a></h3>


<div class="entrygroup" id="https://medium.com/p/a6690114c441">
<h4><a href="https://medium.com/@viterbi.or.not/baseline-model-1-a6690114c441?source=rss-c522ef075bb3------2">Baseline Model #1</a></h4>
<div class="entry">
<div class="content">
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bGQmjLRpsY3UnYhK6e8Lyg.png" /></figure><p>This week, we implemented our <a href="https://github.com/viterbi-or-not-to-be/viterbi-or-not-to-be/tree/master/baseline">first baseline model</a> for conversation summarization. In order to create a baseline that would be useful to build off of and compare our future results to, we decided to base this model off the Naive Bayes implementation described in the paper <a href="http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf"><strong>Summarizing Online Conversations: A Machine Learning Approach</strong></a>.</p><p>To begin with, we identified the W3C Email Threads dataset as a useful starting point because of the fact that we have access to multiple email datasets, and because the paper mentioned above indicated that chatlog data would require a great deal of additional preprocessing to be able to manipulate. By comparison, the email data is fairly simple to interpret because it is rare for a single sender to convey their portion of a message across multiple emails, and the writing is generally more grammatically well-formed. The experiments in the paper run on the <a href="https://flossmole.org/content/software-archaeology-gnue-irc-data-summaries">GNUe Archives</a> and the <a href="https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/bc3.html">BC3 Corpus</a> (annotated W3C Email Threads).</p><p>The paper also uses <a href="https://www.cs.waikato.ac.nz/ml/weka/">Weka Toolkit</a> for all of its model implementations, so we initially aimed to use Weka as well to reproduce our baseline. However, we encountered a few challenges while starting out with Weka that ultimately led to our abandoning the platform in favor of a more flexible Python implementation. The email data is in XML format, whereas Weka appears to work best with a format called ARFF, and in our preliminary examination we found ARFF to be fairly difficult to map directly to XML. In particular, ARFF’s apparent inability to handle dynamic-length data types or nested relations made it challenging to plan around — this was a problem because for each training example (email thread) in our dataset, we have a variable-length amount of information (number of emails, number of sentences per email, number of words per sentence, etc.). Without usage of dynamic-length data coupled with a toolset/interface more suited to data mining and application of basic, pre-made machine learning models than detailed feature engineering, we found Weka specifically difficult to use for feature extraction.</p><p>Instead, we ended up deciding to implement the baseline model in Python — including parsing the XML from our chosen dataset, extracting features, and using SciKit-Learn to train a Naive Bayes model to form the baseline itself. Since our aim with the baseline was to re-implement at least part of the work done by the <a href="http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf"><strong>Summarizing Online Conversations: A Machine Learning Approach</strong></a> paper, we also chose to set up evaluation that would mirror their evaluation techniques, and settled on using a previously-written implementation of ROUGE for the task. Re-implementing the baseline model in Python based on previous work took more effort than using Weka would have, but was advantageous in that it helped us understand the specific challenges of the task and resulted in somewhat extensible code we can adapt for later phases of the project.</p><h4><strong>Feature Selection</strong></h4><p>The Feature Selection section of the paper notes a “basic feature set,” which refers to features that are “not specific to conversations and consider the conversation as a simple piece of text.” For our baseline, we decided to target only the basic features to establish the difference that can be made by using conversation-specific features. Based on this basic set, our Naive Bayes baseline incorporates the following features —</p><p><strong>Mean TF-IDF: </strong>Term frequency-inverse document frequency (TF-IDF) characterizes frequency of a word and reflects its importance to an email thread. Calculating TF-IDF results in a vector of word frequencies per email, and we take the mean of the values in the vector for the mean TF-IDF feature.</p><p><strong>Mean TF-ISF: </strong>Similar to TF-IDF but at a different granularity, term frequency-inverse sentence frequency (TF-ISF) characterizes frequency of a word and reflects its importance to an email. Calculating TF-ISF results in a vector of word frequencies per sentence, and we take the mean of the values in the vector for the mean TF-ISF feature.</p><p><strong>Sentence Length: </strong>Number of characters in the sentence.</p><p><strong>Sentence Position: </strong>Index of the sentence within the email.</p><p><strong>Similarity to Title: </strong>We represent the name of an email thread as a TF-ISF title vector, in the same form as described earlier (without taking the mean). This feature is then the result of the cosine similarity between the TF-ISF vector for a sentence and the title vector.</p><p><strong>Centroid Coherence: </strong>We represent the centroid of an email thread as the average of the TF-ISF sentence vectors. This feature is then the result of the cosine similarity between the averaged centroid vector and the sentence vector.</p><p><strong>Special Terms: </strong>Numbers and proper nouns are deemed as ‘special terms.’ This feature is the count of special terms in a sentence normalized (divided) by the number of special terms in the email thread.</p><h4><strong>Results and Evaluation</strong></h4><p>After implementing our baseline model, we used the ROUGE package to evaluate its summarization on a variety of metrics. We split the data into training and testing segments, placing 80% of email threads in the training set and 20% in the testing set. We then used our model and features to generate summaries and used ROUGE-L to measure their performance. After computing the average ROUGE-L, ROUGE-1, and ROUGE-2 F1 scores across all the email threads in the testing set, we produced the following results (along with the results reported in the baseline paper for comparison):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vjlGJvdBn_k4obtMdkGdIA.png" />Mean F1 scores for various ROUGE metrics, compared to the results from <a href="http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf"><strong>Summarizing Online Conversations: A Machine Learning Approach</strong></a></figure><h4><strong>Error Analysis</strong></h4><p>It is certainly helpful to have a set of metrics like ROUGE to establish a comparison between our model and existing models, but for a task such as automatic summarization it is an unfortunate reality that no existing metrics really come close to fully capturing the “success” of a summary. Comparing the compositions of two summaries provides valuable insight into the performance of a model, but ultimately a great deal of the usefulness of a summary revolves around its ability to convey the information contained in a conversation without needless filler and in a logical, simple format. Because there are no metrics that can describe how well a summary captured the topics in a conversation or how understandable it is, we will have to rely heavily on human interpretation to judge the success of our model.</p><p>For our baseline, we took a look at the generated summaries and discovered that while the general topic of each email thread is easily discernible, it is sometimes difficult to follow the course of the discussion. In addition, we found that our summaries were especially susceptible to the inclusion of “fluff” — short, choppy sentences or signature elements with no relevance to the actual contents of the summary that simply obstructed the reader. Perhaps our future attempts will have to target this with features specifically designed to identify things like email signatures or greetings, to help distinguish between these sources of “fluff” and the actual content of each email. Notably, this is not an issue that chatlog data suffers from, because there are typically fewer signatures or static portions of chats to ignore.</p><p>Our analysis is exemplified by the sample summary and its annotated counterpart below:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ycgusov-GEUfQm7C." />Summary for an email thread generated by our baseline model</figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*aOxikqivuOMXLgnM." />Summary for the same email thread from the dataset annotation</figure><h4><strong>Future Improvements</strong></h4><p>Although our baseline model provided a useful point of comparison for future efforts, we were not able to entirely replicate the results of the paper we based it off of. Despite using the same features that they described, we achieved a significantly lower mean ROUGE-L F1 score of 0.409 as compared to their 0.245 for the same dataset. As a result, it seems that an important next step would be to identify the discrepancies between our implementation and theirs to try and faithfully reproduce their results before moving on to other models.</p><p>One possible issue we identified with our model could be the lack of a topic boundary preprocessing step as found in the research paper. Although their description made it seem as though the technique mainly applied to chatlog data, it is possible that performing such a preprocessing step would bring our model’s performance closer to theirs. The paper describes two different approaches at a very high level, so further research and experimentation would be needed to determine a reasonable implementation to perform such a task. In addition, there were various parameters left out of the paper’s description that could affect the model’s performance, such as ROUGE evaluation parameters and the implementation of Naive Bayes, which could have made an impact.</p><p>After finishing our baseline implementation, we plan to implement further models to address this task. The paper we previously mentioned uses a number of other approaches, so one direction we plan to move in is selecting other potentially effective models from their list and implementing them (decision trees look the most promising and would require very little modification to the code). Afterwards, we plan to address the other datasets, and then move into feature engineering and tweaking our implementations to produce our minimum viable product.</p><h4>Code</h4><p>As mentioned in our first blog post, the full code for our project is <a href="https://github.com/viterbi-or-not-to-be/viterbi-or-not-to-be">available on Github</a>, which currently includes usage instructions located in the README.md file for the baseline directory that can guide a user through the process of setting up the model, producing summaries, and running the ROUGE evaluation. We certainly have a long way to go in terms of code organization and cleanliness, but this baseline implementation at least allows for a preliminary examination of the task and produces the results necessary to interpret the rest of our model approaches with appropriate context.</p><p>Edited 4/24 to add baseline dog.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a6690114c441" width="1" /></div>







<p class="date">
<a href="https://medium.com/@viterbi.or.not/baseline-model-1-a6690114c441?source=rss-c522ef075bb3------2">by Viterbi Or Not To Be at April 18, 2018 06:55 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/15357a82fe06">
<h4><a href="https://medium.com/nlp-capstone-blog/first-impressions-baselines-and-the-evaluation-framework-15357a82fe06?source=rss----9ba3897b6688---4">First Impressions: Baselines and the Evaluation Framework</a></h4>
<div class="entry">
<div class="content">
<p>Ultimately, our goal is to go beyond basic language modeling and create a new text generation architecture conducive to producing technical definitions. To get a feel for the data though, we approach it with familiar, simple baselines that give us a foundation in which we can improve from.</p><p>The baselines that we’ve experimented with are</p><ul><li><strong>Vanilla RNN:</strong> Hidden states as a function of the input and the previous hidden state, with Tanh activation (in particular, we’re using the <a href="https://medium.com/@tamdangnadmat/first-impressions-baselines-and-the-evaluation-framework-15357a82fe06">Elman Network</a>).</li><li><strong>GRU:</strong> An RNN architecture that learns to throttle the influence and usage of particular parameters on inference using a gating mechanism.</li><li><strong>LSTM: </strong>Another RNN architecture that specializes in intelligently remembering relevant details and forgetting irrelevant details through several gating mechanisms and a “cell state” in addition to the conventional hidden states. <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Here’s more detail</a> about this particular kind of RNN.</li></ul><p>All baselines were trained as language models with cross entropy loss and were used to get a sense of how learnable the language of the Semantic Scholar dataset is. The metrics we are focusing on now are</p><ul><li><strong>Perplexity: </strong>A measure of how “confused” the model is at any point it’s attempting to predict the next word.</li><li><strong>Feature extraction through hidden states:</strong> Can the hidden states be used as features for a classification task?</li></ul><p>Our first metric is fairly straightforward; we calculate the aggregate sum of log probabilities for every word in the corpus and normalize by the size of the corpus.</p><p>Our second metric however, is inspired by Dieng et al.’s method for sentiment analysis in <a href="https://www.semanticscholar.org/paper/TopicRNN%3A-A-Recurrent-Neural-Network-with-Semantic-Dieng-Wang/412068c7e8e77b73add471789d58df3d2f3e08d8">TopicRNN</a>, in which the final hidden states after a forward pass of the model on a movie review were used to predict positive or negative sentiment using a single-layer neural network. We aim to adopt this metric from a multi-class classification perspective in which the passages we use are excerpts of research documents with an omitted, technical term. The question we aim to answer with this metric is “is the model capable of representing semantics in a latent space?”</p><p>Our labels will then be a defined set of these omitted, technical terms, and our goal will then be to predict them given the hidden states of the passage via a two-layer neural network. The framework for this evaluation metric can be found in <a href="https://github.com/NLP-Capstone-Project/machine-dictionary/tree/evaluation">this branch</a> of our codebase.</p><h4>Why we chose these metrics</h4><p>Our task is fairly novel given the way we’re approaching it, so currently no dataset exists that pairs domain-specific words with definitions that are to the caliber of research technicality. Because of this, metrics that depend on gold standards such as ROUGE and BLEU are currently out of reach at this time.</p><p>There’s a chance we’ll experiment with these metrics if we can find a labeled dataset to supplement Semantic Scholar’s Open Research Corpus. We are also considering using the publications themselves as the gold standards, which may be helpful since a desirable trait of our model would be its ability to produce language similar to that of the corpus.</p><h3>Challenges Encountered while Baselining</h3><p>In establishing our baselines and metrics, there were several issues we ran into, both in training.</p><h4>Dealing with a larger corpus</h4><p>Given that our baselines are language models, and that our later prototype models will most likely contain an LM component, we have to deal with efficient learning given there are several million documents to process.</p><p>For efficient backpropagation, we opted to introduce a “backpropagation through time” as a hyperparameter defaulted at 50, which specifies the number of words we allow the model to see before updating our parameters.</p><p>Currently, batching is supported by our codebase but was not used in our initial experiments. Given that it takes roughly one minute for the GPUs on the cloud to process a single publication, we plan to concatenate document vectors and reshape into batch-by-length tensors in the future.</p><h4>Sorting By Domain</h4><p>We’d like our model to be trained on a single domain/field of study for our future case studies comparing dictionaries built on one domain versus others. This is further motivated by some of our experiment results discussed later, how loss tends to spike between documents.</p><p>Currently, the Semantic Scholar Open Research Corpus doesn’t include anything in the set of JSON fields that we could find for filtering the data. However, we’ve been assured by AllenNLP researchers that its possible to sort the data by research domain. We may performing another round of baseline experiments once we’ve sorted the data, but for now the results below are on publications of mixed domains.</p><h3>Experimental Results</h3><p>The Semantic Scholar Open Research Corpus provides a sample subset of its dataset: a JSON file containing 4000 entries. Within each entry, the URL of the publication’s online PDF is provided. We use a GET request to <a href="https://github.com/allenai/science-parse">AI2’s Science Parse</a> service to extract the PDF contents.</p><p>From there, we run our experiments on 121 of the 4000 extracted documents, stopping training early at 15 documents and calculated perplexity on a validation set of 30 documents. The total vocabulary used was 11,330 words. Words outside of this vocabulary are replaced with an unknown token at training and test time.</p><p>Loss is calculated and normalized on the last 50 words the model is trained on.</p><ul><li><strong>Elman RNN:</strong> Perplexity of 250.25 with an average loss of 7.908 over the last 50 words.</li><li><strong>GRU:</strong> Perplexity of 265.50 with an average loss of 7.085 over the last 50 words</li><li><strong>LSTM:</strong> Perplexity of 261.95 with an average loss of 6.588 over the last 50 words</li></ul><p>Perplexities calculated using untrained models (with randomized parameters) were several orders of magnitude larger than the ones listed above, so it’s good to know that our baseline models can learn a significant amount of surface-level patterns with such a small subset of the corpus.</p><p>In terms of feature extraction and classification, the framework has been implemented but the data for this has not been created yet. We plan on evaluating our baselines along with our final model on this metric using publications from Semantic Scholar after establishing a vocabulary of semantically significant technical terms and creating the dataset using those terms. This will involve iterating over documents and replacing occurrences of these technical terms with a specialized, unknown token that won’t aid in inference.</p><h3>Conclusion</h3><p>Our methods helped us familiarize ourselves with the data as well as handle significant amounts of overhead in terms of processing the data, toggling between different models, and integration of our evaluation metrics in an organized fashion.</p><p>We are excited to see how novel architectures tailored to the task perform on these metrics!</p><p>To keep up to date with our progress in baselining, evaluation, and other things, you can watch <a href="https://github.com/NLP-Capstone-Project/">this repository</a>.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=15357a82fe06" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/first-impressions-baselines-and-the-evaluation-framework-15357a82fe06">First Impressions: Baselines and the Evaluation Framework</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/first-impressions-baselines-and-the-evaluation-framework-15357a82fe06?source=rss----9ba3897b6688---4">by Tam Dang at April 18, 2018 05:40 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=314">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/18/nlp-capstone-post-3-baseline-and-midi-frustration/">NLP Capstone Post #4: Baseline and MIDI Frustration</a></h4>
<div class="entry">
<div class="content" lang="en">
<h1><b>Baseline model</b></h1>
<p><span style="font-weight: 400;">Our baseline approach is taken from Daniil Pakhomov’s excellent blog post[1]. In this post, two separate RNNs are trained as generators: one for lyrical content and one for music content (in piano roll format). We will begin by using his trained lyrical model, and attempt to do conditional sampled character generation given a starting sequence of characters. We loaded the already trained models from the blog post and generated lyrics according to the style of a given songwriter and with a given seed word. The lyrics are generated via a character-level LSTM and generates the next character conditioned on the preceding characters and choice of songwriter. The model is trained on a corpus of song lyrics, where naturally the “correct” character to generate is the next character in the lyrics. Essentially the same mechanism is applied to the musical note generation.</span></p>
<p><span style="font-weight: 400;">The characters are encoded as a one-hot vector over all letters in the English alphabet plus space, comma, etc. Piano rolls already come in an encoding amenable to feeding into RNNs, modulo additional zero padding to ensure every time slice of every piano role has the same dimension. In particular, at each time step (discretized in an appropriately fine-grained way), we have an indicator 0-1 vector on which notes are currently activated.</span></p>
<p><span style="font-weight: 400;">Here is a song generated in the style of “Queen” with the starting seed sequence of characters “Music”:</span></p>
<blockquote>
<p style="text-align: center;"><i>Music savor valerite – yah  </i></p>
<p style="text-align: center;"><i>Imabribot, bind me – I – well  </i></p>
<p style="text-align: center;"><i>All going down to L</i></p>
<p style="text-align: center;"><i>At the eyes of the universe  </i></p>
<p style="text-align: center;"><i>Agree, five to the Slim  </i></p>
<p style="text-align: center;"><i>I just want to convincide  </i></p>
<p style="text-align: center;"><i>We wash stars and quiet Ich  </i></p>
<p style="text-align: center;"><i>You had a dirty old baby  </i></p>
<p style="text-align: center;"><i>We won’t  </i></p>
<p style="text-align: center;"><i>We does nothing no one ezy? follohin?  </i></p>
<p style="text-align: center;"><i>Sometimes we get down and ooh  </i></p>
<p style="text-align: center;"><i>Nothing do you see all night  </i></p>
<p style="text-align: center;"><i>  </i></p>
<p style="text-align: center;"><i>This is my pries  </i></p>
<p style="text-align: center;"><i>  </i></p>
<p style="text-align: center;"><i>Joyful the world  </i></p>
<p style="text-align: center;"><i>Does their beams  </i></p>
<p style="text-align: center;"><i>Surgeon makes the scule la beat  </i></p>
<p style="text-align: center;"><i>Walking out on my pocket ride  </i></p>
<p style="text-align: center;"><i>My faulty power  </i></p>
<p style="text-align: center;"><i>I wear from the ston</i></p>
</blockquote>
<p><span style="font-weight: 400;">Eventually, since we are actually interested in converting the musical information into plausible lyrics, we will need to modify this baseline in the natural way to take as input time slices of the musical instrumentation in piano roll format and predict characters (or syllables) that are to be enunciated simultaneously with the played notes. In this manner, the lyrics come already aligned in a natural way, and the words can be extracted by compressing the letters occurring between spaces.</span></p>
<h1><b>Dataset parsing</b></h1>
<p><span style="font-weight: 400;">The MIDI format has an unfortunate number of unexpected caveats. We have spent a majority of our time so far cleaning the data and attempting to use it in existing Python libraries that handle MIDI. A brief description of MIDI[2] covers some of the challenges:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">MIDI is made up of a series of messages, such as notes, instruments, and tempo changes. Additional metadata messages exist called meta messages, which can contain text content such as the song title (and lyrics!). In our dataset, lyrics are provided either as “text” messages or as “lyrics” messages.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Messages are grouped into different tracks, often representing separate instruments. Metadata sometimes is located in its own track, and lyrics are sometimes found in a different track from the rest of the metadata.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Durations in the MIDI format are specified as a delta-time relative to the most recent frame. Delta times are in a unit called a tick. Ticks are defined in the file header as a division of the quarter note. The header also defines the number of ticks per frame, which is what the deltas are relative to. Beats per minute (bpm) messages adjust the speed of playback throughout the song.</span></li>
</ul>
<p><span style="font-weight: 400;">The most promising library so far is PrettyMIDI[3], which handles most of the unexpected behavior of the basic MIDI format. This library wraps MIDI messages into structured python objects, and provides a conversion from MIDI into piano roll format. Piano roll in this case is a numpy array of shape (num_notes, num_frames). This allows us to input the musical data directly into an RNN. The units are also converted into absolute seconds, as opposed to relative durations. PrettyMIDI can additionally handle embedded lyrics, but this has proven to be a challenge due to the variety of annotation styles in our dataset. About 200 of our 900 files have parsed lyric data properly, so continuing to clean our data is a high priority.</span></p>
<h1><strong>U</strong>pd<strong>ate</strong></h1>
<p><span style="font-weight: 400;">Unfortunately, we have found the Kara1k dataset[4] to be inapplicable to our project, as the raw sequence of musical notes and lyrical content are not provided, only metadata that the dataset developers have extracted.</span></p>
<h1>References</h1>
<p><span style="font-weight: 400;">[1] </span><a href="http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/"><span style="font-weight: 400;">http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/</span></a></p>
<p><span style="font-weight: 400;">[2] </span><a href="http://www.music.mcgill.ca/~ich/classes/mumt306/StandardMIDIfileformat.html"><span style="font-weight: 400;">http://www.music.mcgill.ca/~ich/classes/mumt306/StandardMIDIfileformat.html</span></a></p>
<p><span style="font-weight: 400;">[3] </span><a href="http://craffel.github.io/pretty-midi/"><span style="font-weight: 400;">http://craffel.github.io/pretty-midi/</span></a></p>
<p><span style="font-weight: 400;">[4] </span><a href="http://yannbayle.fr/karamir/kara1k.php"><span style="font-weight: 400;">http://yannbayle.fr/karamir/kara1k.php</span></a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/18/nlp-capstone-post-3-baseline-and-midi-frustration/">by Nicholas Ruhland at April 18, 2018 04:58 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/first-strawman-attempt">
<h4><a href="http://sarahyu.weebly.com/cse-481n/first-strawman-attempt">First Strawman Attempt</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">Well...here's the first hiccup: the Reddit data I'd like to use is much too large (like 8GB compressed and unknown uncompressed for one month of posts). Since my last post, I had been working on trying to scrape the data manually through Reddit API requests, but I was running into some issues and it was taking quite a while because of the request restrictions.<br /><br />I decided, maybe a little later than I should have, to use the Reddit Data dumps provided by John Baumgartner at pushshift.io instead, but much of the recent data is too large for my computer, attu, as well as my Azure instance. I am working with a grad student to get access to more resources so that I can work with (or even open) some of these files! However, in the meantime I have committed some files that <em>would </em>be my strawman. I have a list of 124 subreddits labeled as the neurodivergent subreddit subset and currently have a model that builds a set of users that post to these subsets, finds the other neurotypical subreddits those users also post to, and aggregates a set of those subreddits. The strawman compares the basic n-grams in each subset of the subreddits to see basic language model differences. I also cloned the vennclouds github project to try and visualize the n-grams (uni, bi, and tri) that are used in these two subsets as well as their overlap however, the current version seems to have a basic bug.<br /><br />I will get the resources hopefully in the next couple of days and update this post with the actual data results! <br /><br /><em>To be continued...</em><br /><br /></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/first-strawman-attempt">April 18, 2018 03:47 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 12, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" title="Stories by Halden Lin on Medium">Halden Lin <br/> Team undef.</a></h3>


<div class="entrygroup" id="https://medium.com/p/7d8e9ec1a8e3">
<h4><a href="https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3?source=rss-2759d54493c0------2">NLP Capstone | 03: Project Proposal</a></h4>
<div class="entry">
<div class="content">
<p><em>previous posts: </em><a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5"><em>01</em></a><em> </em><a href="https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5"><em>02</em></a></p><h3>Towards a Better Understanding of Neural Networks: Visualizing Attention in Sequence-to-Sequence Models</h3><h4>A brief review of attention</h4><p>The idea of ‘attention’ was first introduced to the sphere of natural language processing by Bahdanau et al. (2014) in <em>Neural machine learning by jointly to align and translate</em>. The idea is fairly straightforward: if we have an encoder-decoder model, at each decoding time-step we generate a vector of attention weights corresponding to each of the encoding units. That is to say, when generating each output token, we pay ‘attention’ to certain parts of the input sequence. Intuitively, this is much how we as humans fixate on parts of text to perform tasks such as summarization or question answering.</p><h4>Why visualization?</h4><p>In Machine Learning, neural networks have always been a sort of black box. We know they work incredibly well in certain contexts, but its often difficult to understand why they work so well. The following quote sums up the need for interpretability quite well.</p><blockquote><strong><em>“I believe the most important direction for future research is interpretability.</em></strong><em> The attention mechanism, by revealing what the network is “looking at”, shines some precious light into the black box of neural networks, helping us to debug problems like repetition and copying. To make further advances, we need greater insight into what RNNs are learning from text and how that knowledge is represented.”</em></blockquote><blockquote>- Abigail See, PhD - Stanford University, <em>‘So, is abstractive summarization solved?’</em> from <a href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html">Taming Recurrent Neural Networks for Better Summarization</a></blockquote><p>Visualization provides an avenue for interpretability by mapping the behavior of the complex networks to easy-to-understand visual encodings.</p><h4>A survey of related work</h4><p>Although I am not aware of any papers dedicated to the visualization of attention, examples can be readily found in both published literature and online blogposts. For each example below, I’ll point out strengths and weaknesses. Ultimately, I hope to show that there are improvements we can make that can augment the interpretability of the workings of seq2seq attentional models.</p><p><strong>Heat-maps</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/404/0*9FCWt3AO5oGxLxtg." />Bahdanau et al. (2014). An attention visualization for a seq2seq problem (in this case, translation). Whiter cells represent higher attention.</figure><p>The encoding scheme used by Bahdanau et al. (2014) themselves, heat-maps were the most common encoding of attentional data I found. While making the task of relative correlation lookup efficient, these have a couple of weaknesses.</p><ol><li>Hard to scale. With tasks involving large input or output (e.g. a hundred or more tokens) the size of the heat-map quickly gets out of hand. Scrolling greatly decreases the effectiveness of a visualization with respect to analysis tasks.</li><li>Difficult to read. We generally don’t read in a token-per-line format. Furthermore, source text is rarely in a token-per-line format — we lose insightful information that could be drawn from analyzing the original structure of the text.</li></ol><p><strong>Flow-maps</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*I3cdFcqDAcdKEwuCpAPHTA.png" />Rikters et al (2017). The input sequence is seen on top — output on bottom. Thicker lines denote higher attention.</figure><p>Less common, but interesting nonetheless. This kind of flow-map suffers from problems similar to those of heat-maps. One could also argue that the thinness of the lines and their cross-hatch nature hinder interpretability.</p><p><strong>Interaction</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*3dTXvSI-L3X3M-MKXRraBA.gif" />See et al. (2017). Interactive visualization of attention</figure><p>Interaction solves many of the issues of the static visualizations surveyed above. We retain the structure of both the input and output text, and lookup is quick and efficient. There is a trade-off, however. We are only able to view the attention of a single word at a time, and as a result it is hard to get a sense of the overall coverage or structure of attention.</p><h4>A case study: Summarization</h4><p>In particular,<strong> abstractive summarization</strong>. Summarization is a particularly interesting use case of attention because of the requirement of the condensing of text. The hypothesis is that good abstractive models will be able to cover the majority of the original document. Here I note the difference between <strong>extractive </strong>and<strong> abstractive </strong>summarizations. The former involves selecting pieces of the original text, verbatim. The latter involves compressive paraphrasing.</p><p>Until recently, most of the work in text summarization has revolved around extractive summarization (See et al. 2017). However, the rising prevalence of recurrent neural networks has allowed for further focus in abstractive summarization. Attention has played an important role in improving results. Below is a brief list of relevant work.</p><p><a href="https://arxiv.org/pdf/1509.00685.pdf">Rush, Alexander M. et al. “A Neural Attention Model for Abstractive Sentence Summarization.” <em>EMNLP</em> (2015).</a></p><p><a href="https://arxiv.org/pdf/1602.06023.pdf">Nallapati, Ramesh et al. “Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.” <em>CoNLL</em> (2016).</a></p><p><a href="https://arxiv.org/pdf/1712.06100.pdf">Hasselqvist, Johan et al. “Query-Based Abstractive Summarization Using Neural Networks.” <em>CoRR</em> abs/1712.06100 (2017): n. pag.</a></p><p><a href="https://arxiv.org/pdf/1705.04304.pdf">Paulus, Romain et al. “A Deep Reinforced Model for Abstractive Summarization.” <em>CoRR</em> abs/1705.04304 (2017): n. pag.</a></p><p><a href="https://arxiv.org/pdf/1704.04368.pdf">See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” <em>ACL</em> (2017).</a></p><h4>Summarization Specific Challenges</h4><p>While visualizations of attention are helpful in shedding light on the workings of seq2seq models, summarization models in particular have trouble leveraging this window.</p><ol><li>We care about <strong>where</strong> attention falls just as much as what it falls on. We hope to maximize <strong>coverage</strong>. This is not currently addressed in any interactive visualizations I am aware of.</li><li>We have large input sequences. As discussed in <strong>“A survey of related work,”</strong> this is particularly problematic for static visualizations.</li></ol><p>With this in mind, I propose areas for improvement in both interactive and static visualizations.</p><h4>Where to?</h4><p>With interactive visualizations, two things.</p><p><strong>One. Coverage </strong>is the aggregated attention over a sequence of output tokens. An example given by See et al. can be seen in the figure below. Perhaps allowing brushing to visualizing the aggregate attention over a phrase or sentence can help us understand attention in a more global context.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/956/0*vE-iXohphbWY6Nam." />See et al. (2017). Example of coverage.</figure><p><strong>Two. Extraction vs Abstraction: </strong>Ideally, we want our model to learn how to abstract rather than extract. 1:1 exact match attention is less interesting to see than seeing attention to groups of words. Perhaps emphasizing / de-emphasizing can this in visualizations can help aid understanding of models.</p><p>With static visualizations, there are two analysis tasks that we wish to optimize for.</p><ol><li><strong>Summary. </strong>What is the overall structure of the attention (e.g. coverage).</li><li><strong>Value. </strong>Which input words are attended (i.e. focused on) by each output timestep?</li></ol><p>It is difficult to design an <em>effective</em> static visualization that lends itself well to both of these tasks. Perhaps we need a set of visualizations. For example, one visualization might afford better performance for summary analysis, while another might afford better performance for value analysis. In addition, these static visualizations can incorporate ideas described in previous section.</p><p>Additionally, attention visualizations thus far have been for <strong>specific examples</strong>. Perhaps there a way we can look <strong>across examples</strong> to better understand the behavior of these neural networks. Derived metrics for attention or coverage could be useful in better understanding and diagnosing these models.</p><p>My hope is that addressing these items in both interactive and static visualizations will allow us to better reason about neural networks. In particular, I hope the result can be used as a valuable tool for error analysis, <strong>even</strong> <strong>beyond</strong> hyperparameter tuning. Insights could be gleamed that motivate additions or constraints or mechanisms to optimize coverage (e.g. See et al. (2017)) or abstraction.</p><h4>The Plan</h4><p><strong>Minimum Viable Plan</strong></p><ol><li>Develop a TensorBoard plugin that allows for the static and interactive visualizations described in <strong>Where to?</strong></li><li>Acquire feedback from students / researchers in the Allen School.</li></ol><p>I intend to leverage existing models to retrieve data. For example, that <a href="https://github.com/abisee/pointer-generator">provided publicly</a> by See et al. (2017). The dataset used by them is a <a href="https://github.com/abisee/cnn-dailymail">modified CNN/Daily Mail Dataset</a> [Hermann et al. (2015), See et al. (2017)] — a collection of articles and bullet point summaries.</p><p><strong>Stretch Goals</strong></p><ol><li>Explore and implement aggregate, cross-example visualizations as described in <strong>Where to?</strong></li><li>Release a beta of the TensorBoard plugin on github and acquire feedback there.</li></ol><h4>Works Cited</h4><ul><li><a href="https://arxiv.org/pdf/1704.04368.pdf">See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” <em>ACL</em> (2017).</a></li><li><a href="https://arxiv.org/pdf/1506.02078.pdf">Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. “Visualizing and understanding recurrent networks.” <em>arXiv preprint arXiv:1506.02078</em>(2015).</a></li><li><a href="https://arxiv.org/pdf/1712.06100.pdf">Hasselqvist, Johan et al. “Query-Based Abstractive Summarization Using Neural Networks.” <em>CoRR</em> abs/1712.06100 (2017): n. pag.</a></li><li><a href="https://arxiv.org/pdf/1602.06023.pdf">Nallapati, Ramesh et al. “Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.” <em>CoNLL</em> (2016).</a></li><li><a href="https://arxiv.org/pdf/1705.04304.pdf">Paulus, Romain et al. “A Deep Reinforced Model for Abstractive Summarization.” <em>CoRR</em> abs/1705.04304 (2017): n. pag.</a></li><li><a href="https://arxiv.org/pdf/1509.00685.pdf">Rush, Alexander M. et al. “A Neural Attention Model for Abstractive Sentence Summarization.” <em>EMNLP</em> (2015).</a></li><li><a href="https://arxiv.org/pdf/1409.0473.pdf">Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. “Neural machine translation by jointly learning to align and translate.” <em>arXiv preprint arXiv:1409.0473</em> (2014).</a></li><li><a href="https://arxiv.org/pdf/1506.03340.pdf">Hermann, Karl Moritz et al. “Teaching Machines to Read and Comprehend.” <em>NIPS</em>(2015).</a></li></ul><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7d8e9ec1a8e3" width="1" /></div>







<p class="date">
<a href="https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3?source=rss-2759d54493c0------2">by Halden Lin at April 12, 2018 05:46 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 11, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/formal-proposal">
<h4><a href="http://sarahyu.weebly.com/cse-481n/formal-proposal">Formal Proposal</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">Linguistic Accommodation for Self-Presentation as seen in Neurotypical vs. Neurodivergent Subreddits<br /><br /><u>Hypotheses:</u> <br />1) Users change their language depending on the community, represented by subreddits categorized as Neurotypical vs Neurodivergent<br />      - The divergence from their own baseline is a<span> sign of assimilation through tuned self-presentation*</span><br />2) Language Models, as used by the whole community, differ and not just in topic-specific jargon<br />      - The language changes of the individual user and the change in delta from the community's language model is a sign of their attempt at assimilating language accommodation*<br />          - Do certain users adapt better? If so, what is differentiating those users?<br /><font size="1">*(subpoints are very similar, and I'm still working through if there is a nuance, or if they're the same)</font><br /><br />My <strong>objective</strong>, then, is to address these hypotheses through the following approach:<br /><br /><u>Literature Survey:</u><br />While this project is novel, mainly in the focus on neurotypical vs neurodivergent separation and the use of Reddit data, this project finds guidance from previous work done on similar questions. First, this project aims to extend upon the work of Danescu-Niculescu-Mizil et al. in<em> </em><u><em>Mark My Words! Linguistic Style Accommodation in Social Media​.</em></u> This was the first large-scale endeavor in identifying linguistic accommodation using social media. However, our project extends this work by taking advantage of the siloed nature of Reddit to identify linguistic accommodation employed by a single user across communities as opposed to the one-dimensional view of a user's linguistic accommodation to the general twittersphere in Danescu's paper. Also, this project is informed by Fast and Horvitz in <u><em>Identifying Dogmatism in Social Media: Signals and Models</em></u>, specifically in their methodologies and models; I attempt to extend upon these with more complex models. In this process, I also found several works that were similar in nature: Tamburrini et al. on language change based on social identity on Twitter, Nguyen and Rose on language socialization in online communities, and Michael and Otterbacher on herding in online review language. Two more relevant works for my project are De Choudhury et al.'s work on identifying the shift to Suicidal Ideation in social media and D<span>anescu-Niculescu-Mizil's work on the life-cycle of users in online communities. <br />​<br /><u>Proposed Methodologies</u>: </span><br />In it's most basic form, these questions can be explored with basic language models. First, we will identify a subset of neurotypical and neurodivergent subreddits to explore (100 or so respectively), chosen by a preliminary search on overlapping users posting between these. Based on this preliminary search, we will also gain a set of users who potentially post to both neurotypical and neurodivergent subreddits (we may need to look only at posts within a band of characters, but that is a parameter I'd like to explore). We will aggregate all of a user's posting history, not just in the subset aforementioned, to model the user's language use and do the same for the language of all posts made by any user (not just our set) to the subreddit to model the subreddit's language. I will supplement these models with the LIWC lexicon to characterize the differences between the communities and between users in different subreddits. (I may use a subset of the LIWC categories later on). A more complex model would be to use PPDB to find differences via paraphrasing. Yet another complex model would be to use a graphical model as inspired by Bamman et al's <em><u>Learning Latent Personas of Film Characters</u></em>. A stretch goal would be to train an RNN model for the language model of a neurotypical subreddit and that of a neurodivergent to see the probability of a post to belong to either of these categories. A stretch goal (not in complexity as in the RNN, but rather in interest) is to use the <em>Zelig Quotient</em>, a proposed measure for normalizing linguistic accommodation by Jones et al and see how much this may affect our findings. <br />One special consideration is the use of NSFW language. My only filter will be to disqualify the list of NSFW subreddits, as named by a reddit post (so meta) in being chosen for the subsets, but otherwise we will not do anything special for NSFW language in other subreddits. <br /><br /><u>Resources</u>: Lots of Reddit fun!!!<br /><br /><em>​Here goes nothing...</em></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/formal-proposal">April 11, 2018 07:00 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=14">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/04/11/project-proposal-question-based-knowledge-representation/">Project Proposal: Question-Based Knowledge Representation</a></h4>
<div class="entry">
<div class="content" lang="en">
<p><strong>Project Objective:</strong> The objective of this project is to create a model that can build a representation of the knowledge it’s gathered by reading the input text line by line and asking appropriate clarifying questions to a human for further insight.</p>
<p><strong>Literature Survey:</strong></p>
<ul>
<li><cite class="formatted-citation formatted-citation--style-mla">Williams, Jason D. et al. “Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning.” <em>ACL</em> (2017).</cite></li>
<li>
<div class="padded"><cite class="formatted-citation formatted-citation--style-mla">Zhang, Qianqian et al. “A Review on Entity Relation Extraction.” <em>2017 Second International Conference on Mechanical, Control and Computer Engineering (ICMCCE)</em> (2017): 178-183.</cite></div>
</li>
<li>Arvind Neelakantan’s Doctoral Disseration: <em>Knowledge Representation and Reasoning with Deep Neural Networks (2017)</em></li>
<li>
<div class="padded"><cite class="formatted-citation formatted-citation--style-mla">Zhao, Tiancheng and Maxine Eskénazi. “Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning.” <em>SIGDIAL Conference</em> (2016).</cite></div>
</li>
</ul>
<p><strong>Minimal Viable Action Plan:</strong> My minimum action plan would be to implement and train an RL model to ask certain question template(s) as clarification as it processes input text line by line. Then a human (myself) would assign a reward to indicate the quality of the question and give a brief answer to the question. This brief answer would then be used to expand the model’s stored knowledge representation and the assigned reward would be used to train the RL model. This series of exchanges is treated like a one-on-one conversation even if it’s more like a one-sided lecture.</p>
<p><strong>Stretch Goals:</strong> Stretch goals include extracting features from the existing knowledge base and feeding those into the model to further improve the relevancy of questions (for example not asking questions that the model should already know). Another stretch goal would be to translate the knowledge representation back into everyday English text for tasks such as answering reading comprehension questions. This can be done either by training a Machine Translation model, using a parser like ANTLR, or some other method.</p>
<p><strong>Proposed Methodologies:</strong> My proposed methodologies follows a similar outline as given by Williams et al. in their paper on Hybrid Code Networks (HCNs).</p>
<ul>
<li>The input text is read in line-by-line and goes through various preprocessing steps like entity extraction, word embeddings layer, sentiment analysis, bag of words, etc. These features are concatenated and fed into the model.</li>
<li>In the HCN paper, the model was an RNN followed by a softmax layer (probability distribution over the various actions to take).
<ul>
<li>In this problem, the “actions” to take are the different types of question templates to ask so copying this model and substituting their action templates with my question templates would work.</li>
</ul>
</li>
<li>An alternative approach would be to build a Deep Q Network model with an LSTM (or GRU) layer at the end, so the network would update the Q values for each question type that can be asked (and the one with the highest Q value can be greedily selected). For clarification, each input sentence would represent a time step.</li>
<li>After a question type is selected with either of the 2 approaches above, nouns need to be substituted in to form an actual question (as in the HCN paper). For example, a question template might be “Is there a relationship between ______ and ______?” and the two blanks need to be filled in with nouns. If the substitution results in a good question, a good reward is assigned and otherwise, a very negative reward is assigned. If it’s a good question, then an answer is given to update the knowledge representation.
<ul>
<li>Some possible ways to create this knowledge representation is to use lists, trees, semantic networks, production rules, logical propositions and/or other existing NLP knowledge representation models (I haven’t decided on one yet).</li>
</ul>
</li>
</ul>
<p><strong>Available Resources/Databases:</strong> To train the model, any reading comprehension dataset like MS Marco (Microsoft), SQuAD (Stanford), RACE, etc. can be used. If my project is successful enough, I can even use these datasets for evaluation.</p>
<p><strong>Evaluation Plan:</strong> If I only finish my minimum viable action plan, I’m not sure how I can qualitatively assess my knowledge representation except by comparing the representation against the information I wanted to be recorded (or have other people judge what knowledge should be stored). On the other hand, if I finish my stretch goals (translating knowledge representation to English) I can try to use the knowledge representation to train on and respond to queries in a reading comprehension dataset (which would be a much more qualitative evaluation).</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/04/11/project-proposal-question-based-knowledge-representation/">by ananthgo at April 11, 2018 06:59 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a></h3>


<div class="entrygroup" id="https://medium.com/p/7b6d1a9ec67c">
<h4><a href="https://medium.com/@viterbi.or.not/formal-project-proposal-7b6d1a9ec67c?source=rss-c522ef075bb3------2">Formal Project Proposal</a></h4>
<div class="entry">
<div class="content">
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cr8jfKVLjgd9Y7YmxXWSYA.png" /></figure><p>Introducing our project topic, <strong>automatic conversation summarization</strong>! Our proposal will outline specific objectives, motivations, and plan we have for this project. We also cover here the resources we have gathered — related work, datasets (as promised), and evaluation frameworks, to demonstrate viability and give background on the topic.</p><h4><strong>Objective</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DmM6aXAhZi61Ec7Yhp6YBw.png" />Example summarization of an email chain from the <a href="https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/bc3.html">W3C email threads corpus</a></figure><p>In the field of automatic text summarization, there are numerous techniques that can be used to produce summaries of general text data. However, for our project, we seek to work on a more specific type of data by exploring and evaluating techniques for summarization of conversation logs. Therefore, the models we will attempt to implement and measure will take input in the form of natural text from conversations, such as email threads, chat logs, or transcribed spoken conversations, and output more concise summaries that capture the most important parts of the input.</p><p>While a document or paper typically sticks to a single topic at once and represents communication between an author and the reader, conversational data is characterized by a mixing of sub-topics and, in many cases, contributions from multiple different authors before one topic is finished. As a result, the objective of conversational summarization includes identifying topics and threads among a potentially chaotic conversation in order to make a sensible summarization even without the benefit of a single, linear topic progression.</p><h4><strong>Extractive vs. Abstractive</strong></h4><p>When considering text summarization, a distinction has to be made between two different approaches to summarization that offer different levels of implementation difficulty and usefulness. An extractive summary is produced by identifying the most important sentences from the input text and combining them to form a summary that is the concatenation of those sentences.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iZujvW2rP_HSVU2KXLtvew.png" />Example of a short chatlog and its corresponding extractive summary</figure><p>Alternatively, an abstractive summary consists of new text generated from the topics and important aspects of the input text, but requires the model to create new summary sentences rather than simply re-using existing ones. As a result of this generation process, abstractive summaries are typically considered to be more useful, because they consist of natural-sounding text while still paraphrasing the concepts that are important. However, current techniques perform much better at generating extractive summaries, which are considered much easier to implement because they only require assigning scores to the sentences of the input.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1oiGv-MrfdubjbKv-Vp9fA.png" />Example of a short chatlog and its corresponding abstractive summary</figure><h4><strong>Motivation</strong></h4><p>The <strong>academic</strong> motivations behind our project are to pursue a topic in summarization that has been relatively less explored. This means tackling the challenges that come with the conversation summarization domain — annotated conversation datasets are typically smaller and more unpredictable, and whereas tasks like document summary usually involve one topic written by one author/voice, conversations and involve many participants and have less well-defined topic segmentation.</p><p>The project is also motivated by the possibly impactful <strong>applications </strong>of conversation summarization. Being able to summarize long chains of emails or group IMs is an increasingly important task to tackle in today’s world and can be a useful augmentation to digital group conversations.</p><h4><strong>Related Work</strong></h4><p>In our literature survey, we will first discuss the two papers that have been most impactful in helping establish our project plan.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*U0i-moKx4s_YB3My." />Graph of contributions of general feature categories to the performance of logistic regression classifiers</figure><p><a href="http://www.aclweb.org/anthology/D08-1081"><strong>Summarizing Spoken and Written Conversations</strong></a><strong>¹ </strong>uses meeting and email datasets. The authors of this paper approach extractive summarization with logistic regression classifiers and a mix of general summarization features as well as some basic conversation summarization features. The overall feature categories can be seen in the corresponding figure, which graphs the contributions of each feature category to the classifier’s performance. “Participant” category features were found to help achieve competitive results.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1010/0*wLl3u1tR-GKYuIql." />Table of performances of previously published models and the models used in the experiment (C4.5, NB, MLP, SVM)</figure><p><a href="http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf"><strong>Summarizing Online Conversations: A Machine Learning Approach</strong></a><strong>²</strong> uses chatlog and meeting datasets. The authors of this paper conduct experiments with Decision Tree, Naive Bayes classifier, Multilayer Perceptron (MLP) and Support Vector Machine (SVM) summarizers to create extractive summaries. The input feature vectors to these trainable summarizers use both general summarization features (sentence length, sentence position, similarity to title, etc.) as well as conversation specific summarization features that are more specialized than those mentioned in the previously discussed paper (is question, sentiment score, discourse markers). The paper overall found that the Naive Bayes classifier and MLP performed the best.</p><h4><strong>Datasets</strong></h4><p>The datasets we found cover a variety of conversation domains and are all human-annotated with summaries.</p><p><strong>Email: </strong><a href="https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/bc3.html"><strong>W3C Email Threads</strong></a></p><ul><li>40 email threads of ~80 lines each</li><li>Extractive and Abstractive Summaries</li></ul><p><strong>Chat: </strong><a href="https://flossmole.org/content/software-archaeology-gnue-irc-data-summaries"><strong>GNU Enterprise Chatlogs</strong></a></p><ul><li>~120 chats of ~1200 lines each</li><li>Abstractive Summaries</li></ul><p><strong>Spoken Conversation: </strong><a href="http://groups.inf.ed.ac.uk/ami/corpus/"><strong>AMI Meeting Transcripts</strong></a></p><ul><li>140 meeting transcriptions of ~45 minutes each</li><li>Extractive and Abstractive Summaries</li></ul><h4><strong>Evaluation</strong></h4><p>In terms of <strong>automated</strong> methods of evaluation for the summaries that we will generate, there are a few frameworks we can use:</p><p><strong>BLEU</strong> (BIlingual Evaluation Understudy)</p><ul><li>Precision measure with some enhancements</li></ul><p><strong>METEOR</strong> (Metric for Evaluation of Translation with Explicit ORdering)</p><ul><li>Improves on BLEU by adding recall, synonyms</li><li>Better at sentence-level evaluation</li></ul><p><strong>ROUGE</strong> (Recall-Oriented Understudy for Gisting Evaluation)</p><ul><li><strong>ROUGE-L</strong>: Longest common subsequence</li><li><strong>ROUGE-N</strong>: Overlap of N-Grams between passages</li></ul><p>However, for summarization tasks, <strong>human evaluation</strong> is possibly most ideal and it is unclear if any automatic metric can be as effective.</p><h4><strong>Minimum Viable Product</strong></h4><p>Our plan is to start with two <strong>baseline models</strong> that replicate the best performing models (Naive Bayes and MLP approaches) of “Summarizing Online Conversations: A Machine Learning Approach” using the non-conversation specific features.</p><p>We would then experiment with tweaks to the baseline models that use conversation-specific features, both based off of the ones described in the paper and also based on some of our ideas about what kinds of domain-specific features might benefit conversation summarization. We can also continue to explore alternative models and compare model approaches.</p><h4><strong>Stretch Goals</strong></h4><p>There are a number of ideas we have for stretch goals that can push our project further:</p><p><strong>Neural Network implementation: </strong>Applying general summarization techniques to conversation</p><p><strong>Model entity relationships: </strong>Identifying the role of a contributor or named entity, such as a supervisor in an email thread</p><p><strong>Abstractive summarization: </strong>Convert a previously extractive summary to be natural-sounding and abstractive</p><p><strong>Optimize performance for a specific domain: </strong>For example, in chats about bugfixes, use that specific context to try outperforming the general case</p><p><strong>Relate parameters from different types of data: </strong>For example, use data from spoken corpus to improve results on email</p><p>We hope you enjoyed reading our project proposal and we are excited to get working!</p><p>[1] Murray, G. &amp; Carenini, G. (2008). Summarizing Spoken and Written Conversations. <em>Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, </em>773–782.</p><p>[2] Sood, A. &amp; Varma, V. (2012). Summarizing Online Conversations: A Machine Learning Approach. <em>Centre for Search and Information Extraction Lab International Institute of Information Technology Hyderabad,</em> 500.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7b6d1a9ec67c" width="1" /></div>







<p class="date">
<a href="https://medium.com/@viterbi.or.not/formal-project-proposal-7b6d1a9ec67c?source=rss-c522ef075bb3------2">by Viterbi Or Not To Be at April 11, 2018 06:56 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=309">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/11/nlp-capstone-post-3-proposal/">NLP Capstone Post #3: Proposal</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>Here, we finally present our project proposal in full.</p>
<h3>Project Objectives:</h3>
<p>Our goal for this project is to engineer a model that, given the instrumental (“karaoke”) music for a song in English represented as MIDI data, output a coherent sequence of words corresponding to lyrics for the music. The model will produce timings along with the words to align it with the background instrumentals. Additionally, given the output of the model and the input music, we will automate their combination into a song complete with lyrics and supporting instrumentals. This combined output will be playable and we intend to do live demonstration.</p>
<h3>Proposed Methodology:</h3>
<p>Here, we outline the steps we will need to take in detail.</p>
<li>Data collection (datasets of songs, preferably with instrumentals and lyrics already separated)</li>
<li>Decide on vocabulary and how to handle uncommon words</li>
<li>Decide and implement any required preprocessing of the raw MIDI data. Strip lyrics from MIDI data if not already provided in dataset.</li>
<li>Decide and implement model (see Model Design)</li>
<li>Implement model sanity checks</li>
<li>Model tweaking (we expect this will take the majority of the time; see Model Design)</li>
<li>Implement automated combination of model output (lyrics) and model input (instrumentals)</li>
<li>Further testing</li>
<li>Assuming preceding steps are completed satisfactorily, proceed to stretch goals</li>
<li>Presentation and write-up</li>
<h3>Model Design:</h3>
<p>We will pursue a seq2seq RNN approach, taking in input MIDI data represented as a sequence, and outputting a sequence of words from a specified vocabulary. This model will be referred to as the generator. We will employ adversarial training, simultaneously training a many-to-one RNN discriminator that, given the input instrumentals and corresponding lyrics, output if the lyrics were produced by the generator or not. We will follow approaches taken in previous works such as SeqGAN [2] (and [3, 4]), namely using policy gradient ideas from reinforcement learning to obtain gradients that can be backpropagated from the discriminator network through the generator network. We note that syntactic correctness can be enforced in this manner, as malformed lyrical output can be assigned arbitrarily small reward.</p>
<h3>Stretch Goals:</h3>
<p>There are several stretch goals we will consider, time permitted. They are as follows, in no particular order.</p>
<li>Handling multiple languages, particularly those with less available data</li>
<li>Given a specific songwriter/band, produce the instrumentals along with lyrics for a new song that is in the style of that songwriter/band</li>
<li>Lyrics generation for duets, or multi-singer songs</li>
<li>Playing with phoneme-level generation</li>
<h3>Core Challenges:</h3>
<p>The core challenges we will need to overcome include alignment of lyrics with the music, and production of sensible lyrics. On the more technical side, it is well-known that ensuring convergence in adversarial training is difficult.</p>
<h3>Available Resources:</h3>
<p>Existing music datasets for machine learning tasks are made up of audio samples (such as .wav or .mp3), or MIDI data that specifies timing and notes. For karaoke, lyrics are also provided either as a separate text file (.LRC) specifying the timing of each word, or can be embedded into the MIDI file directly (.KAR). It may also be useful to train a lyric model on a larger corpus of song lyrics, since lyrics are easier to collect than fully time-annotated karaoke files.</p>
<p>The MusicNet dataset [9] provides 330 classical instrumental audio files, each of which has associated timing provided for every note. Since we are primarily interested in lyrical generation and alignment, this dataset is not going to be useful for creating a language model.</p>
<p>An existing karaoke dataset called Kara1k [1] provides many features computed from 1000 lyric-annotated songs. This provides lots of metadata about each song, including annotated chords for each timestep of the song. According to the KaraMIR website, these features are extracted from audio samples using Vamp Plugins, which estimates chords with accuracy up to 70%. </p>
<p>We propose a new dataset (name not yet determined) of MIDI karaoke data with embedded lyrics (.KAR). This dataset contains over 700 files, scraped from a karaoke content aggregator [11]. Timed lyrical data has been extracted from these files, and the precise timing of each note is already available by nature of the MIDI format.</p>
<p>Additional datasets for training a lyric model may be useful, and many are available. One such dataset is the 55000+ Song Lyrics on Kaggle [10]. This could help our model generalize its lyrical output beyond the limited set of vocabulary available within the 1000 or fewer annotated karaoke songs.</p>
<h3>Evaluation Plan:</h3>
<p>Evaluation of our model can be done several ways. The first is simply to listen to the music ourselves. This is the most direct method of evaluation but is not efficient, as likely we will need many iterations of tuning; furthermore, will likely need to listen to several songs to be confident of the model’s quality. Hence, we will also design basic “sanity check” tests for our models.</p>
<p>Recall that in our proposed methodology, we intend to use adversarial training. The discriminator network itself gives a direct evaluation of the generator. As long as the discriminator is of vetted quality, and the discriminator is run on sufficiently many examples (with roughly even number of generated and true examples mixed in), the generator will be deemed also of sufficient quality (as a “sanity check”).</p>
<p>Of course, this leaves the question of ensuring the discriminator is good. We can run the discriminator on instrumentals combined with randomly generated words (according to some distribution), or on instrumentals combined with the original lyrics, which are perturbed in some fashion. As an example, one can perturb the original lyrics temporally (making an utterance off-beat when it should be precisely on the down-beat of a bar) or replacing a few words with randomly selected ones (according to some distribution over the vocabulary). These “test inputs” to the discriminator can be generated before-hand.</p>
<h3>Literature Survey:</h3>
<p>Here are some relevant papers (most were already included in preceding posts).</p>
<p>[1] Y. Bayle, L. Marsik, M. Rusek, M. Robine, P. Hanna, K. Slaninova, J. Martinovic, J. Pokorny. “Kara1k: A Karaoke Dataset for Cover Song Identification and Singing Voice Analysis”. IEEE International Symposium on Multimedia (ISM), 2017. <a href="https://ieeexplore.ieee.org/document/8241597/" rel="nofollow">https://ieeexplore.ieee.org/document/8241597/</a></p>
<p>[2] L. Yu, W. Zhang, J. Wang, Y. Yu. “SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient”. Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, 2017. <a href="https://arxiv.org/abs/1609.05473" rel="nofollow">https://arxiv.org/abs/1609.05473</a></p>
<p>[3] S. Lee, U. Hwang, S. Min, S. Yoon. “A SeqGAN for Polyphonic Music Generation”. 2017. <a href="https://arxiv.org/abs/1710.11418" rel="nofollow">https://arxiv.org/abs/1710.11418</a></p>
<p>[4] H. W. Dong, W. Y. Hsiao, L. C. Yang, Y. H. Yang. “MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment”. 2017. <a href="https://arxiv.org/abs/1709.06298" rel="nofollow">https://arxiv.org/abs/1709.06298</a></p>
<p>[5] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio. “Generative Adversarial Nets”. NIPS, 2014. <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets" rel="nofollow">https://papers.nips.cc/paper/5423-generative-adversarial-nets</a></p>
<p>[6] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, X. Chen. “Improved Techniques for Training GANs”. NIPS, 2016. <a href="https://arxiv.org/abs/1606.03498" rel="nofollow">https://arxiv.org/abs/1606.03498</a></p>
<p>[7] M. Arjovsky,  S. Chintala, L. Bottou. “Wasserstein GAN”. 2017. <a href="https://arxiv.org/abs/1701.07875" rel="nofollow">https://arxiv.org/abs/1701.07875</a></p>
<p>[8] J. Faille, Y. Wang. “Using Deep Learning to Annotate Karaoke Songs”. 2016. <a href="https://www.semanticscholar.org/paper/Using-Deep-Learning-to-Annotate-Karaoke-Songs-Faille-Wang/521361762a7327f8fcc77bd9d76eaa2b503f845a" rel="nofollow">https://www.semanticscholar.org/paper/Using-Deep-Learning-to-Annotate-Karaoke-Songs-Faille-Wang/521361762a7327f8fcc77bd9d76eaa2b503f845a</a></p>
<p>[9] J. Thickstun, Z. Harchaoui, S. Kakade. “Learning Features of Music from Scratch”. 2017. <a href="https://arxiv.org/abs/1611.09827" rel="nofollow">https://arxiv.org/abs/1611.09827</a></p>
<p>[10] Additional data <a href="https://www.kaggle.com/mousehead/songlyrics">here</a></p>
<p>[11] Even more additional data <a href="http://vooch.narod.ru/midi/midi.htm">here</a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/11/nlp-capstone-post-3-proposal/">by Kuikui Liu at April 11, 2018 06:45 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/43368563cf97">
<h4><a href="https://medium.com/nlp-capstone-blog/machine-dictionary-43368563cf97?source=rss----9ba3897b6688---4">Machine Dictionary</a></h4>
<div class="entry">
<div class="content">
<p>We have decided to call our project, the Machine Dictionary. Formally, the goal of this project is to use a large corpus of data to generate definitions for technical terms that are consistent with how those terms are explored in the corpus.</p><h4>Motivation</h4><p>Our aim is to explore novel text generation approaches and apply them to the specific task of generating definitions. One of these techniques includes a specific approach we have termed “Connecting the Dots”. This approach will allow us to loosely structure the definitions such that they contain meaningful content but also keep the model general enough to generate appropriate context. We will discuss this approach further down. Another large motivation is the amount of research paper data we have from the AI2 Semantic Scholar corpus. This will allow us to use domain specific corpora to support definitions.</p><h4>Prior Work</h4><p>Although text generation has been a hot topic in NLP research for a while, we did not discover any prior attempts to generate definitions for technical terms. That said, the text generation task has itself been explored in great detail by many others.</p><p>In <a href="https://arxiv.org/pdf/1707.05501.pdf"><strong>this paper</strong> (Jain et al., 2017)</a>, the authors explore the task of generating short stories given a sequence of independent short descriptions. They approach the problem by using an Encoder-Decoder model to connect the descriptions and the short stories. This method might help us generate short text, but in our case, the input would be a large amount of data in the domain.</p><p>In this <a href="https://pdfs.semanticscholar.org/9dad/f5bb0a2182b1509c5ea60d434bb35d4701c1.pdf?_ga=2.15851958.1083977791.1523309085-1136887644.1523309085">other paper (Ghazvininejad et. al, 2016)</a>, the authors explore generating poetry based on topics. This paper is relevant to our project for several reasons. For instance, the paper generates poems based on a given topic, much like our goal which is to generate definitions based on a given term. In addition, the authors generate poetry by taking advantage of the structure of Shakespearean sonnets such as the unique rhyme scheme and the iambic pentameter cadence. We can use the techniques proposed in the paper to selectively choose information from the training corpus, based on the term we are asked to define and how we believe definitions should be structured.</p><h4>Minimum Viable Plan</h4><p>To reiterate, our model should be able to generate definitions that are based on context received from a large corpus. In this MVP, we can make the simplification that our model should generate definitions of a fixed length (for example, 5 sentences). These definitions should be grammatical and technically correct.</p><h4>Baseline Approach and Evaluation</h4><p>Andrej Karpathy, a former PhD student at Stanford, explores the incredible effectiveness of RNNs in in his blog. The article, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of RNNs</a>, essentially claims that RNNs have an uncanny ability to learn the structure of training data, such as Wikipedia Articles, Shakespeare, and even code. In our baseline approach, we take advantage of this ability by training an RNN language model on the training data. At testing time, we can provide the term we seek to define as a seed to the RNN, which can then generate text until we hit a word limit or the RNN generates the &lt;STOP&gt; character. We can evaluate this approach by using perplexity, to ensure that we have a good language model. We can also cross-check definitions with other sources like Wikipedia articles. Finally, it might also be prudent to have humans evaluate the definitions.</p><h4>Target Approach I and Evaluation</h4><p>We propose two different target approaches to this model. In this first approach, we utilize techniques from work done previously in abstractive summarization. Given a term, we could filter the input data on all sentences associated with that term. We could obtain all the sentences that contain the data and a few sentences in the nearby surroundings, which could capture the context for the data. We would then use these sentences to generate a summary, which we would call the definition of the term. In this approach, we might use an attention mechanism to focus on the most important parts of the input. In terms of evaluation, we would use the cross referencing method from above, where we take the produced definition and the “correct definition” as determined by an external source and compare the number of common words.</p><h4>Target Approach II and Evaluation</h4><p>In this second approach, we explore a concept we have chosen to call “Connecting the Dots”. In this approach, we structure the definition generation by using key words. To elaborate, each term might be closely connected to a certain number of other words which could influence the definition of the term greatly. Consider the term <strong>osteoporosis</strong>. This term might be closely associated with the words <strong>bones, degrade, bone degradation, fractures, </strong>and <strong>women. </strong>We could use these words to structure a definition for <strong>osteoporosis </strong>as a fill in the blank task.</p><p><strong>Osteoporosis is … bones … degrade … bone degradation … fractures … women.</strong></p><p>In the above structure, we would rely on the model to appropriately and grammatically fill in the context between each keyword. As a result, we might generate the following definition:</p><p><strong>Osteoporosis </strong>is a disease that cause <strong>bones </strong>to <strong>degrade.</strong> <strong>Bone degradation </strong>often leads to <strong>fractures.</strong> <strong>Osteoporosis </strong>most commonly affects <strong>women.</strong></p><p>It’s important to note that when we fill in the context between keywords, we must condition on the original term that we are defining. For example, between the words <strong>fracture </strong>and <strong>women</strong>, there might be several sentences we could generate, but we must keep in mind how the keywords are related given that they are about <strong>osteoporosis.</strong></p><p>In this approach, we will build on the neural network model and add task-specific architecture to capture relationships between words. We hope that the model can learn how to define keywords associated with terms and use those terms to structure a definition.</p><p>In terms of evaluation, we introduce another technique. In this evaluation method we take a paragraph in which the technical term appears and omit the term. If we rephrase the problem as a classification task and ask the model to predict the omitted term, we would be able to conclude whether the model has a contextual understanding of the technical term. This evaluation technique would be a good supplement to the human evaluation method where we request users to rank how correct and readable the definitions are.</p><h4>Stretch Goals</h4><p>The ideal goal would be to generate text without any constraints on length, order, or keyword usage. This would be a more “hands-off” approach to text generation and could also allow us to train on different domain based corpora. We might be able to achieve this stretch goal if we perform well on the goals outlined in the Minimum Viable Plan.</p><p>Another stretch goal has to do with ontology matching, whereby we compare two definitions to determine whether they describe the same concept. We could extend this example to generate definitions for all technical terms across a body of research papers, determine which terms are defined similarly in different papers, and unify the terminology across all papers. This goal is definitely a stretch goal, but if we can perfect the architecture for generating definitions, we see this as a future application of our project.</p><h4>Data</h4><p>We plan to use the Semantic Scholar Open Research Corpus for this project. This corpus consists of over 20 million research papers classified into two domains (Computer Science and Medicine). Depending on the approach we take to solve this task, we would filter the data and train accordingly.</p><h4>Resources and Literature Survey</h4><p>We have mentioned several resources above that were the most useful for formulating the project proposal. Here are those resources and a few other resources that we think might be useful in the future.</p><p><strong>Text Generation Techniques</strong></p><p><a href="https://pdfs.semanticscholar.org/797d/7d968b88d5b5dd7c3271d08acd7296950d41.pdf?_ga=2.73597202.1083977791.1523309085-1136887644.1523309085">Using Lexical Chains for Text Summarization (Barzilay et. al, 1997)<br /></a><a href="https://pdfs.semanticscholar.org/9dad/f5bb0a2182b1509c5ea60d434bb35d4701c1.pdf?_ga=2.15851958.1083977791.1523309085-1136887644.1523309085">Generating Topical Poetry (Ghazvininejad et. al, 2016)</a></p><p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of RNNs</a></p><p><a href="https://arxiv.org/pdf/1707.05501.pdf">Story Generation from Sequence of Independent Short Descriptions (Jain et al., 2017)</a></p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=43368563cf97" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/machine-dictionary-43368563cf97">Machine Dictionary</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/machine-dictionary-43368563cf97?source=rss----9ba3897b6688---4">by Karishma Mandyam at April 11, 2018 06:03 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 06, 2018</h2>

<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=12">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/04/06/warm-up-testing-a-codebase/">Warm Up: Testing a Codebase</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>I installed both the Tensorflow and Pytorch API’s since I’m not sure which framework I will use yet. Then, I downloaded the following repository which implemented Hybrid Code Networks for Dialog State tracking in its respective research paper.</p>
<p>Code base URL: <a href="https://github.com/voicy-ai/DialogStateTracking" rel="nofollow">https://github.com/voicy-ai/DialogStateTracking</a></p>
<p>Research Paper URL: <a href="https://www.semanticscholar.org/paper/Hybrid-Code-Networks%3A-practical-and-efficient-with-Williams-Asadi/0645905d70caf180433145be09c9af266a85c863" rel="nofollow">https://www.semanticscholar.org/paper/Hybrid-Code-Networks%3A-practical-and-efficient-with-Williams-Asadi/0645905d70caf180433145be09c9af266a85c863</a></p>
<p>Their implementation uses Keras (built on Tensorflow) to build the network. The model stores a predetermined set of action templates to execute based on what the user requests. By feeding in features like the previous action taken, a bag of words vector, an entity tracking feature vector, etc. their RNN outputs a softmax distribution over the possible action templates. The action taken is the one with the highest probability. Because the conversation is restricted to a particular domain such as searching for a restaurant, the model performed well when I ran and tested it. The model generally recognized the type of request I was making, but its responses were extremely robotic and towards the end, gave me yes or no questions to answer to narrow down what action it should take. My goal is to generalize the type of information the model can store between time steps to be able to provide responses for requests outside of a restricted domain (like searching for a restaurant in this case).</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/04/06/warm-up-testing-a-codebase/">by ananthgo at April 06, 2018 06:58 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" title="Stories by Halden Lin on Medium">Halden Lin <br/> Team undef.</a></h3>


<div class="entrygroup" id="https://medium.com/p/96fb908765f5">
<h4><a href="https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5?source=rss-2759d54493c0------2">NLP Capstone | 02: Getting Started</a></h4>
<div class="entry">
<div class="content">
<p><a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5">previous post</a></p><p>Alright, it’s been only 2 days since my last entry, so this will be a relatively short post. The direction I proposed in <strong>Option 1 </strong>of that post was towards a more robust, interpretable, and informative visualization of attention, particularly in the context of text summarization. A quick recap:</p><blockquote>Perhaps interaction can be used to create a more insightful and interpretable visualization framework for understanding attention. For example, text heat-maps are already used widely to visualize sentiment analysis.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*lsgeuBXGGBog4YkuQNgJVw.png" />Lin et al. (2017) [6]. Visualization of sentiment analysis on a token-by-token basis.</figure><blockquote>In a static context, using this method for attention would require repeat of the same input sequence for each word in the output sequence. Using interaction, however, a model creator could brush over single or sequences of words in the output sequence to view corresponding soft-alignment in the input sequence. Aggregate visualizations could be shown to supplement this view (either aggregates over a particular input / output sequence, or aggregates over all input / output sequences).</blockquote><p>I’m currently working on laying out the groundwork for such a project. Task 1: implement a model. Without one, there’s no data to visualize!</p><p>With that in mind, here’s what I’ve been up to:</p><h4>Finding a Text Summarization Dataset</h4><p>A quick survey of recent research papers [1–5] on text summarization points, as well as online forums, points to three commonly used datasets.</p><ol><li><a href="https://cs.nyu.edu/~kcho/DMQA/">CNN/Daily Mail Corpus</a>. A collection of articles and their bullet point summaries, with each bullet split for Q/A purposes. <a href="https://github.com/abisee/cnn-dailymail">A script</a> [1] can be ran over the original dataset to restore the original bullet point summaries, to be used as a summarization corpus.</li><li><a href="https://www-nlpir.nist.gov/projects/duc/data.html">DUC Corpus</a>. In particular, DUC 2003 and DUC 2004. These contain a collection of documents, each accompanied by a short (~10 word) summary. There is also a longer summary for each cluster of documents.</li><li><a href="https://catalog.ldc.upenn.edu/ldc2003t05">Gigaword Corpus</a>. An annotated collection of millions of documents. The summarization task here would be to predict the headline of each [5]</li></ol><p>The accessibility of the <strong>CNN/Daily Mail Corpus</strong> (a process is required for the other two), in addition to the prevalence of projects that used it as a primary dataset [1, 2, 4], made it the most attractive option. The relatively longer summaries (~4 bullet points as opposed a short blurb in the other two datasets) also lends itself conveniently to the case of an interactive visualization with multi-token selection (e.g. select a whole bullet point and see where it attended). For a baseline, this will be my dataset!</p><h4>Identifying a Baseline Model</h4><p>See et al. (2017) [1] lay out a seq2seq attentional model as their baseline (a bidirectional LSTM). I’ll be using this as a baseline model with which to obtain data.</p><h4>Getting Some Code Up</h4><p>I’ll be using <a href="http://pytorch.org/">PyTorch</a> and the <a href="http://allennlp.org/">AllenNLP</a> toolkit [7] to implement my NN models. These are both ready to go on both my machine and Azure. I’m currently in the process of writing a DatasetReader for the dataset described above.</p><h3>Next Steps</h3><ul><li>Finish writing the DatasetReader for the CNN/Daily Mail Corpus.</li><li>Begin work on a baseline seq2seq attentional model, as described in <strong>Identifying a Baseline Model</strong></li></ul><h4>Works Cited</h4><p>[1] <a href="https://arxiv.org/pdf/1704.04368.pdf">See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” <em>ACL</em> (2017).</a></p><p>[2] <a href="https://arxiv.org/pdf/1712.06100.pdf">Hasselqvist, Johan et al. “Query-Based Abstractive Summarization Using Neural Networks.” <em>CoRR</em> abs/1712.06100 (2017): n. pag.</a></p><p>[3] <a href="https://arxiv.org/pdf/1602.06023.pdf">Nallapati, Ramesh et al. “Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.” <em>CoNLL</em> (2016).</a></p><p>[4] <a href="https://arxiv.org/pdf/1705.04304.pdf">Paulus, Romain et al. “A Deep Reinforced Model for Abstractive Summarization.” <em>CoRR</em> abs/1705.04304 (2017): n. pag.</a></p><p>[5] <a href="https://arxiv.org/pdf/1509.00685.pdf">Rush, Alexander M. et al. “A Neural Attention Model for Abstractive Sentence Summarization.” <em>EMNLP</em> (2015).</a></p><p>[6] <a href="https://arxiv.org/pdf/1703.03130.pdf">Lin, Zhouhan, <em>et al.</em>, “A structured self-attentive sentence embedding.”<em>arXiv preprint arXiv:1703.03130</em> (2017).</a></p><p>[7] <a href="https://pdfs.semanticscholar.org/a550/2187140cdd98d76ae711973dbcdaf1fef46d.pdf?_ga=2.150901366.1370831839.1522970228-1363309632.1522194596">Gardner, Matt et al. “AllenNLP: A Deep Semantic Natural Language Processing Platform.” (2017).</a></p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=96fb908765f5" width="1" /></div>







<p class="date">
<a href="https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5?source=rss-2759d54493c0------2">by Halden Lin at April 06, 2018 06:53 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a></h3>


<div class="entrygroup" id="https://medium.com/p/278789e4d04a">
<h4><a href="https://medium.com/@viterbi.or.not/warming-up-278789e4d04a?source=rss-c522ef075bb3------2">Warming Up</a></h4>
<div class="entry">
<div class="content">
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qc9b3NkzkWe1kNnqmlHukA.png" /></figure><p>In order to begin implementing our baseline model, for which we intend to duplicate the results presented in another research paper covering discussion summarization, we began by identifying software that was referenced by other papers as being useful for their implementations.</p><p>Using these leads, we decided to “warm up” by installing the software and gaining some familiarity with it. The main libraries that we identified are listed here:</p><h4><a href="https://www.nltk.org/"><strong>Natural Language Toolkit (nltk)</strong></a></h4><p>Perhaps it is no surprise that this resource ended up first on our list, but it was a clear choice to familiarize ourselves with because of the sheer variety of tools it provides. While performing an initial survey of conversation summarization papers, we discovered a reference to the TextTiling algorithm, described in <a href="http://www.aclweb.org/anthology/J97-1003">this paper</a> and referenced as a technique used in <a href="http://www.aclweb.org/anthology/D08-1081">another paper</a> about summarization. Broadly, the algorithm detects boundaries between topics in text, so it was used by this summarization paper as part of a pipeline before assigning scores to those topics representing their importance. For our baseline model, one possibility is to implement a similar pipeline, so having access to an implementation of the TextTiling algorithm would allow us to quickly implement that component and spend more time on other design decisions and implementation details. The nltk library provides a <a href="https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.texttiling">TextTiling module</a> with this functionality.</p><h4><a href="http://scikit-learn.org/stable/"><strong>SciKit-Learn</strong></a></h4><p><a href="http://www.aclweb.org/anthology/P05-1037">Another research paper</a> we found concerning the topic of conversation summarization had several sections dedicated to the task of identifying portions of a chatlog with direct relevance to one another — for example, a question asked by one contributor and answered by another contributor several messages down would be considered a pair of directly relevant sections. As part of their technique for identifying these pairs, the researchers used <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">Support Vector Machines</a> and <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">Maximum Entropy</a> models in order to determine the sections that most likely directly respond to previous portions of the conversation. SciKit-Learn provides these functionalities, and in order to familiarize ourselves with additional existing tools that might be useful in building a baseline model we have installed this tool and begun experimenting with it.</p><h4><a href="https://www.cs.waikato.ac.nz/ml/weka/"><strong>Weka Toolkit</strong></a></h4><p>Another less commonly seen method for chat summarization can be found in a more recent (in comparison to the others we have referenced) <a href="http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf">research paper</a> which explores the usage of Multilayer Perceptrons (MLP) for the task, among several other approaches. The MLP approach in the paper is broadly composed of a feedforward neural network with more layers between the input and output layers using backpropagation to train the network and built with the Weka toolkit, a collection of machine learning algorithms that can applied to a dataset and which contains tools for developing a variety of schemes for processing data. Although the paper finds an approach using Naive Bayes to be the most effective on the GNUe archives, their MLP implementation comes in relatively close second and we think the idea is worth pursuing further. For a baseline model, we could start by working on a similar MLP system to the one in the paper using the same Weka toolkit implementation and strive to improve from there.</p><h4><a href="http://pytorch.org/"><strong>PyTorch</strong></a></h4><p>Though not referenced by any of the papers we have encountered so far, PyTorch would be a really helpful tool for us to further the explore the usage of neural network models in chat summarization, which we can use for the aforementioned MLP approach as well as apply to our stretch goal of working on a less commonly used deep learning based model.</p><p>In addition to researching libraries and tools that we might use in our project, we have begun the process of finding and enumerating datasets that might be useful for our project ideas — more to come on the datasets and other resources in our next blog post!</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=278789e4d04a" width="1" /></div>







<p class="date">
<a href="https://medium.com/@viterbi.or.not/warming-up-278789e4d04a?source=rss-c522ef075bb3------2">by Viterbi Or Not To Be at April 06, 2018 06:41 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=304">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/06/milestone-2-music-as-a-natural-language-task/">Milestone #2: Music as a Natural Language Task</a></h4>
<div class="entry">
<div class="content" lang="en">
<h3>Framing the problem</h3>
<p>The focus of Natural Language Processing relies on patterns in the structure of language and models that find ways to encode the complexities of these structures. Many forms of music also have large amounts of structure which could potentially be discovered using similar models as a standard natural language.</p>
<p>Music datasets for machine learning purposes have recently become available through projects like MusicNet in 2016 [1]. This music is primarily classical, and provided as both audio and MIDI.</p>
<h3>Project ideas</h3>
<p>For our project we are interested in music with lyrical content – both for the potential to create a creative demo and for the interest of making this a language task. The current direction we are most interested in is the generation of lyrics for a song, given its nonlyrical content. This will be broken up into subtasks depending on the feasible scale of the project. Not all of the following points will necessarily be parts of our project, but we will use them as as starting point as we see the success of our models.</p>
<ul>
<li>Creating a machine learning model for MIDI music</li>
<li>Translating MIDI into specific artists or styles</li>
<li>Creating models for the lyrical content of specific artists or styles of music</li>
<li>Generating lyrics given an artist or style</li>
<li>Seq2seq conversion of MIDI into lyrical content</li>
<li>GANs for either side of the conversion – MIDI encoding or lyrical generating</li>
</ul>
<h3>Using MIDIs in RNNs</h3>
<p>Work by Pakhomov [2] has already used RNNs to create models for lyrics. In his <a href="http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/">blog post</a> he additionally discusses a method for forming any MIDI into piano roll format. This is essentially a matrix where each column represents a different time step, and each row represents a different note. Having a 1 corresponds to that note sounding at that time. The individual time vectors can be used as the inputs to an RNN at each time step to create a model representing the various songs.</p>
<p>One possible data source for our project is karaoke data available from various sources online. If available in large enough quantities this could be extremely convenient because it already contains many pairings of MIDI music to their lyrics.</p>
<h3>Azure</h3>
<p>We intend to use PyTorch to train our models, and have begun setting up an instance on Microsoft Azure.</p>
<h3>Relevant work</h3>
<p>[1] <a href="https://homes.cs.washington.edu/~thickstn/musicnet.html" rel="nofollow">https://homes.cs.washington.edu/~thickstn/musicnet.html</a></p>
<p>[2] <a href="http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/" rel="nofollow">http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/</a></p>
<p>[3] Dong, Hao-Wen. 2017. MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment. <a href="https://arxiv.org/pdf/1709.06298" rel="nofollow">https://arxiv.org/pdf/1709.06298</a></p>
<p>[4] Yu, Lantao. 2016. SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. <a href="https://arxiv.org/abs/1609.05473" rel="nofollow">https://arxiv.org/abs/1609.05473</a></p>
<p>[5] Lee, Sang-gil. 2017. A SeqGAN for Polyphonic Music Generation. <a href="https://arxiv.org/abs/1710.11418" rel="nofollow">https://arxiv.org/abs/1710.11418</a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/06/milestone-2-music-as-a-natural-language-task/">by Nicholas Ruhland at April 06, 2018 06:30 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/3a2f40b355a5">
<h4><a href="https://medium.com/nlp-capstone-blog/getting-started-for-the-capstone-software-installation-pipeline-brainstorming-3a2f40b355a5?source=rss----9ba3897b6688---4">Getting Started for the Capstone: Software Installation &amp; Pipeline Brainstorming</a></h4>
<div class="entry">
<div class="content">
<p>Currently, our top two choices for the capstone is</p><ol><li><strong>Machine Dictionary: </strong>learning definitions of technical terms whose semantics are averaged over all places it is mentioned in training (in this case, research publications in the given field of study)</li><li><strong>Visual Reasoning: </strong>Given three windows, each of which containing a random arrangement of colored, geometric shapes, and a statement about the image, predict whether the statement is true or false.</li></ol><p>Despite being problems with very different needs and challenges, the bulk of the tools and frameworks we’ll be using overlap for both tasks. Here, we discuss those tools and frameworks, followed by things we need specific to <strong>Machine Dictionary </strong>and <strong>Visual Reasoning </strong>separately.</p><h3>Resources Used for Both Tasks</h3><h4>PyTorch</h4><p>Given the limited time that we have, a neural-based approach using an established framework is preferred over implementing all of the model architecture from scratch, and to help avoid complications that can accompany other methods such as deriving parameter updates for bayesian models. PyTorch is an excellent framework that abstracts away differentiation and tensor arithmetic while still allowing a healthy amount of flexibility with it’s ability to dynamically produce computation graphs.</p><h4>AllenNLP</h4><p>After the crash course on the framework provided by AI2 in class, along with our experience from using it in the undergraduate NLP class, we’re convinced that the integration of AllenNLP with PyTorch is the best way to be as productive as possible. We plan to use the libraries it provides to make training more streamline and organized.</p><p>Since we’ve taken the undergraduate NLP class, we’ve already installed PyTorch and AllenNLP. We installed PyTorch through conda and AllenNLP through pip. Deep Learning projects also tend to involve complicated models which might require more computing resources, so we will also utilize the Azure credits available through the capstone. This process involved installing PyTorch and AllenNLP on Ubuntu VMs on Azure configured into include NVIDIA GPUs so that we can take advantage of PyTorch’s .</p><h3>Resources Specific to Machine Dictionary</h3><h4>Semantic Scholar Open Research Corpus</h4><p>The best dataset we’ve seen so far for this task is the <a href="http://labs.semanticscholar.org/corpus/">Semantic Scholar Open Research Corpus</a> provided by <a href="http://allenai.org/">AI2</a>. The dataset specifically consists of JSON files with metadata for each publication. The most relevant files will be <em>title, pdfUrls, </em>and <em>year</em>. Given all of the content besides the paper abstract (which is included as a field called <em>paperAbstract</em>), we resort to using the <em>pdfUrls</em> and extracting the text from each.</p><h4>Textract</h4><p><a href="https://github.com/deanmalmgren/textract">Textract</a> is a Python package that allows the extraction of text from PDFs. We plan to rely on this package given that it’s robust to both compiled and scanned PDFs.</p><p>We downloaded Textract to the Azure Linux VM using the Ubuntu installation directions:</p><pre>apt-get install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr \<br />flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig<br />pip install textract</pre><p>but ran into an issue that the developers haven’t dealt with yet. In the first line, they are missing a dependency to the libpulse-dev package, which causes the build to fail when downloading Textract. Doing a pip install libpulse-dev takes care of it.</p><p>Grabbing the text from a PDF file is then fairly convenient. We tested this on a <a href="https://www.semanticscholar.org/paper/Effects-of-anthocyanins-on-the-prevention-and-of-Lin-Gong/1bcf9ae84d4ec5c0aba7918e6784dbfd0e8514b6">cancer research paper</a> taken from the Semantic Scholar dataset:</p><pre><strong>import</strong> textract<br />text = textract.process("path-to-doc.pdf", encoding="ascii")</pre><p>which produced an excellent parse of the PDF.</p><blockquote>b’BJP\n\nBritish Journal of\nPharmacology\n\nBritish Journal of Pharmacology (2016) \n\n1\n\nREVIEW ARTICLE THEMED ISSUE\nEffects of anthocyanins on the prevention and\ntreatment of cancer\nCorrespondence Ying-Yu Cui, Department of Regenerative Medicine, Tongji University School of Medicine, Shanghai 200092,\nChina. E-mail: yycui@tongji.edu.cn\n\nReceived 13 June 2016; Revised 17 August 2016; Accepted 13 September 2016\n\nBo-Wen Lin1, Cheng-Chen Gong1, Hai-Fei Song1 and Ying-Yu Cui1,2,3\n1\n\nDepartment of Regenerative Medicine, Tongji University School of Medicine, Shanghai, China, 2Key Laboratory of Arrhythmias, Ministry of\n\nEducation (Tongji University), Shanghai, China, and 3Institute of Medical Genetics, Tongji University School of Medicine, Shanghai, China\n\nAnthocyanins are a class of water-soluble avonoids, which show a range of pharmacological effects, such as prevention of\ncardiovascular disease, obesity control and antitumour activity.</blockquote><h3>Resources Specific to Visual Reasoning</h3><h4>Cornell NLVR Dataset</h4><p>The dataset for the Visual Reasoning task is easily available through <a href="https://github.com/clic-lab/nlvr">github</a>. This dataset was also very conveniently organized into three folders: testing data, development data, and training data. Each folder contains the actual images which are provided for us to train on.</p><p>In addition to the images, each folder also includes a JSON file which contains basic JSON representations of each image and the sentence that needs to be validated in each data point. This JSON representation is very useful because we do not need to parse the image to retrieve the raw elements (shape, color, location).</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3a2f40b355a5" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/getting-started-for-the-capstone-software-installation-pipeline-brainstorming-3a2f40b355a5">Getting Started for the Capstone: Software Installation &amp; Pipeline Brainstorming</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/getting-started-for-the-capstone-software-installation-pipeline-brainstorming-3a2f40b355a5?source=rss----9ba3897b6688---4">by Tam Dang at April 06, 2018 06:06 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2">
<h4><a href="http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2">Technical Details (Blog Post #2)</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph"><span style="color: rgb(0, 0, 0);">For my project I am planning to do some deep learning at the end if I have time and if the results up to that point lead to that track. (I have pytorch installed from NLP so that's nice to have). <br /><br />With that said, I've been working with the Reddit API's and Reddit datadumps to get started on gathering the necessary data for pursuing the Language Accommodation project. I've been trying to figure out if the best approach is to work with the limited requests, the direct json files, or if some of the data dumps will suffice. I hope to have most of that and some basic data visualizations ready in the next couple of days to inform some of the choices I should make regarding the data (i.e. what time period to gather data from, what subreddits to pull from, etc.)</span><br /></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2">April 06, 2018 12:10 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 04, 2018</h2>

<div class="channelgroup">







<h3><a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a></h3>


<div class="entrygroup" id="https://medium.com/p/9a0f5382cff5">
<h4><a href="https://medium.com/@viterbi.or.not/preliminary-ideas-9a0f5382cff5?source=rss-c522ef075bb3------2">Project Ideas</a></h4>
<div class="entry">
<div class="content">
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5yHodb3mhK0Ec-CEkw4o-A.png" /></figure><p>Welcome to Team Viterbi Or Not To Be’s first blog post, where we will be discussing our top three project ideas and linking them to the preliminary research we’ve done. As a start for our to-be-determined project, we’ve created a <a href="https://github.com/viterbi-or-not-to-be/viterbi-or-not-to-be">git repository</a> where you can access our code and see our progress!</p><p>While considering various ideas to explore, our team ultimately decided to approach the project in “research mode”, with a greater focus on novel models and analysis of their performance. Although it would be exciting to build a complete platform incorporating Natural Language Processing techniques, we decided it would be more educational overall to examine the workings of the models themselves. With that overarching direction in mind, we began to think about the issues in the field that would be most interesting to work with, and settled on a few ideas:</p><h4><strong>Automatic Conversation Summarization</strong></h4><p>One idea is to explore various approaches for the automatic summarization of written conversations. As a topic, the usefulness of written conversation summaries is undeniable — as so many methods of communication are being powered through online e-mail or chat interfaces, there is value in being able to extract relevant topics or action items from conversations just as there is for documents or news articles. As a task, however, there are plenty of challenges to overcome to determine what properties of a topic in a discussion influence its importance to the overall summary and how a summary can be generated based off that data.</p><p>In our preliminary research, we found that relatively few efforts to automatically summarize natural language text have focused on conversational input specifically, but those that have show promise. One paper in particular examines the <a href="https://pdfs.semanticscholar.org/efe0/fffe080ac4b1a943f62cc56f2baa27c6e195.pdf">summarization of both spoken and written conversations</a>, and notes that there are substantial differences in the two types of data. Although the results of the summarization efforts presented in this paper are well below the baseline established by human summarizers, we think it would be interesting to implement a comparable system and explore modifications that could be made or alternative models that could be used to get a more complete picture of the possibilities. By reading the research papers of related projects, we have identified a number of datasets that would allow us to train our model, including <a href="http://groups.inf.ed.ac.uk/ami/corpus/">meeting summaries</a>, <a href="https://www.cs.cmu.edu/~./enron/">summaries of email threads</a>, and <a href="https://flossmole.org/content/software-archaeology-gnue-irc-data-summaries">summaries of chat logs</a>.</p><p>If we were to pursue this option, we would initially focus on replicating the approaches used by previous research projects to automatically summarize conversations as a baseline. Once that is working, we would move toward creating a minimum viable product by examining the features and models used in past approaches and performing an analysis of possible alternatives. Although it may not be within the scope of this quarter-long project to apply an entirely novel model to the problem, at a minimum we would seek to determine the relationship between different features and the various types of conversation available through experimentation.</p><p>Beyond the minimum viable product, there are several stretch goals we would like to tackle. One of the more seemingly impactful would be using model parameters and features extracted from one type of data to improve the model’s performance on another, such as using the result of training a model on spoken meeting data to improve the automatic summarization of emails. In the previously linked research paper, the authors mention that a future goal for their research is to implement such a system, and they assert that preliminary results are promising. Another stretch goal would be attempting to beat previous approaches by focusing on one specific domain and using the unique properties of that data to produce better summaries. An example could be to focus on chat logs from the GNU dataset that deal specifically with bugfixes — by restricting the domain to a set of code-related topics that likely share a much smaller vocabulary and a consistent notion of “importance”, such as action items during the lifecycle of a bugfix, it may be possible to produce better summaries than in the case of general summarization. Finally, we are interested in comparing the results of extractive and abstractive summarization — while the former works by identifying the most important sentences in a text and combining them, the latter attempts to make a more “human” summary by identifying topics in a text and generating new sentences that paraphrase the intent. One extension could therefore be to try extending extractive models proposed previously to be abstractive.</p><p>While our primary interest would be in conversation summarization, we would also consider doing a similar summarization project based on research papers if the conversation data proved to be insufficient. To do so, we would identify corpuses of research papers, and use the author-written abstracts as the summaries for our training data.</p><h4><strong>Multiple Premise Entailment</strong></h4><p>Another idea is to pursue a project that tackles the problem of Multiple Premise Entailment, which involves being able to make inferences based on multiple premises, as shown in the example below. This would contribute to the making of more “knowledgeable” models that aim to use and understand contexts across multiple ideas, a more challenging problem than making inferences from a single sentence as is done in standard entailment tasks.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7M-V9YHW9UtascpkTEK5Nw.png" />Example of a Multiple Premise Entailment, Lai et al. 2017</figure><p>If we were to take on this challenge, we would start by looking at <a href="http://aclweb.org/anthology/I17-1011">existing research</a> and begin our approach in a similar way, first using baseline neural models for standard entailment. Once this is running on the MPE dataset, we would do some study and error analysis of these runs, then aim to improve upon these models in a way that takes our findings on the baseline models into account and includes adaptations to tackle multiple premises to create a minimum viable product.</p><p>Beyond the minimum viable product, a stretch goal for this problem would be to present a unique model that possibly uses a novel approach from the baseline models to tackle multiple premises in a way that best suits this new challenge, relying less on models for standard entailment.</p><h4><strong>Natural Language Visual Reasoning</strong></h4><p>The last idea we are interested in involves the <a href="http://lic.nlp.cornell.edu/nlvr/">Cornell Natural Language Visual Reasoning dataset</a>, which contains 92.244 pairs of natural language statements grounded in synthetic images like the one shown below. The challenge that involves language is to determine whether a statement about the image is true or false. Doing so would typically involve reasoning based on spatial relations, quantities, and other qualities about sets of objects that might appear in the NLVR dataset images.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Y4-XNexoEp2I5lGKm9MTmg.png" />Examples of image-statement pairs from the NLVR dataset, Suhr et al. 2017</figure><p>If we were to work on this problem, we would focus first on reimplementation of the <a href="https://arxiv.org/pdf/1511.02799.pdf">state of the art model</a> from UC Berkeley. As noted in the “Future Work” section of the UC Berkeley paper, their model maintains a strict separation between predicting network structures and learning network parameters. As a stretch goal, we could work on integrating the current approach with existing tools for learning semantic parsers to achieve an integration between the two components that would possibly improve performance or make way for a novel approach to the problem.</p><p>Edited 4/5 to fix formatting, revise introduction (plus dog mascot!), and add example images.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9a0f5382cff5" width="1" /></div>







<p class="date">
<a href="https://medium.com/@viterbi.or.not/preliminary-ideas-9a0f5382cff5?source=rss-c522ef075bb3------2">by Viterbi Or Not To Be at April 04, 2018 07:03 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=277">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/04/nlp-capstone-post-1-ideation/">NLP Capstone Post #1: Ideation</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>In this post, I’d like to briefly discuss three different ideas I have for my capstone project.</p>
<p>UPDATE (04/05/2018): I am fortunate to be joined by a fellow student, Nicholas Ruhland, for this capstone project.</p>
<h1>A Theoretical Analysis of RNNs (Research Mode):</h1>
<p> A recent <a href="https://arxiv.org/abs/1703.00810">paper of Professor Naftali Tishby</a> provided some useful observations on the behavior of feedforward neural networks, and proposed a promising approach to understanding their performance. Earlier empirical work done in the vision community showed that when a convolutional neural network is trained, layers closer to the input learn lower level features (such as edges and corners) and layers closer to the output learn higher level features (“this part of the image resembles a nose, and this other part resembles an eye”). One might expect similar behavior to occur with general feedforward neural networks: that earlier layers learn lower level features of the input and later levels learn higher level features of the input. The key insight here was to think of each layer of a neural network as a Markov chain, where each layer <img alt="L_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{i}" /> is a (vector-valued) random variable that is conditionally independent of <img alt="L_{j}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{j}" /> for all <img alt="j &lt; i - 1" class="latex" src="https://s0.wp.com/latex.php?latex=j+%3C+i+-+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="j &lt; i - 1" /> given <img alt="L_{i-1}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bi-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{i-1}" />. In this way, information flowing forward in the network can be quantified via notions of entropy from traditional information theory.</p>
<p>The paper contains some empirical work, observing that there are generally two phases to learning artificial neural networks via stochastic gradient descent: the fitting phase, and the compression phase. The fitting phase is the shorter phase, where the model is quickly tuning itself to minimize the empirical loss function. At the end of this phase, we don't necessarily have a model that will generalize to new data. The compression phase is where the model begins to learn the relevant features in the input, with the intuition that there are many irrelevant parts of the input (I don't need to know every atom in an object to identify it). </p>
<p>The goal of this project would be to perform a similar theoretical analysis and empirical work for RNN architectures (whose "natural" Markov chain isn't as simple, as there are cycles) on some traditional NLP task, such as Machine Translation, with the goal of studying the flow of information in an RNN architecture, rather than performing comparably to state-of-the-art Machine Translation models (although this can be a stretch goal).</p>
<p>The relevant steps in this project will likely look like the following:<br />
1. Reading up on the relevant work by Tishby et. al. (and any other theoretical papers on deep learning).<br />
2. Understand basic and traditional RNN architectures.<br />
3. Learning PyTorch.<br />
4. Implementing several of these architectures and testing (for example, to see if learning also comes in two distinct phases: fitting and compression)<br />
5. Using these empirical observations, and information theory to analyze these architectures.<br />
6. Time permitted, play around with new RNN architectures.</p>
<h1>Musical Style Learning from Musical Scores (Research/Start-Up Mode):</h1>
<p> This idea lies somewhat outside traditional NLP in that it tackles the language of music. While the alphabet of a musical score consist chiefly of the 12 musical notes, there is added challenge in that several notes may be played simultaneously, especially if there are several instruments involved or simply the two hands of a pianist. Furthermore, the exact timing of each note played matters, note merely the ordering of the notes.</p>
<p>The idea here is simply to, given the score of a musical piece, represented as a sequence of notes at each time, predict the era (Baroque, Classical, Romantic, etc.) or even, the composer of the piece (Bach, Beethoven, Brahms, etc.) There are several problems to be solved step by step for this project.</p>
<p>1. Data collection from a large library of musical scores (ex: <a href="http://imslp.org/">IMSLP</a>)<br />
2. Data formatting so as to be usable.<br />
3. Model selection.<br />
4. Model implementation (PyTorch).<br />
5. Model testing.</p>
<p>There are also several extensions that can be viewed as stretch goals. For these, the first two can be reused.</p>
<h3>Musical Score Generation:</h3>
<p> Now, we learn how to compose a piece that “sounds” similar to a given composer. This will involve learning from the pieces written by a given input composer, and outputting a new piece. One core challenge here is ensuring that the output is syntactically correct.</p>
<h1>Story Illustration (Start-Up Mode):</h1>
<p> Given a short story and a specific scene (or place in the text), produce an image that is representative of the scene. This project combines aspects of NLP and vision. This project may also explore generative adversarial methods. One well-known challenge here is convergence.</p>
<p>Here are the general steps for this project:<br />
1. Data collection (image captioning dataset can be helpful)<br />
2. Model selection.<br />
3. Model implementation (PyTorch).<br />
4. Model testing.</p>
<p>As an extension, one can also generate several frames to form a short “movie”. Another can be comic book pane generation.</p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/04/nlp-capstone-post-1-ideation/">by Kuikui Liu at April 04, 2018 06:53 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" title="Stories by Halden Lin on Medium">Halden Lin <br/> Team undef.</a></h3>


<div class="entrygroup" id="https://medium.com/p/ee873b6885d5">
<h4><a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5?source=rss-2759d54493c0------2">NLP Capstone | 01: Options</a></h4>
<div class="entry">
<div class="content">
<p>Hello! This post is the first in a series that will document my progression through CSE 481n, taught by Prof. Yejin Choi at the University of Washington.</p><p>My github for this project can be found at: <a href="https://github.com/haldenl/nlpcapstone">https://github.com/haldenl/nlpcapstone</a></p><h3>What I hope to explore</h3><p>Over the course of the next 10 weeks, I intend to explore the intersection of V<strong>isualization (Vis) and Natural Language Processing (NLP)</strong>. I am particularly excited to explore the avenues through which Vis can be used to augment the interpretability of Neural Networks (NNs). I’ve spent the past year working around Vis (through classes and research) and am excited to bring what I’ve learned to problems in NLP. I intend to take a <strong>research oriented approach</strong> to this project.</p><h3>Relevant Work (a brief and incomplete list)</h3><ul><li><a href="https://arxiv.org/pdf/1506.02078.pdf">Visualizing and Understanding Recurrent Neural Networks, Karpathy et al. (2015)</a></li><li><a href="http://www.aclweb.org/anthology/N16-1082">Visualizing and Understanding Neural Networks in NLP, Li et al. (2016)</a></li><li><a href="https://distill.pub/2018/building-blocks/">The Building Blocks of Interpretability, Olah et al. (2017)</a></li><li><a href="https://arxiv.org/pdf/1612.08220.pdf">Understanding Neural Networks through Representation Erasure, Li et al. (2017)</a></li><li><a href="https://ufal.mff.cuni.cz/pbml/109/art-rikters-fishel-bojar.pdf">Visualizing Neural Machine Translation Attention and Confidence, Rikters et al. (2017)</a></li></ul><h3>Directions for Exploration</h3><h4>Option 1: Attention</h4><p>Visualizing and understanding <strong>attention</strong>, with a focus on its role in text summarization.</p><p>This is my most realized idea at this time, so I will spend more time here explaining my thoughts.</p><p><strong>Background:</strong></p><p>Visualizations of attention are often used in an effort to understand, and subsequently improve, decisions made by neural networks. To my knowledge, the most common way of visualization attention in seq2seq models is via a 2-dimensional heat-map, wherein the attention each decoding unit gives to each input token can be seen.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/612/1*KvfXxucogv8flZHndSJvHg.png" />Rikters et al. (2017). An attention visualization for a seq2seq problem (in this case, translation). Whiter cells represent higher attention.</figure><p>There are a few issues with this format: (1) it is difficult to fit the words (as seen above) on the x-axis, harming readability; (2) this does not scale well with large input or output (e.g. summarization); and (3) we do not read single-tokens at a time (i.e. y-axis), and input and output are generally not in this format either.</p><p>As a whole, this format, while simple, is lacking in interpretability. The cognitive work-load of a viewer is less than optimal.</p><p>Alternatives can be found in literature, such as the following from Rikters et al. (2017).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jtRrxf5pIB-OCBsoqUjgJQ.png" />Rikters et al. (2017). The input sequence is seen on top — output on bottom. Thicker lines denote higher attention.</figure><p>However, this also suffers from similar interpretability and scalability issues. Moreover, the thickness of lines as an encoding scheme does not lend itself easily to comparison between words.</p><p><strong>Proposed Exploration:</strong></p><p>Perhaps interaction can be used to create a more insightful and interpretable visualization framework for understanding attention. For example, text heat-maps are already used widely to visualize sentiment analysis.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*lsgeuBXGGBog4YkuQNgJVw.png" />Lin et al. (2017). Visualization of sentiment analysis on a token-by-token basis.</figure><p>In a static context, using this method for attention would require repeat of the same input sequence for each word in the output sequence. Using interaction, however, a model creator could brush over single or sequences of words in the output sequence to view corresponding soft-alignment in the input sequence. Aggregate visualizations could be shown to supplement this view (either aggregates over a particular input / output sequence, or aggregates over all input / output sequences).</p><p><strong>Baseline: </strong>A user-interface for an interactive visualization of attention for seq2seq models (e.g. text summarization models). Exploration of how this visualization method can be used to either improve an existing models or understand the differences between models. This would, of course, require building a model from which to obtain data.</p><p><strong>Reach: </strong>Integration into an existing platform, e.g. in PyTorch to be viewable in TensorBoard.</p><h4>Option 2: Neural Network Cells</h4><p>Visualizing and understanding the role of cells in a neural network. In particular, the meaning of each cell within a model.</p><p><strong>Background:</strong></p><p>Though more exploration of relevant work will be required if this becomes the path of choice, Kaparthy et al. (2015) present work that serves as the basis for this idea. They show that the role of cells in a neural network can be interpreted by viewing their activations at each point in a given passage. For example, they found in their model a cell that fired at the end of a line (top left).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6yoF78PlE5ETNc9-kbya6w.png" />Kaparthy et al. (2015). Different cells fire in different contexts.</figure><p><strong>Proposed Exploration:</strong></p><p>Perhaps a deeper exploration of the activations of neural network cells could lead to insights for visualizations that may aid the interpretability of neural networks.</p><p><strong>Baseline: </strong>Create a program or UI that generates a visualization (either interactive or static), of the activations of each cell in a neural network. Use this information to posit the meanings of each cell and build further visualizations as needed for exploration. This would require building a model from which to obtain observations.</p><p><strong>Reach:</strong> This is a little loose, as it is unclear how much exploration will be needed to produce results. Perhaps as in Option 1, integration of whatever tool created into an existing framework.</p><h4>Option 3: Gates and Value flow in RNN models.</h4><p>Visualizing and understanding the flow of values through gates in a Recurrent Neural Network (RNN). In particular, allowing for interpretability of gates and the ‘memory’ of cells.</p><p><strong>Background:</strong></p><p>My motivation for this idea is largely anecdotal. I recall while taking the undergraduate NLP course that the concepts of ‘gates’ and ‘memory’ in RNN models (e.g. GRU, LSTM) were difficult for me to wrap my head around. We have these giant networks with variable length ‘memory’ determined by gates that preform well as a result of this mechanism, but how exactly is this mechanism is used, and on what tokens at what points? Kaparthy et al. (2015) present a way of visualizing the ‘memory’ of cells in an LSTM model via plotting of cells and the time spent right or left saturated (right saturated cells remember values for longer periods of time).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/975/1*Qq9KrefGEl7tM4mPxUi99g.png" />Kaparthy et al. (2015). Visualizing the cells and their time spent right or left saturated.</figure><p>Data-flow graphs have already been developed for neural networks (e.g. Wongsuphasawat et al. (2017)). An RNN specific model with a focus on interpretability of gates may be valuable for the NLP community.</p><p><strong>Proposed Exploration:</strong></p><p>Perhaps producing visualizations of a network of cells using information similar to above could lead to improvements in interpretability and aid model improvement.</p><p><strong>Baseline: </strong>Create a program or UI to generate a visualization, as described above, of an small RNN models. Use this generate insight into the model’s behavior (e.g. compare between models), and explore further from there.</p><p><strong>Reach: </strong>Support larger-scale networks. Integration into existing frameworks.</p><h3>Works Cited</h3><ul><li><a href="https://arxiv.org/pdf/1506.02078.pdf">Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. “Visualizing and understanding recurrent networks.” <em>arXiv preprint arXiv:1506.02078</em> (2015).</a></li><li><a href="https://arxiv.org/pdf/1506.01066.pdf">Li, Jiwei, <em>et al.</em> “Visualizing and understanding neural models in NLP.” <em>arXiv preprint arXiv:1506.01066</em> (2015).</a></li><li><a href="https://arxiv.org/pdf/1612.08220.pdf">Li, Jiwei, Will Monroe, and Dan Jurafsky. “Understanding neural networks through representation erasure.” <em>arXiv preprint arXiv:1612.08220</em> (2016).</a></li><li><a href="https://ufal.mff.cuni.cz/pbml/109/art-rikters-fishel-bojar.pdf">Rikters, Matīss, Mark Fishel, and Ondřej Bojar. “Visualizing neural machine translation attention and confidence.” <em>The Prague Bulletin of Mathematical Linguistics</em> 109.1 (2017): 39–50.</a></li><li><a href="http://idl.cs.washington.edu/files/2018-TensorFlowGraph-VAST.pdf">K. Wongsuphasawat <em>et al</em>., “Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow,” in <em>IEEE Transactions on Visualization and Computer Graphics</em>, vol. 24, no. 1, pp. 1–12, Jan. 2018.</a></li><li><a href="https://arxiv.org/pdf/1703.03130.pdf">Lin, Zhouhan, <em>et al.</em>, “A structured self-attentive sentence embedding.” <em>arXiv preprint arXiv:1703.03130</em> (2017).</a></li></ul><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ee873b6885d5" width="1" /></div>







<p class="date">
<a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5?source=rss-2759d54493c0------2">by Halden Lin at April 04, 2018 06:43 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a></h3>


<div class="entrygroup" id="http://deeplearningturingtest.wordpress.com/?p=3">
<h4 lang="en"><a href="https://deeplearningturingtest.wordpress.com/2018/04/04/the-journey-begins/">Top 3 Project Ideas I’m Excited For</a></h4>
<div class="entry">
<div class="content" lang="en">
<ol>
<li><strong>Visualizing a text-based description:</strong> Train a model to learn a language to image mapping with simple descriptions. The minimum plan would be to feed the model short text descriptions like “white hat” and “black cat” with their corresponding visual outputs. Then, if the text “white cat” is input at test time, the model should output the cat with the same shade of white as the hat. Stretch goals include visualizing multiple objects in the same picture and/or visualizing them in different spatial orientations with respect to each other (e.g. on top of, inside, underneath, next to, etc.).</li>
<li><strong>Generating Multimodal Word Embeddings:</strong> The goal is to create word embeddings that describe a word more holistically from multiple modalities like audio and visual inputs. One possible approach is to concatenate pre-trained word embeddings (e.g. GloVe vector) with additional features based on what context the word is in the present sentence. Then, concatenate this with features generated from a deep fully connected layer of a ConvNet where the input is an image of the actual word (e.g. car). Stretch goals include using these augmented embeddings to enhance performance in applications like sentiment analysis or further augmenting these embeddings with audio features of the word being pronounced (which can help distinguish different meanings of the word).</li>
<li><strong>Dialogue and Information State Tracking:</strong> The goal is to create a model that can either receive contextual information as text or probe the environment with questions and receive an answer as text. This text can either be input into a linear or tree LSTM for entity extraction, coreference resolution, parsing, and/or other algorithms which can extract valuable contextual information. Then, this text can be used to update the current dialogue and/or information state using deep reinforcement learning. The policy then uses the updated state to choose the next action and hopefully keep repeating this until the text is satisfactorily understood. Stretch goals include using this model to answer test questions about the reading material using a machine comprehension model like the ReasoNet architecture.</li>
</ol>
<p> </p>
<p>Git Repo URL: <a href="https://gitlab.cs.washington.edu/ananthgo/cse481n-capstone" rel="nofollow">https://gitlab.cs.washington.edu/ananthgo/cse481n-capstone</a></p>
<p>This project is in research mode with a heavy focus on making improvements in keeping track of the meaning behind lines of input text.</p></div>







<p class="date">
<a href="https://deeplearningturingtest.wordpress.com/2018/04/04/the-journey-begins/">by ananthgo at April 04, 2018 05:42 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a></h3>


<div class="entrygroup" id="https://medium.com/p/93d82eed396c">
<h4><a href="https://medium.com/nlp-capstone-blog/a-discussion-of-language-tasks-for-the-nlp-capstone-blog-post-1-93d82eed396c?source=rss----9ba3897b6688---4">A Discussion of Language Tasks for the NLP Capstone (Blog Post #1)</a></h4>
<div class="entry">
<div class="content">
<p>Welcome to Team Illimitatum! In this first blog post, we’ll discuss the top three/four ideas that emerged over several days of brainstorming. We started the week with seven exciting potential projects and carefully narrowed them down to these three ideas after taking into consideration feasibility, dataset availability, scope, and complexity for each project. Here they are!</p><h3>1. Machine Dictionary</h3><h4>Motivation &amp; Context</h4><p>The breadth and ubiquity of data and online material is both a gift and a curse. Availability of resources ultimately leads to better-informed decisions, but puts an enormous strain on a researcher to</p><ul><li>Search for sources while having to determine which ones are credible and which ones are not</li><li>Read passages and excerpts of these sources</li><li>Reach an understanding that is consistent with all of the sources they have seen</li></ul><p>On one hand, a concept or idea embedded in lengthy text (a textbook, article, or research paper, etc.), makes it so the reader has to achieve some basic understanding of the context surrounding the concept of interest as well as find all related details in order to learn the concept. On the other hand, basic web definitions and blogs <em>may</em> provide a more presentable and explicit definition of a particular concept, but may be lacking in the amount detail a researcher is looking for along with confidence in its credibility.</p><p>Summarization is a task in NLP that aims to reduce passages to a more condense form without sacrificing semantic meaning. The trouble then, of needing to make sense of lengthy text can be alleviated by solving this task.</p><p>Entity recognition is another task in NLP where the goal is to highlight “entities” or figures of significance in text that conventially refer to nouns people, places, and organizations. In a medical context, entities could include chemical names, technical terms pertaining to the anatomy of the human body, and diseases.</p><p>We aim to combine aspects of both summarization and entity recognition to solve the three “maladies” of research we’ve outlined above: to produce a model that produces the most probable definition for a given entity that would be consistent with as much of the data as possible.</p><p>With the intention of training such a model only on publications and research papers, we hope to pave the way for “specialized dictionaries”. Training the data on all publications from a particular field of study would simultaneously restrict the search space to only credible and relevant sources for any definition contained. Providing a generated definition would also prevent the need for the researcher to hunt for it in every source and make sense of it, themselves.</p><h4>Minimum Viable Product</h4><p>Steps toward the realization of this model involve finding a <strong>dataset</strong>, defining <strong>metrics of accuracy and performance</strong>, and determining a reasonable <strong>baseline</strong>.</p><p>Semantic Scholar’s <a href="http://labs.semanticscholar.org/corpus/">Open Research Corpus</a> provides over 20 million publications in Computer Science, Neuroscience, and Medicine. While also providing the metadata for each publication, the dataset is primed for the task as it provides credible sources while opening up the possibility of testing the model in three different fields of study.</p><p>Metrics of accuracy and performance will be difficult to define. At the moment, we plan on using precision and recall of relevant terms found in Google, Oxford, and Webster definitions and whether they appear in a generated definition.</p><p>As for determining a baseline, we plan to implement a standard approach to the Question-Answering task, where the question is always “What is X?”. Ideally, our model should be able to provide more complete definitions with more tangential detail than a standard QA model.</p><h4>Stretch Goal: Ontology Matching</h4><p>If we were to succeed at the task, applications of our methodology could lead to advancement in both summarization and entity recognition. However, a working version of the model could also pave the way for a novel approach to Ontology Matching.</p><p>We could extend our model to solve Ontology Matching by first extracting all significant entities from the corpora. We could then assign each of these entities a definition through our model, and cluster terms based on the definitions accordingly. By doing so, we can in a sense eliminate entities that are linearly dependent with another, reducing the set of entities to one in which their definitions are unique and different enough to be stand-alone definitions.</p><p>Through this, we could achieve a new means of standardizing vocabulary in order to clean up knowledge bases or assist in the advent of a new field. Deep Learning in particular has experienced a lot of redundancy in notation and definitions and would benefit from this application.</p><h3><strong>2. Natural Language Visual Reasoning</strong></h3><h4><strong>Motivation &amp; Context</strong></h4><p>Natural Language Visual Reasoning is a task which involves answering questions about an image. Extracting information from an image is an incredibly important but difficult task. Pictures provide a plethora of context about the world, especially about the relationships between objects. It’s also important to be able to extract knowledge about these relationships through natural language. If we can successfully reason about image content, we can change the way we train models and how they integrate with the physical world.</p><p>This task is also exciting because it combines Computer Vision and Natural Language Processing. While this goal is fairly lofty, researchers at Cornell and Facebook have collaborated to create a new visual reasoning language dataset (<a href="http://lic.nlp.cornell.edu/nlvr/">http://lic.nlp.cornell.edu/nlvr/</a>), which is a simplified set of images (which contain shapes and colors of a certain format) and sentences. One of the major motivations of this project is the fact that this dataset is so vast and complete.</p><h4><strong>Minimum Viable Product</strong></h4><p>The Visual Reasoning task can be described as follows. The below image represents one instance of training data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/924/1*O1uEL-Ljs3OgfkVeTJZJuQ.png" />Example training instance</figure><p>As you can see, each piece of data consists of a statement and an image of three boxes. If the statement is true of the image, then we output true, otherwise false. There are two viable courses of action for this problem. In the first approach, we can take the current best performing model (<a href="https://arxiv.org/pdf/1511.02799.pdf">Neural Module Networks</a>) and improve the model to produce better results. In the second approach, we can start from scratch and try to develop a model that is uniquely trained for this dataset. It’s important to note that the Neural Module Network can be applied to any image, whereas we can train a model that can only be applied to images of this type.</p><p>Putting it all together, the end product should be able to take the sentence and image as input, extract all relevant information from the image into some intermediate form, validate the criteria described in the sentence for each image, and output the correct answer. We can start by coming up with a basic model which represents each image as a list of objects (yellow triangle at (a, b), blue square at (c, d), black circle at (e, f)) and finds a relationship between the sentence and the list of objects. A more effective model might try to represent the image in a different way, or utilize some of the techniques from the Neural Module Network paper.</p><p>The evaluation metrics for this project are very simple. The output of the model will always be true or false and we can do a simple accuracy test to determine the strength of our model.</p><h4><strong>Stretch Goals</strong></h4><p>A stretch goal for this project would be to generalize to different pictures of the same type. For example, what would happen if we changed the shapes and colors in the picture? What would happen if we changed the type of picture? Trying to achieve a high accuracy on different types of pictures would definitely be a stretch for this project.</p><h3><strong>3. You Can Read This, Machines Can’t</strong></h3><h4><strong>Motivation and Context</strong></h4><p>The motivation for this project idea came from one of the datasets that Yonatan introduced to the class. Research has shown that humans are able to read the following sentence, but it turns out that computers are not able to do the same:</p><blockquote>“Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn’t mttaer in waht oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and lsat ltteer be at the rghit pclae”</blockquote><p>This is an interesting project because it explores how robust Machine Learning models can be to error (like misspellings). It also explores how much computers might be able to infer given surrounding words.</p><p>Another interesting aspect of this project is the available data. We can constantly generate new scrambled sentences with a simple algorithm.</p><h4><strong>Minimum Viable Product</strong></h4><p>The goal of this task is to take a scrambled English sentence and produce the unscrambled version. In particular, each word in the English sentence will have the correct first and last letter but the letters in between might all be incorrect. There are several variations of this data. In the easiest version, the letters in the middle of each word are all correct but they are scrambled. A slightly more difficult challenge would be evaluating sentences where the letters in the middle of each word are semi-random and the number of letters may not correspond to the number of letters in the actual word.</p><p>In order to approach this problem, we can start with a very basic approach (for the easier version of the dataset) where we create a very efficient dictionary search algorithm. This approach would not use any Machine Learning and might be very slow. A better approach might be to train a very effective sequence to sequence model which maps a scrambled sentence to the correct sentence. Since we can mutate the data in many ways, we can train several different versions of the same sentence.</p><p>We could also explore a more effective way of training the model, rather than training on several mutations of the same sentence. One approach would be to represent words as a volume in N-Dimensional space, so a model would be able to recognize any data point within that volume as the original word. Another interesting approach would be to infer what the next word might be based on the most likely translation of the previous word. This would not only involve choosing a likely translation of the word, but also choosing the most likely translation of the sentence. A third approach might be to treat the the scrambled sentence as a language and use Machine Translation techniques to “translate” the sentence to English.</p><p>The performance metric for this would be the number of correctly predicted words in each sentence.</p><h4><strong>Stretch Goals</strong></h4><p>The stretch goal for this project would be to recognize any mutated sentence as the original sentence. This includes random letters, swapped letters, dropped letters, etc. We also could generate new datasets in different languages pretty easily. It might be interesting to see how our model performs on languages like Spanish or French.</p><h3>Conclusion</h3><p>Questioning the immediate vs. long-term impact, along with the tractability and scope of each task, helped define our motivations while ensuring to the best of our knowledge, the viability of each potential project. We asked each other questions along the lines of “why is this task important?”,<br />“if other people have done this before, how can we do it better?”, and “can it be done in 10 weeks?”.</p><p>Ultimately, we want our efforts and insights to have potentially long-term and significant consequences. Without consciously keeping track of whether an idea was research or industry/product oriented, we both came up with only research-oriented proposals. These proposals, if executed properly, would have impacts on related works, having fundamental and methodological influence through novel approaches and perspectives rather than aiding in the downstream application of any particular product. For the time being, we consider ourselves a research mode team.</p><p>Link to Code: <a href="https://github.com/NLP-Capstone-Project">https://github.com/NLP-Capstone-Project</a></p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=93d82eed396c" width="1" /><hr /><p><a href="https://medium.com/nlp-capstone-blog/a-discussion-of-language-tasks-for-the-nlp-capstone-blog-post-1-93d82eed396c">A Discussion of Language Tasks for the NLP Capstone (Blog Post #1)</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>







<p class="date">
<a href="https://medium.com/nlp-capstone-blog/a-discussion-of-language-tasks-for-the-nlp-capstone-blog-post-1-93d82eed396c?source=rss----9ba3897b6688---4">by Karishma Mandyam at April 04, 2018 03:24 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 03, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/inaugural-blog-post">
<h4><a href="http://sarahyu.weebly.com/cse-481n/inaugural-blog-post">Inaugural Blog Post</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">Welcome to the first blog post of <u><em>Jekyll-Hyde</em></u><em> </em><u>(</u>my very cool and somewhat related group-of-1 name).<br />As the name might reveal, I’m interested in using NLP to uncover the duality of language, the ability to simultaneously present both sides of a coin, whether in everyday conversation or more curated prose. Because language matters; so much so that we mend and mold our language to navigate the different social environments and spaces we inhabit, whether for power, survival, or acceptance, and often in the most primitive and subconscious ways. And because in today’s (supposedly) civilized world, the language we employ with another can be a sort of proxy for the relationship we share. I'd like to get at some of these ideas through the NLP capstone and think the next three topics are a potential start. <br /><br />1) Language Accommodation (or my unlikely paper title: <em>Nice Guy by Day, A**hole by Night: Language Accommodation for Self-Presentation in Subreddit Communities)</em><ul><li>​MVP: Scrape Reddit user data, identify language baselines for a given subreddit or capture linguistic differences of a single user across subreddits</li><li>Stretch Goals: Do both (subreddit baselines and user difference) and not only identify a user's language accommodation, but how they fall in line with the communities' baseline (a kind of hive mentality)</li></ul><br />2) Identifying Condescension (another working title: "<em>Well, Actually": Identifying Ambiguities in emails from 'helpful' colleagues</em>)<ul><li>​MVP: Identify an appropriate data source (ideally emails or more personal interactions), manually identify possible ambiguities, train model (maybe one that doesn't require a large dataset) to identify ambiguous spans of a sentence. </li><li>Stretch Goals: Begin identifying entity-entity-relationships with cues from ambiguous interactions or maybe something cooler about context and the different meanings if ambiguous...</li></ul><br />3) Identifying Disrespect (might as well for consistency: <em>Linguistic (dis)R.E.S.P.E.C.T. - Addressing 90% of Comment Sections)</em><ul><li>​MVP: Scrape Youtube comment data, classify comments as hateful/not hateful, train model on classified comments, test and tune</li><li>Stretch Goals: Train a portable model that can work with content from other mediums such as Twitter and Reddit</li></ul><br />I'll be pursuing one of these ideas in <strong>research mode</strong> and you can follow along at:<br />                                      https://github.com/sarahyu17/481n</div>  <div class="wsite-spacer" style="height: 50px;"></div>  <div> 				<form action="http://www.weebly.com/weebly/apps/formSubmit.php" enctype="multipart/form-data" id="form-147197638403517045" method="POST"> 					<div class="wsite-form-container" id="147197638403517045-form-parent" style="margin-top: 10px;"> 						<ul class="formlist" id="147197638403517045-form-list"> 							<h2 class="wsite-content-title">Any Favorite Paper Titles?</h2>  <label class="wsite-form-label wsite-form-fields-required-label"><span class="form-required">*</span> Indicates required field</label><div><div class="wsite-form-field" style="margin: 5px 0px 0px 0px;">   <label class="wsite-form-label" for="input-789590629342031487">Paper Title <span class="form-required">*</span></label>   <div class="wsite-form-radio-container">     <span class="form-radio-container"><input id="radio-0-_u789590629342031487" name="_u789590629342031487" type="radio" value="#1" /><label for="radio-0-_u789590629342031487">#1</label></span><span class="form-radio-container"><input id="radio-1-_u789590629342031487" name="_u789590629342031487" type="radio" value="#2" /><label for="radio-1-_u789590629342031487">#2</label></span><span class="form-radio-container"><input id="radio-2-_u789590629342031487" name="_u789590629342031487" type="radio" value="#3" /><label for="radio-2-_u789590629342031487">#3</label></span>   </div>   <div class="wsite-form-instructions" id="instructions-Paper Title" style="display: none;"></div> </div></div> 						</ul> 					</div> 					<div style="display: none;"> 						<input name="weebly_subject" type="text" /> 					</div> 					<div style="text-align: left; margin-top: 10px; margin-bottom: 10px;"> 						<input name="form_version" type="hidden" value="2" /> 						<input id="weebly-approved" name="weebly_approved" type="hidden" value="approved" /> 						<input name="ucfid" type="hidden" value="147197638403517045" /> 						<input name="recaptcha_token" type="hidden" /> 						<input name="opted_in" type="hidden" value="0" /> 						<input type="submit" /> 						<a class="wsite-button"> 							<span class="wsite-button-inner">vote</span> 						</a> 					</div> 				</form> 				<div class="recaptcha" id="g-recaptcha-147197638403517045"></div> 			  			</div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/inaugural-blog-post">April 03, 2018 07:00 AM</a>
</p>
</div>
</div>


</div>

</div>


<div class="sidebar">

<h2>Subscriptions</h2>
<ul>
<li>
<a href="https://medium.com/feed/@viterbi.or.not" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" title="Stories by Viterbi Or Not To Be on Medium">Aaron Johnston, Lynsey Liu <br/> Team Viterbi Or Not To Be</a>
</li>
<li>
<a href="https://deeplearningturingtest.wordpress.com/feed/" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://deeplearningturingtest.wordpress.com" title="NLP Capstone Project Updates – Ananth">Ananth Gottumukkala <br/> Team Turing Test</a>
</li>
<li>
<a href="https://medium.com/feed/@halden.lin" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" title="Stories by Halden Lin on Medium">Halden Lin <br/> Team undef.</a>
</li>
<li>
<a href="https://mathstoc.wordpress.com/category/nlp-capstone/feed/" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a>
</li>
<li>
<a href="https://teamoverfit.blogspot.com/feeds/posts/default?alt=rss" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a>
</li>
<li>
<a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://cse481n.blogspot.com/" title="PrimeapeNLP">Ron Fan, Aditya Saraf <br/> Team PrimeapeNLP</a>
</li>
<li>
<a href="http://sarahyu.weebly.com/6/feed" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a>
</li>
<li>
<a href="https://medium.com/feed/nlp-capstone-blog" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" title="NLP Capstone Blog - Medium">Tam Dang, Karishma Mandyam <br/> Team Illimitatum</a>
</li>
</ul>

<p>
<strong>Last updated:</strong><br>
July 30, 2018 04:41 PM<br>
<em>All times are UTC.</em><br>

<!--
<br>
Powered by:<br>
<a href="http://www.planetplanet.org/"><img src="images/planet.png" width="80" height="15" alt="Planet" border="0"></a>
</p>

<p>
<h2>Planetarium:</h2>
<ul>
<li><a href="http://www.planetapache.org/">Planet Apache</a></li>
<li><a href="http://planet.freedesktop.org/">Planet freedesktop.org</a></li>
<li><a href="http://planet.gnome.org/">Planet GNOME</a></li>
<li><a href="http://planet.debian.net/">Planet Debian</a></li>
<li><a href="http://planet.fedoraproject.org/">Planet Fedora</a></li>
<li><a href="http://planets.sun.com/">Planet Sun</a></li>
<li><a href="http://www.planetplanet.org/">more...</a></li>
</ul>
</p>
!-->
</div>
</body>

</html>
