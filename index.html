<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>NLP Capstone Spring 2018</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="planet.css" type="text/css">
<link rel="alternate" href="https://nlpcapstone.github.io/atom.xml" title="" type="application/atom+xml">
</head>

<body>
<h1>NLP Capstone Spring 2018</h1>

<div class="daygroup">
<h2>May 29, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/last-update">
<h4><a href="http://sarahyu.weebly.com/cse-481n/last-update">Last Update</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">The final update for the project! Unfortunately, there is not as much to report on results as usual and not as many fun visualizations. For the 2nd Advanced Model, my plan was to begin on the stretch goals I had initially outlined and train a neural model for Reddit Post classification and Generation. The idea took cue from the Affect-LM paper. Basically it would be similar to this model</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 13.331751602564%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 68.227199377828%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-29-at-9-42-04-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 18.441049019608%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph">which has inputs of the context words, on which to build up the rest of the sentence from; the Affect category which is chosen beforehand to generate the desired output; and an Affect strength to determine the intensity of the affect category defined. <br /><br />My model would be similar to this, but instead look more like the following where we take out the strength factor and choose the mental category  to be fed into the Mental LM.</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 24.666352941176%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 56.902274509804%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/published/screen-shot-2018-05-29-at-10-56-19-pm.png?1527659857" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 18.43137254902%; padding: 0 15px;"> 					 						  <div class="wsite-spacer" style="height: 50px;"></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph">My current model seems to have some issues generating posts containing any language beyond the exact topic of the mental category itself (i.e. using the word depressed for the F30 category), with the model using the cross entropy loss. <br /><br />At this point, the game plan is to fix the bugs and get a working model to test out the post generation. The results will be interesting to see in and of themselves, but I will also compare the language model of the generated posts against the metrics we hav seen throughout the quarter (vennclouds, idp). </div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/last-update">May 29, 2018 05:35 PM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 17, 2018</h2>

<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=339">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/05/17/nlp-capstone-post-8-training-challenges/">NLP Capstone Post #8: Training challenges</a></h4>
<div class="entry">
<div class="content" lang="en">
<h1><span style="font-weight: 400;">Challenges with training TSL model</span></h1>
<p><span style="font-weight: 400;">In our previous post we proposed a three-model system that would allow us to take advantage of a larger corpus of higher quality lyrics data for the production of lyrics. We also finally tackle the alignment task with a simple approach of determining whether a lyric token should be produced at each timestep. This seems sensible since we have begun dividing the MIDIs into pianorolls with a constant frequency.</span></p>
<p><span style="font-weight: 400;">Unfortunately, even after several bugs bashed, we’ve been still unable to produce even sensible timings. We find the RNN collapses to repeatedly generating 0 (for no lyric event), even though a randomly initialized RNN will repeatedly generate 1 (and perform better with respect to classification accuracy).</span></p>
<p> </p>
<h1><span style="font-weight: 400;">Future direction</span></h1>
<p><span style="font-weight: 400;">If we are able to produce something reasonable from our existing architecture, we would like to move on to a second model that structures the problem as machine translation. We have decided to focus on the paper Attention is All You Need by Vaswani et al. [1] for our presentation in two weeks. The structure of our problem is straightforward to apply to translation as converting pianoroll format into english sentences. Incorporating attention has shown promising results in the literature, though that is no guarantee that our noisy dataset would be able to take advantage of this proposed architecture.</span></p>
<p> </p>
<h1><span style="font-weight: 400;">References</span></h1>
<p><span style="font-weight: 400;">[1] </span><a href="https://arxiv.org/abs/1706.03762"><span style="font-weight: 400;">https://arxiv.org/abs/1706.03762</span></a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/05/17/nlp-capstone-post-8-training-challenges/">by Nicholas Ruhland at May 17, 2018 05:55 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 10, 2018</h2>

<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=335">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/05/10/nlp-capstone-post-7-tsl-pipeline/">NLP Capstone Post #7: TSL Pipeline</a></h4>
<div class="entry">
<div class="content" lang="en">
<p> </p>
<h2><span style="font-weight: 400;">Modeling issues</span></h2>
<p><span style="font-weight: 400;">As seen in our results last week, the RNN architecture we have been training has not been able to produce any coherent series of tokens based on the music data provided in the clean Lakh dataset. To analyze the poor results of this model, we have considered various features of the quality of the data. To simplify the issue of timing the lyric tokens, this model attempts to predict a lyric token at every timestep. Between each token we have summed all the musical data, producing a piano roll that looks approximately like the following image.</span></p>
<p><img alt="Screen Shot 2018-05-09 at 4.31.53 PM" class="  wp-image-331 aligncenter" height="303" src="https://mathstoc.files.wordpress.com/2018/05/screen-shot-2018-05-09-at-4-31-53-pm.png?w=501&amp;h=303" width="501" /></p>
<p><span style="font-weight: 400;">In the event that two lyrics occur at exactly the same time step, we end up with a gap in the notes, here highlighted in red.</span></p>
<p><img alt="Screen Shot 2018-05-09 at 4.31.53 PM" class="  wp-image-334 aligncenter" height="302" src="https://mathstoc.files.wordpress.com/2018/05/screen-shot-2018-05-09-at-4-31-53-pm1.png?w=500&amp;h=302" width="500" /></p>
<p><span style="font-weight: 400;">At first we expected this problem to occur in only a small number of cases, but it is often the result of the newline character appearing in a message simultaneously with the first lyric of the next sentence. This processing poses several problems to the task of learning the lyrical content based on the structure of the music. First, the large number of musical gaps may be confounding the model due to the large variety in lyrics that will be seen at those time steps. Additionally, we lose all information about the song timing since all regions without lyrics are compressed into a single time step. In theory, gaps in lyrics could hint to the model that the next section should start a new verse or chorus.</span></p>
<h2><span style="font-weight: 400;">The TSL Pipeline</span></h2>
<p><span style="font-weight: 400;">As suggested in the previous blogpost, we would like to be able to augment the results of the musical model with a higher quality lyrical dataset. The Kaggle lyrics dataset has shown promising results in previous blogposts at the quality of the lyric sequences it has been able to produce.</span></p>
<p><span style="font-weight: 400;">The TSL Pipeline is a combination of three models: Timing, Seed, and Lyrics. The architecture may look something like the following diagram:</span></p>
<p><img alt="Training" class="  wp-image-333 aligncenter" height="221" src="https://mathstoc.files.wordpress.com/2018/05/training.png?w=531&amp;h=221" width="531" /></p>
<p><span style="font-weight: 400;">During training, each pianoroll will be separated into data representing the timing, notes and lyrics. These get passed into respective models to learn timing and “seed” information. Additional lyrics information from the Kaggle dataset is used to train a lyrical model.</span></p>
<p><img alt="Evaluation" class="  wp-image-332 aligncenter" height="220" src="https://mathstoc.files.wordpress.com/2018/05/evaluation.png?w=538&amp;h=220" width="538" /></p>
<p><span style="font-weight: 400;">At evaluation time, the lyrics from the original pianoroll are not passed into the Seed model. Instead, the Seed model attempts to predict some seed based on the musical content, and will pass its result into the lyrics model. The combination of these lyrics and timing information constitute the complete description of our karaoke output.</span></p>
<h2><span style="font-weight: 400;">Timing Model</span></h2>
<p><span style="font-weight: 400;">In all previous posts we ignored the issue of lyrics timing in the interest of creating a reasonable lyrical model. Our current timing model is similar to our previous model attempt, but the data is generated differently. Instead of computing a pianoroll sample at each lyrical timestep, we us a constant sampling frequency of 10 timesteps per second. We then annotate each timestep with a 1 or 0 based on if a lyric was annotated at that step. The model will then attempt to predict for each step of a given pianoroll the probability there should be a lyric at that time.</span></p>
<h2><span style="font-weight: 400;">Seed Model</span></h2>
<p><span style="font-weight: 400;">The seed model will be a simplified version of the poorly performing model from before. Instead of predicting all lyrics, it will attempt to predict a small subset of the initial lyrics. This would also allow us to create a dataset with more training examples by splitting each song into smaller samples.</span></p>
<h2><span style="font-weight: 400;">Lyrics Model</span></h2>
<p><span style="font-weight: 400;">The lyrics model will be similar to the one described in the second blog post, which is a character level RNN for generating lyrics. This will take the first few words predicted by the seed model and generate the remainder of the lyrics. Since it’s trained on the large Kaggle dataset the quality seems to be much higher than what our MIDI training has produced.</span></p>
<h2>Results</h2>
<p>As of this blog post, we are still testing various hyperparameters and waiting for models to converge.  Additional results will follow once we can examine the various output.</p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/05/10/nlp-capstone-post-7-tsl-pipeline/">by Nicholas Ruhland at May 10, 2018 06:56 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>May 08, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1-part-21">
<h4><a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1-part-21">Advanced Model Attempt #1 (Act 2 Scene 1)</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">This week, I had to do a little backtracking and restructuring, but all for the better! <ul><li>More Data! I have had a couple jobs still running that has allowed me to have more reddit data to work with overall, which is especially good news for the next advanced model</li><li>Subreddit ReOrg - A large portion of this week has been on identifying different subsets for the neurodivergent and neurotypical groups. In conversation with Maarten, we discussed that the subreddits in the current state are a bit too broad to address the hypotheses and may not produce a focused language model that makes sense. Currently, the neurodivergent group is a broad strokes grouping of all types of differences. With that in mind, I spent some time researching, following some Reddit rabbit holes, and learning about ICD 10 <font color="#515151">(International Statistical Classification of Diseases and Related Health Problems (ICD), a medical classification list by the World Health Organization (WHO)) to classify the subreddits and the mental illnesses they speak to. Among these topics, we specifically wanted to look at the the two 'Blocks' - Mood (affective) disorders and Neurotic, Stress-Related and Somatoform disorders. This would be defined as the new 28 subreddits constituting the neurodivergent set, and the 22 neurotypical subreddits are those that are general advice, community support, or discussions on mental health, broadly speaking. These were chosen from the initial ND set because while some of them discussed mental health and neurodiversity, they were open to more than just users dealing with mental health related issues. They had similar types of posts where people are more open and discussing in detail their personal lives, a kind of venting space. And choosing from this group rather than to look at more general reddit, was a decision made so that I could find the nuances rather than the more general difference seen in discussing psychology and not. Despite the significant reduction of subreddits (from 126 to 28 ND and 100 to 22 NT), the data collection I mentioned above has resulted in 5x the original vocabulary size and we now have a combined 1 million words in the newest set (being added to as we speak)</font></li></ul><br />With that, I was able to run the previous vennclouds and idp models (and Act 2 Scene 2, before next week will hopefully include the Connotation Frames model results). For the vennclouds, I think we see some similarity from before of the "my/me/I" terms speaking in first person, but the NT posts has more discussion of a second person or a more general reference to an other or friends. It might also be useful to find a list of prepositional phrases to remove and to find something more interesting in the venn diagram middle portion. </div>  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-09-at-2-32-31-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>  <div class="paragraph"><br />I also ran the IDP model from before and found the following top categories:<br /><strong>ND</strong><br /><em>I/<span>my/</span></em><span><em>me/myself</em> - the personal discussion makes sense, but I'm a bit curious as to why it would be more prevalent than in the other discussion forums that are also predominantly venting spaces focused on the self seeking the community. </span><br /><em>anxiety/depression/OCD</em> - these are the subreddit categories we basically chose<br /><em>feel/feeling </em>- I was surprised to see a significantly higher mention of feeling in the ND categories despite the other subreddits still being communities discussing mental illness along with the larger public in discussing general problems <br /><br /><strong>NT</strong><br /><em>you / his/he/him / girls/she/her - </em>This is a bit surprising given that I had made a bit of a fuss around the female mentions being significantly higher in the ND categories. I'm interested to see what this shift might mean, but even more so, I think the discussion of another is an interesting contrast between the two that might be better suited for the connotation frames results that I hope to do in the next couple of days. <br /><em>https</em> - links are back! But more interestingly, while they were prevalent in the original NT group, which we hypothesized were due to the higher proportion of anecdotal advice in the ND categories, it seems that even in regards to discussions around mental illness, people bring in outside links and whatnot to advise, or uplift other users. <br /><br />Next Steps: As I mentioned above, I'd like to run this same new and larger dataset with the connotation frames model. Beyond that, I'd like to spend the next two weeks working on either implementing a SAGE log-linear model to describe these language models, or a deep learning model. If possible, I hope to also get the graphical model going <br /><br /></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1-part-21">May 08, 2018 07:00 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 25, 2018</h2>

<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=323">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/25/nlp-capstone-post-5-a-new-hope/">NLP Capstone Post #5: A New Hope</a></h4>
<div class="entry">
<div class="content" lang="en">
<h1><span style="font-weight: 400;">Dataset Improvements</span></h1>
<p><i><span style="font-weight: 400;">Last time, on Music NLP.</span></i><span style="font-weight: 400;"> We ran into many midi data parsing issues. Since then, we have discovered a new dataset called the Lakh MIDI Dataset (</span><a href="http://colinraffel.com/projects/lmd/"><span style="font-weight: 400;">http://colinraffel.com/projects/lmd/</span></a><span style="font-weight: 400;">) that comes with reasonably well-formed midi files. Using the “Clean MIDI Subset”, we found thousands of midi files with their associated song names and songwriters. From these midi files, we extracted all with nonempty “lyric” fields when parsed via the pretty_midi package (which, incidentally, is also developed by Colin Raffel). After this step, we were left with ~1200 midi files that contain lyrics.</span></p>
<p><span style="font-weight: 400;">We currently clean the lyrical content by removing all unusual symbols and setting all characters to lowercase. We leave all lyrical tokens as is, which typically means syllable. Due to the inconsistent quality of the MIDI annotations, many songs are tokenized instead to characters, words, or even sentences. We will explore other methods for processing data if this is not sufficient for our results.</span></p>
<p><span style="font-weight: 400;">It is unfortunate we did not find this dataset sooner, because most of our challenges up to this point have been dealing with the poor quality of the gathered data.</span></p>
<h1><span style="font-weight: 400;">Alignment</span></h1>
<p><span style="font-weight: 400;">For our task of producing karaoke style output, there are two main tasks we have to solve. The first task is the generation of plausible lyrics, and the second is to align the lyrics to the proper time along the musical data. The alignment task has been studied extensively, but specifically aligning lyrical content to MIDI has not been covered in literature we have found. The most common alignment task is lyrics to audio data, as opposed to MIDI. The other common task is to align audio data to the notes defined in a MIDI file. In [1], they show a method that takes a MIDI file with annotated lyrics and uses this to align the lyrics to the raw audio. Unfortunately this is not our task, because we are trying to generate the annotated MIDI.</span></p>
<p><span style="font-weight: 400;">This week, we have decided to ignore the alignment task and focus primarily on making a reasonable lyrical model. We will return to alignment next week.</span></p>
<p><span style="font-weight: 400;">The next step was to align the lyrics with pianoroll. Fortunately, well-formed midi data parsed into PrettyMIDI objects come with a “get_piano_roll” function that takes as input a list of “times” which correspond to where in time pretty_midi will attempt to sample the music. As each syllable in the lyrics comes with a start time for when the singer enunciates it, we can pass in these start times to produce pianoroll that is aligned (up to small error) with the lyrics.</span></p>
<p><span style="font-weight: 400;">For some implementation reasons that are difficult to explain in English, it is possible for “get_piano_roll” to produce NaN entries, which we have replaced with zeros. Due to this and the potential for other such problems, we have forked the pretty_midi package and will be able to modify the code for our needs. For example, as pointed out in [2], “in a given MIDI file there is no reliable way of determining which instrument is a transcription of the vocals in a song”. As such, there are many choices for how to do alignment; pretty_midi has implemented just one. It is an interesting task to see how different alignment methods help or hurt our models.</span></p>
<h1><span style="font-weight: 400;">Lyric prediction</span></h1>
<p><span style="font-weight: 400;">Now that we have aligned pianoroll to lyrics data, we can begin engineering the model. Last time, we used an LSTM to generate lyrics given starting characters. Here, we will again use LSTMs, but instead, work at the syllable level and take as input the pianoroll of a song. As each column of a pianoroll is a time slice, each input vector to the LSTM is a single time slice. Each time slice is a 128-dimensional vector, with each entry representing the activation of an instrument; there are 128 midi recognized “instruments”.</span></p>
<p> </p>
<p><span style="font-weight: 400;">All that is left is to play with the architecture. </span></p>
<p><img alt="RNN model" class="alignnone size-full wp-image-322" src="https://mathstoc.files.wordpress.com/2018/04/rnn-model1.png?w=676" /></p>
<p><span style="font-weight: 400;">At the moment, our pipeline looks like what is shown in the diagram. At each iteration, we take a song, extract the lyrics and the corresponding pianoroll data. We then feed each time slice of the pianoroll data through an encoder unit, then through an LSTM unit, then through a decoder unit, and finally through a softmax to produce the prediction. Our loss is the negative log-likelihood (negative logarithm of the RNN softmax probability of the true syllable).</span></p>
<p><span style="font-weight: 400;">We will compare our final model to this baseline with respect to the loss on a held-out validation set. We will also experiment with loss functions other than cross entropy to see how it affects the actual lyrical output.</span></p>
<h1><span style="font-weight: 400;">Model results</span></h1>
<p><span style="font-weight: 400;">We have so far only trained our model for a single iteration over the training set. For an empirical evaluation on the current model quality, we ran a single MIDI through the input and computed the argmax word for each output. This produced a result in which every predicted lyric was an empty message, which is the most common string in the training set. We will explore methods to handle this class imbalance as our next task.</span></p>
<h1>References</h1>
<p><span style="font-weight: 400;">[1] Müller, Meinard &amp; Kurth, Frank &amp; Damm, David &amp; Fremerey, Christian &amp; Clausen, Michael. (2007). Lyrics-Based Audio Retrieval and Multimodal Navigation in Music Collections. 4675. 112-123. 10.1007/978-3-540-74851-9_10.</span><br />
<span style="font-weight: 400;">[2] </span><span style="font-weight: 400;">Raffel, Colin and Daniel P. W. Ellis. “Extracting Ground-Truth Information from MIDI Files: A MIDIfesto.” </span><i><span style="font-weight: 400;">ISMIR</span></i><span style="font-weight: 400;"> (2016). </span><span style="font-weight: 400;"> </span></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/25/nlp-capstone-post-5-a-new-hope/">by Nicholas Ruhland at April 25, 2018 04:44 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 24, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/actual-strawman-update">
<h4><a href="http://sarahyu.weebly.com/cse-481n/actual-strawman-update">Actual Strawman Update</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph"><strong>Real Data and Results Have Been Seen! </strong><br />As I mentioned in my last post, I was struggling with accessing the data, but I've since solved my problems and got to learn some cool tools along the way (like apparently you can read a compressed file without decompressing??? wild). I've also spent a large part of the week learning and fighting with SqlAlchemy, PyMySQL, MySQL, and UTF-8 issues. With the interaction of all of these, I was able to read (most of the) Reddit posts of January 2017 (thanks to Jason Baumgartner publishing these dumps on pushshift.io, I will donate when I have an income) which amounted to 80 million posts, find the users we are interested in, find neurotypical subreddits these users post to, and then get posts of our two (neurotypical and neurodivergent) subreddit subsets. <br /><br />Side Note: I'm going to start referencing the Neurodivergent set as ND, and Neurotypical as NT, trying to save some typing<br /><br /><strong>Baseline #1 (kind of an update of the Strawman #1):</strong><ul><li><span>Glen Coppersmith and Erin Kelly (2014). <strong><em>Dynamic Wordclouds and Vennclouds for Exploratory Data Analysis. </em></strong></span><span><font size="2">Association for Computational Linguistics Workshop on Interactive Language Learning and Visualization</font></span></li></ul> With thanks to Coppersmith and Kelly, I was able to make a Term Frequency Venncloud as seen below that show in black the most frequent terms found in both subreddit subsets, and then separated into the most frequent terms in neurodivergent subreddits and neurotypical subreddits in blue and red, respectively. </div>  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0px; margin-right: 0px; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-04-26-at-1-48-22-pm_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;">Naurodivergent vs. Neurotypical Subreddit Venncloud</div> </div></div>  <div class="paragraph"><br />As we can see, the middle of the venncloud is pretty uninteresting, but here are some notable points:<ul><li>Personalization: Frequency of you/me, your/my words which indicate some sense of relationship and more personalization between ND posters. This contrasts the frequency of they, indicating some discussion of an other, in the NT subreddits.</li><li>"is": I interpreted the frequency of this word in the NT subreddits as a more definitive and declarative way of speech, rather than other words such as "think", "feel" and "maybe" (in the ND subreddits) which signal more hesitation. This is a point touched on and described as dogma in Fast &amp; Horvitz which is one of the papers I discussed in a previous post.</li><li>"www", "imagesofnetwork" :  This is something I cold probably fix; the way I pre-process the data scrubs and separates the links into separate words. At the end of the day though, this shows that there are significantly more links in NT subreddits. My thought is that the lack of such in the ND subreddits might mean more anecdotal and personal interactions than when compared to ND subreddits</li><li>Moral Adjectives: Some of the ND frequently used terms are what I am going to call Morale Adjectives (let me know if there's an actual term for this); here I mean, we see words like "good", "right", "bad", which are often used to describe habits or behavior.</li><li>SURPRISE GENDER DIFF: As you can see, 'she' is one of the most frequent ND words, whereas 'he' is  one of the most frequently used NT words. Some thoughts: 1) doesn't show anything, there are some partner subreddits and may just show that the predominantly male reddit user base talks about different genders in the two, but they themselves may not be a different gender distribution or 2) could show different gender engagement in the different subsets.</li></ul><br /><strong>Baseline #2: Connotation Frames</strong><ul><li><span style="color: rgb(0, 0, 0);">Hannah Rashkin, Sameer Singh, Yejin Choi. 2016. <strong><em>Connotation Frames: A Data-Driven Investigation.</em></strong><font size="2"> In Proceedings of ACL 2016</font></span></li><li><span style="color: rgb(0, 0, 0);">Maarten Sap, Marcella Cindy Prasettio, Ari Holtzman, Hannah Rashkin, &amp; Yejin Choi. 2017. <em><strong>Connotation Frames of Power and Agency in Modern Films.</strong></em> <font size="2">sched. to appear EMNLP 2017 short papers. </font></span></li></ul> With help from Maarten Sap, another model I explored was the Connotation Frames formalism, to look at the verbs used in both our NT and ND subreddits and the sentiments these provide between agent and subject. However, we found no significant differences between the two (output below).</div>  <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap"> 	<table class="wsite-multicol-table"> 		<tbody class="wsite-multicol-tbody"> 			<tr class="wsite-multicol-tr"> 				<td class="wsite-multicol-col" style="width: 50%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-04-26-at-9-45-10-am_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>				<td class="wsite-multicol-col" style="width: 50%; padding: 0 15px;"> 					 						  <div><div class="wsite-image wsite-image-border-none " style="padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;"> <a> <img alt="Picture" src="http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/nt-verbs_1_orig.png" style="width: auto;" /> </a> <div style="display: block; font-size: 90%;"></div> </div></div>   					 				</td>			</tr> 		</tbody> 	</table> </div></div></div>  <div class="paragraph"><br /><strong>Baseline #3: LIWC2015</strong><br />Finally, I used LIWC2015 to count and classify the psychological meanings and categories for both NT and ND subreddits. This serves as another type of language model to define these two 'languages' and offers us another metric on which to find similarities and differences. ​<br /><br />Just for some clarification, the way that this model works is by having 73 categories (more information available <a href="https://liwc.wpengine.com/compare-dictionaries/" target="_blank">here</a>), anywhere from topics - PRONOUN, HEALTH, BIO - to grammar - VERB, ARTICLE - and gives the percentage of the language that each category accounts for in that 'language'. In our case, we see the distribution of categories in NT subreddits compared to ND subreddits. <br /><br /><span>My hypotheses were:</span><ul><li>[You, Heshe, Pronoun, Health, Feel, They] categories would be significantly higher in ND </li><li>[Anger, Power, Swear] categories would be significantly lower in ND than in NT</li></ul><br />After getting the results, I report the top 10 categories with the largest % difference between the two. <ul><li>HEALTH(3.34x), <span>INGEST(2.69x), BIO(2.13x)</span><br /><ul><li>These categories are topic specific (ingestion related to drug subreddits and bio on biological processes) and align with what we expect in mental health topic subreddits</li></ul></li><li>FEEL(2.04x), SAD(1.88x), ANX - anxiety (<span>2.86x)</span><br /><ul><li><span>​Also make sense for the support communities within the ND group, potentially, topical for "anxiety" as a temporary and consistent feeling</span><br /></li></ul></li><li>FAMILY(2.07x), HOME (2.05x)<br /><ul><li>This was a bit surprising, I believe appeals to the family and home tend are prominent in support groups as well as the "partners of" subreddits we have in the ND group</li></ul></li><li>I (1.73x)<ul><li>There seems to also be a lot of personal discussion, which we expect in subreddits that are meant to discuss personal problems</li></ul></li><li>FEMALE (1.81x)<br /><ul><li>ITS HERE AGAIN WHY AND HOW</li></ul></li></ul>​<br />That's all for baseline models, here's to my first attempt at the more advanced model this week...</div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/actual-strawman-update">April 24, 2018 07:00 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 18, 2018</h2>

<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=314">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/18/nlp-capstone-post-3-baseline-and-midi-frustration/">NLP Capstone Post #4: Baseline and MIDI Frustration</a></h4>
<div class="entry">
<div class="content" lang="en">
<h1><b>Baseline model</b></h1>
<p><span style="font-weight: 400;">Our baseline approach is taken from Daniil Pakhomov’s excellent blog post[1]. In this post, two separate RNNs are trained as generators: one for lyrical content and one for music content (in piano roll format). We will begin by using his trained lyrical model, and attempt to do conditional sampled character generation given a starting sequence of characters. We loaded the already trained models from the blog post and generated lyrics according to the style of a given songwriter and with a given seed word. The lyrics are generated via a character-level LSTM and generates the next character conditioned on the preceding characters and choice of songwriter. The model is trained on a corpus of song lyrics, where naturally the “correct” character to generate is the next character in the lyrics. Essentially the same mechanism is applied to the musical note generation.</span></p>
<p><span style="font-weight: 400;">The characters are encoded as a one-hot vector over all letters in the English alphabet plus space, comma, etc. Piano rolls already come in an encoding amenable to feeding into RNNs, modulo additional zero padding to ensure every time slice of every piano role has the same dimension. In particular, at each time step (discretized in an appropriately fine-grained way), we have an indicator 0-1 vector on which notes are currently activated.</span></p>
<p><span style="font-weight: 400;">Here is a song generated in the style of “Queen” with the starting seed sequence of characters “Music”:</span></p>
<blockquote>
<p style="text-align: center;"><i>Music savor valerite – yah  </i></p>
<p style="text-align: center;"><i>Imabribot, bind me – I – well  </i></p>
<p style="text-align: center;"><i>All going down to L</i></p>
<p style="text-align: center;"><i>At the eyes of the universe  </i></p>
<p style="text-align: center;"><i>Agree, five to the Slim  </i></p>
<p style="text-align: center;"><i>I just want to convincide  </i></p>
<p style="text-align: center;"><i>We wash stars and quiet Ich  </i></p>
<p style="text-align: center;"><i>You had a dirty old baby  </i></p>
<p style="text-align: center;"><i>We won’t  </i></p>
<p style="text-align: center;"><i>We does nothing no one ezy? follohin?  </i></p>
<p style="text-align: center;"><i>Sometimes we get down and ooh  </i></p>
<p style="text-align: center;"><i>Nothing do you see all night  </i></p>
<p style="text-align: center;"><i>  </i></p>
<p style="text-align: center;"><i>This is my pries  </i></p>
<p style="text-align: center;"><i>  </i></p>
<p style="text-align: center;"><i>Joyful the world  </i></p>
<p style="text-align: center;"><i>Does their beams  </i></p>
<p style="text-align: center;"><i>Surgeon makes the scule la beat  </i></p>
<p style="text-align: center;"><i>Walking out on my pocket ride  </i></p>
<p style="text-align: center;"><i>My faulty power  </i></p>
<p style="text-align: center;"><i>I wear from the ston</i></p>
</blockquote>
<p><span style="font-weight: 400;">Eventually, since we are actually interested in converting the musical information into plausible lyrics, we will need to modify this baseline in the natural way to take as input time slices of the musical instrumentation in piano roll format and predict characters (or syllables) that are to be enunciated simultaneously with the played notes. In this manner, the lyrics come already aligned in a natural way, and the words can be extracted by compressing the letters occurring between spaces.</span></p>
<h1><b>Dataset parsing</b></h1>
<p><span style="font-weight: 400;">The MIDI format has an unfortunate number of unexpected caveats. We have spent a majority of our time so far cleaning the data and attempting to use it in existing Python libraries that handle MIDI. A brief description of MIDI[2] covers some of the challenges:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">MIDI is made up of a series of messages, such as notes, instruments, and tempo changes. Additional metadata messages exist called meta messages, which can contain text content such as the song title (and lyrics!). In our dataset, lyrics are provided either as “text” messages or as “lyrics” messages.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Messages are grouped into different tracks, often representing separate instruments. Metadata sometimes is located in its own track, and lyrics are sometimes found in a different track from the rest of the metadata.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Durations in the MIDI format are specified as a delta-time relative to the most recent frame. Delta times are in a unit called a tick. Ticks are defined in the file header as a division of the quarter note. The header also defines the number of ticks per frame, which is what the deltas are relative to. Beats per minute (bpm) messages adjust the speed of playback throughout the song.</span></li>
</ul>
<p><span style="font-weight: 400;">The most promising library so far is PrettyMIDI[3], which handles most of the unexpected behavior of the basic MIDI format. This library wraps MIDI messages into structured python objects, and provides a conversion from MIDI into piano roll format. Piano roll in this case is a numpy array of shape (num_notes, num_frames). This allows us to input the musical data directly into an RNN. The units are also converted into absolute seconds, as opposed to relative durations. PrettyMIDI can additionally handle embedded lyrics, but this has proven to be a challenge due to the variety of annotation styles in our dataset. About 200 of our 900 files have parsed lyric data properly, so continuing to clean our data is a high priority.</span></p>
<h1><strong>U</strong>pd<strong>ate</strong></h1>
<p><span style="font-weight: 400;">Unfortunately, we have found the Kara1k dataset[4] to be inapplicable to our project, as the raw sequence of musical notes and lyrical content are not provided, only metadata that the dataset developers have extracted.</span></p>
<h1>References</h1>
<p><span style="font-weight: 400;">[1] </span><a href="http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/"><span style="font-weight: 400;">http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/</span></a></p>
<p><span style="font-weight: 400;">[2] </span><a href="http://www.music.mcgill.ca/~ich/classes/mumt306/StandardMIDIfileformat.html"><span style="font-weight: 400;">http://www.music.mcgill.ca/~ich/classes/mumt306/StandardMIDIfileformat.html</span></a></p>
<p><span style="font-weight: 400;">[3] </span><a href="http://craffel.github.io/pretty-midi/"><span style="font-weight: 400;">http://craffel.github.io/pretty-midi/</span></a></p>
<p><span style="font-weight: 400;">[4] </span><a href="http://yannbayle.fr/karamir/kara1k.php"><span style="font-weight: 400;">http://yannbayle.fr/karamir/kara1k.php</span></a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/18/nlp-capstone-post-3-baseline-and-midi-frustration/">by Nicholas Ruhland at April 18, 2018 04:58 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 11, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/formal-proposal">
<h4><a href="http://sarahyu.weebly.com/cse-481n/formal-proposal">Formal Proposal</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">Linguistic Accommodation for Self-Presentation as seen in Neurotypical vs. Neurodivergent Subreddits<br /><br /><u>Hypotheses:</u> <br />1) Users change their language depending on the community, represented by subreddits categorized as Neurotypical vs Neurodivergent<br />      - The divergence from their own baseline is a<span> sign of assimilation through tuned self-presentation*</span><br />2) Language Models, as used by the whole community, differ and not just in topic-specific jargon<br />      - The language changes of the individual user and the change in delta from the community's language model is a sign of their attempt at assimilating language accommodation*<br />          - Do certain users adapt better? If so, what is differentiating those users?<br /><font size="1">*(subpoints are very similar, and I'm still working through if there is a nuance, or if they're the same)</font><br /><br />My <strong>objective</strong>, then, is to address these hypotheses through the following approach:<br /><br /><u>Literature Survey:</u><br />While this project is novel, mainly in the focus on neurotypical vs neurodivergent separation and the use of Reddit data, this project finds guidance from previous work done on similar questions. First, this project aims to extend upon the work of Danescu-Niculescu-Mizil et al. in<em> </em><u><em>Mark My Words! Linguistic Style Accommodation in Social Media​.</em></u> This was the first large-scale endeavor in identifying linguistic accommodation using social media. However, our project extends this work by taking advantage of the siloed nature of Reddit to identify linguistic accommodation employed by a single user across communities as opposed to the one-dimensional view of a user's linguistic accommodation to the general twittersphere in Danescu's paper. Also, this project is informed by Fast and Horvitz in <u><em>Identifying Dogmatism in Social Media: Signals and Models</em></u>, specifically in their methodologies and models; I attempt to extend upon these with more complex models. In this process, I also found several works that were similar in nature: Tamburrini et al. on language change based on social identity on Twitter, Nguyen and Rose on language socialization in online communities, and Michael and Otterbacher on herding in online review language. Two more relevant works for my project are De Choudhury et al.'s work on identifying the shift to Suicidal Ideation in social media and D<span>anescu-Niculescu-Mizil's work on the life-cycle of users in online communities. <br />​<br /><u>Proposed Methodologies</u>: </span><br />In it's most basic form, these questions can be explored with basic language models. First, we will identify a subset of neurotypical and neurodivergent subreddits to explore (100 or so respectively), chosen by a preliminary search on overlapping users posting between these. Based on this preliminary search, we will also gain a set of users who potentially post to both neurotypical and neurodivergent subreddits (we may need to look only at posts within a band of characters, but that is a parameter I'd like to explore). We will aggregate all of a user's posting history, not just in the subset aforementioned, to model the user's language use and do the same for the language of all posts made by any user (not just our set) to the subreddit to model the subreddit's language. I will supplement these models with the LIWC lexicon to characterize the differences between the communities and between users in different subreddits. (I may use a subset of the LIWC categories later on). A more complex model would be to use PPDB to find differences via paraphrasing. Yet another complex model would be to use a graphical model as inspired by Bamman et al's <em><u>Learning Latent Personas of Film Characters</u></em>. A stretch goal would be to train an RNN model for the language model of a neurotypical subreddit and that of a neurodivergent to see the probability of a post to belong to either of these categories. A stretch goal (not in complexity as in the RNN, but rather in interest) is to use the <em>Zelig Quotient</em>, a proposed measure for normalizing linguistic accommodation by Jones et al and see how much this may affect our findings. <br />One special consideration is the use of NSFW language. My only filter will be to disqualify the list of NSFW subreddits, as named by a reddit post (so meta) in being chosen for the subsets, but otherwise we will not do anything special for NSFW language in other subreddits. <br /><br /><u>Resources</u>: Lots of Reddit fun!!!<br /><br /><em>​Here goes nothing...</em></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/formal-proposal">April 11, 2018 07:00 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=309">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/11/nlp-capstone-post-3-proposal/">NLP Capstone Post #3: Proposal</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>Here, we finally present our project proposal in full.</p>
<h3>Project Objectives:</h3>
<p>Our goal for this project is to engineer a model that, given the instrumental (“karaoke”) music for a song in English represented as MIDI data, output a coherent sequence of words corresponding to lyrics for the music. The model will produce timings along with the words to align it with the background instrumentals. Additionally, given the output of the model and the input music, we will automate their combination into a song complete with lyrics and supporting instrumentals. This combined output will be playable and we intend to do live demonstration.</p>
<h3>Proposed Methodology:</h3>
<p>Here, we outline the steps we will need to take in detail.</p>
<li>Data collection (datasets of songs, preferably with instrumentals and lyrics already separated)</li>
<li>Decide on vocabulary and how to handle uncommon words</li>
<li>Decide and implement any required preprocessing of the raw MIDI data. Strip lyrics from MIDI data if not already provided in dataset.</li>
<li>Decide and implement model (see Model Design)</li>
<li>Implement model sanity checks</li>
<li>Model tweaking (we expect this will take the majority of the time; see Model Design)</li>
<li>Implement automated combination of model output (lyrics) and model input (instrumentals)</li>
<li>Further testing</li>
<li>Assuming preceding steps are completed satisfactorily, proceed to stretch goals</li>
<li>Presentation and write-up</li>
<h3>Model Design:</h3>
<p>We will pursue a seq2seq RNN approach, taking in input MIDI data represented as a sequence, and outputting a sequence of words from a specified vocabulary. This model will be referred to as the generator. We will employ adversarial training, simultaneously training a many-to-one RNN discriminator that, given the input instrumentals and corresponding lyrics, output if the lyrics were produced by the generator or not. We will follow approaches taken in previous works such as SeqGAN [2] (and [3, 4]), namely using policy gradient ideas from reinforcement learning to obtain gradients that can be backpropagated from the discriminator network through the generator network. We note that syntactic correctness can be enforced in this manner, as malformed lyrical output can be assigned arbitrarily small reward.</p>
<h3>Stretch Goals:</h3>
<p>There are several stretch goals we will consider, time permitted. They are as follows, in no particular order.</p>
<li>Handling multiple languages, particularly those with less available data</li>
<li>Given a specific songwriter/band, produce the instrumentals along with lyrics for a new song that is in the style of that songwriter/band</li>
<li>Lyrics generation for duets, or multi-singer songs</li>
<li>Playing with phoneme-level generation</li>
<h3>Core Challenges:</h3>
<p>The core challenges we will need to overcome include alignment of lyrics with the music, and production of sensible lyrics. On the more technical side, it is well-known that ensuring convergence in adversarial training is difficult.</p>
<h3>Available Resources:</h3>
<p>Existing music datasets for machine learning tasks are made up of audio samples (such as .wav or .mp3), or MIDI data that specifies timing and notes. For karaoke, lyrics are also provided either as a separate text file (.LRC) specifying the timing of each word, or can be embedded into the MIDI file directly (.KAR). It may also be useful to train a lyric model on a larger corpus of song lyrics, since lyrics are easier to collect than fully time-annotated karaoke files.</p>
<p>The MusicNet dataset [9] provides 330 classical instrumental audio files, each of which has associated timing provided for every note. Since we are primarily interested in lyrical generation and alignment, this dataset is not going to be useful for creating a language model.</p>
<p>An existing karaoke dataset called Kara1k [1] provides many features computed from 1000 lyric-annotated songs. This provides lots of metadata about each song, including annotated chords for each timestep of the song. According to the KaraMIR website, these features are extracted from audio samples using Vamp Plugins, which estimates chords with accuracy up to 70%. </p>
<p>We propose a new dataset (name not yet determined) of MIDI karaoke data with embedded lyrics (.KAR). This dataset contains over 700 files, scraped from a karaoke content aggregator [11]. Timed lyrical data has been extracted from these files, and the precise timing of each note is already available by nature of the MIDI format.</p>
<p>Additional datasets for training a lyric model may be useful, and many are available. One such dataset is the 55000+ Song Lyrics on Kaggle [10]. This could help our model generalize its lyrical output beyond the limited set of vocabulary available within the 1000 or fewer annotated karaoke songs.</p>
<h3>Evaluation Plan:</h3>
<p>Evaluation of our model can be done several ways. The first is simply to listen to the music ourselves. This is the most direct method of evaluation but is not efficient, as likely we will need many iterations of tuning; furthermore, will likely need to listen to several songs to be confident of the model’s quality. Hence, we will also design basic “sanity check” tests for our models.</p>
<p>Recall that in our proposed methodology, we intend to use adversarial training. The discriminator network itself gives a direct evaluation of the generator. As long as the discriminator is of vetted quality, and the discriminator is run on sufficiently many examples (with roughly even number of generated and true examples mixed in), the generator will be deemed also of sufficient quality (as a “sanity check”).</p>
<p>Of course, this leaves the question of ensuring the discriminator is good. We can run the discriminator on instrumentals combined with randomly generated words (according to some distribution), or on instrumentals combined with the original lyrics, which are perturbed in some fashion. As an example, one can perturb the original lyrics temporally (making an utterance off-beat when it should be precisely on the down-beat of a bar) or replacing a few words with randomly selected ones (according to some distribution over the vocabulary). These “test inputs” to the discriminator can be generated before-hand.</p>
<h3>Literature Survey:</h3>
<p>Here are some relevant papers (most were already included in preceding posts).</p>
<p>[1] Y. Bayle, L. Marsik, M. Rusek, M. Robine, P. Hanna, K. Slaninova, J. Martinovic, J. Pokorny. “Kara1k: A Karaoke Dataset for Cover Song Identification and Singing Voice Analysis”. IEEE International Symposium on Multimedia (ISM), 2017. <a href="https://ieeexplore.ieee.org/document/8241597/" rel="nofollow">https://ieeexplore.ieee.org/document/8241597/</a></p>
<p>[2] L. Yu, W. Zhang, J. Wang, Y. Yu. “SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient”. Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, 2017. <a href="https://arxiv.org/abs/1609.05473" rel="nofollow">https://arxiv.org/abs/1609.05473</a></p>
<p>[3] S. Lee, U. Hwang, S. Min, S. Yoon. “A SeqGAN for Polyphonic Music Generation”. 2017. <a href="https://arxiv.org/abs/1710.11418" rel="nofollow">https://arxiv.org/abs/1710.11418</a></p>
<p>[4] H. W. Dong, W. Y. Hsiao, L. C. Yang, Y. H. Yang. “MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment”. 2017. <a href="https://arxiv.org/abs/1709.06298" rel="nofollow">https://arxiv.org/abs/1709.06298</a></p>
<p>[5] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio. “Generative Adversarial Nets”. NIPS, 2014. <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets" rel="nofollow">https://papers.nips.cc/paper/5423-generative-adversarial-nets</a></p>
<p>[6] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, X. Chen. “Improved Techniques for Training GANs”. NIPS, 2016. <a href="https://arxiv.org/abs/1606.03498" rel="nofollow">https://arxiv.org/abs/1606.03498</a></p>
<p>[7] M. Arjovsky,  S. Chintala, L. Bottou. “Wasserstein GAN”. 2017. <a href="https://arxiv.org/abs/1701.07875" rel="nofollow">https://arxiv.org/abs/1701.07875</a></p>
<p>[8] J. Faille, Y. Wang. “Using Deep Learning to Annotate Karaoke Songs”. 2016. <a href="https://www.semanticscholar.org/paper/Using-Deep-Learning-to-Annotate-Karaoke-Songs-Faille-Wang/521361762a7327f8fcc77bd9d76eaa2b503f845a" rel="nofollow">https://www.semanticscholar.org/paper/Using-Deep-Learning-to-Annotate-Karaoke-Songs-Faille-Wang/521361762a7327f8fcc77bd9d76eaa2b503f845a</a></p>
<p>[9] J. Thickstun, Z. Harchaoui, S. Kakade. “Learning Features of Music from Scratch”. 2017. <a href="https://arxiv.org/abs/1611.09827" rel="nofollow">https://arxiv.org/abs/1611.09827</a></p>
<p>[10] Additional data <a href="https://www.kaggle.com/mousehead/songlyrics">here</a></p>
<p>[11] Even more additional data <a href="http://vooch.narod.ru/midi/midi.htm">here</a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/11/nlp-capstone-post-3-proposal/">by Kuikui Liu at April 11, 2018 06:45 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 06, 2018</h2>

<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=304">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/06/milestone-2-music-as-a-natural-language-task/">Milestone #2: Music as a Natural Language Task</a></h4>
<div class="entry">
<div class="content" lang="en">
<h3>Framing the problem</h3>
<p>The focus of Natural Language Processing relies on patterns in the structure of language and models that find ways to encode the complexities of these structures. Many forms of music also have large amounts of structure which could potentially be discovered using similar models as a standard natural language.</p>
<p>Music datasets for machine learning purposes have recently become available through projects like MusicNet in 2016 [1]. This music is primarily classical, and provided as both audio and MIDI.</p>
<h3>Project ideas</h3>
<p>For our project we are interested in music with lyrical content – both for the potential to create a creative demo and for the interest of making this a language task. The current direction we are most interested in is the generation of lyrics for a song, given its nonlyrical content. This will be broken up into subtasks depending on the feasible scale of the project. Not all of the following points will necessarily be parts of our project, but we will use them as as starting point as we see the success of our models.</p>
<ul>
<li>Creating a machine learning model for MIDI music</li>
<li>Translating MIDI into specific artists or styles</li>
<li>Creating models for the lyrical content of specific artists or styles of music</li>
<li>Generating lyrics given an artist or style</li>
<li>Seq2seq conversion of MIDI into lyrical content</li>
<li>GANs for either side of the conversion – MIDI encoding or lyrical generating</li>
</ul>
<h3>Using MIDIs in RNNs</h3>
<p>Work by Pakhomov [2] has already used RNNs to create models for lyrics. In his <a href="http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/">blog post</a> he additionally discusses a method for forming any MIDI into piano roll format. This is essentially a matrix where each column represents a different time step, and each row represents a different note. Having a 1 corresponds to that note sounding at that time. The individual time vectors can be used as the inputs to an RNN at each time step to create a model representing the various songs.</p>
<p>One possible data source for our project is karaoke data available from various sources online. If available in large enough quantities this could be extremely convenient because it already contains many pairings of MIDI music to their lyrics.</p>
<h3>Azure</h3>
<p>We intend to use PyTorch to train our models, and have begun setting up an instance on Microsoft Azure.</p>
<h3>Relevant work</h3>
<p>[1] <a href="https://homes.cs.washington.edu/~thickstn/musicnet.html" rel="nofollow">https://homes.cs.washington.edu/~thickstn/musicnet.html</a></p>
<p>[2] <a href="http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/" rel="nofollow">http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/</a></p>
<p>[3] Dong, Hao-Wen. 2017. MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment. <a href="https://arxiv.org/pdf/1709.06298" rel="nofollow">https://arxiv.org/pdf/1709.06298</a></p>
<p>[4] Yu, Lantao. 2016. SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. <a href="https://arxiv.org/abs/1609.05473" rel="nofollow">https://arxiv.org/abs/1609.05473</a></p>
<p>[5] Lee, Sang-gil. 2017. A SeqGAN for Polyphonic Music Generation. <a href="https://arxiv.org/abs/1710.11418" rel="nofollow">https://arxiv.org/abs/1710.11418</a></p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/06/milestone-2-music-as-a-natural-language-task/">by Nicholas Ruhland at April 06, 2018 06:30 AM</a>
</p>
</div>
</div>



</div>
<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2">
<h4><a href="http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2">Technical Details (Blog Post #2)</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph"><span style="color: rgb(0, 0, 0);">For my project I am planning to do some deep learning at the end if I have time and if the results up to that point lead to that track. (I have pytorch installed from NLP so that's nice to have). <br /><br />With that said, I've been working with the Reddit API's and Reddit datadumps to get started on gathering the necessary data for pursuing the Language Accommodation project. I've been trying to figure out if the best approach is to work with the limited requests, the direct json files, or if some of the data dumps will suffice. I hope to have most of that and some basic data visualizations ready in the next couple of days to inform some of the choices I should make regarding the data (i.e. what time period to gather data from, what subreddits to pull from, etc.)</span><br /></div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2">April 06, 2018 12:10 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 04, 2018</h2>

<div class="channelgroup">







<h3><a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a></h3>


<div class="entrygroup" id="http://mathstoc.wordpress.com/?p=277">
<h4 lang="en"><a href="https://mathstoc.wordpress.com/2018/04/04/nlp-capstone-post-1-ideation/">NLP Capstone Post #1: Ideation</a></h4>
<div class="entry">
<div class="content" lang="en">
<p>In this post, I’d like to briefly discuss three different ideas I have for my capstone project.</p>
<p>UPDATE (04/05/2018): I am fortunate to be joined by a fellow student, Nicholas Ruhland, for this capstone project.</p>
<h1>A Theoretical Analysis of RNNs (Research Mode):</h1>
<p> A recent <a href="https://arxiv.org/abs/1703.00810">paper of Professor Naftali Tishby</a> provided some useful observations on the behavior of feedforward neural networks, and proposed a promising approach to understanding their performance. Earlier empirical work done in the vision community showed that when a convolutional neural network is trained, layers closer to the input learn lower level features (such as edges and corners) and layers closer to the output learn higher level features (“this part of the image resembles a nose, and this other part resembles an eye”). One might expect similar behavior to occur with general feedforward neural networks: that earlier layers learn lower level features of the input and later levels learn higher level features of the input. The key insight here was to think of each layer of a neural network as a Markov chain, where each layer <img alt="L_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{i}" /> is a (vector-valued) random variable that is conditionally independent of <img alt="L_{j}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{j}" /> for all <img alt="j &lt; i - 1" class="latex" src="https://s0.wp.com/latex.php?latex=j+%3C+i+-+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="j &lt; i - 1" /> given <img alt="L_{i-1}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bi-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{i-1}" />. In this way, information flowing forward in the network can be quantified via notions of entropy from traditional information theory.</p>
<p>The paper contains some empirical work, observing that there are generally two phases to learning artificial neural networks via stochastic gradient descent: the fitting phase, and the compression phase. The fitting phase is the shorter phase, where the model is quickly tuning itself to minimize the empirical loss function. At the end of this phase, we don't necessarily have a model that will generalize to new data. The compression phase is where the model begins to learn the relevant features in the input, with the intuition that there are many irrelevant parts of the input (I don't need to know every atom in an object to identify it). </p>
<p>The goal of this project would be to perform a similar theoretical analysis and empirical work for RNN architectures (whose "natural" Markov chain isn't as simple, as there are cycles) on some traditional NLP task, such as Machine Translation, with the goal of studying the flow of information in an RNN architecture, rather than performing comparably to state-of-the-art Machine Translation models (although this can be a stretch goal).</p>
<p>The relevant steps in this project will likely look like the following:<br />
1. Reading up on the relevant work by Tishby et. al. (and any other theoretical papers on deep learning).<br />
2. Understand basic and traditional RNN architectures.<br />
3. Learning PyTorch.<br />
4. Implementing several of these architectures and testing (for example, to see if learning also comes in two distinct phases: fitting and compression)<br />
5. Using these empirical observations, and information theory to analyze these architectures.<br />
6. Time permitted, play around with new RNN architectures.</p>
<h1>Musical Style Learning from Musical Scores (Research/Start-Up Mode):</h1>
<p> This idea lies somewhat outside traditional NLP in that it tackles the language of music. While the alphabet of a musical score consist chiefly of the 12 musical notes, there is added challenge in that several notes may be played simultaneously, especially if there are several instruments involved or simply the two hands of a pianist. Furthermore, the exact timing of each note played matters, note merely the ordering of the notes.</p>
<p>The idea here is simply to, given the score of a musical piece, represented as a sequence of notes at each time, predict the era (Baroque, Classical, Romantic, etc.) or even, the composer of the piece (Bach, Beethoven, Brahms, etc.) There are several problems to be solved step by step for this project.</p>
<p>1. Data collection from a large library of musical scores (ex: <a href="http://imslp.org/">IMSLP</a>)<br />
2. Data formatting so as to be usable.<br />
3. Model selection.<br />
4. Model implementation (PyTorch).<br />
5. Model testing.</p>
<p>There are also several extensions that can be viewed as stretch goals. For these, the first two can be reused.</p>
<h3>Musical Score Generation:</h3>
<p> Now, we learn how to compose a piece that “sounds” similar to a given composer. This will involve learning from the pieces written by a given input composer, and outputting a new piece. One core challenge here is ensuring that the output is syntactically correct.</p>
<h1>Story Illustration (Start-Up Mode):</h1>
<p> Given a short story and a specific scene (or place in the text), produce an image that is representative of the scene. This project combines aspects of NLP and vision. This project may also explore generative adversarial methods. One well-known challenge here is convergence.</p>
<p>Here are the general steps for this project:<br />
1. Data collection (image captioning dataset can be helpful)<br />
2. Model selection.<br />
3. Model implementation (PyTorch).<br />
4. Model testing.</p>
<p>As an extension, one can also generate several frames to form a short “movie”. Another can be comic book pane generation.</p></div>







<p class="date">
<a href="https://mathstoc.wordpress.com/2018/04/04/nlp-capstone-post-1-ideation/">by Kuikui Liu at April 04, 2018 06:53 AM</a>
</p>
</div>
</div>


</div>

</div>
<div class="daygroup">
<h2>April 03, 2018</h2>

<div class="channelgroup">







<h3><a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a></h3>


<div class="entrygroup" id="http://sarahyu.weebly.com/cse-481n/inaugural-blog-post">
<h4><a href="http://sarahyu.weebly.com/cse-481n/inaugural-blog-post">Inaugural Blog Post</a></h4>
<div class="entry">
<div class="content">
<div class="paragraph">Welcome to the first blog post of <u><em>Jekyll-Hyde</em></u><em> </em><u>(</u>my very cool and somewhat related group-of-1 name).<br />As the name might reveal, I’m interested in using NLP to uncover the duality of language, the ability to simultaneously present both sides of a coin, whether in everyday conversation or more curated prose. Because language matters; so much so that we mend and mold our language to navigate the different social environments and spaces we inhabit, whether for power, survival, or acceptance, and often in the most primitive and subconscious ways. And because in today’s (supposedly) civilized world, the language we employ with another can be a sort of proxy for the relationship we share. I'd like to get at some of these ideas through the NLP capstone and think the next three topics are a potential start. <br /><br />1) Language Accommodation (or my unlikely paper title: <em>Nice Guy by Day, A**hole by Night: Language Accommodation for Self-Presentation in Subreddit Communities)</em><ul><li>​MVP: Scrape Reddit user data, identify language baselines for a given subreddit or capture linguistic differences of a single user across subreddits</li><li>Stretch Goals: Do both (subreddit baselines and user difference) and not only identify a user's language accommodation, but how they fall in line with the communities' baseline (a kind of hive mentality)</li></ul><br />2) Identifying Condescension (another working title: "<em>Well, Actually": Identifying Ambiguities in emails from 'helpful' colleagues</em>)<ul><li>​MVP: Identify an appropriate data source (ideally emails or more personal interactions), manually identify possible ambiguities, train model (maybe one that doesn't require a large dataset) to identify ambiguous spans of a sentence. </li><li>Stretch Goals: Begin identifying entity-entity-relationships with cues from ambiguous interactions or maybe something cooler about context and the different meanings if ambiguous...</li></ul><br />3) Identifying Disrespect (might as well for consistency: <em>Linguistic (dis)R.E.S.P.E.C.T. - Addressing 90% of Comment Sections)</em><ul><li>​MVP: Scrape Youtube comment data, classify comments as hateful/not hateful, train model on classified comments, test and tune</li><li>Stretch Goals: Train a portable model that can work with content from other mediums such as Twitter and Reddit</li></ul><br />I'll be pursuing one of these ideas in <strong>research mode</strong> and you can follow along at:<br />                                      https://github.com/sarahyu17/481n</div>  <div class="wsite-spacer" style="height: 50px;"></div>  <div> 				<form action="http://www.weebly.com/weebly/apps/formSubmit.php" enctype="multipart/form-data" id="form-147197638403517045" method="POST"> 					<div class="wsite-form-container" id="147197638403517045-form-parent" style="margin-top: 10px;"> 						<ul class="formlist" id="147197638403517045-form-list"> 							<h2 class="wsite-content-title">Any Favorite Paper Titles?</h2>  <label class="wsite-form-label wsite-form-fields-required-label"><span class="form-required">*</span> Indicates required field</label><div><div class="wsite-form-field" style="margin: 5px 0px 0px 0px;">   <label class="wsite-form-label" for="input-789590629342031487">Paper Title <span class="form-required">*</span></label>   <div class="wsite-form-radio-container">     <span class="form-radio-container"><input id="radio-0-_u789590629342031487" name="_u789590629342031487" type="radio" value="#1" /><label for="radio-0-_u789590629342031487">#1</label></span><span class="form-radio-container"><input id="radio-1-_u789590629342031487" name="_u789590629342031487" type="radio" value="#2" /><label for="radio-1-_u789590629342031487">#2</label></span><span class="form-radio-container"><input id="radio-2-_u789590629342031487" name="_u789590629342031487" type="radio" value="#3" /><label for="radio-2-_u789590629342031487">#3</label></span>   </div>   <div class="wsite-form-instructions" id="instructions-Paper Title" style="display: none;"></div> </div></div> 						</ul> 					</div> 					<div style="display: none;"> 						<input name="weebly_subject" type="text" /> 					</div> 					<div style="text-align: left; margin-top: 10px; margin-bottom: 10px;"> 						<input name="form_version" type="hidden" value="2" /> 						<input id="weebly-approved" name="weebly_approved" type="hidden" value="approved" /> 						<input name="ucfid" type="hidden" value="147197638403517045" /> 						<input name="recaptcha_token" type="hidden" /> 						<input name="opted_in" type="hidden" value="0" /> 						<input type="submit" /> 						<a class="wsite-button"> 							<span class="wsite-button-inner">vote</span> 						</a> 					</div> 				</form> 				<div class="recaptcha" id="g-recaptcha-147197638403517045"></div> 			  			</div></div>







<p class="date">
<a href="http://sarahyu.weebly.com/cse-481n/inaugural-blog-post">April 03, 2018 07:00 AM</a>
</p>
</div>
</div>


</div>

</div>


<div class="sidebar">

<h2>Subscriptions</h2>
<ul>
<li>
<a href="https://mathstoc.wordpress.com/category/nlp-capstone/feed/" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://mathstoc.wordpress.com" title="NLP Capstone – Mathematical Distractions">Kuikui Liu, Nicholas Ruhland <br/> Team INLP</a>
</li>
<li>
<a href="https://teamoverfit.blogspot.com/feeds/posts/default?alt=rss" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://teamoverfit.blogspot.com/" title="NLP Capstone">Pinyi Wang, Dawei Shen, Xukai Liu <br/> Team Overfit</a>
</li>
<li>
<a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="https://cse481n.blogspot.com/" title="PrimeapeNLP">Ron Fan, Aditya Saraf <br/> Team PrimeapeNLP</a>
</li>
<li>
<a href="http://sarahyu.weebly.com/6/feed" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)"></a> <a href="http://sarahyu.weebly.com/cse-481n" title="Sarah yu - CSE 481N">Sarah Yu <br/> Team Jekyll-Hyde</a>
</li>
</ul>

<p>
<strong>Last updated:</strong><br>
August 16, 2018 08:07 PM<br>
<em>All times are UTC.</em><br>

<!--
<br>
Powered by:<br>
<a href="http://www.planetplanet.org/"><img src="images/planet.png" width="80" height="15" alt="Planet" border="0"></a>
</p>

<p>
<h2>Planetarium:</h2>
<ul>
<li><a href="http://www.planetapache.org/">Planet Apache</a></li>
<li><a href="http://planet.freedesktop.org/">Planet freedesktop.org</a></li>
<li><a href="http://planet.gnome.org/">Planet GNOME</a></li>
<li><a href="http://planet.debian.net/">Planet Debian</a></li>
<li><a href="http://planet.fedoraproject.org/">Planet Fedora</a></li>
<li><a href="http://planets.sun.com/">Planet Sun</a></li>
<li><a href="http://www.planetplanet.org/">more...</a></li>
</ul>
</p>
!-->
</div>
</body>

</html>
