<?xml version="1.0"?>
<rdf:RDF
	xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:foaf="http://xmlns.com/foaf/0.1/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns="http://purl.org/rss/1.0/"
>
<channel rdf:about="https://nlpcapstone.github.io/">
	<title>NLP Capstone Spring 2018</title>
	<link>https://nlpcapstone.github.io/</link>
	<description>NLP Capstone Spring 2018 - https://nlpcapstone.github.io/</description>
	<atom:link rel="self" href="https://nlpcapstone.github.io/rss10.xml" type="application/rss+xml"/>

	<items>
		<rdf:Seq>
			<rdf:li rdf:resource="https://medium.com/p/ab3d796c422e" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-5600014144802012716.post-6768023392170538237" />
		</rdf:Seq>
	</items>
</channel>

<item rdf:about="https://medium.com/p/ab3d796c422e">
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Project Ideas</title>
	<link>https://medium.com/@ryanp97/project-ideas-ab3d796c422e?source=rss-6378d85d3a9b------2</link>
	<content:encoded>&lt;p&gt;I plan on following a research track and hope to pursue one of the following ideas:&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Neural Machine Translation with Semantic Transfer&lt;/strong&gt; (as outlined by Jan Buys)&lt;/h4&gt;&lt;p&gt;&lt;em&gt;Minimal Viable Action Plan:&lt;/em&gt; &lt;br /&gt;1) Use statistical parser (&lt;a href=&quot;http://sweaglesw.org/linguistics/ace/&quot;&gt;ACE&lt;/a&gt;) to get MRS graphs of English/Japanese sentences and convert to DMRS graph (using &lt;a href=&quot;https://github.com/delph-in/pydelphin&quot;&gt;PyDelphin&lt;/a&gt; interface to do parsing and conversion)&lt;br /&gt;2) Linearize DMRS graph&lt;br /&gt;3) Train a seq2seq that takes linearized English DMRS graph and outputs linearized Japanese DMRS graph&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*J3l_TjWr3A-jOAcrCznqvA.png&quot; /&gt;Example of penmen format that DMRS can be represented with (non-linearized). Note that the representation shown is an AMR graph, not a DMRS graph. Figure taken from this &lt;a href=&quot;https://arxiv.org/pdf/1704.08381.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Stretch Goals:&lt;br /&gt;&lt;/em&gt;1) Explore different architectures for semantic transfer (e.g. TreeLSTM as opposed to seq2seq)&lt;br /&gt;2) Explore ways to learn correspondences between semantic concepts in the two languages&lt;/p&gt;&lt;h4&gt;DMRS to Text Generation (as outlined by Jan Buys)&lt;/h4&gt;&lt;p&gt;&lt;em&gt;Minimal Viable Action Plan:&lt;br /&gt;&lt;/em&gt;1) Generate DMRS graph serialization similar to what was outlined above&lt;br /&gt;2) Train seq2seq model to generate text directly from graph serialization&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Bndfdhs3ixwG6-JUQPC72A.png&quot; /&gt;Example of different graph representations. Figure taken from this &lt;a href=&quot;http://www.lrec-conf.org/proceedings/lrec2016/pdf/634_Paper.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Stretch Goals:&lt;/em&gt;&lt;br /&gt;1) Experiment with different seq2seq architectures&lt;br /&gt;2) Attempt semi-supervised training using a high-precision grammar-based parser&lt;/p&gt;&lt;h4&gt;Reproduce results / expand on a Paper&lt;/h4&gt;&lt;p&gt;In this option, I would be attempting to expand on this &lt;a href=&quot;https://arxiv.org/pdf/1704.04859.pdf&quot;&gt;paper&lt;/a&gt; regarding hybrid models. The paper attempts to alleviate and/or solve the issue of out-of-vocabulary words and characters, specifically in Chinese and Japanese. It describes using a CNN in conjunction with an RNN to do so; the CNN learns the radicals of characters and attempts to choose characters with similar radicals since the meaning of radicals remains constant between characters.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/822/1*5GyInYVxc8ifP_YRCHiHHA.png&quot; /&gt;Image taken from the linked paper.&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Minimal Viable Action Plan:&lt;br /&gt;&lt;/em&gt;1) Obtain/create a visual dataset of the characters in order to train the CNN&lt;br /&gt;2) Design and explore different methods for joining the two models such as described in the paper&lt;br /&gt;3) Train the model end-to-end&lt;/p&gt;&lt;p&gt;&lt;em&gt;Stretch Goals:&lt;br /&gt;&lt;/em&gt;1) Error analysis on different methods for joining and potential points of error&lt;br /&gt;2) Exploration of model variations and architecture (incremental changes similar to this &lt;a href=&quot;https://arxiv.org/pdf/1708.04755.pdf&quot;&gt;paper&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;For current progress, visit the &lt;a href=&quot;https://github.com/ryanp97/NeuralEmpty&quot;&gt;repo&lt;/a&gt;.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=ab3d796c422e&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-03T00:55:49+00:00</dc:date>
	<dc:creator>Ryan Pham</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-5600014144802012716.post-6768023392170538237">
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog Post 1</title>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/03/blog-post-1.html</link>
	<content:encoded>&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: arial;&quot;&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt;I am interested in working in the code generation/semantic parsing space on the research track. My code &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: arial;&quot;&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt;will be in various branches of my fork of allennlp (https://github.com/rajasagashe/allennlp). I will keep you&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: arial;&quot;&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt; updated on which branch/commits I worked on during each blog post. Also note that project idea 1 has &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: arial;&quot;&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt;the most detail since I have picked it as my project!&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;h2 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;&quot;&gt;&lt;span&gt;Project Idea 1&lt;/span&gt;&lt;/h2&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Minimum Viable Plan:&lt;/span&gt;&lt;span&gt; Implement the model in the recent UW paper which introduces the task of &lt;/span&gt;&lt;br /&gt;&lt;span&gt;generating the code for a java function from a natural language description. To further aid code &lt;/span&gt;&lt;br /&gt;&lt;span&gt;generation, the class in which the generated function is to reside is provided, i.e. the class variables &lt;/span&gt;&lt;br /&gt;&lt;span&gt;and methods. Thus the encoder encodes the class as well as the utterance and the decoder uses a &lt;/span&gt;&lt;br /&gt;&lt;span&gt;two step attention mechanism and decodes through the java grammar production rules. &lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Stretch Goals:&lt;/span&gt;&lt;span&gt; Reproduce the state of the art results in the paper. I’m putting this in the stretch goals &lt;/span&gt;&lt;br /&gt;&lt;span&gt;since successfully implementing a neural semantic parser with type constraints is pretty challenging. &lt;/span&gt;&lt;br /&gt;&lt;span&gt;In addition, I hope to experiment with other improvements like encoding the entire class method body&lt;/span&gt;&lt;br /&gt;&lt;span&gt; which wasn’t done.&lt;/span&gt;&lt;/div&gt;&lt;h2 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;&quot;&gt;&lt;span&gt;Project Idea 2&lt;/span&gt;&lt;/h2&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Minimum Viable Plan: &lt;/span&gt;&lt;span&gt;Implement this paper: &lt;/span&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.01696.pdf&quot; style=&quot;text-decoration: none;&quot;&gt;&lt;span&gt;https://arxiv.org/pdf/1704.01696.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;. The model is &lt;/span&gt;&lt;br /&gt;&lt;span&gt;similar to that of the previous idea, but the datasets are for python and ifttt instead.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Stretch Goals: &lt;/span&gt;&lt;span&gt;Improve the paper’s result.&lt;/span&gt;&lt;/div&gt;&lt;h2 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;&quot;&gt;&lt;span&gt;Project Idea 3&lt;/span&gt;&lt;/h2&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Minimum Viable Plan: &lt;/span&gt;&lt;span&gt;Perform transfer learning across several code generation tasks by using the &lt;/span&gt;&lt;br /&gt;&lt;span&gt;same encoder for them all. This technique would be similar to what was used in the Cove paper &lt;/span&gt;&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.00107.pdf&quot; style=&quot;text-decoration: none;&quot;&gt;&lt;span&gt;https://arxiv.org/pdf/1708.00107.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;. &lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Stretch Goals: &lt;/span&gt;&lt;span&gt;Improve the individual paper results with this technique.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-03-30T22:41:00+00:00</dc:date>
	<dc:creator>nlpcapstone</dc:creator>
</item>

</rdf:RDF>
