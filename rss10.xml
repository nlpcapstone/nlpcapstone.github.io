<?xml version="1.0"?>
<rdf:RDF
	xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:foaf="http://xmlns.com/foaf/0.1/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns="http://purl.org/rss/1.0/"
>
<channel rdf:about="https://nlpcapstone.github.io/">
	<title>NLP Capstone Spring 2018</title>
	<link>https://nlpcapstone.github.io/</link>
	<description>NLP Capstone Spring 2018 - https://nlpcapstone.github.io/</description>
	<atom:link rel="self" href="https://nlpcapstone.github.io/rss10.xml" type="application/rss+xml"/>

	<items>
		<rdf:Seq>
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-5600014144802012716.post-8581531123022969204" />
			<rdf:li rdf:resource="https://medium.com/p/4626fe37cadb" />
			<rdf:li rdf:resource="https://medium.com/p/d73810e0c390" />
			<rdf:li rdf:resource="https://medium.com/p/d01e84c5e1da" />
			<rdf:li rdf:resource="https://medium.com/p/6f773ae418d0" />
			<rdf:li rdf:resource="http://deeplearningturingtest.wordpress.com/?p=21" />
			<rdf:li rdf:resource="http://cse481n-capstone.azurewebsites.net/?p=64" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-9203775015655831448.post-3361785683406757277" />
			<rdf:li rdf:resource="https://medium.com/p/e05ee9a7eaa8" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-5600014144802012716.post-7248717662742163548" />
			<rdf:li rdf:resource="https://medium.com/p/e7e525549f94" />
			<rdf:li rdf:resource="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1" />
			<rdf:li rdf:resource="http://deeplearningturingtest.wordpress.com/?p=18" />
			<rdf:li rdf:resource="https://medium.com/p/306dca636d3a" />
			<rdf:li rdf:resource="https://medium.com/p/27c90aa3c1aa" />
			<rdf:li rdf:resource="https://medium.com/p/a21b51cdd27c" />
			<rdf:li rdf:resource="https://medium.com/p/fd61ae20a2f1" />
			<rdf:li rdf:resource="https://medium.com/p/79ed786e5c74" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-9203775015655831448.post-597849658553454254" />
			<rdf:li rdf:resource="http://cse481n-capstone.azurewebsites.net/?p=51" />
			<rdf:li rdf:resource="http://mathstoc.wordpress.com/?p=323" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-5600014144802012716.post-1597347106413431292" />
			<rdf:li rdf:resource="http://sarahyu.weebly.com/cse-481n/actual-strawman-update" />
			<rdf:li rdf:resource="https://medium.com/p/79ba10bbc70c" />
			<rdf:li rdf:resource="http://deeplearningturingtest.wordpress.com/?p=16" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-5600014144802012716.post-2741407940361589303" />
			<rdf:li rdf:resource="https://medium.com/p/be87c31976b7" />
			<rdf:li rdf:resource="https://medium.com/p/a6690114c441" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-3753031463594823927.post-8569998071322028844" />
			<rdf:li rdf:resource="https://medium.com/p/15357a82fe06" />
			<rdf:li rdf:resource="http://mathstoc.wordpress.com/?p=314" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-9203775015655831448.post-1250926726356516395" />
			<rdf:li rdf:resource="http://sarahyu.weebly.com/cse-481n/first-strawman-attempt" />
			<rdf:li rdf:resource="http://cse481n-capstone.azurewebsites.net/?p=41" />
			<rdf:li rdf:resource="https://medium.com/p/d1d1b2d1f34c" />
			<rdf:li rdf:resource="https://medium.com/p/7de5277b5be" />
			<rdf:li rdf:resource="https://medium.com/p/613328b9a85e" />
			<rdf:li rdf:resource="https://medium.com/p/7d8e9ec1a8e3" />
			<rdf:li rdf:resource="https://medium.com/p/b951950ad9a5" />
			<rdf:li rdf:resource="http://sarahyu.weebly.com/cse-481n/formal-proposal" />
			<rdf:li rdf:resource="http://deeplearningturingtest.wordpress.com/?p=14" />
			<rdf:li rdf:resource="https://medium.com/p/7b6d1a9ec67c" />
			<rdf:li rdf:resource="https://medium.com/p/c8a12d3ae611" />
			<rdf:li rdf:resource="http://mathstoc.wordpress.com/?p=309" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-3753031463594823927.post-4531878816260312232" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-9203775015655831448.post-5878905571398539101" />
			<rdf:li rdf:resource="https://medium.com/p/43368563cf97" />
			<rdf:li rdf:resource="http://cse481n-capstone.azurewebsites.net/?p=37" />
			<rdf:li rdf:resource="https://medium.com/p/a1903faeadb7" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-5600014144802012716.post-8898628104121215850" />
			<rdf:li rdf:resource="https://medium.com/p/45c89bec2c2e" />
			<rdf:li rdf:resource="http://deeplearningturingtest.wordpress.com/?p=12" />
			<rdf:li rdf:resource="https://medium.com/p/96fb908765f5" />
			<rdf:li rdf:resource="https://medium.com/p/278789e4d04a" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-3753031463594823927.post-2253300890173394060" />
			<rdf:li rdf:resource="http://mathstoc.wordpress.com/?p=304" />
			<rdf:li rdf:resource="https://medium.com/p/3a2f40b355a5" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-9203775015655831448.post-34377626932024049" />
			<rdf:li rdf:resource="https://medium.com/p/3d3651220219" />
			<rdf:li rdf:resource="http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2" />
		</rdf:Seq>
	</items>
</channel>

<item rdf:about="tag:blogger.com,1999:blog-5600014144802012716.post-8581531123022969204">
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog 7</title>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/05/blog-7.html</link>
	<content:encoded>&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;I've read through a lot of the data, to understand the data set and view the different types of error &lt;/span&gt;&lt;br /&gt;&lt;span&gt;cases. in general, most(⅔) of the data is just noise, meaning that a programmer won't be able to &lt;/span&gt;&lt;br /&gt;&lt;span&gt;generate the target code from the utterance. &lt;/span&gt;&lt;/div&gt;&lt;b id=&quot;docs-internal-guid-42510a76-41f6-9607-1678-1da3293c9502&quot; style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Error breakdown(whats consistently wrong). Rule based fixes.&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Not understanding functions versus fields&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;return func_debug; or func_x = …&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Fix &quot;Expression--&amp;gt;Expression___(___)&quot;, &lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Expresion -&amp;gt;identifer&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;identifier-&amp;gt;classfields variables&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;&quot;Expression--&amp;gt;Expression___.___Nt_33&quot;&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;If parent has parenthesis then allow nt_33 to generate functions otherwise fields ok&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Not able to initialize correctly&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;List y = new File&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;sriniclass_event = new StatisticGenerationEvent()&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt; text-indent: 36pt;&quot;&gt;&lt;span&gt;&quot;Expression--&amp;gt;Expression___Nt_68___Expression&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Expression--&amp;gt;Primary&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Primary--&amp;gt;IdentifierNT&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;IdentifierNT--&amp;gt;sriniclass_event&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Nt_68--&amp;gt;=&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Expression--&amp;gt;new___Creator&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Creator--&amp;gt;CreatedName___Nt_37&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;CreatedName--&amp;gt;IdentifierNT&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;IdentifierNT--&amp;gt;StatisticGenerationEvent&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Nt_37--&amp;gt;ClassCreatorRest&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;ClassCreatorRest--&amp;gt;Arguments&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Arguments--&amp;gt;(___ExpressionList___)&quot;&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Generics incorrect and missing generics&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span&gt;List&amp;lt;List&amp;gt;  Map&amp;lt;Map, Map&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;span&gt;&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span&gt;List x&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;I want pivot now since adding all these rules doesn't seem interesting from a language perspective. I am considering finding a filtering heuristic to run on this dataset to get better quality utterances.&lt;/span&gt;</content:encoded>
	<dc:date>2018-05-08T22:55:00+00:00</dc:date>
	<dc:creator>nlpcapstone</dc:creator>
</item>
<item rdf:about="https://medium.com/p/4626fe37cadb">
	<title>Aaron Johnston, Lynsey Liu &lt;br/&gt; Team Viterbi Or Not To Be: Advanced Model #1, Part 1</title>
	<link>https://medium.com/@viterbi.or.not/advanced-model-1-part-1-4626fe37cadb?source=rss-c522ef075bb3------2</link>
	<content:encoded>&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*_Pn2CYQZkoIzZYLwVPV2MA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Work on our advanced model is underway! To recap, we aim to upgrade our baseline model by including other forms of conversational data (chatlogs and meeting transcripts in addition to the email data we have been using) into our training and adding features that target characteristics specific to conversational text.&lt;/p&gt;&lt;h4&gt;Extensibility and Other Datasets&lt;/h4&gt;&lt;p&gt;To achieve this, we first worked on restructuring our baseline code to be extensible to allow addition of inputs from other datasets and to allow different methods of feature extraction based on the dataset. We’ve started splitting our baseline code into smaller parts (parser, feature extractor, model) and creating a structure for the new code segments and datasets. We’ve also parsed one additional data type (GNUe chatlogs) and begun the process of integrating the data into the structure. Unfortunately, we have not yet been able to produce summaries using this dataset — while we have the parser working, several of our features and evaluation scripts do not translate easily to the new formatting and we are still working on refactoring our codebase to handle it. As we complete our first advanced model, bringing these new data sources into the fold will be a top priority so that we can begin to compare the differences between the dataset and work toward our stretch goal of training on both datasets in order to improve performance on each.&lt;/p&gt;&lt;h4&gt;Preprocessing&lt;/h4&gt;&lt;p&gt;Looking at the generated summaries from our baseline model, we found a few mistakes that could be easily avoided using automated preprocessing. The summaries often included quoted replies, email signatures, and other insignificant lines that could be eliminated by looking at simple characteristics of the sentence (for example, a line or sentence ending in a comma usually indicates that it is a greeting or closing clause). We previously added a feature based on the sentence starting with a ‘&amp;gt;’ character to eliminate sentences that were quotes from previous emails in the thread, which improved performance. We are now moving this to the preprocessing step. The preprocessing will occur during after parsing the dataset, but before feature extraction, and because it is highly specific to the types of data we are parsing it will have to be implemented separately for both email and chat data.&lt;/p&gt;&lt;p&gt;Another addition we have been looking into has been detection of email signatures. While we have not yet created a very successful system for this, we have some ideas on how to proceed as we finish our first advanced model. We have already tried adding a feature for a sentence’s proximity to the end of an email, but unfortunately it was not very successful in removing signatures. Therefore, we are hoping that preprocessing will fit the task better — for example, we might examine the email for the last contiguous block of non-empty lines, and remove them under the assumption that the email signature will always be at the end and will never be useful in a summary. But this comes with its own challenges, as some emails in the dataset (and in general use) do not have a signature, and thus the last contiguous block might be content or even the entire email in some cases. One idea we have to combat this would be application of a language model to the sentences in a contiguous chunk. If a language model (such as an n-gram model) were trained on the English language and exhibited a high perplexity when looking at a given sentence, it might be safe to say that it is not prose and is instead a collection of names, emails, or organization names as one might expect to see in a signature. Be sure to check back in our Part 2 blog post after we have implemented this preprocessing and reported on the results!&lt;/p&gt;&lt;h4&gt;Conversation-Specific Features&lt;/h4&gt;&lt;p&gt;We started our foray into conversation-specific features with the those listed in the complete features set (beyond the basic set in our baseline) in the Sood et. al. paper, before expanding upon this set with our own contributions that target email data specifically. So far, we’ve implemented the following features based on the paper —&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Is Question:&lt;/strong&gt; Important issues and concerns are often expressed in the form of questions. To take advantage of this idea, this feature represents whether or not (1 or 0) the sentence is a question. When training using this feature, we found that&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Sentiment Score: &lt;/strong&gt;Based on the idea that strong sentiments and opinions are important to a conversation, this feature captures the sentiment of a sentence. The score is determined by taking the difference between the positive and negative and score for each word in the sentence, then adding up the scores of all the words in the sentence and normalizing by the number of words in the sentence.&lt;/p&gt;&lt;p&gt;In addition to the features in the Sood et. al. paper, we came up with some more feature ideas based on our error analysis of the baseline model —&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Numbers:&lt;/strong&gt; Even with the ‘Special Terms’ feature, our baseline model often missed lines containing numbers or statistics that we would want to preserve in a summary because they were short or eliminated by the importance of other features. In order to address this shortcoming, this feature uses a simple regular expression mechanism to determine the occurrences of numeric data in a sentence.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;URLs:&lt;/strong&gt; The baseline model also often missed URLs, which are typically important information and usually contained in the summary annotations of the dataset. This feature would address this problem by using a regular expression to determine how many URLs are contained in the sentence.&lt;/p&gt;&lt;h4&gt;Results&lt;/h4&gt;&lt;p&gt;At first, we tested the features that were described in the Sood et. al. paper, evaluating them on the Naive Bayes model that we had found to be most generally successful in our previous baseline experimentation. As corroborated by the paper’s findings, we discovered that these features were somewhat impactful in improving the performance of our model, at least as far as ROUGE scores were concerned:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*X9KXUGGowXEptmVn3kv-jg.png&quot; /&gt;Results of the model with the conversation-specific features from Sood et. al. compared with the baseline&lt;/figure&gt;&lt;p&gt;Noticeably, the addition of these conversation features was strictly better for ROUGE evaluation. Both the question feature and sentiment feature were roughly equivalent, although we noticed that when we subjected the summaries to human evaluation it was easier to notice an improvement from the question feature. Many of the important sentences in the reference summaries seem to have been questions, perhaps because asking a question is such a common way to transition to a new topic in an email thread. As a result, we found that this feature helped populate our summaries with many of the questions from the reference summaries, establishing clear boundaries. Even if some of the sentences in our summaries were less impactful, having the questions made a big difference for a human reader because it was possible to get the general idea of the thread from the questions alone. As an example, this is one of our generated summaries, compared with a reference:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*ONLBMtMpzebsdPUa1AzWBQ.png&quot; /&gt;Comparison of reference summary (left) to our generated summary (right) using the Is Question feature&lt;/figure&gt;&lt;p&gt;It is noticeable that the questions add a significant amount of relevance to our summary, although there is still work to be done as far as capturing the shorter, choppier, but still more important non-question sentences.&lt;/p&gt;&lt;p&gt;However, because another significant goal of our advanced model is to explore other conversation-specific features not addressed in the research paper, we did some additional exploration on our own to find features that could take our model’s performance beyond the results of Sood et. al., even if only for email data. So far, we have found two that improved our ROUGE metrics while also seeming to make more relevant summaries by detecting numbers and detecting URLs in a given sentence. Our results are as follows:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*xiga8kKb3O5cYHYP2sDbTQ.png&quot; /&gt;Results of the model with our conversation-specific features compared with the baseline&lt;/figure&gt;&lt;p&gt;Using the numbers feature in particular, we found that our model was able to outperform the baseline considerably, although upon human examination we had trouble determining exactly why this had happened as the summaries did not capture that many numbers. However, as far as human examination went, we discovered that the URL feature made a clear impact, adding a considerable number of important URLs to our summaries that had originally been passed over by the model even though human annotators had marked them as being especially important.&lt;/p&gt;&lt;p&gt;Finally, we tried all of these new features together to see how they compared to the baseline:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*l7tr_T6H5jnqWRbSf7NeGA.png&quot; /&gt;Results of the model with the all of the additional features discussed compared with the baseline&lt;/figure&gt;&lt;p&gt;Ultimately, while these features were relatively few in number, their use of conversation-specific data clearly led to a great deal of performance increase for our model.&lt;/p&gt;&lt;h4&gt;Next Steps&lt;/h4&gt;&lt;p&gt;For Part 2 of our advanced model, we would like to finish restructuring and incorporating the other datasets into our model input. Because one of our stretch goals is to be able to find a way to incorporate multiple types of data into a single model, we are hoping to do this in a way that allows us to compute the same (or at least similar) features for each type of data.&lt;/p&gt;&lt;p&gt;Another goal is to implement the preprocessing steps described above as a part of dataset parsing. Significantly, there are some advanced preprocessing techniques described by the Sood et. al. paper that deal with chat data specifically, so we hope to incorporate those once the chat data is working correctly. Finally, we want to continue adding features — although it is certainly unlikely that all of the features we try will be as successful as the ones described above, we hope that this project will culminate in a thorough examination of the impacts that various features can make, so we will continue to explore in this regard as we finish up our first advanced model.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=4626fe37cadb&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-05-03T06:56:41+00:00</dc:date>
	<dc:creator>Viterbi Or Not To Be</dc:creator>
</item>
<item rdf:about="https://medium.com/p/d73810e0c390">
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #6: Overfitting and Advanced Model I</title>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-6-overfitting-of-neural-baseline-and-advanced-model-d73810e0c390?source=rss-fad49d942bf3------2</link>
	<content:encoded>&lt;p&gt;This week, I continued working on correcting the apparent train/development performance discrepancy in baseline neural model through modifying the training data as well as model itself. I also attempted to construct a new advanced model. Unfortunately, the performance on the dev data, of both models, is still not as good as I’d like it to be.&lt;/p&gt;&lt;h3&gt;Dataset and Work on Baseline&lt;/h3&gt;&lt;p&gt;I’ve ran my baseline model on a total of 4 different datasets.&lt;/p&gt;&lt;h4&gt;Dataset A: Original&lt;/h4&gt;&lt;p&gt;I ran the baseline model using the original dataset provided by &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;Choi et al., 2016&lt;/a&gt;. Results are reported from previous blog posts, reposted below:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*iPBZ5X-TctKEGWXub_8Klw.png&quot; /&gt;f1 scores across 20 epochs&lt;/figure&gt;&lt;h4&gt;Dataset B: Splitting train&lt;/h4&gt;&lt;p&gt;This dataset was created through splitting the original training dataset provided by &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;Choi et al.&lt;/a&gt; into an 80/10/10 train/dev/test ratio. The motivations behind this were stated in my previous blog post, but I basically wanted to see how my model would perform if the train/dev/test distributions were similar. Results are reported from previous blog posts, reposted below:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*JYd_FY4ZYnUJj5Fnn-kLKg.png&quot; /&gt;f1 scores across 20 epochs&lt;/figure&gt;&lt;h4&gt;Dataset C: Training on weakly generated data&lt;/h4&gt;&lt;p&gt;This dataset used the dev/test of dataset A, but supplemented the original training set by generating weakly labeled ‘null’ examples to mimic the distribution of the development dataset. This was done by assuming all unlabeled entity pairs in the training set express no sentiment to each other.&lt;/p&gt;&lt;p&gt;As I had reported in my previous blog posts, the discrepancy in performance between the dev and train data is large on the original dataset (dataset A). I originally hypothesized this discrepancy as due to the difference in distribution between the training and development dataset, and this seemed to be supported by my results in dataset B. So I believed that by modifying my training dataset in this way, I would be able to improve performance. The results are plotted below:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*ZrOouM_61fTPOUZWIE3zTg.png&quot; /&gt;f1 scores across 20 epochs&lt;/figure&gt;&lt;p&gt;Unfortunately, there is still a large discrepancy in performance, especially on positive and negative sentiment. This led me to wonder what would happened if I trained my model on data similar to the original development data. That is, rather than using splitting the development data off the training data as in dataset B, what would happen if I split the training data off the development data…&lt;/p&gt;&lt;h4&gt;Dataset D: 3rd Modification (Training on subset of dev data)&lt;/h4&gt;&lt;p&gt;Originally, &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;Choi et al.&lt;/a&gt; had split the development set into two subsets: “eval” (development data used for error analysis and ablations) and “tune” (development data used for tuning hyper-parameters). For each of the above datasets A-C, I had used “eval” as the development data and thrown away the “tune” set. However, I wondered what would happen if I used the “tune” set as my training set. The distribution of my training data and development data should be similar in this case. The results are as follows:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*3ET0QKkUWLLhInHQuXOKxA.png&quot; /&gt;f1 scores across 100 epochs&lt;/figure&gt;&lt;p&gt;Note that there is still a large discrepancy, and performance on the new development data still barely, if ever, exceeds 0.2 for positive and negative labels. This is strongly indicative of overfitting. I have additionally plotted the losses for the train and development results, respectively, and on this plot overfitting is extremely apparent. Development loss does not even begin to decrease, suggesting that the baseline neural model is not learning what its intended to learn — perhaps it’s just memorizing particular configurations of text in the training set.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/640/1*KBxQ8qLMwxgU4AtPUuPjZQ.png&quot; /&gt;loss across 100 epochs&lt;/figure&gt;&lt;h3&gt;Dealing with the Overfitting&lt;/h3&gt;&lt;p&gt;To deal with the issue, the first thing I tried was applying some regularization. Unfortunately all it seemed to do was decrease/slow down improvement in performance on the train set, and leaving dev set performance unaffected.&lt;/p&gt;&lt;p&gt;(Results from running neural baseline model on dataset D.)&lt;/p&gt;&lt;p&gt;Dropout = 0.2: I added a dropout layer after the LSTM and set its dropout rate to 0.2.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*FT7361ZyaXIGgp4aXkf-nA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;I’m still not exactly sure as to what is causing the overfitting. Just looking at positive and negative sentiment performance, what’s really strange is the fact that the model’s dev performance was relatively high on dataset B where both train and dev were based on the training distribution, but not on dataset D where both train and dev were based on the development distribution. My guess is that perhaps since there’s relatively little positive and negative examples in the development set, the model had an easier time overfitting them for dataset D. Whereas for the training set, there was a plethora of positive and negative examples (and relatively little no sentiment examples), making it harder for the model to overfit the positives and negatives for dataset B.&lt;/p&gt;&lt;p&gt;Following Eunsol’s recommendation, I plan next time to experiment with strategically increase the size of the training data, and hopefully the model will demonstrate less overfitting.&lt;/p&gt;&lt;h3&gt;Advanced Model&lt;/h3&gt;&lt;p&gt;The final thing I did this week was implement a new model. The model combines elements from the two papers: &lt;a href=&quot;https://arxiv.org/pdf/1707.07045.pdf&quot;&gt;Lee et al.&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/pdf/1802.10569.pdf&quot;&gt;Verga et al.&lt;/a&gt; Basically, like my baseline neural model, this model encodes the inputs through a biLSTM. However, it then extracts span representations of the holder and target mentions using the encoded endpoints of the span, as in Lee et al. Then it runs the representations through separate FFNN as in Verga et al., and aggregates across holder and target mentions through a summation expression, and extracts the final score through a bi-affine operation. This is a very rough architecture at this stage, and there’s definitely parts of it that aren’t very well thought through, so I’ll definitely tinker with it in the future. However, the following are some preliminary results.&lt;/p&gt;&lt;p&gt;Results on Dataset C:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*9tPF6HytReejUKE3NDwVOw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;At this point, it looks like this model has very similar performance, if not worse, to the neural baseline. I will continue tinkering with it, as well as the data, in hopes of improving performance. In particular, in accordance with the &lt;a href=&quot;https://arxiv.org/pdf/1707.07045.pdf&quot;&gt;Lee et al.&lt;/a&gt; paper, perhaps adding features (i.e. the paper used distance and width features, which improved f1 scores by 3.1) or the head-finding attention mechanism will improve performance.&lt;/p&gt;&lt;h3&gt;Future Plans&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;Continue expanding training data by strategically combining the training data of Dataset C and D.&lt;/li&gt;&lt;li&gt;If this doesn’t work, do some error analysis to hopefully gain more insight into why the overfit is occurring.&lt;/li&gt;&lt;li&gt;Tinker with the advanced model by adding features or the head-finding attention mechanism. Also experiment more with the architecture.&lt;/li&gt;&lt;/ol&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=d73810e0c390&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-05-03T04:25:50+00:00</dc:date>
	<dc:creator>Belinda Zou Li</dc:creator>
</item>
<item rdf:about="https://medium.com/p/d01e84c5e1da">
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: Advanced Model Attempt #1: Neural-Based Definition Extraction</title>
	<link>https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-neural-based-definition-extraction-d01e84c5e1da?source=rss----9ba3897b6688---4</link>
	<content:encoded>&lt;p&gt;We last left off on the idea of using an FSA with a restricted vocabulary; restricted in the sense that we extract sentences coupled with a neural language model to assure semantic quality while allowing a generative RNN model a reasonable amount of improvisation to produce abstractive definitions.&lt;/p&gt;&lt;p&gt;Here, we discuss our approach for the extractive component of this model, and consider it our first attempt at an advanced model for the task.&lt;/p&gt;&lt;h3&gt;Introducing Extractive Summarization&lt;/h3&gt;&lt;p&gt;Recall that extractive summarization is the idea of reducing text down to a subset of its sentences that still preserves its semantic integrity. In particular, we intend to build on the work of a successful nerual-based extractive summarizer and tailor it to solve our task.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.04230&quot;&gt;SummaRuNNer&lt;/a&gt; is an RNN-based extractive summarization algorithm developed by Nallapati et al. that encodes documents from the word level up to and across the sentence level before making inference. Essentially, the model is a binary classifier on sentences within a document on whether it should be included in a summary. Its decisions are conditioned on&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Averaged-pooled word-level hidden states of the sentence&lt;/li&gt;&lt;li&gt;Average-pooled sentence-level hidden states of the document&lt;/li&gt;&lt;li&gt;An abstract representation of the summary built so far (average-pooling of the word-level pooled hidden states of sentences selected thus far)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;After which, there are several affine transformations conducive to selecting and filtering sentences:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Content: affine on the abstract sentence representation that measures semantic richness&lt;/li&gt;&lt;li&gt;Salience: bilinear affine on the abstract sentence representation and the document representation to measure cohesiveness&lt;/li&gt;&lt;li&gt;Novelty: bilinear affine on the abstract sentence representation and the running summary representation to address redundancy&lt;/li&gt;&lt;li&gt;Absolute and Relative Positioning: two separate affines on the embedded index of the sentence to allow how far we are into the document to influence inference&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As of now, we have built from scratch our own &lt;a href=&quot;https://github.com/NLP-Capstone-Project/machine-dictionary/blob/development-tam/machine_dictionary_rc/models/SummaRuNNer.py&quot;&gt;unofficial implementation of this model&lt;/a&gt; with inspiration from another &lt;a href=&quot;https://github.com/hpzhao/SummaRuNNer&quot;&gt;unofficial implementation&lt;/a&gt; and is capable of summarizing documents the way we’ve formatted them. What’s left is for us to tailor this model to fit the task.&lt;/p&gt;&lt;h3&gt;A Slight Twist on an Established Task&lt;/h3&gt;&lt;p&gt;As of now, the model summarizes documents. We’d like it so that it instead zeroes in on query terms we give it given a research paper, to intelligently extract only sentences from that paper conducive to defining that term.&lt;/p&gt;&lt;p&gt;Our approach for augmenting SummaRuNNer to be a definition extractor involves&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Encoding the query term with a character-level RNN and using its concatenated hidden states as its representation&lt;/li&gt;&lt;li&gt;Introducing this new query-term abstract representation when constructing the document representation through a bilinear affine&lt;/li&gt;&lt;li&gt;Further introducing this query term by converting many of the non-bilinear affines (content, positioning, and possibly new ones for the task) to further condition inference on the query term.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Essentially, the sentences we extract from the document are being conditioned on the term we’re trying to define. Encoding technical terms using a character level RNN allows similar technical terms to have similar hidden representations. For example, if we see the term “Chronic Lymphocytic Leukemia” in the training data and encounter “Chronic Myelogenous Leukemia” in the testing data, we would have more of an idea of how to approach this new term because of its character level similarities to the term we have already seen during training time. This might help us break down more complicated novel technical terms at testing time.&lt;/p&gt;&lt;p&gt;Experiments have yet to be conducted on the effectiveness of this approach but will be discussed later in &lt;strong&gt;Advanced Model Attempt #1 (cont.):&lt;/strong&gt; another post later in this series discussing the results of the groundwork we’ve laid out here.&lt;/p&gt;&lt;h3&gt;Training Methods&lt;/h3&gt;&lt;h4&gt;Collection Training Data with UMLS and ROUGE:&lt;/h4&gt;&lt;p&gt;Recall that SummaRuNNer is a model that aims to extract the sentences in a document that summarize it best. It does so by training on examples that teach the model which sentences to extract from the document.&lt;/p&gt;&lt;p&gt;SummaRuNNeR uses a &lt;em&gt;distant supervision&lt;/em&gt; method that relies on ROUGE in order to produce training examples for the model. This portion of the architecture, which we refer to as the “extractor”, extracts the sentences out of each document which maximize the ROUGE score when compared against the gold standard definition for the term in question. The extractor in a summarization context can use a greedy approach as follows:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Look at each sentence in the document one at a time and consider appending it to the extracted sentences that we have already chosen.&lt;/li&gt;&lt;li&gt;Calculate the ROUGE score of the old extracted sentences + this new sentence in comparison to the gold standard summarization for the document.&lt;/li&gt;&lt;li&gt;If the ROUGE score increases from the previous ROUGE score, keep the new sentence.&lt;/li&gt;&lt;li&gt;Otherwise, we don’t keep the new sentence and move on.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Although this method may not produce the most optimal and compact set of sentences that are relevant, this approach will be faster and is reasonable. The output of the extractor for each document is a tensor whose length is the number of sentences in the document, and is 0 if the sentence is tagged with O or 1 if the sentence is tagged with I.&lt;/p&gt;&lt;p&gt;To tailor this style of data collection to our task however, we optimize on ROUGE with respect to an entity’s gold-standard definition instead of a gold-standard summarization of the document. We collect entity-definition pairs through &lt;a href=&quot;https://www.nlm.nih.gov/research/umls/&quot;&gt;UMLS&lt;/a&gt; and creating training examples of the form&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Entity (the technical term to define)&lt;/li&gt;&lt;li&gt;Gold-standard definition for the entity&lt;/li&gt;&lt;li&gt;The target sentence IO tags found via distant supervision with ROUGE on sentences of a research paper with the gold-standard definition being the reference&lt;/li&gt;&lt;li&gt;A Semantic Scholar research paper in which the sentences came from (provides the sentences in which to perform inference)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;With this data, we can train the definition extraction model discussed earlier; we train using these &amp;lt;entity, IO-tagged sentences, publication&amp;gt; examples to learn a tagger that can extract sentences most relevant to a term given a publication.&lt;/p&gt;&lt;p&gt;While this may result in an unnecessarily large number of training data points, we can also consider pruning this dataset later on if we have irrelevant entities for a particular document. For example, if we were trying to find a training example that used the entity “dental cavity” for a document that was about blood cancers, we might not want to keep this training example because there wouldn’t be much of a correlation between the two. In order to do this, we can introduce a ROUGE threshold, where we only keep the training example if the ROUGE score of the sentences extracted by the tagger are above a particular threshold. This might be an optimization for the future.&lt;/p&gt;&lt;p&gt;Our previous approach was unsupervised and it relied only on the semantic scholar dataset to produce definitions. Our current approach is an extension of SummaRuNNer which requires gold standard definitions for entities that we’d like to define in each paper. We chose to focus on medical terms, and one of the most complete datasets for medical terms and their definitions happens to be the UMLS dataset. This dataset contains a &lt;em&gt;Metathesaurus&lt;/em&gt; which contains, amongst many other pieces of data, medical terms and their definitions. The technical terms in the dataset serve as references for ROUGE in the tagging phase above.&lt;/p&gt;&lt;h4&gt;In summary&lt;/h4&gt;&lt;p&gt;Training is fairly straightforward; loss between predicted and target sentences is computed with log loss (each sentence in a document is IO-tagged where sentences labeled with &lt;em&gt;I &lt;/em&gt;are to be included in the definition). Essentially, the definition extractor, much like SummaRuNNer, is trained as a sentence tagger.&lt;/p&gt;&lt;h4&gt;Attention as a Stretch Goal&lt;/h4&gt;&lt;p&gt;The first part of our basic SummaRuNNer-based model uses a document representation to predict tags for sentences in a document. The current document representation is constructed by averaging the hidden states from words in each sentence and averaging the hidden states from each sentence in the document. However, we believe that simply averaging the sentences may not be the best approach to constructing the latent document representation. One of our stretch goals for us to optimize the model will be to attend to the most important parts of sentences in each document. We can do this using the method proposed in Hierarchical Attention Networks for Document Classification (Yang et. al 2016).&lt;/p&gt;&lt;p&gt;This approach introduces a word level context vector and a sentence level context vector which allow us to calculate attention coefficients on the fly for every word in each sentence and every sentence in the document. In this manner, we can take a weighted sum of the hidden states in the sentences and will hopefully produce better document representations overall. The word level and sentence level context vectors can be initialized randomly and learned throughout training.&lt;/p&gt;&lt;h4&gt;Conclusion&lt;/h4&gt;&lt;p&gt;We are very excited to have found a supervised approach to this task per the advice of AI2 researchers. It’s a straightforward approach with measurable loss and clearer metrics.&lt;/p&gt;&lt;p&gt;We also hope to have enough time before the capstone is over to introduce attention!&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=d01e84c5e1da&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-neural-based-definition-extraction-d01e84c5e1da&quot;&gt;Advanced Model Attempt #1: Neural-Based Definition Extraction&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
	<dc:date>2018-05-03T03:32:53+00:00</dc:date>
	<dc:creator>Tam Dang</dc:creator>
</item>
<item rdf:about="https://medium.com/p/6f773ae418d0">
	<title>Halden Lin &lt;br/&gt; Team undef.: NLP Capstone | 06: Uncertainty</title>
	<link>https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0?source=rss-2759d54493c0------2</link>
	<content:encoded>&lt;p&gt;previous posts: &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;01&lt;/a&gt; &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5&quot;&gt;02&lt;/a&gt; &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3&quot;&gt;03&lt;/a&gt; &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7&quot;&gt;04&lt;/a&gt; &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a&quot;&gt;05&lt;/a&gt;&lt;/p&gt;&lt;p&gt;I’ve begun to realize I may not be getting as much out of the project I chose I had hoped. My initial motivation for my project was a hope of expanding my knowledge and developing insights on the NLP front by leveraging the familiarity of Visualization. While I am certainly learning a lot by reading papers on Attention and Neural Networks as a whole (especially through my in-class paper presentation), I feel the work I am doing in building a tool for visualizing and debugging attention models may not be providing me the space to explore NLP that I had hoped for. While the tool will certainly &lt;strong&gt;enable &lt;/strong&gt;exploration, my concern is that this exploration will not occur until after the tool is completed at the end of the quarter.&lt;/p&gt;&lt;p&gt;The good news is that there have been two recent developments that, while increasing my uncertainty, offer potential for greater depth in exploration along the NLP front.&lt;/p&gt;&lt;h4&gt;1. Potential Pivot&lt;/h4&gt;&lt;p&gt;I voiced these concerns with Prof. Choi this past week and was given a good amount of valuable advice. Per her suggestion, the beginning of my last cycle began with three tasks.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Read &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=2470718&quot;&gt;&lt;em&gt;The Efficacy of Human Post-editing for Language Translation&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;authored by&lt;em&gt; &lt;/em&gt;Spence Green, Jeff Heer, and Christopher Manning. This paper is unique in that it presents the value of Visualization and HCI within Natural Language Processing, but not as a window into a model. Rather, the authors explore a specific task integral to the Language Translation pipeline and present suggestions for future work in improving Language Translation.&lt;/li&gt;&lt;li&gt;Do in-depth human error-analysis of existing summarization models. I used examples from See et al.’s paper &lt;em&gt;Get To The Point: Summarization with Pointer-Generator Networks &lt;/em&gt;(2017).This was helpful gaining a better intuition as to the problem space and the challenges currently posed by machine summarization.&lt;/li&gt;&lt;li&gt;Think about how summarization as a task, whether that be the development of models, the model’s task itself, or end-user tasks that use the model, can be re-framed in order to leverage Visualization. This was especially time consuming, as it was difficult for me, but it helped immensely in taking a step back to understand the purpose of these models. This, in turn, helped me understand how my work can fit into this purpose.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The next few days consisted largely of brainstorming pivots for my project. The most promising direction that came out of these sessions is very briefly outlined below.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Assisted Cognitive Document Abstraction&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Machine-generated document summaries, even the state-of-the-art, are infrequently used in practice because their summaries are quite poor. Perhaps we can leverage existing models to, rather than produce definite summaries which may be error-prone or difficult to understand, create visualizations over the source text in order to assist humans in comprehension and abstraction.&lt;/p&gt;&lt;p&gt;For example, instead of treating attention weights as input for an output of text, as we do in summarization models, we can view them as output for human interpretation. Aggregate attention distributions (in summarization) highlight areas of the input text that are salient for the summary produced. Note that this is potentially more valuable than highlighting extractive summaries in the text because attention could potentially point towards different areas of the text that relate to a summary sequence. In this way, generation of summaries becomes a proxy task for creating salient highlights for text. We could then use this as a starting point from which ‘related’ sections in an article may be highlighted for users upon interaction (e.g. mousing over an attended-to sequence).&lt;/p&gt;&lt;p&gt;The hope is that these visualizations will increase the speed (over no summarization) or accuracy (over machine summarization) at which readers can abstract / understand key ideas in a document.&lt;/p&gt;&lt;p&gt;Most excitingly, with this re-framing of the task for these models, from sequence output to highlighting, perhaps the models can be modified by adding or removing constraints and mechanisms in order to improve performance for this new task.&lt;/p&gt;&lt;p&gt;Upon presenting this idea (in longer form) to Prof. Choi, I was encouraged to (1) think more about weaknesses of removing summaries altogether and (2) push for more novelty in the approach — is there any meaningful insight about attention models or summarization as a task that can be gleamed from this pivot, and if not, how can I work towards that. While I do not yet have answers to these concerns, the next development may result in a few.&lt;/p&gt;&lt;h4&gt;2. Related work, here at the Allen School&lt;/h4&gt;&lt;p&gt;It was just recently brought to my attention that a Tongshuang (Sherry) Wu, a PhD student in the Interactive Data Lab (in which I am currently working), is also working on visualizations for understanding attention models in NLP. As a part of her project, she and a few of her peers have developed a preliminary visualization tool for an attentive QA model (on the SQuAD dataset).She and my mentor, Kanit (Ham) Wongsuphasawat (whom I have been bouncing ideas off recently), have kindly offered to meet and discuss her work and insights on the problem space. Perhaps collaboration is a possibility — this is exciting! In any case, I suspect talking with Sherry and Ham will provide me the insight and guidance to make a decision on the direction of my project.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Future Work&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;I hope to make another blog post in the coming few days as I iron out a future direction. Before this, however, future work is unclear. Until next time!&lt;/p&gt;&lt;h4&gt;In the meanwhile (supplementary material)&lt;/h4&gt;&lt;p&gt;I’ve also been playing around with my visualization prototypes, even as I am uncertain as to whether or not they will be relevant to my project after this week. Here’s what I’ve discovered and implemented in that time.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Interactive heat-maps likely won’t work.&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*_bVKvDPn6jBADKwFU9VG3g.gif&quot; /&gt;Interactive heat-maps result in a large degree of lag between input and visual update. This is likely due to the extremely large size of the attention matrix in summarization (24,000 individual squares in the heat-map).&lt;/figure&gt;&lt;p&gt;This is unfortunate, but browser limitations are limitations that must be worked around.&lt;/p&gt;&lt;p&gt;2. Selection over output text.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/999/1*KfMQd6YnDzdH9dZOyjVEIw.gif&quot; /&gt;Mousing over words in the summary results in a view of the attention distribution over the article for that decoder time-step.&lt;/figure&gt;&lt;p&gt;This is similar to the interactive visualizations presented by See et al. in their &lt;a href=&quot;http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html&quot;&gt;blogpost&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;3. Brushing over output text.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1000/1*nwGIF3VgNDeuFxRml8gGzg.gif&quot; /&gt;Brushing over the summary results in an aggregate attention distribution (i.e. coverage) over the article for the selected decoder time-steps.&lt;/figure&gt;&lt;p&gt;This is an interaction technique I have yet to see in work involving attention analysis, so this is exciting! It looks to be somewhat useful in identifying sections of input text that are salient to an &lt;strong&gt;idea&lt;/strong&gt; rather than a &lt;strong&gt;single word&lt;/strong&gt; in the output text.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=6f773ae418d0&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-05-02T06:59:16+00:00</dc:date>
	<dc:creator>Halden Lin</dc:creator>
</item>
<item rdf:about="http://deeplearningturingtest.wordpress.com/?p=21">
	<title>Ananth Gottumukkala &lt;br/&gt; Team Turing Test: Advanced Model Attempt 1 (Part 1)</title>
	<link>https://deeplearningturingtest.wordpress.com/2018/05/02/advanced-model-attempt-1-part-1/</link>
	<content:encoded>&lt;p&gt;This week I created and populated my SQL database with the birth year, birth place, industry, gender, profession, and continent information corresponding to 100 random famous people spanning across all time periods. Furthermore, I created a little over 30 questions that the model can ask as well as the corresponding SQL queries for each question. During each game for the simulation, the user will randomly pick a person for the model to guess and the model picks from a list of these questions to ask. Then, the query corresponding to this question is used to extract the truth value of the question from the database (Yes, No, Unknown). This answer is used as the response to simulate a real person giving that answer through user input. Now that the code has been written to interact with the database, the model can now fully create the observation at any point, which is the input vector to the DRQN. Next, I will hardcode the sample rewards as well as the rules of the game (maximum 20 questions, rewards for winning/losing/wrong guess, terminating game, etc.). This week I will be focusing on getting the simulation to run end-to-end, use tensorflow-gpu, and do hyperparameter tuning.&lt;/p&gt;</content:encoded>
	<dc:date>2018-05-02T06:38:52+00:00</dc:date>
	<dc:creator>ananthgo</dc:creator>
</item>
<item rdf:about="http://cse481n-capstone.azurewebsites.net/?p=64">
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: Advanced Attempt I</title>
	<link>http://cse481n-capstone.azurewebsites.net/2018/05/01/advanced-attempt-i/</link>
	<content:encoded>&lt;h3&gt;&lt;b&gt;Data Collection: &lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We have made a lot of progress increasing our data quality since the last blog post. We have fine-tuned our filtering parameters and experimented with a few different definitions for set similarity. On top of Jaccard Index, we tried Dice Index and Cosine Similarity. We found that depending on the threshold, these different methods gave very similar results, but Dice Index seemed to provide the highest quality sentences while being more tolerant of long sentences (unlike Jaccard, which favored short sentences). Although it’s very picky, we’re certain that due to the huge amount of raw Reddit data we have we can still get a dataset big enough to train our complex neural nets.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;As an example of our improvement, consider this example from the last post:&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;MeanJokes Post: “Don’t be offended but Fuck you”
&lt;/span&gt;Similar Post: “fuck Foligno”
Similar Post: “fuck narek”
Similar Post: “fuck”
Similar Post: “Fuck me?”
Similar Post: “Fuck me”
Similar Post: “fuck me”
Similar Post: “Fuck it”
Similar Post: “Fuck”
Similar Post: “Who the fuck are you?”&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Now our output would look like:&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;MeanJokes Post: Don't be offended but Fuck you
&lt;/span&gt;Similar Post: why the fuck does he have to talk in a screaming voice
Similar Post 171137: &quot;Officer, I have no idea what in the fuck you're talking about.
Similar Post 92163: Or maybe you just fuck me in public for all too see.
Similar Post 18052: &quot;you know, I'm finally happy&quot;. UGH, fuck off.
Similar Post 2567: So reddit, that's my fuck up. Any advice if any of you are in HR?
Similar Post 160778: Now I'm questining what numbers are real and what was put down to fuck with me and what's serious.
Similar Post 210956: And when i ask him about it, he cusses me out (tells me to fuck off) and i just die/break down internally.&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;As well, since Dice is so picky and because the MeanJokes set tends to have a very particular structure to all its posts, we are also adding in some other obviously offensive posts to use for our set similarity step. We’re using a hate speech lexicon developed by Tom Davidson (linked below) to extract hateful posts from the general Reddit set. We will concatenate this with the MeanJokes set before running Set Similarity against all of the posts again, hopefully giving us a wider range of language structure for our dataset. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our final improvement was discovering that a handful of subreddits contribute a majority of the noise in our data. This noise is mostly of two varieties: 1. Personal ads for intimate encounters and 2. Trading requests, for both physical and virtual items. A handful of these subreddits are very activate and are surprisingly a large chunk of Reddit’s posts, although none of them ever get nearly enough upvotes to be noticed by the average user. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;So in order to combat having a lot of posts of this sort in our dataset:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; class=&quot;alignnone wp-image-65&quot; height=&quot;144&quot; src=&quot;http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/Picture1-300x110.png&quot; width=&quot;393&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We have put together a blacklist of subs and filtered them out of the posts we consider for set similarity.&lt;/span&gt;&lt;/p&gt;
&lt;table style=&quot;height: 620px;&quot; width=&quot;414&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;100k posts&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Top 10 Black List&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Top 10 White List&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;RocketLeagueExchange’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;1860&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;AskReddit’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;5978&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;A5XHE’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;1373&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Showerthoughts’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;1709&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dirtykikpals’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;1128&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The_Donald’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;850&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dirtypenpals’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;870&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;teenagers’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;720&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;DirtySnapchat’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;792&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;GlobalOffensiveTrade’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;681&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dirtyr4r’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;438&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Bitcoin’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;651&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;AppNana’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;372&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;relationships’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;586&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Roleplaykik’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;368&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;FIFA’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;558&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;buildapc’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;364&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;explainlikeimfive’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;500&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;AgeplayPenPals’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;329&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Fireteams’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;469&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;&lt;br /&gt;
Running the filter on 100k posts, we can see that most of the most common subreddits that remained are conversational in nature, while those that were removed would not make very useful sentences.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Advanced Model Attempt: &lt;/b&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Combining Datasets: &lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In our last blog post, we mentioned our concern about the small size of &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem’s twitter dataset&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;. This week, we combined that dataset with another twitter hate speech dataset made by Thomas Davidson. The Davidson dataset contains 24,802 labeled tweets. Each tweet is coded by at least 3 CrowdFlower users. Each row contains 5 columns:&lt;/span&gt;&lt;/p&gt;
&lt;table style=&quot;height: 336px;&quot; width=&quot;584&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;count&lt;/td&gt;
&lt;td&gt;number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hate_speech&lt;/td&gt;
&lt;td&gt;number of CF users who judged the tweet to be hate speech.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;offensive_language&lt;/td&gt;
&lt;td&gt;number of CF users who judged the tweet to be offensive.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;neither&lt;/td&gt;
&lt;td&gt;number of CF users who judged the tweet to be neither offensive nor non-offensive.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;class label for majority of CF users. 0 – hate speech 1 – offensive language 2 – neither&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Davidson et. al. used the following definition for hate speech: language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. According to the paper, &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;“only 5% of tweets were coded by the majority of coders”&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;. If we directly combine Waseem Data with this we might get a even more skewed class distribution (31% ‘hate’, 69% ‘none’). Therefore, we decided to change the class labels of Davidson a little bit: if all CF users unanimously coded a tweet hate_speech or offensive_language, the tweet would be labeled ‘hate’; otherwise, the tweet would be labeled ‘none’. The modified Davidson dataset has a class distribution of 76% ‘hate’ and 24% ‘none’. Then we combined these two datasets (removed duplicate tweets if there are any). The new combined dataset has 40,509 tweets and a class distribution of 59% ‘hate’ and 41% ‘none. The combined dataset is much larger than the altered Waseem dataset (~15k tweets) and the labels are more balanced. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We do have the concern whether this more these more generously labeled ‘hate’ tweets are noisy. However, because the Waseem dataset is also more generous to ‘none’ labels (as long as the tweet is neither racist or sexist), we believe they would have some counter effect on each other. After all, data noise is very unlikely to be completely removed. &lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Preprocessing: &lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Since the model we tried requires each sentence to have at least 4 tokens, we decided to ignore sentences with less than 4 tokens after pre-processing.&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The effectiveness of Combined Dataset:&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;To illustrate the effectiveness of the combined dataset, we chose the best NN model set up from baseline II to train on Waseem dataset and combined dataset separately and evaluated the two trained models on Waseem dev data. We decided not to evaluate on test data yet because we don’t want any leaked info from test.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Set up — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer GRU&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Trained on Waseem Dataset, epoch chosen: 13&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem-Dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8235&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8022&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7876&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.7940&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Trained on Combined Dataset, epoch chosen: 16&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem-Dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8152&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7979&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7672&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.7788&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Although the two models had the same setup, the one trained on the combined dataset got performance close to the one trained on the original Waseem dataset despite the fact that we now have really different class distributions in the two datasets. &lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Retrain Some Baseline Models on Combined Dataset:&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Here we retained some baseline models with different set ups on the combined dataset and evaluated them on both Waseem dev data and combined dev data.&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model1 — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer GRU, epoch chosen: 16&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Combined-dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem-dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8665&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8152&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8614&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7979&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8628&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7672&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8621&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;0.7788&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model2 — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer LSTM, epoch chosen: 19&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Combined-dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem-dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8625&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8091&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8577&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7875&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8578&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7648&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8578&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;0.7739&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; Model3 — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer BiGRU, epoch chosen: 16&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Combined-dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem-dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8618&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8104&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8567&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7916&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8575&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7620&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8571&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;0.7732&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model4 — embedding: 100 dimensional glove twitter embeddings, encoder: 1 layer GRU, epoch chosen: 11&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Combined-dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem-dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8651&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8194&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8605&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7971&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8603&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7834&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8604&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;0.7894&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model:&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our first Advanced model will be a CNN model.&lt;/span&gt;&lt;/p&gt;
&lt;h5&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The intuition of choosing this model:&lt;/span&gt;&lt;/h5&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;CNN provides us a convenient way to extract the most important information within the given fragment of a sentence through filters and max pooling. We found it might be a worth trying model on our task.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our model looks like:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; class=&quot;alignnone wp-image-67&quot; height=&quot;158&quot; src=&quot;http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/Picture2-300x116.png&quot; width=&quot;409&quot; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Image credit:&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Gambäck, B., &amp;amp; Sikdar, U.K. (2017). Using Convolutional Neural Networks to Classify Hate-Speech.&lt;/span&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Preprocess all words and encode them using pretrained glove embeddings.&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Feed result into a convolution neural network, taking 2, 3 and 4-grams into consideration. Output dimension is 28, 26 for English alphabets, 1 for digits and 1 for all other symbols. &lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Feed result into a 2-layer feed-forward neural net, with dimension (28, 2)  and dropout (0.3, 0.3)&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Softmax on the result and pick the major class&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;With not much tuning, here’s what our best model looks like:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;200 dimension embedding, filters=100, trained on Waseem twitter dataset&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.79169&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.78274&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.80086&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.82142&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;200 dimension embedding, filters=100, trained on the combined dataset&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;*revised, due to an imperfection in the combined dataset, there was a mistake in numbers&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;combined dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.85983&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;0.77451&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.86029&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.76156&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.85937&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.78791&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.86437&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.80837&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;del&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;With no doubt, our combined dataset provides a huge boost on performance on original Waseem twitter dataset.&lt;/span&gt;&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;From the above results, it seems CNN hasn’t show an improvement on our job. We think doing more hyper parameter tuning should give us some improvement. Furthermore, we would like to incorporate Elmo to see if that will help us our not.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;del&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;However, we do have some concern about our models: almost all of the best models we have with a large number of filters have their best epoch generally to be the first few epochs. We are a little bit concerned about that since that may be a sign of overfitting.&lt;/span&gt;&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;Since our result hasn’t show any improvement, we think it’s more appropriate to do error analysis once we gain some improvement.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;del&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Therefore, before we start to do any error analysis, we would like to do a little bit more hyperparameters since we haven’t really try different drop out rate or other output dimension values other than the one specified in the paper we referenced.&lt;/span&gt;&lt;/del&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Next Step:&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;First, we will dig deeper on the model we have right now. We will first play with its parameters and then conduct error analysis on it.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;As suggested in previous blog post feedback, we would like to try Elmo and see how much can we improve with it. Furthermore, we would like to try things like character level embedding as well as another very interesting model which combines CNN with GRU to make prediction.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Work Cited:&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06&quot;&gt;Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Automated-Hate-Speech-Detection-and-the-Problem-of-Davidson-Warmsley/6ccfff0d7a10bf7046fbfd109b301323293b67da&quot;&gt;Davidson, Thomas J et al. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM (2017).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Using-Convolutional-Neural-Networks-to-Classify-Gamb%C3%A4ck-Sikdar/0dca29b6a5ea2fe2b6373aba9fe0ab829c06fd78&quot;&gt;Gambäck, Björn and Utpal Kumar Sikdar. “Using Convolutional Neural Networks to Classify Hate-Speech.” (2017).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;</content:encoded>
	<dc:date>2018-05-02T06:17:57+00:00</dc:date>
	<dc:creator>Team Watch Your Language!</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-9203775015655831448.post-3361785683406757277">
	<title>Pinyi Wang, Dawei Shen, Xukai Liu &lt;br/&gt; Team Overfit: #6 Milestone: Advanced model attempt #1</title>
	<link>https://teamoverfit.blogspot.com/2018/05/6-milestone-advanced-model-attempt-1.html</link>
	<content:encoded>&lt;h2 style=&quot;height: 0px;&quot;&gt;&lt;span&gt;Team Overfit&lt;/span&gt;&lt;/h2&gt;&lt;h3&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3&gt;&lt;span&gt;Project repo: &lt;span style=&quot;font-size: 18.72px;&quot;&gt;&lt;a href=&quot;https://github.com/pinyiw/nlpcapstone-teamoverfit&quot;&gt;https://github.com/pinyiw/nlpcapstone-teamoverfit&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h4&gt;&lt;span&gt;Team members: Dawei Shen, Pinyi Wang, Xukai Liu&lt;/span&gt;&lt;/h4&gt;&lt;div style=&quot;text-align: start; text-indent: 0px;&quot;&gt;&lt;div style=&quot;margin: 0px;&quot;&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;Blog Post: #6: 05/01/2018&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;span&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Social Media Predicts Stock Price (StartUp Mode)&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre;&quot;&gt;This week, we added more data source and have finer process of the data. We also convert &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre;&quot;&gt;Keras model to Tensorflow for better future improvement.   &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;b&gt;Keras to TensorFlow&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;We updated our code from using Keras to Tensorflow which helps to do more improvement on model. For example, TensorFlow is a lower level library which allow us to have more control over the variables we used as the input and output. For example, if we want to try with more complicated input with adding the voerall market stock price and the competitors' stock prices, TensorFlow will be more helpful.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;Add One More Target Company: Tesla&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;As there are too many Tweets that have tagged #Apple but are unrelated to the company Apple, we need to filter out such tweets. However, if we apply the model on the company Tesla, who's company name is uniquer than 'Apple' and has its products naming closer to its company name, the Tweets and news that have tagged #Tesla will contain much less noisy information and thus improve the prediction to its stock price.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;Try Out Different Layers Structure to Tune LSTM&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;ul&gt;&lt;span id=&quot;docs-internal-guid-1d495372-1f45-7a06-476d-fd5b4661d619&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;Add the NASDAQ index to the input&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;The composition of the NASDAQ Composite is heavily weighted towards information technology companies. Therefore, company Apple’s and Tesla’s stock prices may be influenced by the Nasdaq index.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;Add a 'STAY' category as the output prediction of stock price&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;If the change of the stock price for the next day is within 1%, we will mark the stock price of that day as a ‘STAY’. Otherwise, we mark it as ‘UP’ or ‘DOWN’&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;Lemmatization &amp;amp; Stemming&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;Lemmatization and stemming help collect the same words with different tenses together, which reduce the total vocabulary size.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/span&gt;&lt;/ul&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-1d495372-1f45-7a06-476d-fd5b4661d619&quot;&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;&lt;b&gt;Update Of Evaluation Plan&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-1d495372-1f45-7a06-476d-fd5b4661d619&quot;&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;&lt;b&gt;Results &amp;amp; Error Analysis&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;span id=&quot;docs-internal-guid-1d495372-1f45-7a06-476d-fd5b4661d619&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;After we have improved the model and the training process, the prediction accuracy for Apple will be above 85% if we choose '&lt;i&gt;+/-1% change of the stock price&lt;/i&gt;' as 'STAY'.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;The prediction accuracy for Apple will be above 60% if we choose '&lt;i&gt;+/-1% change of the stock price&lt;/i&gt;' as 'STAY'.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/span&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;span id=&quot;docs-internal-guid-1d495372-1f45-7a06-476d-fd5b4661d619&quot;&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;&lt;b&gt;What To Investigate For The Next Week&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;span id=&quot;docs-internal-guid-1d495372-1f45-7a06-476d-fd5b4661d619&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;Apply F1 scores. For the evaluation ,we would like to know:&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;The rate of stocks are marked 'UP/DOWN/STAY' that are predicted correctly as 'UP/DOWN/STAY' separately.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;The rate of stocks are marked 'UP/DOWN/STAY' that are predicted wrongly as 'UP/DOWN/STAY' separately.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;Add competitors' stock prices to the input&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/span&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-05-02T05:53:00+00:00</dc:date>
	<dc:creator>Team Overfit</dc:creator>
</item>
<item rdf:about="https://medium.com/p/e05ee9a7eaa8">
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Image Captioning Model: Advanced model attempt #1</title>
	<link>https://medium.com/@hongnin1/image-captioning-model-advanced-model-attempt-1-e05ee9a7eaa8?source=rss-c450eb982161------2</link>
	<content:encoded>&lt;p&gt;To recap, this is our baseline approach:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/@hongnin1/image-annotation-model-baseline-dataset-and-evaluation-framework-d1d1b2d1f34c&quot;&gt;Image Annotation Model Baseline, Dataset and Evaluation Framework&lt;/a&gt;&lt;/p&gt;&lt;p&gt;For the past week, we have been trying to improve our model’s performance from two different aspect: training speed and model accuracy. In order to improve speed, we further investigated in TensorFlow’s API. We realized that the bottleneck of our model’s performance is in extracting and batching our data. After some research, we have decided to try tf.record (API: &lt;a href=&quot;https://www.tensorflow.org/programmers_guide/datasets#consuming_tfrecord_data&quot;&gt;https://www.tensorflow.org/programmers_guide/datasets#consuming_tfrecord_data&lt;/a&gt;). We are working on using tf.record and tf.dataset to prepare and feed our data as proposed in the link, and we are hoping to see a performance increase (fingers crossed).&lt;/p&gt;&lt;p&gt;In order to improve model accuracy, we mentioned in previous post that we are working towards finding the right way to calculate attention weights. The APIs we have tried are: &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention&quot;&gt;https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention&lt;/a&gt; and &lt;a href=&quot;https://www.tensorflow.org/programmers_guide/datasets#consuming_tfrecord_data&quot;&gt;https://www.tensorflow.org/programmers_guide/datasets#consuming_tfrecord_data&lt;/a&gt;, however the way they are calculating attention weights do not align with what the paper did, so we are considering implementing our own small neural network to extract attention weights, but it will take a while. Our teammates are currently working on developing our own neural network for this week and see if it will work. It’s a trail and error process so it might take some time.&lt;/p&gt;&lt;p&gt;In terms of the model itself, we have previously implemented InceptionNet as our CNN encoder. However we encountered the same problem as we had for calculating attention weights, this InceptionNet is a different implementation from the paper we are basing our model on, so we decided to switch to VGG. We are still trying to find a suitable VGG16 (CNN) implementation with checkpoint and this is what we are currently considering: &lt;a href=&quot;https://github.com/machrisaa/tensorflow-vgg&quot;&gt;https://github.com/machrisaa/tensorflow-vgg&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Overall speaking, our baseline model has a lot of problems we have been fixing for the past week in terms of training speed and attention visualization. Also changing our neural network for CNN encoder is taking a large chunk of our time as well. We are hoping to fix these problems but it might take a while for us to figure out the best approach. We are trying our best and hopefully come up with an even more advanced model sometime soon.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=e05ee9a7eaa8&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-05-02T04:26:35+00:00</dc:date>
	<dc:creator>Ning Hong</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-5600014144802012716.post-7248717662742163548">
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Advanced model attempt one</title>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/05/advanced-model-attempt-one.html</link>
	<content:encoded>I fixed the performance issues, and I have been working on adding enhancements to the model such as restricting the action space, by only allowing the model to generate types from the class, as well as debugging it to improve accuracy. The results are: 7.7 EM and 22.1 Bleu.</content:encoded>
	<dc:date>2018-05-01T15:57:00+00:00</dc:date>
	<dc:creator>nlpcapstone</dc:creator>
</item>
<item rdf:about="https://medium.com/p/e7e525549f94">
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Fully-featured Model Results and More Ideas for Evaluation</title>
	<link>https://medium.com/@ryanp97/fully-featured-model-results-and-more-ideas-for-evaluation-e7e525549f94?source=rss-6378d85d3a9b------2</link>
	<content:encoded>&lt;p&gt;In the past week, I’ve been working on training a fully-featured baseline model as well as a large, fully-featured model. This post will cover the results from these two models as well as a comparison between the featureless model.&lt;/p&gt;&lt;p&gt;Additionally, in the past week Jan, Michael, and I have been discussing different metrics for evaluating graphs. Based on this, the project may pivot away from building more complex models for the semantic transfer task to focus on exploring new and novel techniques for evaluation.&lt;/p&gt;&lt;p&gt;So in the coming week, I’ll be continuing to explore with slightly larger models as well as exploring different evaluation techniques.&lt;/p&gt;&lt;h4&gt;Fully-featured Model Results&lt;/h4&gt;&lt;p&gt;To be expected, evaluating the fully-featured model is not as simple as running SMATCH and taking the F1 score at face value. After expanding the model’s predictions, the baseline model received an F1 score of 0.77, with all features included. As mentioned in the previous post, this score is inflated since SMATCH equally weights predicate and feature accurate despite features generally being easier to predict compared to predicates or arguments.&lt;/p&gt;&lt;p&gt;Considering this, I stripped away all features from the predictions except for tense and mood and evaluated the model again. Note the only thing that has changed is an extra post-processing step has been added to the same model and the same predictions. This change resulted in a SMATCH score of 0.65. From here, I removed all features from the predictions and re-ran SMATCH and got an F1 score of 0.63. It seems that we are able to train a model using data with squashed features with little to no risk of performance loss.&lt;/p&gt;&lt;p&gt;For recap, here is a small table with the results so far.&lt;/p&gt;&lt;pre&gt;Fully-featured (all)          -  0.77&lt;br /&gt;Fully-featured (tense, mood)  -  0.65&lt;br /&gt;Fully-featured (none)         -  0.63&lt;/pre&gt;&lt;pre&gt;Featureless (baseline)        -  0.65&lt;/pre&gt;&lt;p&gt;As a side note: I also attempted to train a larger model with 600 embedding size, 750 hidden size, 3 encoder and decoder layers. This, however, resulted in extreme overfitting. The model predictions (without features) received an abysmal F1 of 0.32. In an effort to not go as crazy with the parameters while making the model larger to account for adding extra information in the input, I’m training a model with 600 hidden size and 3 layers and reverting back to a 500 embedding size.&lt;/p&gt;&lt;h4&gt;Other Ideas for Evaluation Exploration&lt;/h4&gt;&lt;p&gt;As mentioned earlier in this post, an issue with SMATCH is that it weights predicates, arguments, and features all equally when features are significantly easier to predict. To address this, Jan, Michael, and I have discussed some different evaluation ideas that we might want to explore.&lt;/p&gt;&lt;p&gt;The first idea is to separate the predicates from the features and calculate precision and recall on just the predicates. We can also further divide the predicates into surface predicates and abstract predicates (abstract predicates often have the _ prefix). This allows us to get a more detailed insight on the model’s ability to predict predicates that SMATCH has issues revealing.&lt;/p&gt;&lt;p&gt;A second idea is to replace all predicted features with some dummy feature so that we can upper bound the score when features are being predicted. By doing this, we can get a more accurate gauge of how well the model is predicting features.&lt;/p&gt;&lt;p&gt;Finally, the third idea is to eventually do something similar to this &lt;a href=&quot;https://github.com/mdtux89/amr-evaluation&quot;&gt;AMR evaluator&lt;/a&gt;. It computes SMATCH and F-scores on various different versions of the predictions by selectively cleaning up portions that are not useful to a specific metric. This would be an interesting idea and an interesting thing to reimplement/repurpose for the use of DMRS graphs.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=e7e525549f94&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-05-01T07:10:59+00:00</dc:date>
	<dc:creator>Ryan Pham</dc:creator>
</item>
<item rdf:about="http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1">
	<title>Sarah Yu &lt;br/&gt; Team Jekyll-Hyde: Advanced Model Attempt #1</title>
	<link>http://sarahyu.weebly.com/cse-481n/advanced-model-attempt-1</link>
	<content:encoded>&lt;div class=&quot;paragraph&quot;&gt;The first part of implementing my advanced model attempt was to work on implementing the IDP Algorithm presented in  Monroe, B. L., Colaresi, M. P., &amp;amp; Quinn, K. M. (2008). &lt;em&gt;Fightin' words: Lexical feature selection and evaluation for identifying the content of political conflict. &lt;/em&gt;&lt;font size=&quot;2&quot;&gt;Political Analysis, 16(4), 372-403. &lt;/font&gt;&lt;br /&gt;&lt;br /&gt;In doing so, I was able to find the weighted log odds ratio of each word present in both ND and NT posts, ultimately showing which type of subreddit each word was 'more affiliated' with. The findings were as one might expect, especially with my previous baselines and were in line with the results from those. As seen below we see some familiar words within the ND (I, you, &lt;strong&gt;she&lt;/strong&gt;​) and NT (http) - so sorry for the ugly terminal output, but I need to find a prettier CSV presentation:&lt;/div&gt;  &lt;div&gt;&lt;div class=&quot;wsite-multicol&quot;&gt;&lt;div class=&quot;wsite-multicol-table-wrap&quot;&gt; 	&lt;table class=&quot;wsite-multicol-table&quot;&gt; 		&lt;tbody class=&quot;wsite-multicol-tbody&quot;&gt; 			&lt;tr class=&quot;wsite-multicol-tr&quot;&gt; 				&lt;td class=&quot;wsite-multicol-col&quot; style=&quot;width: 50%; padding: 0 15px;&quot;&gt; 					 						  &lt;div&gt;&lt;div class=&quot;wsite-image wsite-image-border-none &quot; style=&quot;padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;&quot;&gt; &lt;a&gt; &lt;img alt=&quot;Picture&quot; src=&quot;http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-02-at-12-42-36-am_orig.png&quot; style=&quot;width: auto;&quot; /&gt; &lt;/a&gt; &lt;div style=&quot;display: block; font-size: 90%;&quot;&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;   					 				&lt;/td&gt;				&lt;td class=&quot;wsite-multicol-col&quot; style=&quot;width: 50%; padding: 0 15px;&quot;&gt; 					 						  &lt;div&gt;&lt;div class=&quot;wsite-image wsite-image-border-none &quot; style=&quot;padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;&quot;&gt; &lt;a&gt; &lt;img alt=&quot;Picture&quot; src=&quot;http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-05-02-at-12-43-09-am_orig.png&quot; style=&quot;width: auto;&quot; /&gt; &lt;/a&gt; &lt;div style=&quot;display: block; font-size: 90%;&quot;&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;   					 				&lt;/td&gt;			&lt;/tr&gt; 		&lt;/tbody&gt; 	&lt;/table&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;  &lt;div class=&quot;paragraph&quot;&gt;This was a good first step, and will need some more work hashing out some final implementation details, but my next step in making this an actual advanced model, is to now utilize some of that reddit data that I've been harvesting for the past week or so. With that, we have a lot more data and might need to make some changes on the subreddit subsets depending on how the data has developed (changes in sentence length and number of total number of posts in each subreddit). Off to more data!!!&lt;/div&gt;</content:encoded>
	<dc:date>2018-05-01T07:00:00+00:00</dc:date>
</item>
<item rdf:about="http://deeplearningturingtest.wordpress.com/?p=18">
	<title>Ananth Gottumukkala &lt;br/&gt; Team Turing Test: Strawman/Baseline 2: Same DRQN Model with a Different Policy</title>
	<link>https://deeplearningturingtest.wordpress.com/2018/04/26/strawman-baseline-2-same-drqn-model-with-a-different-policy/</link>
	<content:encoded>&lt;p&gt;I decided to narrow the scope of my problem by changing the types of questions asked to the user to be only yes/no questions. This simplifies interpreting the user response into a classification task. More specifically, the questions will be about figuring out the attributes of any entities brought up in the text.  Therefore, I will use a relational database to store the accumulated entity-attribute relationships instead of a semantic network since it will be easier to extract quantifiable features by performing specific queries. In addition to asking the user questions, I decided to involve database queries in the training process. This way the model can query the database about any inferences or guesses it has about the attributes of an entity, and the database can return a list of entities that fit the hypothesis. Then, the model is given a reward based on how much it was able to narrow down the list of entities. This allows the model to get frequent reward signals from the database to speed up training.&lt;/p&gt;
&lt;p&gt;Because of this change, my model architecture has also changed. The DRQN will now take as inputs the action from the previous time step (one-hot vector), the user response (word embeddings passed through CNN), and the database response (number of entities the previous query narrowed it down to). In addition, the outputs to the LSTM at each time step will feed into A+1 policy networks where A = number of attributes. The first A policy networks are needed because the model needs to learn how to guess each attribute independently. The last policy network determines which question the model will ask the user. So far I have implemented most of this architecture but still need to add in the policy networks and debug.&lt;/p&gt;
&lt;p&gt;Finally, I will implement a question simulator to randomly pick an entity and have the model guess what it is, similar to 20 questions. During this simulation, rewards will be automatically given every time the model queries the database and whenever the game ends (win or loss). Furthermore, a small penalty is given for a wrong guess. By implementing this simulator, the need for user input to give rewards is eliminated and this should completely automate and greatly speed up training.&lt;/p&gt;
&lt;p&gt;My goal for next week is to finish implementing and debugging the model to start this training simulation.&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-26T06:58:22+00:00</dc:date>
	<dc:creator>ananthgo</dc:creator>
</item>
<item rdf:about="https://medium.com/p/306dca636d3a">
	<title>Halden Lin &lt;br/&gt; Team undef.: NLP Capstone | 05: Experimenting</title>
	<link>https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a?source=rss-2759d54493c0------2</link>
	<content:encoded>&lt;p&gt;&lt;em&gt;previous posts: &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;&lt;em&gt;01&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5\&quot;&gt;&lt;em&gt;02&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3&quot;&gt;&lt;em&gt;03&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7&quot;&gt;&lt;em&gt;04&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Hi! Here’s what I’ve been up to in the past week.&lt;/p&gt;&lt;h4&gt;Progress on the TensorBoard Plugin&lt;/h4&gt;&lt;p&gt;Real data collection and the backend are functioning!&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*l6M8uMcswVoaEtL0_HGt0w.png&quot; /&gt;Architecture of the Attention Plugin.&lt;/figure&gt;&lt;p&gt;At this point last week, I had implemented the nodes in green above. These were the operators / functions required to produce Summary protobufs that are in turn saved to disk.&lt;/p&gt;&lt;p&gt;This week, I completed a number of tasks to produce a bare-bones functioning plugin (sans visualizations).&lt;/p&gt;&lt;p&gt;First, I modified the source code for &lt;a href=&quot;https://github.com/abisee/pointer-generator&quot;&gt;See et al.’s (2017) attentional models&lt;/a&gt; to use the Attention Plugin API to save input text, output text, and attention distributions during evaluation.&lt;/p&gt;&lt;p&gt;Next, I implemented the Attention Plugin’s back-end, which is used to fulfill requests made by the front-end. This service currently offers two services:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;tags This route returns all tags associated for each run in the log. This should include 3 tags for each run: one for each of the input, output, and attention tensors.&lt;/li&gt;&lt;li&gt;attention This route returns a list values associated with the given tag (including time and step stamps). This can be used by the front-end to acquire each of the input, output, and attention lists (converted from tensors) by passing the corresponding tag (retrieved using the tags route).&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Finally, as a proof of concept, I modified the front-end provided in the &lt;a href=&quot;https://github.com/tensorflow/tensorboard-plugin-example&quot;&gt;TensorBoard Plugin Example&lt;/a&gt; to consume this back-end, showing it is able to retrieve summaries. Now we just need some visualizations to consume the data!&lt;/p&gt;&lt;h4&gt;Visualization Prototyping Begins&lt;/h4&gt;&lt;p&gt;While data collection and back-end development has been wrapping up, I’ve begun to prototype static visualizations for the plugin. To do this, I used data produced by &lt;a href=&quot;https://github.com/abisee/pointer-generator&quot;&gt;See et al.’s (2017) pre-trained attentional models&lt;/a&gt; (produced only at decode time without the Attention Plugin). Through this process, I hope to gain two things in particular.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;A idea of what will/won’t work as visualizations for summarization tasks.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;A better understanding of the behavior of attentive models&lt;/strong&gt;, and through that a better idea of how static and/or interactive visualizations can further interpretability and understanding.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The first idea I decided to pursue was that of a &lt;strong&gt;condensed heat-map&lt;/strong&gt;. You may recall the conventional heat-map used for attention visualizations described in &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;my first blog post&lt;/a&gt;.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/612/1*_sq2Vy_Py7hEXp2tWBBXxg.png&quot; /&gt;Rikters et al. (2017). A heat-map with relatively large cells, allowing for display of text along the axes.&lt;/figure&gt;&lt;p&gt;The issues I noted with this visualization pattern are as follows:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;It is difficult to fit the words (as seen above) on the x-axis, harming readability.&lt;/li&gt;&lt;li&gt;This does not scale well with large input or output (e.g. summarization)&lt;/li&gt;&lt;li&gt;We do not read single-tokens at a time (i.e. y-axis), and input and output are generally not in this format either.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;To address the point (2), scalability, I decided to try producing a heat-map with no text labels, and thus each cell could be as small as a single pixel.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*WOlVYgmeZ0DmutTZkDXn2g.png&quot; /&gt;A prototype of a condensed heat-map, where x-axis represents input and y-axis represents output.&lt;/figure&gt;&lt;p&gt;The color scale is a discrete scale, where each step is determined by the quantiles of the weight distribution. The x-axis represents the input text, and the y-axis represents the output text, with each cell representing the amount of attention paid for that pair (output paid to input). The good news here is that the attention distribution is relatively easy to understand at a quick glance. The downside is that cells that are not part of a larger trend (you may notice a lone red spot near the top of the heat-map, approximately a quarter of the way through the x-axis) are harder to make out, as the cells are so small. Further, the distribution is contextless — we don’t know the structure of the input text or what words these high weights are associated with. In the example above, we understand that the model focused primarily on the beginning of the article, but we can’t tell whether that is good or bad without seeing the text.&lt;/p&gt;&lt;p&gt;To remedy this, I decided to also display the input text, with the input text highlighted according to the maximum of the weights it received. This also solves concerns (1) and (3) for the conventional heat-map.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*dJXS9s_391dACPuUwAgPmA.png&quot; /&gt;The input text corresponding to the heat-map above, where each token is highlighted according to its max attention weight received.&lt;/figure&gt;&lt;p&gt;By putting these two together (along with the output text for reference), we can gain a better understanding of how the model arrived at its summary. A viewer can now map the attention distribution shown in the heat-map to text in the input sequence by looking for patches of similar color intensity.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*dMqdcl01Za5U4nYrXpye6g.png&quot; /&gt;A prototype static visualization including both heat-map and highlighted text.&lt;/figure&gt;&lt;p&gt;To get a better sense of how this visualization pattern would play out, I built a light web-page that allows users to cycle through different input / output examples. The gif below shows several of these.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*_ACE44hvUSrwfE3F04gslg.gif&quot; /&gt;The described visualization pattern over several input / output sequences.&lt;/figure&gt;&lt;p&gt;More exploration (inside and outside of this pattern) will need to occur, but this seems promising!&lt;/p&gt;&lt;h4&gt;What’s Next&lt;/h4&gt;&lt;p&gt;Lots to get done this next week. Here’s what’s in my plan:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Continue working on data collection and cleaning up the TensorBoard plug-in. Move beyond the proof-of-concept front-end and show that meaningful visualizations (perhaps extremely basic ones) can be generated using the plugin back-end as a data source.&lt;/li&gt;&lt;li&gt;Read more into the model provided by See et al. (2017), as well as related work, to gain a better understanding of the architecture and function/behavior of attention. A closer study of the works cited in my &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5&quot;&gt;second blog post&lt;/a&gt; will be a good starting point. The better I understand this mechanism the more equipped I’ll be to create meaningful visualizations.&lt;/li&gt;&lt;li&gt;Continue prototyping static visualizations, move on to interactive visualizations. Acquire feedback from peers for both.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Thanks for reading!&lt;/p&gt;&lt;h4&gt;Works Cited&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.04368.pdf&quot;&gt;See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” &lt;em&gt;ACL&lt;/em&gt; (2017).&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://ufal.mff.cuni.cz/pbml/109/art-rikters-fishel-bojar.pdf&quot;&gt;Rikters, Matīss, Mark Fishel, and Ondřej Bojar. “Visualizing neural machine translation attention and confidence.” &lt;em&gt;The Prague Bulletin of Mathematical Linguistics&lt;/em&gt; 109.1 (2017): 39–50.&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=306dca636d3a&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-25T06:48:23+00:00</dc:date>
	<dc:creator>Halden Lin</dc:creator>
</item>
<item rdf:about="https://medium.com/p/27c90aa3c1aa">
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #5: More Baselines and Error Analysis</title>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-5-more-baselines-and-error-analysis-27c90aa3c1aa?source=rss-fad49d942bf3------2</link>
	<content:encoded>&lt;p&gt;I spent this week implementing other baselines, running last week’s neural baseline on the re-split training dataset, and analyzing results and errors from the run on the new dataset.&lt;/p&gt;&lt;h3&gt;Baselines&lt;/h3&gt;&lt;h4&gt;Random Baseline&lt;/h4&gt;&lt;p&gt;This baseline randomly assigned labels to examples in accordance with the training distribution. If 45% of the training examples were negative, it would assign a negative label to any given example with a 45% probability. I implemented this baseline as a comparison tool to ensure that other model I implemented were actually learning something. The f1 scores are as reported:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/503/1*GhPCrHpMs81djY06j3cFTA.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;Sentence-level Neural Baseline&lt;/h4&gt;&lt;p&gt;In the &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;Choi et al., 2016&lt;/a&gt; paper that I’m working on improving, this model is used as a baseline. I decided the report the results here and use them as baseline results for my own project as well.&lt;/p&gt;&lt;p&gt;This baseline is derived from &lt;a href=&quot;https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf&quot;&gt;Socher et al., 2013&lt;/a&gt;, in which the authors implemented a sentence-level RNN model for sentiment classification. To adapt this model for the purpose of entity-entity relation extraction, Choi et al. performed four steps:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Collect all sentences where entities co-occur (appear in the same sentence)&lt;/li&gt;&lt;li&gt;Run each of the collected sentences through the classifier, and amalgamate the positive/negative sentiment results for each&lt;/li&gt;&lt;li&gt;Assign positive label to entity pair if at least one collected sentence was classified positively&lt;/li&gt;&lt;li&gt;Otherwise, assign negative label to entity pair if all sentences classified negatively&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The f1 scores for this model are as reported:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/402/1*H0uAoCZuPsanMJKNYfhebg.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;ILP Model Baseline&lt;/h4&gt;&lt;p&gt;This model is the main model introduced by the &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;Choi et al., 2016&lt;/a&gt; paper. It is an ILP model that is basically combines a base SVM model, which predicts entity-entity pairwise sentiment, with soft constraints from social science theories.&lt;/p&gt;&lt;p&gt;The f1 scores for this model are as reported:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/402/1*tCbKTqxFbNYcuGfbtrlyzA.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;Neural Baseline&lt;/h4&gt;&lt;p&gt;This model and the f1 scores for it are described in my previous post (see &lt;a href=&quot;https://medium.com/@be.li.nda/nlp-capstone-blog-4-baseline-model-i-7de5277b5be&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;In my last blog post, I mentioned that one of the challenges I was facing was the fact that the train set and the dev/test sets were drastically different. This week, I have brainstormed a few ideas to approach this problem:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Weak supervision approach: generate more training examples with the “no sentiment” label by assuming all unlabeled entity pairs in the training set express no sentiment to each other. Since the training set has many examples, but is very sparsely annotated (annotates few entities per document), it should be relatively easy to find many unlabeled entity pairs.&lt;/li&gt;&lt;li&gt;Adjust class weights to have the model pay more attention to minority classes in the training data, such as the “no sentiment” class.&lt;/li&gt;&lt;li&gt;Adjust the decision threshold of each sentiment class for the model. Given the final LSTM output (a probability score for each of the three classes), rather than just labeling the example by the majority class, perhaps tune the threshold for labeling an example as each of the three sentiments.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;In the meantime, I wanted to see how the model would perform if the datasets were more similar in distribution. I created a new dataset (which I will refer to as train’/dev’/test’) by splitting the train data by document using an 80/10/10 ratio. The results of the neural baseline on this dataset are report below:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*JYd_FY4ZYnUJj5Fnn-kLKg.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;The performance on the new dev’ set is much better than on the original dev set, although there is still some discrepancy.&lt;/p&gt;&lt;h3&gt;Error Analysis&lt;/h3&gt;&lt;p&gt;The final thing I did this week is error analysis. I used the results on the new dev’ of the neural baseline model for this analysis. The main thing I looked for was how each entity within a document was being classified, as I had hypothesized that the target/holder-ness of entities weren’t being encoded well enough.&lt;/p&gt;&lt;p&gt;The statistics from my error analysis appear to support my hypothesis.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Of documents that had multiple entity pairs annotated (52% of documents), the model predicted 84.8% of them to have the same sentiment relation for all entity pairs, whereas only 30.3% of them actually classify the same sentiment for all entity pairs.&lt;/li&gt;&lt;li&gt;Up to 77% of the mistakes on the dev set could have been due to the model focusing on the classifying the central sentiment of the document, rather than classifying sentiments between specific (possibly peripheral) entity pairs in the document.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;For the second statistic, I defined the “central sentiment” of the document as the sentiment dynamic that the document was focused on , expressed by the main entities of the document. As a more concrete example, take an article headlined “&lt;em&gt;Protestors Storm BBC over Far-Right Leader’s TV Slot&lt;/em&gt;,” which has a negative central sentiment. I manually went through the misclassified examples and counted how many of them were predicted in line with the central sentiment of the document (i.e. for the above example article, how many of its entity pairs are mistakenly predicted “negative”).&lt;/p&gt;&lt;p&gt;In terms of possible solutions for this problem, I’m planning to augment the architecture of my neural model to encode target/holder-ness in the architecture itself, rather than just within the input embeddings. Two possible architectures I plan to experiment with are either the &lt;a href=&quot;https://homes.cs.washington.edu/~luheng/files/emnlp2017_lhlz.pdf&quot;&gt;Lee et al. paper&lt;/a&gt; or the &lt;a href=&quot;https://arxiv.org/pdf/1802.10569.pdf&quot;&gt;Verga et al. paper&lt;/a&gt;. I describe both of these architecture, as well as how I’m going to adjust them for the entity-entity sentiment analysis task, in more detail in my project proposal blog post (see &lt;a href=&quot;https://medium.com/@be.li.nda/nlp-capstone-blog-3-project-proposal-c8a12d3ae611&quot;&gt;here&lt;/a&gt;). While I haven’t figured out the exact mechanics of how I’m going to incorporate them, I definitely plan to experiment with some ideas from each paper next time.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=27c90aa3c1aa&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-25T06:30:23+00:00</dc:date>
	<dc:creator>Belinda Zou Li</dc:creator>
</item>
<item rdf:about="https://medium.com/p/a21b51cdd27c">
	<title>Aaron Johnston, Lynsey Liu &lt;br/&gt; Team Viterbi Or Not To Be: Baseline Model #2</title>
	<link>https://medium.com/@viterbi.or.not/baseline-model-2-a21b51cdd27c?source=rss-c522ef075bb3------2</link>
	<content:encoded>&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*M0h3Wof_sgtxGwgnT2IncQ.png&quot; /&gt;Bassline = Baseline?&lt;/figure&gt;&lt;p&gt;This week, we completed the baseline portion of our project (see the roadmap below!) by finishing the implementation of our various baseline models and evaluating their performance with the goal of providing context for the rest of the project. In last week’s blog post, we described the process of replicating the findings of a related research paper using a single model, and incorporating only features that apply to text summarization in general. This week, we expanded upon that start by adding an additional simple, single-feature baseline and by evaluating our implementation using other models as well! Finally, we built upon our codebase from last week with various bugfixes and feature additions, such as implementing k-fold cross validation for more reliable metrics.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*K0NWdjTpiYezkyqh34fdow.png&quot; /&gt;Roadmap of our project — At this point, we’ve completed the first big block, highlighted in orange!&lt;/figure&gt;&lt;h4&gt;Simple Baseline&lt;/h4&gt;&lt;p&gt;The simplest baseline we put together used only the sentence length feature, a very basic but often reliable measure of sentence importance, with a Naive Bayes model.&lt;/p&gt;&lt;h4&gt;More Realistic Baselines&lt;/h4&gt;&lt;p&gt;The more realistic and comprehensive baseline models that we implemented use the full “basic feature set” described in our &lt;a href=&quot;https://medium.com/@viterbi.or.not/baseline-model-1-a6690114c441&quot;&gt;last blog post&lt;/a&gt; with Naive Bayes, Decision Tree, and Multilayer Perceptron models. The comprehensive Naive Bayes baseline is the same as the baseline model described in our previous post — the main progress on this week’s baselines is the addition of a simple feature to catch email lines that are quoted replies (a problem we found in a lot of the summaries generated by our models) as well as experimentation with the different types of models.&lt;/p&gt;&lt;h4&gt;Results and Evaluation&lt;/h4&gt;&lt;p&gt;As expected, the simple Naive Bayes baseline performed pretty poorly. The models using all of the features in the basic set all performed similarly well, with slight fluctuations in which ROUGE metric they did better in (seen in the table below). Overall, we found that our Naive Bayes model performed competitively with the other models in ROUGE-L and the best in ROUGE-1 and ROUGE-2, though most importantly, we thought the summaries generated by Naive Bayes were the most satisfactory when actually reading them.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Hv_d7fQj-ATlUEaV79Moiw.png&quot; /&gt;Table comparing the performances of the various baseline models we implemented, the best performing in each ROUGE metric highlighted in orange&lt;/figure&gt;&lt;p&gt;Our best baseline model does better than the corresponding model in &lt;a href=&quot;http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf&quot;&gt;Summarizing Online Conversations: A Machine Learning Approach&lt;/a&gt; in the ROUGE-1 and ROUGE-2 metrics&lt;strong&gt;, &lt;/strong&gt;but does significantly worse in the ROUGE-L metric. However, it is not clear which of the ROUGE metrics is more “important” to score well in, and it is hard to do a complete comparison between our model and theirs without a sample of the summaries generated by their model (which is not provided in the paper).&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*sFxZT2d-C_jKMnFxTWUHuA.png&quot; /&gt;Table comparing the performance of the our best baseline model with the corresponding model in &lt;a href=&quot;http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf&quot;&gt;&lt;strong&gt;Summarizing Online Conversations: A Machine Learning Approach&lt;/strong&gt;&lt;/a&gt;, the best performing in each ROUGE metric highlighted in orange&lt;/figure&gt;&lt;p&gt;In an effort to better understand what these automated metrics are measuring, here are descriptions of what ROUGE-L and ROUGE-N (ROUGE-1 and ROUGE-2) measure —&lt;/p&gt;&lt;p&gt;&lt;strong&gt;ROUGE-L: &lt;/strong&gt;Based on Longest Common Subsequence statistics, identifies longest co-occurring in sequence n-grams.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;ROUGE-1: &lt;/strong&gt;Unigram overlap between system and reference summaries.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;ROUGE-2: &lt;/strong&gt;Bigram overlap between system and reference summaries.&lt;/p&gt;&lt;p&gt;ROUGE-L and ROUGE-N clearly measure summary quality very differently, and rather than increase together, the metrics vary greatly and sometimes change inversely. While performing a mini ablation study with our baselines, we noticed that removing certain features increased ROUGE-L but cause large drops in both ROUGE-1 and ROUGE-2, as well as generated less satisfactory summaries (judged by us reading the generated summaries).&lt;/p&gt;&lt;p&gt;Because ROUGE does not necessarily reliably measure the quality of a conversation summary, human monitoring of generated summaries and error analysis are crucial to this project.&lt;/p&gt;&lt;h4&gt;Error Analysis&lt;/h4&gt;&lt;p&gt;So, although we used the ROUGE metrics for our model in order to compare it to our baseline research paper, we put an emphasis on human evaluation due to the inability of ROUGE to capture all the elements of successful summarization. Namely, although ROUGE is capable of determining whether the words and subsequences used in a summary match the human-annotated reference, it cannot capture critical aspects of the text such as its coverage of the source document’s most important points or whether it makes logical sense when read.&lt;/p&gt;&lt;p&gt;Upon reading through the summaries produced by our model, we discovered an interesting mix of results. Several sentences that appeared key to establishing the email thread’s topic were included in the summary by the baseline, indicating that its features for topic identification and term similarity were contributing in a positive manner. However, although it is expected that any extractive summary will produce grammatically imperfect results, there were noticeably major issues with the formation of our baseline model’s summaries.&lt;/p&gt;&lt;p&gt;An excerpt of one fairly representative summary produced by our model appears below, unedited except for truncation and formatting for clarity. This is a summary of an email chain concerning accessible technology from the BC3 corpus:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*JGjecxwGpwd-rxwnf_t_Cw.png&quot; /&gt;Comparison with reference summary&lt;/figure&gt;&lt;p&gt;As one can see, only the first two sentences (green) of this particular summary match the human-annotated summary; the rest (red) diverge and never again are the same sentences shared. This trend is seen across most of the summaries produced by our baseline model, where early sentences tend to be shared — one hypothesis is that the first email in a thread has clear significance in establishing that thread’s topic, while subsequent emails are less certain, leading to divergence between the model and human understanding.&lt;/p&gt;&lt;p&gt;Beyond the matching of sentences, there are certain aspects of the summary that, through a human evaluation, can be identified as clearly not belonging in the summary:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*ni7uIemzWgCsW4Yvh0jlwA.png&quot; /&gt;Email-specific formatting leads to summarization mistakes&lt;/figure&gt;&lt;p&gt;In this example, there are sequences of text that are directly repeated (orange). This occurs due to emails having quoted text from previous emails in their message bodies, and as the general text summarization features attribute the same likelihood of being relevant to the summary to both versions of the text based solely on content (and not, for instance, sentence position), it makes sense that both would be included. Doing so seems not to vastly impact the ROUGE score, but provides another example of the importance of human evaluation because it seems reasonable that no human would prefer to have those repetitions in a summary. Another example of the baseline model’s errors can be found in the email signature present in the summary (blue), which a human reader would clearly not find meaningful for the summary.&lt;/p&gt;&lt;p&gt;Both of these mistakes can be attributed to the fact that our baseline model uses general features for text summarization, and does not factor in conversation-specific features such as recognizing an email signature as being irrelevant to the topics being discussed. Once we begin work on our minimum viable product, we expect it to perform much better in this category, because we will have the chance to add these features.&lt;/p&gt;&lt;p&gt;Finally, the following examples show cases in which our model makes the opposite mistake from above, failing to capture portions of the original conversation that are important to the summary through human eyes. As expected, both of the common cases of failure we identified seem to stem from the fact that the missed content is formatted in a different way than normal text.&lt;/p&gt;&lt;p&gt;For the same conversation as above, the following shows just the URLs that were included in a human-annotated summary. Highlighted in green are the URLs which our baseline model also included in its generated summary:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*P9C4OsXf9w3KK4ishqa9Tg.png&quot; /&gt;Almost all important URLs are not captured by the baseline model&lt;/figure&gt;&lt;p&gt;As one can see, the baseline model had a 10% success rate in identifying URLs from the email thread that the human annotator deemed as important. In a thread that primarily dealt with identifying resources from the internet, having these URLs in a summary would be highly desirable, so we consider this to be a major failing of the baseline model. Because URLs are much different from typical text, using general text summarization features likely led to this absence because there are no features that can ascribe importance to URLs based purely off their general format.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Wfd7cl-8KZ36YvAWXG4wHw.png&quot; /&gt;Structured data, such as ordered lists, are not captured&lt;/figure&gt;&lt;p&gt;As another example, the following is a comparison between our baseline model and a human-annotated reference summary for a different conversation that includes discussion of poll results:&lt;/p&gt;&lt;p&gt;Although the baseline model does relatively well in identifying sentences related to the discussion of the poll results, its summary is noticeably lacking the results themselves. We hypothesize that the general text summarization features are insufficient to detect numerical data or items that are presented in a list, such as the one above, and therefore because the actual text within each poll result item is very generic the baseline model was unable to successfully select them for the summary.&lt;/p&gt;&lt;p&gt;Ultimately, these few examples are not enough to capture the exact failings of our baseline model, but by analyzing them in conjunction with the other summaries generated for our validation set, we were able to get an impression of the types of improvements that will be needed. In our next step of including conversation-specific features, we plan to make our top priority addressing structured data unique to emails by creating features to target email summaries, quoted text (and repeated text in general), and certain entities such as URLs or lists.&lt;/p&gt;&lt;h4&gt;Steps Towards an Advanced Model&lt;/h4&gt;&lt;p&gt;Keeping the weaknesses of our baseline in mind, our next steps towards creating a more advanced conversation summarization model (and approaching our Minimum Viable Product!) include the following:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Incorporate conversation-specific features&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;We would like to explore using topic segmentation to preprocess conversation data and potentially improve model performance.&lt;/li&gt;&lt;li&gt;We would also like to add features that incorporate detection of&lt;strong&gt; structured data&lt;/strong&gt; (URLs, Lists, Numbers), &lt;strong&gt;sentiment scores&lt;/strong&gt; of sentences, and &lt;strong&gt;discourse markers&lt;/strong&gt; (defining the purpose of a sentence in the text, for example, identifying a sentence as an email signature).&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Train the models on other datasets&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;We will need preprocess the other datasets and restructure some of our code to handle input of the other datasets.&lt;/li&gt;&lt;li&gt;We anticipate some challenges based on the differences of the other datasets — the chatlog data will require much more preprocessing and meeting transcripts will likely have different vocabulary (i.e. no URLs, fewer abbreviations) which may impact our feature engineering process.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Check back next week to see our initial work on the most exciting part — the advanced model!&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=a21b51cdd27c&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-25T06:29:12+00:00</dc:date>
	<dc:creator>Viterbi Or Not To Be</dc:creator>
</item>
<item rdf:about="https://medium.com/p/fd61ae20a2f1">
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Image Annotation Model Improved Baseline</title>
	<link>https://medium.com/@hongnin1/image-annotation-model-improved-baseline-fd61ae20a2f1?source=rss-c450eb982161------2</link>
	<content:encoded>&lt;p&gt;Our original baseline can be found here:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/@hongnin1/image-annotation-model-baseline-dataset-and-evaluation-framework-d1d1b2d1f34c&quot;&gt;Image Annotation Model Baseline, Dataset and Evaluation Framework&lt;/a&gt;&lt;/p&gt;&lt;p&gt;There are several major improve we have done to our baseline:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;We added Attention to our deep learning network to improve our decoding scheme: as can be seen in the snapshot below, we decided to change LSTM (previous baseline) to GRU due to performance reasons. We discovered that because our training corpus is extremely large, it takes way to long to train our model using LSTM, hence the switch to GRU.&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/838/0*mAvISRtQGogv4cQ-.&quot; /&gt;code block we added for adding attention/GRU&lt;/figure&gt;&lt;p&gt;2. Now that we are more familiar with TensorFlow (we have been watching tutorials online), we improved our APIs and incorporated better TensorFlow code to make our code base more organized and efficient. More specifically, we added “tf.contrib.seq2seq.GreedyEmbeddingHelper” to help feed data into the network, as well as adding “tf.contrib.seq2seq.BasicDecoder” and “tf.contrib.seq2seq.dynamic_decode” to perform dynamic unroll of RNN when doing decoding.&lt;/p&gt;&lt;p&gt;3. We had some bugs with our data parsing, and we have spent a large chunk of time debugging and eventually fixed the problem.&lt;/p&gt;&lt;p&gt;As for evaluation, we dived deep into the code base we found online for evaluation (for more detail see our previous post for baseline) and have decided to use the same evaluation method as the baseline approach because it is pretty established and work pretty well in determining whether an annotation is valid or not by using BLEU score).&lt;/p&gt;&lt;p&gt;4. We kept trying different attention weights and visualized the attention weight on input graphs.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=fd61ae20a2f1&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-25T05:48:06+00:00</dc:date>
	<dc:creator>Ning Hong</dc:creator>
</item>
<item rdf:about="https://medium.com/p/79ed786e5c74">
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: Baseline II and Updates</title>
	<link>https://medium.com/nlp-capstone-blog/baseline-ii-and-updates-79ed786e5c74?source=rss----9ba3897b6688---4</link>
	<content:encoded>&lt;p&gt;This week, our focus was to improve the original baseline model with an approach more tailored to the task of generating definitions. There were a few key challenges that the original baseline approach did not address. This included generating grammatically sound English sentences, and incorporating keywords. Over the past few weeks, we also explored text generation techniques used in poetry (Ghazvininejad et. al). Our new approach is inspired by the techniques used in this paper and aims to address the two major problems with our first baseline.&lt;/p&gt;&lt;h4&gt;Revised Approach&lt;/h4&gt;&lt;p&gt;One of the biggest issues with the first baseline models, which were neural language models, was that the sentence outputs were not coherent or grammatical. Ghazvininejad et. al addressed a similar structural issue by creating a large Finite State Machine of all possible paths one could take while generating a sonnet. Each path is grounded in the filtered vocabulary developed in earlier steps and technically would have produced a structurally sound sonnet. Though all of these paths were not great, the FSM provided a foundation for generating the best sonnet. We use a similar approach, where we create an FSM of all possible paths through the training corpus. For example, if we encountered the following sentence in the corpus, it’s corresponding FSM would look like this:&lt;/p&gt;&lt;blockquote&gt;Osteoporosis is a bone disease.&lt;/blockquote&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*uWBb5Vc4EerUU9hEvzHDuQ.png&quot; /&gt;Example FSM&lt;/figure&gt;&lt;p&gt;In this manner, we construct an FSM for the entire corpus.&lt;/p&gt;&lt;h4&gt;Extraction&lt;/h4&gt;&lt;p&gt;Another challenge of the original baseline was that it did not focus on the topic we provided to start with. Even though we provided the seed word, it tended to stray off topic very quickly. In order to fix this, we chose to use some techniques from extractive summarization and the keyword identification process from the poetry generation paper. To start, we have to identify some common words associated with the term we’re defining. In Generating Topical Poetry, the authors use distance between word vectors as a metric for similarity, which we also do in our model. This allows us to determine the top few keywords associated with the term.&lt;/p&gt;&lt;p&gt;Now, we can proceed to look through the corpus and extract all sentences that contain a keyword or the term itself. This allows us to capture the context in which a term is mentioned. In the future, we can capture more sentences than this, especially if we use a window larger than one sentence for capturing context. Once we have our extracted sentences, we can proceed to modify the FSM.&lt;/p&gt;&lt;p&gt;Our modified FSM will only contain words that are included in the extracted sentences and a set of pre-selected connective words required to generate grammatical sentences. We hope that this will narrow down the search space in our decoding phase.&lt;/p&gt;&lt;h4&gt;Definition Generation via Beam Search&lt;/h4&gt;&lt;p&gt;The FSM mentioned previously needs a scoring mechanism with which to extract likely paths. We plan on using an RNN language model (LSTM, GRU, etc.) to decode the FSM and produce the generated definition one sentence at a time. Although it is asking a bit much to trust the neural language model to produce coherent, semantically rich sentences, we trust that the amount of structure we’re introducing before inference nudges the model to connect the dots in the most sensible way.&lt;/p&gt;&lt;p&gt;An issue we’re left with is the actual beam search itself. Beam search is a useful alternative to exhaustive search for fixed sized sequences in that we only continue paths that are one of the top K likely paths at every step. The difficulty lies in the fact that beam search is best used for deriving most likely paths of a fixed length. When we’re at the stage of generating definitions, we’ll have to figure out how exactly sentence length will be enforced or relaxed. It’ll be especially difficult to enforce grammar, in particular, how to terminate sentences. In the most cases this can be mitigated by generating the sentences backwards and appending punctuation.&lt;/p&gt;&lt;p&gt;Difficulties also lie in what seed to use per generated sentence. It may make sense for semantically relevant terms to appear at the end of each sentence but that isn’t necessarily how all of these words are used in practice. Regardless, an FSA using a restricted vocabulary from extracted sentences coupled with a neural language model, we believe, will be the best of both worlds. We gain assurance in semantic quality uses aspects of extractive summarization and structure we introduce while allowing the generation a reasonable amount of improvisation.&lt;/p&gt;&lt;h4&gt;BIO-Tagging Approach for Sentence Extraction&lt;/h4&gt;&lt;p&gt;For what could be a part of the FSM-style definition generation, or even a standalone definition extractor, we plan on labeling sentences throughout the corpus using BIO tags. This approach proposed by AI2 researchers involves the use of distant supervision to label sentences conducive to definition structure and semantics by picking sentences that meet a ROUGE threshold w.r.t to gold standard definitions. We would then collect triples of terms, their gold standard definitions, and their BIO-tagged sentences. We could then train a sequence tagger to recognize what sentences in a paper are conducive to definitions and which ones aren’t.&lt;/p&gt;&lt;p&gt;Possible sources of gold standards to use for ROUGE are include WordNet, which has a large breadth of glosses but each gloss tends to be very short. We are also exploring the idea of using UMLS which would provide technical medical terms along the lines of what we’d like the model to be able to define, and another data set composed of NELL and Freebase which can be found &lt;a href=&quot;http://rtw.ml.cmu.edu/wk/WebSets/glossFinding_wsdm_2015_online/index.html&quot;&gt;here&lt;/a&gt;. With “Automatic Gloss Finding for a Knowledge Base using Ontological Constraints”, Dalvi et al. set out to simplify KBs the same way we are, and they were kind enough to make this dataset of ~500k glosses available to the research community for continuing this work.&lt;/p&gt;&lt;p&gt;A supervised aspect of this project has been lacking until now, and we believe that incorporating this sequence-tagging or other intelligent forms of extracting rich, definition-like sentences will mean the language model and beam search won’t have to work as hard. The added assurance of a restricted search space to only what is relevant is better both for inference and training.&lt;/p&gt;&lt;h4&gt;Progress&lt;/h4&gt;&lt;p&gt;So far, the most difficult part of our project has been determining a more advanced approach to start with. Though the initial baseline model was easy to come up with, this model took several days to design. Most of the work we accomplished over the past few weeks involved talking with Waleed Ammar from AI2 and reading several research papers in order to define the architecture we have proposed above. As such, we have not made enough progress on this approach to evaluate it thoroughly. In this section, we list a breakdown of all the tasks we have.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The infrastructure and interface for constructing the FSM is complete&lt;/li&gt;&lt;li&gt;The system used to determine keywords given a term is in progress. We are currently debating whether or not to use pre-trained word vectors and are working on finishing the code&lt;/li&gt;&lt;li&gt;The extraction phase of the model (after retrieving the keywords) is not complete as it relies on the keywords. However, this part should be fairly straightforward and should be complete by the end of the week&lt;/li&gt;&lt;li&gt;Beam search decoding is in progress and the functionality to find the next beam is complete but the infrastructure for deciding when to terminate the search is in progress.&lt;/li&gt;&lt;li&gt;Although the RNN for beam search through the FSM is already written (we can use the same RNN from the original baseline), it needs to be trained on a corpus, preferably the Semantic Scholar corpus&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Another challenge we are currently facing involves organizing the data. In our original baseline model, the loss would steadily decrease as we trained on a single document but would then suddenly spike upward when we switched to a new document. The AI2 Semantic Scholar dataset consists of many different types of research papers including Computer Science, Medical, and various other domains. As such, the language in each domain differs drastically, so organizing the papers into linguistically similar groups has remained a challenge. Currently, the API provides no such tools for categorizing the papers.&lt;/p&gt;&lt;h4&gt;Conclusion&lt;/h4&gt;&lt;p&gt;Overall, we hope that this approach is a step closer to defining an architecture specific to the definition generation task. That said, there are several ways to improve the individual pieces of this architecture. We can change the hyper parameters of models in every phase, change the way they are trained, and introduce new concepts such as sequence tagging to improve the quality of the text generated by the model. This current architecture gives us a baseline on which we can continually improve.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=79ed786e5c74&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/baseline-ii-and-updates-79ed786e5c74&quot;&gt;Baseline II and Updates&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-25T05:43:39+00:00</dc:date>
	<dc:creator>Karishma Mandyam</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-9203775015655831448.post-597849658553454254">
	<title>Pinyi Wang, Dawei Shen, Xukai Liu &lt;br/&gt; Team Overfit: #5 Milestone: Strawman/Baseline II</title>
	<link>https://teamoverfit.blogspot.com/2018/04/5-milestone-strawmanbaseline-ii.html</link>
	<content:encoded>&lt;h2 style=&quot;height: 0px;&quot;&gt;&lt;span&gt;Team Overfit&lt;/span&gt;&lt;/h2&gt;&lt;h3&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3&gt;&lt;span&gt;Project repo: &lt;span style=&quot;font-size: 18.72px;&quot;&gt;&lt;a href=&quot;https://github.com/pinyiw/nlpcapstone-teamoverfit&quot;&gt;https://github.com/pinyiw/nlpcapstone-teamoverfit&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h4&gt;&lt;span&gt;Team members: Dawei Shen, Pinyi Wang, Xukai Liu&lt;/span&gt;&lt;/h4&gt;&lt;div style=&quot;text-align: start; text-indent: 0px;&quot;&gt;&lt;div style=&quot;margin: 0px;&quot;&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;Blog Post: #5: 04/24/2018&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;span&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Social Media Predicts Stock Price (StartUp Mode)&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;span id=&quot;docs-internal-guid-8a395488-fb3a-d9a7-cec2-8d3cb31f0a59&quot;&gt;&lt;span&gt;This week, we tried to use tf-idf and seq2vec to process news headlines as input to our LSTM &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;model to predict UP/DOWN for APPLE stock price.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;&lt;b&gt;Data Preprocessing&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Crawling historical news headlines from Twitter&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Categorize tweets with date and correlated stock price&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Cleanup unrelated data&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Map vocabulary to index with removing words that appear less than 3 times in the tweet news corpus.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;Seq2One with TF-IDF&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;TF-IDF&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Correlate word frequencies with price changes (convert words into normalized frequency count vector)&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;Seq2One&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Correlate average word embeddings with price changes (convert words into normalized embedding vector)&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;Bidirectional RNN&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Enhance the performance by knowing before and after prices&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;Evaluation&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Accuracy on predicting Up or Down&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Accuracy on predicting Stock Price (TODO)&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;Result&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;TF-IDF: 60.7% accuracy predictin UP/DOWN on test set with 273 total data points and 9/1 train-test split&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Seq2Vec: 53.6% accuracy predicting UP/DOWN on test set with 273 total data points and 9/1 train-test split&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;Error Analysis&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;b&gt;Things that may impair the precision of prediction&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Words with high frequency but have low effect on stock prices:&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;A, is, of, for, the&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Tweets talking about fruit &lt;i&gt;apple&lt;/i&gt; instead of the company&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;Tweets news that have high frequency but low effect on stock prices and news that may have high effect on stock prices but with low frequencies&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Losing information of each single tweet as we have combined all tweets in a day together to form a word frequency vector and do the prediction&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;&lt;b&gt;Things that could be done to improve the performance&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Add the overall market stock price as part of the input because it usually also has big impact on a single companies stock price.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Crawl more data for more companies and longer duration&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Add financial&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Add the competitor companies stock prices and tweets news as part of the input.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span id=&quot;docs-internal-guid-8a395488-fb3c-6f01-c575-279c6f3d6421&quot;&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-04-25T05:20:00+00:00</dc:date>
	<dc:creator>Team Overfit</dc:creator>
</item>
<item rdf:about="http://cse481n-capstone.azurewebsites.net/?p=51">
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: More Data Collection and Baseline</title>
	<link>http://cse481n-capstone.azurewebsites.net/2018/04/24/more-data-collection-and-baseline/</link>
	<content:encoded>&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Data Collection: &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In the last blog we discussed the challenges of trying to find general Reddit posts that were similar to the collected MeanJokes posts. Even limiting to posts with Jaccard Similarity &amp;gt; .3 a lot of the data looked like the following:&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;MeanJokes Post: “Don’t be offended but Fuck you”
    Similar Post: “fuck Foligno”
    Similar Post: “fuck narek”
    Similar Post: “fuck&quot;
    Similar Post: “Fuck me?”
    Similar Post: “Fuck me”
    Similar Post: “fuck me”
&lt;/span&gt;    Similar Post: “Fuck it”
    Similar Post: “Fuck”
    Similar Post: “Who the fuck are you?”&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;These wouldn’t be very interesting examples to eventually train a model on. We also noticed that because of the nature of Jaccard Similarity and the sparsity of language in our collected Reddit posts, most of the posts that matched our MeanJokes posts would be very short, containing one or two key phrases from the MJ post. Posts made to Reddit are typically either very long or very short, so to make use of those long posts we decided to split them up by sentence and consider every sentence individually. We would also filter out sentences that are below a certain number of tokens, so that we avoid examples like the above.&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;MeanJokes Post: “How is ScizorSci like Hoss McDank? They’re both faggots!”
&lt;/span&gt;    Similar Post: “How long was he like that?”
    Similar Post: “More like CRAPitalism (this but unironically)
    Similar Post: “Volcanoes are like earth pimples”
    Similar Post: “I cried like a bitch”
    Similar Post: “She doesn’t like jewelry”
    Similar Post: “Everyone was like daaaaayum”
    Similar Post: “Don’t speak to me like that”
    Similar Post: “Don’t like the smell of this at all”
    Similar Post: “A few others I like are”
    Similar Post: “It’s like I’m on fire”&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;These are better than the previous examples but still the similarities are very shallow. Most of the matches are just because there were one or two content phrases that matched between them. This could be expected from having a Jaccard Index cutoff as low as .3, since usually you want one that is somewhere above .7, but the language used in these posts is too sparse to be this picky and still have enough data to train a neural network. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We could possibly try similarity on word embeddings or sentence embeddings, but we liked using Jaccard Index because we actually care about the specific words used and not just the semantic meaning. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our main issue ended up being that we assumed most posts would be conversationally structured with short-ish text, when in reality we found that posts are either really short, some collection of tags for indexing or trading, really long posts with at least a paragraph of text about some abstract subject, or requests for sexual favors. Overall this makes general reddit posts quite different from r/meanjokes, so at the surface level jaccard index won’t really do much, and furthermore general reddit posts won’t be conversational in structure the way r/meanjokes posts are. For these reasons we will have to move on to looking at comments instead, since we believe that they will be more conversational than posts. We originally wanted to use posts instead of comments since posts are contextually self-containing while comments are typically responses to multi-person conversations. However, we ended up splitting posts into independent sentences anyway, so this reasoning for avoiding comments became moot.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Baseline Model:&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;This week, we developed a baseline Neural Network model using allennlp. The model architecture is simple. We used pretrained &lt;/span&gt;&lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;glove twitter word embeddings&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;, encode each tweet with a recurrent neural network (e.g. RNN, LSTM, GRU) sequence to vector encoder, and finally feed the vector into a feed-forward network with softmax at the end. We experimented with glove twitter word embeddings with 50 dimensions. We also tried different flavors of 1 layer recurrent neural network sequence to vector encoders, more specifically, GRU, LSTM, BiLSTM, and RNN. By the time this blog is written, we have yet performed extensive hyperparameter tuning. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Among the 4 different model setups that we tried, one of the models (Model1) got the highest accuracy, recall, and f1 score on test data, while another model (Model2) got the highest precision on test data. Below are their performances on dev and test dataset. &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model1 — embedding: 50-dimensional glove twitter embeddings, encoder: 1 layer GRU, epoch chosen: 20&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Test&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8245&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8181&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7934&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7896&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7995&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7947&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7964&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7921&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model2: — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer BiLSTM, epoch chosen: 5&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Test&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8409&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8175&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8239&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8004&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7909&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7627&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8070&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7811&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;These two models’ setups are almost the same except for model1 uses a GRU encoder while model2 uses an BiLSTM encoder. Surprisingly, Model1 ends up having better overall performance on test data than the ones with more complex encoders like BiLSTM. &lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Error Analysis:&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We will use the Model2’s errors in our error analysis:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;First we will look into sentences that are hateful but our model classified as none:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;@RichardDawkins @AbuelitoSerge Really, Muslims understand this. They just want to be able to use the name “racism” to shut us up.&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;@RTUKnews An Islamist human rights group? LOL. Now there is a contradiction in terms.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;From the above examples, we found that our model is not good at understanding the underlying meaning of a sentence. For example, the 2rd one implies Islamist doesn’t care about human rights, which is attacking Islam people. However, since this sentence does not have any words that are very sensitive, our model considered it as OK instead of hateful.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Here are some other sentences that are not hateful but our model classified then as hateful:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;@Strubbestition Name one thing that is not an opinion but is still sexist. I will wait.&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;@Bipartisanism @AllooCharas Terrorism involves a political or religious objective to the terror.Most mass murderers have personal objectives&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;On the other hand, we found a trend that sentences including words like “sexist”, “crime” are classified as hateful disregarding what exactly the post means. For a concrete example, the 3rd sentence from 2nd group is not saying anything hateful but our model considered it as hateful. We suspect that because “murderers” appeared in that sentence and in our training data and most other sentences with such word is hateful, our model picked up such pattern and made a wrong decision.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Why we end up have a pretty bad result? We have two possible reasons:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our dataset used is really small (15k sentences in total) and dataset itself is really noisy. For example, “@dgbattaglia Saw this this morning… http://t.co/9YUwOuZugw” is somehow labeled as hateful as true label in original dataset.&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our model is not expressive enough to recognize more complicated patterns. This also has something to do with the dataset. With such a small dataset, we cannot really train a deep or more complicated model.&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Next Steps: &lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;What we are seeing in training is is this general pattern. We suspect it is because the dataset we have (around 15k tweets) is too small for a neural network model. We would want to try combine another &lt;/span&gt;&lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;twitter hate speech dataset (by Thomas Davidson et. al.)&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; and Waseem’s twitter dataset and train different neural net models on the combined dataset. &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;&lt;br /&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; class=&quot;alignnone wp-image-54&quot; height=&quot;192&quot; src=&quot;http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/04/P3-300x138.png&quot; width=&quot;417&quot; /&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Data Sources: &lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06&quot;&gt;Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Automated-Hate-Speech-Detection-and-the-Problem-of-Davidson-Warmsley/6ccfff0d7a10bf7046fbfd109b301323293b67da&quot;&gt;Davidson, Thomas J et al. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM (2017).&lt;/a&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-25T05:00:21+00:00</dc:date>
	<dc:creator>Team Watch Your Language!</dc:creator>
</item>
<item rdf:about="http://mathstoc.wordpress.com/?p=323">
	<title>Kuikui Liu, Nicholas Ruhland &lt;br/&gt; Team INLP: NLP Capstone Post #5: A New Hope</title>
	<link>https://mathstoc.wordpress.com/2018/04/25/nlp-capstone-post-5-a-new-hope/</link>
	<content:encoded>&lt;h1&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Dataset Improvements&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Last time, on Music NLP.&lt;/span&gt;&lt;/i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; We ran into many midi data parsing issues. Since then, we have discovered a new dataset called the Lakh MIDI Dataset (&lt;/span&gt;&lt;a href=&quot;http://colinraffel.com/projects/lmd/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;http://colinraffel.com/projects/lmd/&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;) that comes with reasonably well-formed midi files. Using the “Clean MIDI Subset”, we found thousands of midi files with their associated song names and songwriters. From these midi files, we extracted all with nonempty “lyric” fields when parsed via the pretty_midi package (which, incidentally, is also developed by Colin Raffel). After this step, we were left with ~1200 midi files that contain lyrics.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We currently clean the lyrical content by removing all unusual symbols and setting all characters to lowercase. We leave all lyrical tokens as is, which typically means syllable. Due to the inconsistent quality of the MIDI annotations, many songs are tokenized instead to characters, words, or even sentences. We will explore other methods for processing data if this is not sufficient for our results.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;It is unfortunate we did not find this dataset sooner, because most of our challenges up to this point have been dealing with the poor quality of the gathered data.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Alignment&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For our task of producing karaoke style output, there are two main tasks we have to solve. The first task is the generation of plausible lyrics, and the second is to align the lyrics to the proper time along the musical data. The alignment task has been studied extensively, but specifically aligning lyrical content to MIDI has not been covered in literature we have found. The most common alignment task is lyrics to audio data, as opposed to MIDI. The other common task is to align audio data to the notes defined in a MIDI file. In [1], they show a method that takes a MIDI file with annotated lyrics and uses this to align the lyrics to the raw audio. Unfortunately this is not our task, because we are trying to generate the annotated MIDI.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;This week, we have decided to ignore the alignment task and focus primarily on making a reasonable lyrical model. We will return to alignment next week.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The next step was to align the lyrics with pianoroll. Fortunately, well-formed midi data parsed into PrettyMIDI objects come with a “get_piano_roll” function that takes as input a list of “times” which correspond to where in time pretty_midi will attempt to sample the music. As each syllable in the lyrics comes with a start time for when the singer enunciates it, we can pass in these start times to produce pianoroll that is aligned (up to small error) with the lyrics.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For some implementation reasons that are difficult to explain in English, it is possible for “get_piano_roll” to produce NaN entries, which we have replaced with zeros. Due to this and the potential for other such problems, we have forked the pretty_midi package and will be able to modify the code for our needs. For example, as pointed out in [2], “in a given MIDI file there is no reliable way of determining which instrument is a transcription of the vocals in a song”. As such, there are many choices for how to do alignment; pretty_midi has implemented just one. It is an interesting task to see how different alignment methods help or hurt our models.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Lyric prediction&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Now that we have aligned pianoroll to lyrics data, we can begin engineering the model. Last time, we used an LSTM to generate lyrics given starting characters. Here, we will again use LSTMs, but instead, work at the syllable level and take as input the pianoroll of a song. As each column of a pianoroll is a time slice, each input vector to the LSTM is a single time slice. Each time slice is a 128-dimensional vector, with each entry representing the activation of an instrument; there are 128 midi recognized “instruments”.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;All that is left is to play with the architecture. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;RNN model&quot; class=&quot;alignnone size-full wp-image-322&quot; src=&quot;https://mathstoc.files.wordpress.com/2018/04/rnn-model1.png?w=676&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;At the moment, our pipeline looks like what is shown in the diagram. At each iteration, we take a song, extract the lyrics and the corresponding pianoroll data. We then feed each time slice of the pianoroll data through an encoder unit, then through an LSTM unit, then through a decoder unit, and finally through a softmax to produce the prediction. Our loss is the negative log-likelihood (negative logarithm of the RNN softmax probability of the true syllable).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We will compare our final model to this baseline with respect to the loss on a held-out validation set. We will also experiment with loss functions other than cross entropy to see how it affects the actual lyrical output.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model results&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We have so far only trained our model for a single iteration over the training set. For an empirical evaluation on the current model quality, we ran a single MIDI through the input and computed the argmax word for each output. This produced a result in which every predicted lyric was an empty message, which is the most common string in the training set. We will explore methods to handle this class imbalance as our next task.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[1] Müller, Meinard &amp;amp; Kurth, Frank &amp;amp; Damm, David &amp;amp; Fremerey, Christian &amp;amp; Clausen, Michael. (2007). Lyrics-Based Audio Retrieval and Multimodal Navigation in Music Collections. 4675. 112-123. 10.1007/978-3-540-74851-9_10.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;font-weight: 400;&quot;&gt;[2] &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Raffel, Colin and Daniel P. W. Ellis. “Extracting Ground-Truth Information from MIDI Files: A MIDIfesto.” &lt;/span&gt;&lt;i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;ISMIR&lt;/span&gt;&lt;/i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; (2016). &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; &lt;/span&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-25T04:44:57+00:00</dc:date>
	<dc:creator>Nicholas Ruhland</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-5600014144802012716.post-1597347106413431292">
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog 5: Strawman II</title>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/04/blog-5-strawman-ii.html</link>
	<content:encoded>&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Currently my model is at 20.1 Bleu and .02 EM. The state of the art has 23 Bleu and .08 EM. I'm&lt;/span&gt;&lt;br /&gt;&lt;span&gt; not doing error analysis yet since I'm only able to train my model on 1/3 of the data due to &lt;/span&gt;&lt;br /&gt;&lt;span&gt;performance and memory issues. I've cut training time in half through several optimization &lt;/span&gt;&lt;br /&gt;&lt;span&gt;but there is a memory bug which I haven't found yet which prevents me from training on the &lt;/span&gt;&lt;br /&gt;&lt;span&gt;whole dataset. Thus, I will list what I've done on the performance end, and my action plan.&lt;/span&gt;&lt;/div&gt;&lt;span id=&quot;docs-internal-guid-17abcdb6-f838-25e9-608e-d57755b90e92&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Speed Optimizations:&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Problem - Training ⅓ dataset taking 6 hours per epoch.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Solutions implemented:&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;Get_states_to_consider slowest part of take_step. This was optimized along with the expensive padding operation in get_action_embeddings, and the inefficient looping and sorting in compute_new_states. This cuts the training time in half, but still takes 3 hours on training set for 1 epoch&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre-wrap;&quot;&gt;Solutions to explore:&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;Further areas to improve which cut down by about ½ hour are create_grammar_state, embed_actions, map_entity_productions. Here since there are 10,000 global rules which are processed for every batch in every iteration, just process them once in the constructor.&lt;/li&gt;&lt;/ul&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Memory Optimization:&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Problem - Low identifier threshold or high embedding dim like in paper, on 40,000 or more instances causes gpu out of memory error.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Debugging:&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;Printing all tensors in memory at the end of forward using python garbage collection package. Total size is around 115mb but gpu uses 12gb! Perhaps this is a memory leak?&lt;/li&gt;&lt;li&gt;Tried different configurations to see where its crashing, high embedding dim causes crash in action index select, while just large dataset causes crash in backward.&lt;/li&gt;&lt;/ul&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Solutions that I've tried:&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;Doing index select on action_embeddings for previous embedding to save space.&lt;/li&gt;&lt;li&gt;All finished states del’d&lt;/li&gt;&lt;li&gt;Del keyword used frequently after tensor no longer used.&lt;/li&gt;&lt;li&gt;Disabling cudnn backend&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Solutions to explore:&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Using pytorch DataParallel package.&lt;/li&gt;&lt;li&gt;Split only when a state has finished.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;Let me know if you guys have any ideas on solving this gpu out of memory issue!&lt;/div&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-04-24T15:14:00+00:00</dc:date>
	<dc:creator>nlpcapstone</dc:creator>
</item>
<item rdf:about="http://sarahyu.weebly.com/cse-481n/actual-strawman-update">
	<title>Sarah Yu &lt;br/&gt; Team Jekyll-Hyde: Actual Strawman Update</title>
	<link>http://sarahyu.weebly.com/cse-481n/actual-strawman-update</link>
	<content:encoded>&lt;div class=&quot;paragraph&quot;&gt;&lt;strong&gt;Real Data and Results Have Been Seen! &lt;/strong&gt;&lt;br /&gt;As I mentioned in my last post, I was struggling with accessing the data, but I've since solved my problems and got to learn some cool tools along the way (like apparently you can read a compressed file without decompressing??? wild). I've also spent a large part of the week learning and fighting with SqlAlchemy, PyMySQL, MySQL, and UTF-8 issues. With the interaction of all of these, I was able to read (most of the) Reddit posts of January 2017 (thanks to Jason Baumgartner publishing these dumps on pushshift.io, I will donate when I have an income) which amounted to 80 million posts, find the users we are interested in, find neurotypical subreddits these users post to, and then get posts of our two (neurotypical and neurodivergent) subreddit subsets. &lt;br /&gt;&lt;br /&gt;Side Note: I'm going to start referencing the Neurodivergent set as ND, and Neurotypical as NT, trying to save some typing&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;Baseline #1 (kind of an update of the Strawman #1):&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Glen Coppersmith and Erin Kelly (2014). &lt;strong&gt;&lt;em&gt;Dynamic Wordclouds and Vennclouds for Exploratory Data Analysis. &lt;/em&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;font size=&quot;2&quot;&gt;Association for Computational Linguistics Workshop on Interactive Language Learning and Visualization&lt;/font&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt; With thanks to Coppersmith and Kelly, I was able to make a Term Frequency Venncloud as seen below that show in black the most frequent terms found in both subreddit subsets, and then separated into the most frequent terms in neurodivergent subreddits and neurotypical subreddits in blue and red, respectively. &lt;/div&gt;  &lt;div&gt;&lt;div class=&quot;wsite-image wsite-image-border-none &quot; style=&quot;padding-top: 10px; padding-bottom: 10px; margin-left: 0px; margin-right: 0px; text-align: center;&quot;&gt; &lt;a&gt; &lt;img alt=&quot;Picture&quot; src=&quot;http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-04-26-at-1-48-22-pm_orig.png&quot; style=&quot;width: auto;&quot; /&gt; &lt;/a&gt; &lt;div style=&quot;display: block; font-size: 90%;&quot;&gt;Naurodivergent vs. Neurotypical Subreddit Venncloud&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;  &lt;div class=&quot;paragraph&quot;&gt;&lt;br /&gt;As we can see, the middle of the venncloud is pretty uninteresting, but here are some notable points:&lt;ul&gt;&lt;li&gt;Personalization: Frequency of you/me, your/my words which indicate some sense of relationship and more personalization between ND posters. This contrasts the frequency of they, indicating some discussion of an other, in the NT subreddits.&lt;/li&gt;&lt;li&gt;&quot;is&quot;: I interpreted the frequency of this word in the NT subreddits as a more definitive and declarative way of speech, rather than other words such as &quot;think&quot;, &quot;feel&quot; and &quot;maybe&quot; (in the ND subreddits) which signal more hesitation. This is a point touched on and described as dogma in Fast &amp;amp; Horvitz which is one of the papers I discussed in a previous post.&lt;/li&gt;&lt;li&gt;&quot;www&quot;, &quot;imagesofnetwork&quot; :  This is something I cold probably fix; the way I pre-process the data scrubs and separates the links into separate words. At the end of the day though, this shows that there are significantly more links in NT subreddits. My thought is that the lack of such in the ND subreddits might mean more anecdotal and personal interactions than when compared to ND subreddits&lt;/li&gt;&lt;li&gt;Moral Adjectives: Some of the ND frequently used terms are what I am going to call Morale Adjectives (let me know if there's an actual term for this); here I mean, we see words like &quot;good&quot;, &quot;right&quot;, &quot;bad&quot;, which are often used to describe habits or behavior.&lt;/li&gt;&lt;li&gt;SURPRISE GENDER DIFF: As you can see, 'she' is one of the most frequent ND words, whereas 'he' is  one of the most frequently used NT words. Some thoughts: 1) doesn't show anything, there are some partner subreddits and may just show that the predominantly male reddit user base talks about different genders in the two, but they themselves may not be a different gender distribution or 2) could show different gender engagement in the different subsets.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;strong&gt;Baseline #2: Connotation Frames&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;&lt;span style=&quot;color: rgb(0, 0, 0);&quot;&gt;Hannah Rashkin, Sameer Singh, Yejin Choi. 2016. &lt;strong&gt;&lt;em&gt;Connotation Frames: A Data-Driven Investigation.&lt;/em&gt;&lt;/strong&gt;&lt;font size=&quot;2&quot;&gt; In Proceedings of ACL 2016&lt;/font&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;color: rgb(0, 0, 0);&quot;&gt;Maarten Sap, Marcella Cindy Prasettio, Ari Holtzman, Hannah Rashkin, &amp;amp; Yejin Choi. 2017. &lt;em&gt;&lt;strong&gt;Connotation Frames of Power and Agency in Modern Films.&lt;/strong&gt;&lt;/em&gt; &lt;font size=&quot;2&quot;&gt;sched. to appear EMNLP 2017 short papers. &lt;/font&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt; With help from Maarten Sap, another model I explored was the Connotation Frames formalism, to look at the verbs used in both our NT and ND subreddits and the sentiments these provide between agent and subject. However, we found no significant differences between the two (output below).&lt;/div&gt;  &lt;div&gt;&lt;div class=&quot;wsite-multicol&quot;&gt;&lt;div class=&quot;wsite-multicol-table-wrap&quot;&gt; 	&lt;table class=&quot;wsite-multicol-table&quot;&gt; 		&lt;tbody class=&quot;wsite-multicol-tbody&quot;&gt; 			&lt;tr class=&quot;wsite-multicol-tr&quot;&gt; 				&lt;td class=&quot;wsite-multicol-col&quot; style=&quot;width: 50%; padding: 0 15px;&quot;&gt; 					 						  &lt;div&gt;&lt;div class=&quot;wsite-image wsite-image-border-none &quot; style=&quot;padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;&quot;&gt; &lt;a&gt; &lt;img alt=&quot;Picture&quot; src=&quot;http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/screen-shot-2018-04-26-at-9-45-10-am_orig.png&quot; style=&quot;width: auto;&quot; /&gt; &lt;/a&gt; &lt;div style=&quot;display: block; font-size: 90%;&quot;&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;   					 				&lt;/td&gt;				&lt;td class=&quot;wsite-multicol-col&quot; style=&quot;width: 50%; padding: 0 15px;&quot;&gt; 					 						  &lt;div&gt;&lt;div class=&quot;wsite-image wsite-image-border-none &quot; style=&quot;padding-top: 10px; padding-bottom: 10px; margin-left: 0; margin-right: 0; text-align: center;&quot;&gt; &lt;a&gt; &lt;img alt=&quot;Picture&quot; src=&quot;http://sarahyu.weebly.com/uploads/2/4/3/0/24307463/nt-verbs_1_orig.png&quot; style=&quot;width: auto;&quot; /&gt; &lt;/a&gt; &lt;div style=&quot;display: block; font-size: 90%;&quot;&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;   					 				&lt;/td&gt;			&lt;/tr&gt; 		&lt;/tbody&gt; 	&lt;/table&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;  &lt;div class=&quot;paragraph&quot;&gt;&lt;br /&gt;&lt;strong&gt;Baseline #3: LIWC2015&lt;/strong&gt;&lt;br /&gt;Finally, I used LIWC2015 to count and classify the psychological meanings and categories for both NT and ND subreddits. This serves as another type of language model to define these two 'languages' and offers us another metric on which to find similarities and differences. ​&lt;br /&gt;&lt;br /&gt;Just for some clarification, the way that this model works is by having 73 categories (more information available &lt;a href=&quot;https://liwc.wpengine.com/compare-dictionaries/&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;), anywhere from topics - PRONOUN, HEALTH, BIO - to grammar - VERB, ARTICLE - and gives the percentage of the language that each category accounts for in that 'language'. In our case, we see the distribution of categories in NT subreddits compared to ND subreddits. &lt;br /&gt;&lt;br /&gt;&lt;span&gt;My hypotheses were:&lt;/span&gt;&lt;ul&gt;&lt;li&gt;[You, Heshe, Pronoun, Health, Feel, They] categories would be significantly higher in ND &lt;/li&gt;&lt;li&gt;[Anger, Power, Swear] categories would be significantly lower in ND than in NT&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;After getting the results, I report the top 10 categories with the largest % difference between the two. &lt;ul&gt;&lt;li&gt;HEALTH(3.34x), &lt;span&gt;INGEST(2.69x), BIO(2.13x)&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;These categories are topic specific (ingestion related to drug subreddits and bio on biological processes) and align with what we expect in mental health topic subreddits&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;FEEL(2.04x), SAD(1.88x), ANX - anxiety (&lt;span&gt;2.86x)&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;​Also make sense for the support communities within the ND group, potentially, topical for &quot;anxiety&quot; as a temporary and consistent feeling&lt;/span&gt;&lt;br /&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;FAMILY(2.07x), HOME (2.05x)&lt;br /&gt;&lt;ul&gt;&lt;li&gt;This was a bit surprising, I believe appeals to the family and home tend are prominent in support groups as well as the &quot;partners of&quot; subreddits we have in the ND group&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;I (1.73x)&lt;ul&gt;&lt;li&gt;There seems to also be a lot of personal discussion, which we expect in subreddits that are meant to discuss personal problems&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;FEMALE (1.81x)&lt;br /&gt;&lt;ul&gt;&lt;li&gt;ITS HERE AGAIN WHY AND HOW&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;​&lt;br /&gt;That's all for baseline models, here's to my first attempt at the more advanced model this week...&lt;/div&gt;</content:encoded>
	<dc:date>2018-04-24T07:00:00+00:00</dc:date>
</item>
<item rdf:about="https://medium.com/p/79ba10bbc70c">
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Baseline Evaluation and Ideas for Evaluating Fully-featured Graphs</title>
	<link>https://medium.com/@ryanp97/baseline-evaluation-and-ideas-for-evaluating-fully-featured-graphs-79ba10bbc70c?source=rss-6378d85d3a9b------2</link>
	<content:encoded>&lt;p&gt;Since the last week, I’ve been working on cleaning the model’s predictions in order to be able to evaluate it using SMATCH (note the following sections focus on the model trained on and predicting featureless graphs). There were a couple of cases in which the model was unable to produce a well formed graph, but those were few and far between.&lt;/p&gt;&lt;h4&gt;Dealing with Ill-formed Graphs&lt;/h4&gt;&lt;p&gt;I decided to deal with ill-formed graphs in a harsh way until Jan any I can decide on a better heuristic for dealing with them. Though, for now, whenever I encounter an invalid graph, I immediately replace it with (999999999 / invalid). I’ve verified that this graph does not appear in the development set and works as a dummy graph. Whenever SMATCH encounters this graph, it will output an F1 score of 0.0 for that prediction, label pair since there are no matched triples.&lt;/p&gt;&lt;p&gt;Ideally, we would like to be more generous with partially correct graphs, but dealing with that is non-trivial and is probably something I will explore more after training and evaluating a model that is able to predict features.&lt;/p&gt;&lt;h4&gt;Example of Ill-formed Graphs&lt;/h4&gt;&lt;p&gt;Below is an example of an ill-formed graph that the model predicted. Though the parenthesis structure is well-formed (i.e. none are missing or mismatched), the model predicted that the node _hito_n was a re-entrancy, despite it not appearing anywhere else in the graph&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/482/1*YlYEDUelqNqShF1bnjc5RA.png&quot; /&gt;An example of the model predicting a re-entrancy that has not yet occurred in the graph.&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/439/1*2QvzOMoTdU_BhDB8Zhj-ig.png&quot; /&gt;The target graph for the above example. The model’s prediction is pretty far off, but it’s interesting to see that it is able to have fairly high precision on the nodes that it is able to generate.&lt;/figure&gt;&lt;p&gt;The model that was trained on the featureless data was able to learn the parenthesis structure and there were no cases, that I could find, where the model predicted mismatched or missing parenthesis.’&lt;/p&gt;&lt;h4&gt;Baseline Evaluation Results&lt;/h4&gt;&lt;p&gt;Even with the harsh treatment of ill-formed graphs, the model was able to achieve a F1 score of 0.65 on the development dataset. This was much higher than I was expecting and seems to be promising for further exploration. I suspect that once I begin to train the model on data that includes features, the model will have a harder time making completely accurate predictions (though the F1 score will likely be inflated as explained in the next section)&lt;/p&gt;&lt;h4&gt;Ideas for Evaluating the Feature-full Model&lt;/h4&gt;&lt;p&gt;Though it’s nice to have the model correctly predict the mood, tense, etc. of a sentence, these may not be entirely helpful when evaluating the model. By including these features, we increase the number of triples that SMATCH looks for. There are a couple different ideas that Jan, Michael, and I discussed when it comes to evaluating the predicted graphs:&lt;/p&gt;&lt;p&gt;The first option is to leave the features squashed. This is probably the simplest approach, but is not ideal since it does not allow for partial credit. If the model misses a single feature (i.e. predicts the mood as tensed rather than untensed, the entire triple would be wrong even though other features are correct).&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*iQNyCAiGWw0w5YIXND8THw.png&quot; /&gt;Example of squashed features. Note this is the same example from last week’s post.&lt;/figure&gt;&lt;p&gt;The second option is to uncompress the features. In the case of the previous picture, it would be removing the equals signs and splitting the “super feature” based on the colons. This approach is a bit nicer in terms of giving partial credit, but is still not ideal since in increases the number of triples. We believe that this would inflate the SMATCH score to something that is not entirely useful and most of the score would be based on the feature triples and not the actual semantic meaning.&lt;/p&gt;&lt;p&gt;The third and final option we discussed was to selectively include features during evaluation. During training and prediction, the model would take in the entire squashed feature as input and output squashed features during prediction. However, we would uncompress the features and then choose a couple (or none at all) to keep when we post-process the graphs. With this, we can evaluate using a couple different combinations of features and see how those affect the model’s performance.&lt;/p&gt;&lt;p&gt;Some of the features the model predicts are: tense, mood, pass, etc. We can choose to keep only mood and tense if we believe that these are the most useful features for semantic transfer and see how that affects performance vs. a featureless model or some other combination of features.&lt;/p&gt;&lt;h4&gt;Training a Fully-featured Model&lt;/h4&gt;&lt;p&gt;Currently I’m training a model on data that includes the squashed features, though it will not be evaluated in time for this post or for the presentation. The model is the same exact model as the featureless model, so I expect that it may have a more noticeable issue matching parentheses and generating well-formed graphs in general until I have some time to experiment with the hyperparameters.&lt;/p&gt;&lt;p&gt;For reference, the models (both featureless and feature-full) are trained with the following architecture and hyperparameters:&lt;/p&gt;&lt;pre&gt;Encoder:&lt;br /&gt;    RNN Type       :  LSTM    &lt;br /&gt;    Embedding Dim  :  500&lt;/pre&gt;&lt;pre&gt;Decoder:&lt;br /&gt;    RNN Type       : Stacked LSTM&lt;br /&gt;    Layers         : 2&lt;br /&gt;    Embedding Dim  : 500&lt;br /&gt;    Dropout        : 0.3&lt;/pre&gt;&lt;pre&gt;Attention:&lt;br /&gt;    Type           : Global (&lt;a href=&quot;https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb&quot;&gt;general [Luong]&lt;/a&gt;)&lt;/pre&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=79ba10bbc70c&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-24T06:17:52+00:00</dc:date>
	<dc:creator>Ryan Pham</dc:creator>
</item>
<item rdf:about="http://deeplearningturingtest.wordpress.com/?p=16">
	<title>Ananth Gottumukkala &lt;br/&gt; Team Turing Test: Strawman/Baseline 1: Deep Recurrent Q Network</title>
	<link>https://deeplearningturingtest.wordpress.com/2018/04/20/strawman-baseline-1-deep-recurrent-q-network/</link>
	<content:encoded>&lt;p&gt;For this week, I decided to first develop the RL model for asking questions. I chose to try a Deep Recurrent Q Network (DRQN) first (and the policy gradient method later) using the following repository as a starting point:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/awjuliani/DeepRL-Agents/blob/master/Deep-Recurrent-Q-Network.ipynb&quot; rel=&quot;nofollow&quot;&gt;https://github.com/awjuliani/DeepRL-Agents/blob/master/Deep-Recurrent-Q-Network.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository implements the DRQN for games by using LSTM cells to encode sequential information as successive frames are passed in for each time step. These frames are then passed through a CNN, then the LSTM cell, and then output the Q values. In the code I replaced where the input image (frame) is passed into the DRQN with the 2D word embedding matrix (GloVe vector for each word in sentence) and passed it straight into the CNN. I also gave a default reward of 0 to the model and managed to run the model without errors.&lt;/p&gt;
&lt;p&gt;My goal for next week is to change the reward allocation to be user input, hardcode all the question templates, and begin training the model.&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-20T06:47:18+00:00</dc:date>
	<dc:creator>ananthgo</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-5600014144802012716.post-2741407940361589303">
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog 4: Strawman I</title>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/04/blog-4-strawman-i.html</link>
	<content:encoded>I'm implementing the parser within the the wikitables semantic parser(Neural Semantic parsing with type constraints by Krishnamurthy et al) since both are solving similar tasks and it'd be a cool result if the same architecture worked for both tasks. The similarity is that both tasks need to generate the logical form which incorporates elements of a context. For wikitables its cell and column names and for this task its variable and method names.&lt;br /&gt;&lt;br /&gt;Results:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Wikitables framework - 4% accuracy.&lt;/li&gt;&lt;li&gt;Wikitables framework + parent states - 12% accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The wikitables baseline performs poorly and integrating the parent production rule used as input to the decoder cell in the java paper but not the wikitables paper results in a eight percent improvement. This is since the java production rule sequences are much longer and cannot just rely on the previous rule.&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;br /&gt;These results demonstrate that while that tasks are similar there are some key differences in the datasets that won't allow the exact same architecture to be used. Nonetheless, I will still be implementing the java parser within the wikitables framework incorporating necessary elements to boost performance.&lt;br /&gt;&lt;br /&gt;My code is here:&lt;br /&gt;&lt;b&gt;https://github.com/rajasagashe/allennlp/tree/enviro-linking&lt;/b&gt;</content:encoded>
	<dc:date>2018-04-19T19:13:00+00:00</dc:date>
	<dc:creator>nlpcapstone</dc:creator>
</item>
<item rdf:about="https://medium.com/p/be87c31976b7">
	<title>Halden Lin &lt;br/&gt; Team undef.: NLP Capstone | 04: First Steps</title>
	<link>https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7?source=rss-2759d54493c0------2</link>
	<content:encoded>&lt;p&gt;&lt;em&gt;previous posts: &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;&lt;em&gt;01&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5&quot;&gt;&lt;em&gt;02&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3&quot;&gt;&lt;em&gt;03&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Nearing 4 weeks in — I’ve finally got a foothold in the development process. Over this past week I’ve been looking through TensorBoard and TensorFlow source-code and documentation in an attempt to develop a foundation for plugin development.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Notable Resources:&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard&quot;&gt;TensorBoard Documentation&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/tensorboard&quot;&gt;TensorBoard Source Code&lt;/a&gt; and &lt;a href=&quot;https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins&quot;&gt;Existing Plugins&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/tensorboard-plugin-example/blob/master/README.md&quot;&gt;Developing a TensorBoard Plugin&lt;/a&gt; (a simple example)&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Completed Milestones:&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;Understand the structure of a TensorBoard Plugin, specifically how the architecture of the Attention Plugin should look like.&lt;/li&gt;&lt;li&gt;Design and write the data fetching layer.&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;Plugin Architecture&lt;/h4&gt;&lt;p&gt;In TensorFlow, data from iterations of training / evaluation is stored as a set of &lt;strong&gt;summaries&lt;/strong&gt;. These can take the form of any tensor, including text, image, scalars, or time series. These are written to disk as the computation graph is executed. Each ‘summary’ takes the form of a &lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/summary.proto&quot;&gt;Summary protocol buffer&lt;/a&gt;, which holds, in addition to the data stored, critical identifying information (tags and metadata). A plugin can then read summaries associated with particular tags and sessions from disk to serve to the TensorBoard front-end via a plugin back-end, where a visualization is rendered.&lt;/p&gt;&lt;p&gt;Following &lt;a href=&quot;https://github.com/tensorflow/tensorboard-plugin-example/blob/master/README.md&quot;&gt;&lt;em&gt;Developing a TensorBoard plugin&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;the Attention Plugin with have three primary components:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Data API layer. This is what allows users to capture relevant summaries from within their models.&lt;/li&gt;&lt;li&gt;Plugin backend, which serves said summaries.&lt;/li&gt;&lt;li&gt;Frontend, where the visualizations are displayed.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Unlike commonly used plugins such as the &lt;a href=&quot;https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/scalar]&quot;&gt;scalar&lt;/a&gt; and &lt;a href=&quot;https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/histogram&quot;&gt;histogram&lt;/a&gt; plugins, the Attention Plugin consumes 3 distinct values: the input text, the decoded output text, and the attention matrix that correlates the two. Initially, I attempted to store these all in a single Summary protobuf, where the first two rows of the encapsulating matrix would contain the text, and the rest would contain the attention weights. This results in a mixing of string and float types in a single Tensor, which is not valid (to my knowledge) in TensorFlow. I then realized I could store these separately, each in their own summary, and retrieve them via an identifying name. The resulting architecture is shown in the diagram below.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*QK7USkxsSC3u6F0LKbKWXQ.png&quot; /&gt;The architecture of the Attention Plugin.&lt;/figure&gt;&lt;p&gt;I decided to make the output text an optional summary, as models don’t necessarily need to decode (via Viterbi, Beam Search, or otherwise) an output sequence while training. The input text and attention matrix are still valuable, as summary statistics (e.g. coverage, important words, etc) can be gleamed without the decoded text.&lt;/p&gt;&lt;h4&gt;Data API Layer&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*0m1-fllpNYMP666FhR0nbA.png&quot; /&gt;Data API Layer highlighted in green.&lt;/figure&gt;&lt;p&gt;As the TensorBoard authors suggest, the data API layer, defined in attention_summary.py, provides two methods for creating Summary protobufs, which can then be written to disk via a FileWriter. The first is via a TensorFlow op, which can be thought of as a node in the computation graph, that produces a Summary when the graph is executed. The second is by directly creating the protobuf, which allows for data to be saved outside the execution of a TensorFlow session. I’ve implemented both of these. There is a separate method for each of the three datum used by the plugin (input text, output text, attention matrix), and each of the three summary datum are tagged differently (e.g. name/attention_input_summary, name/attention_output_summary) in order to allow for distinguishable retrieval later.&lt;/p&gt;&lt;p&gt;An example of the usage of both methods can be found in attention_demo.py.&lt;/p&gt;&lt;h4&gt;Next Steps&lt;/h4&gt;&lt;p&gt;I suspect the work completed this week was the biggest hurdle in terms of time:code ratio. I would not be surprised if I had to revisit the work done here in order to clean things up or fix small bugs. However, with this understanding and architecture nailed down, I expect implementation of the rest of the plugin will come at a faster pace. With that said, three tasks stand as immediate goals for the next week.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Modify my forked repository of the summarization model created by &lt;a href=&quot;https://arxiv.org/pdf/1704.04368.pdf&quot;&gt;See et al.&lt;/a&gt; (original found &lt;a href=&quot;https://github.com/abisee/pointer-generator&quot;&gt;here&lt;/a&gt;) to produce and save the appropriate summaries for the Attention Plugin. I’ve already started looking into this, and expect to have to fiddle around with the training / evaluation scheme in order to grab an appropriate amount of data.&lt;/li&gt;&lt;li&gt;Implement the backend of the Attention Plugin.&lt;/li&gt;&lt;li&gt;Begin prototyping visualizations (pen &amp;amp; paper) and acquire preliminary feedback.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Until next time.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=be87c31976b7&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-19T06:33:40+00:00</dc:date>
	<dc:creator>Halden Lin</dc:creator>
</item>
<item rdf:about="https://medium.com/p/a6690114c441">
	<title>Aaron Johnston, Lynsey Liu &lt;br/&gt; Team Viterbi Or Not To Be: Baseline Model #1</title>
	<link>https://medium.com/@viterbi.or.not/baseline-model-1-a6690114c441?source=rss-c522ef075bb3------2</link>
	<content:encoded>&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*bGQmjLRpsY3UnYhK6e8Lyg.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;This week, we implemented our &lt;a href=&quot;https://github.com/viterbi-or-not-to-be/viterbi-or-not-to-be/tree/master/baseline&quot;&gt;first baseline model&lt;/a&gt; for conversation summarization. In order to create a baseline that would be useful to build off of and compare our future results to, we decided to base this model off the Naive Bayes implementation described in the paper &lt;a href=&quot;http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf&quot;&gt;&lt;strong&gt;Summarizing Online Conversations: A Machine Learning Approach&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;To begin with, we identified the W3C Email Threads dataset as a useful starting point because of the fact that we have access to multiple email datasets, and because the paper mentioned above indicated that chatlog data would require a great deal of additional preprocessing to be able to manipulate. By comparison, the email data is fairly simple to interpret because it is rare for a single sender to convey their portion of a message across multiple emails, and the writing is generally more grammatically well-formed. The experiments in the paper run on the &lt;a href=&quot;https://flossmole.org/content/software-archaeology-gnue-irc-data-summaries&quot;&gt;GNUe Archives&lt;/a&gt; and the &lt;a href=&quot;https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/bc3.html&quot;&gt;BC3 Corpus&lt;/a&gt; (annotated W3C Email Threads).&lt;/p&gt;&lt;p&gt;The paper also uses &lt;a href=&quot;https://www.cs.waikato.ac.nz/ml/weka/&quot;&gt;Weka Toolkit&lt;/a&gt; for all of its model implementations, so we initially aimed to use Weka as well to reproduce our baseline. However, we encountered a few challenges while starting out with Weka that ultimately led to our abandoning the platform in favor of a more flexible Python implementation. The email data is in XML format, whereas Weka appears to work best with a format called ARFF, and in our preliminary examination we found ARFF to be fairly difficult to map directly to XML. In particular, ARFF’s apparent inability to handle dynamic-length data types or nested relations made it challenging to plan around — this was a problem because for each training example (email thread) in our dataset, we have a variable-length amount of information (number of emails, number of sentences per email, number of words per sentence, etc.). Without usage of dynamic-length data coupled with a toolset/interface more suited to data mining and application of basic, pre-made machine learning models than detailed feature engineering, we found Weka specifically difficult to use for feature extraction.&lt;/p&gt;&lt;p&gt;Instead, we ended up deciding to implement the baseline model in Python — including parsing the XML from our chosen dataset, extracting features, and using SciKit-Learn to train a Naive Bayes model to form the baseline itself. Since our aim with the baseline was to re-implement at least part of the work done by the &lt;a href=&quot;http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf&quot;&gt;&lt;strong&gt;Summarizing Online Conversations: A Machine Learning Approach&lt;/strong&gt;&lt;/a&gt; paper, we also chose to set up evaluation that would mirror their evaluation techniques, and settled on using a previously-written implementation of ROUGE for the task. Re-implementing the baseline model in Python based on previous work took more effort than using Weka would have, but was advantageous in that it helped us understand the specific challenges of the task and resulted in somewhat extensible code we can adapt for later phases of the project.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Feature Selection&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The Feature Selection section of the paper notes a “basic feature set,” which refers to features that are “not specific to conversations and consider the conversation as a simple piece of text.” For our baseline, we decided to target only the basic features to establish the difference that can be made by using conversation-specific features. Based on this basic set, our Naive Bayes baseline incorporates the following features —&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Mean TF-IDF: &lt;/strong&gt;Term frequency-inverse document frequency (TF-IDF) characterizes frequency of a word and reflects its importance to an email thread. Calculating TF-IDF results in a vector of word frequencies per email, and we take the mean of the values in the vector for the mean TF-IDF feature.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Mean TF-ISF: &lt;/strong&gt;Similar to TF-IDF but at a different granularity, term frequency-inverse sentence frequency (TF-ISF) characterizes frequency of a word and reflects its importance to an email. Calculating TF-ISF results in a vector of word frequencies per sentence, and we take the mean of the values in the vector for the mean TF-ISF feature.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Sentence Length: &lt;/strong&gt;Number of characters in the sentence.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Sentence Position: &lt;/strong&gt;Index of the sentence within the email.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Similarity to Title: &lt;/strong&gt;We represent the name of an email thread as a TF-ISF title vector, in the same form as described earlier (without taking the mean). This feature is then the result of the cosine similarity between the TF-ISF vector for a sentence and the title vector.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Centroid Coherence: &lt;/strong&gt;We represent the centroid of an email thread as the average of the TF-ISF sentence vectors. This feature is then the result of the cosine similarity between the averaged centroid vector and the sentence vector.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Special Terms: &lt;/strong&gt;Numbers and proper nouns are deemed as ‘special terms.’ This feature is the count of special terms in a sentence normalized (divided) by the number of special terms in the email thread.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Results and Evaluation&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;After implementing our baseline model, we used the ROUGE package to evaluate its summarization on a variety of metrics. We split the data into training and testing segments, placing 80% of email threads in the training set and 20% in the testing set. We then used our model and features to generate summaries and used ROUGE-L to measure their performance. After computing the average ROUGE-L, ROUGE-1, and ROUGE-2 F1 scores across all the email threads in the testing set, we produced the following results (along with the results reported in the baseline paper for comparison):&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*vjlGJvdBn_k4obtMdkGdIA.png&quot; /&gt;Mean F1 scores for various ROUGE metrics, compared to the results from &lt;a href=&quot;http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf&quot;&gt;&lt;strong&gt;Summarizing Online Conversations: A Machine Learning Approach&lt;/strong&gt;&lt;/a&gt;&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;Error Analysis&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;It is certainly helpful to have a set of metrics like ROUGE to establish a comparison between our model and existing models, but for a task such as automatic summarization it is an unfortunate reality that no existing metrics really come close to fully capturing the “success” of a summary. Comparing the compositions of two summaries provides valuable insight into the performance of a model, but ultimately a great deal of the usefulness of a summary revolves around its ability to convey the information contained in a conversation without needless filler and in a logical, simple format. Because there are no metrics that can describe how well a summary captured the topics in a conversation or how understandable it is, we will have to rely heavily on human interpretation to judge the success of our model.&lt;/p&gt;&lt;p&gt;For our baseline, we took a look at the generated summaries and discovered that while the general topic of each email thread is easily discernible, it is sometimes difficult to follow the course of the discussion. In addition, we found that our summaries were especially susceptible to the inclusion of “fluff” — short, choppy sentences or signature elements with no relevance to the actual contents of the summary that simply obstructed the reader. Perhaps our future attempts will have to target this with features specifically designed to identify things like email signatures or greetings, to help distinguish between these sources of “fluff” and the actual content of each email. Notably, this is not an issue that chatlog data suffers from, because there are typically fewer signatures or static portions of chats to ignore.&lt;/p&gt;&lt;p&gt;Our analysis is exemplified by the sample summary and its annotated counterpart below:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/0*ycgusov-GEUfQm7C.&quot; /&gt;Summary for an email thread generated by our baseline model&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/0*aOxikqivuOMXLgnM.&quot; /&gt;Summary for the same email thread from the dataset annotation&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;Future Improvements&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Although our baseline model provided a useful point of comparison for future efforts, we were not able to entirely replicate the results of the paper we based it off of. Despite using the same features that they described, we achieved a significantly lower mean ROUGE-L F1 score of 0.409 as compared to their 0.245 for the same dataset. As a result, it seems that an important next step would be to identify the discrepancies between our implementation and theirs to try and faithfully reproduce their results before moving on to other models.&lt;/p&gt;&lt;p&gt;One possible issue we identified with our model could be the lack of a topic boundary preprocessing step as found in the research paper. Although their description made it seem as though the technique mainly applied to chatlog data, it is possible that performing such a preprocessing step would bring our model’s performance closer to theirs. The paper describes two different approaches at a very high level, so further research and experimentation would be needed to determine a reasonable implementation to perform such a task. In addition, there were various parameters left out of the paper’s description that could affect the model’s performance, such as ROUGE evaluation parameters and the implementation of Naive Bayes, which could have made an impact.&lt;/p&gt;&lt;p&gt;After finishing our baseline implementation, we plan to implement further models to address this task. The paper we previously mentioned uses a number of other approaches, so one direction we plan to move in is selecting other potentially effective models from their list and implementing them (decision trees look the most promising and would require very little modification to the code). Afterwards, we plan to address the other datasets, and then move into feature engineering and tweaking our implementations to produce our minimum viable product.&lt;/p&gt;&lt;h4&gt;Code&lt;/h4&gt;&lt;p&gt;As mentioned in our first blog post, the full code for our project is &lt;a href=&quot;https://github.com/viterbi-or-not-to-be/viterbi-or-not-to-be&quot;&gt;available on Github&lt;/a&gt;, which currently includes usage instructions located in the README.md file for the baseline directory that can guide a user through the process of setting up the model, producing summaries, and running the ROUGE evaluation. We certainly have a long way to go in terms of code organization and cleanliness, but this baseline implementation at least allows for a preliminary examination of the task and produces the results necessary to interpret the rest of our model approaches with appropriate context.&lt;/p&gt;&lt;p&gt;Edited 4/24 to add baseline dog.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=a6690114c441&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-18T06:55:14+00:00</dc:date>
	<dc:creator>Viterbi Or Not To Be</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-3753031463594823927.post-8569998071322028844">
	<title>Ron Fan, Aditya Saraf &lt;br/&gt; Team PrimeapeNLP: Blog Post #4</title>
	<link>https://cse481n.blogspot.com/2018/04/blog-post-4.html</link>
	<content:encoded>&lt;div dir=&quot;ltr&quot; id=&quot;docs-internal-guid-7937c8af-d777-ed0f-f77b-cf09623b985e&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;A big part of our work for setting up a baseline is the creation of a reasonably good dataset for training and evaluating extractive text summarization. Our goal was to build a dataset with all sentences from the articles marked with binary labels indicating whether or not they were part of the extracted summary.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;We used the DailyMail and CNN story dataset, which is meant for abstractive summarization, to build our own dataset. The data set includes automatically-parsed lines from news articles, as well as a few bullet point highlights for each article. These highlights are not sentences directly from the article, but are overall a decent indicator and considered to be one of the most useable datasets out there (if for no other reason than sheer volume of data).&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;To build our data set, we used a toy metric roughly based off of ROUGE-N, with the understanding that we would only need a way of relative ranking for each sentence.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;The procedure to generate the data set was as follows:&lt;/span&gt;&lt;/div&gt;&lt;ol style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Read in a single .story file from the abstractive dataset into highlights and sentences.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Post-process sentences- in the original data, new lines do not correspond exactly with sentence endings. Additionally, some sentences are originally split arbitrarily by new lines. We do an additional split by the regex (?&amp;lt;=[.?!])\s+ (whitespace with period, question mark, or exclamation mark lookbehind), but sentence “re-joining” is off by default, as some sentences in the original dataset do not end with punctuation.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Score each sentence with our metric.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;ol style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Tokenize each sentence and highlight.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Build unigram, bigram, and trigram lists.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Count the number of matches between the sentence and all the highlights, giving more weight to bigram and trigram matches and also weighting by word length.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Sort sentences by score.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Let S be the number of sentences and H be the number of highlights. The algorithm picks up to min(1, floor(S/2)) sentences as part of the summary. It picks at least the H sentences with the highest scores. After sentence H is picked, subsequent sentences will only be picked if their score was at least (80+3*X)% of the previously-picked sentence’s score, where X is the number of sentences currently picked. This is effectively a fairly arbitrary metric to pick the most likely sentences roughly in accordance with the size of the abstractive summary, while also allowing for particularly similar sentences to both be chosen for completeness.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;All data input and output with UTF-8 encoding.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Output format:&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;&amp;lt;#&amp;gt; &amp;lt;SENTENCE PLAINTEXT&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;# is either 0 = normal sentence in the article, 1 = sentence chosen as part of extractive summary, 2 = original abstractive summary (should not be touched by model, only left there for human reference). &lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Overall, we think our data set is generally reasonable and makes sense. Nonetheless, there are a number of weaknesses with both extractive models in general and this specific type of dataset (including some problems which derive from the original abstractive dataset).&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Key weaknesses of data set:&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;In the original data, highlights do not always correspond to actual information in the article. For example, this short story has four highlights that are all new information: 008fc24ca9f4c48a54623bef423a3f2f8db8451a.story.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Frequent formatting problems - sentences that don’t terminate with punctuation, repeated sentences, bugged unicode characters, etc.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Key weaknesses specific to extractive summarization:&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Articles that are particularly short are effectively impossible to meaningfully summarize with extractive techniques.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Some sentences in articles contain the same content with slightly different wording. In these cases, we decided to choose both sentences, with the idea that post-processing could take care of highly-similar sentences, but a model could not be expected to accurately distinguish between two such sentences if they have different labels.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;The Baseline Model:&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;For our baseline combinatorial approach, we did a simple Maximum Coverage Problem implementation. The Maximum Coverage Problem is a classic NP-complete problem. Given a collection of n sets, the goal is to maximize the number of chosen (or “covered”) elements while only choosing k of the sets (where k &amp;lt; n). It’s very straightforward to reduce summarization to this problem. The sentences in the document are the sets, and the vocabulary of the document represents the “universe”, or the elements in the sets. The goal is to cover as many words as possible while picking the same number of sentences as in the labelled summary. The intuition is that sentences with more words capture more semantic meaning, and once a word is listed in the summary, that word need not be considered again. This reduction makes some intuitive sense, but two main aspects must be refined. First, not all words are equally meaningful. Some words, like “a”, “the”, or “and”, have little semantic content, while named entities and other important words have much higher semantic content. Second, we should at least account for sentence lengths -- instead of constraining the algorithm to pick the same number of sentences, we should constrain it to the same word count. This two additional criteria turn the problem into the Budgeted Maximum Coverage Problem: elements in the sets have specified values, and the sets have specified costs. The new objective is to maximize the total value of the covered elements while remaining with a given budget.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Of course, this problem is NP-complete, so it may seem like a strange choice to model summarization with. There are two approaches to this. First, a simple greedy algorithm achieves an approximation of ~63%, which means that the greedy algorithm is guaranteed to get at least 63% of the value of the optimal solution. For the standard MCP, the greedy algorithm just chooses the set with the most uncovered elements until no more sets are allowed. For the budgeted MCP, the approximation algorithm is slightly more complex. See Khuller et al. for a detailed look at it -- the algorithm is slightly more work, but still very understandable, and achieves the same approximation bound.  We can also formulate the problem as an Integer Linear Program (ILP) and use an ILP solver to generate optimal solutions on realistic instance sizes. This works because the documents and sentences are both small.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;For now, we are using the greedy algorithm for the standard MCP. As a simple metric, we used the number of correct chosen sentences divided by the number of chosen sentences in total (correct means that the gold standard also picked that sentence). With a basic MCP approach, we had an accuracy of 0.218. This was measured on a small sample of 2000 stories. In order to approximate a weighting scheme, we use a stop list to simply remove common articles and other semantically empty words. With this small modification, we had an accuracy of 0.2328.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;We plan to extend this to a better baseline model in the next few days. We will formulate the problem as an ILP and pass it to an ILP solver for an optimal solution. We will also implement a more sophisticated weighting scheme. One idea is to use some of the labelled data as training data (right now, we don’t train) to build a weight matrix for common words -- we can build this matrix with a logistic regression, using n-gram (probably unigram) similarity as the loss function.This will hopefully give us a more reasonable baseline model.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Evaluation Framework:&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;For now, we’re using a simple accuracy metric. We plan to switch to ROUGE, which counts n-gram similarity (ROUGE measures recall, not precision). We will work with the ROUGE-2.0 Java package or pyrouge, depending on which language we are working with. &lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;So to summarize our goals for the next week, we will improve this baseline model by formulating the problem as an ILP and we may implement a weighting scheme (if that’s too complicated, we’ll save it for the advanced model). We will also build a baseline neural model. Then, we’ll flesh out our evaluation framework using ROUGE metrics, train/test using more of the corpus, and have detailed error analysis.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;References:&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Khuller, S., Moss, A., &amp;amp; Naor, J. (Seffi). (1999). The Budgeted Maximum Coverage Problem. &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Inf. Process. Lett.&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;, &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;70&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;(1), 39–45. https://doi.org/10.1016/S0020-0190(99)00031-9&lt;/span&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-04-18T06:25:00+00:00</dc:date>
	<dc:creator>Ron &amp; Aditya</dc:creator>
</item>
<item rdf:about="https://medium.com/p/15357a82fe06">
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: First Impressions: Baselines and the Evaluation Framework</title>
	<link>https://medium.com/nlp-capstone-blog/first-impressions-baselines-and-the-evaluation-framework-15357a82fe06?source=rss----9ba3897b6688---4</link>
	<content:encoded>&lt;p&gt;Ultimately, our goal is to go beyond basic language modeling and create a new text generation architecture conducive to producing technical definitions. To get a feel for the data though, we approach it with familiar, simple baselines that give us a foundation in which we can improve from.&lt;/p&gt;&lt;p&gt;The baselines that we’ve experimented with are&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Vanilla RNN:&lt;/strong&gt; Hidden states as a function of the input and the previous hidden state, with Tanh activation (in particular, we’re using the &lt;a href=&quot;https://medium.com/@tamdangnadmat/first-impressions-baselines-and-the-evaluation-framework-15357a82fe06&quot;&gt;Elman Network&lt;/a&gt;).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;GRU:&lt;/strong&gt; An RNN architecture that learns to throttle the influence and usage of particular parameters on inference using a gating mechanism.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;LSTM: &lt;/strong&gt;Another RNN architecture that specializes in intelligently remembering relevant details and forgetting irrelevant details through several gating mechanisms and a “cell state” in addition to the conventional hidden states. &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Here’s more detail&lt;/a&gt; about this particular kind of RNN.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All baselines were trained as language models with cross entropy loss and were used to get a sense of how learnable the language of the Semantic Scholar dataset is. The metrics we are focusing on now are&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Perplexity: &lt;/strong&gt;A measure of how “confused” the model is at any point it’s attempting to predict the next word.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Feature extraction through hidden states:&lt;/strong&gt; Can the hidden states be used as features for a classification task?&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Our first metric is fairly straightforward; we calculate the aggregate sum of log probabilities for every word in the corpus and normalize by the size of the corpus.&lt;/p&gt;&lt;p&gt;Our second metric however, is inspired by Dieng et al.’s method for sentiment analysis in &lt;a href=&quot;https://www.semanticscholar.org/paper/TopicRNN%3A-A-Recurrent-Neural-Network-with-Semantic-Dieng-Wang/412068c7e8e77b73add471789d58df3d2f3e08d8&quot;&gt;TopicRNN&lt;/a&gt;, in which the final hidden states after a forward pass of the model on a movie review were used to predict positive or negative sentiment using a single-layer neural network. We aim to adopt this metric from a multi-class classification perspective in which the passages we use are excerpts of research documents with an omitted, technical term. The question we aim to answer with this metric is “is the model capable of representing semantics in a latent space?”&lt;/p&gt;&lt;p&gt;Our labels will then be a defined set of these omitted, technical terms, and our goal will then be to predict them given the hidden states of the passage via a two-layer neural network. The framework for this evaluation metric can be found in &lt;a href=&quot;https://github.com/NLP-Capstone-Project/machine-dictionary/tree/evaluation&quot;&gt;this branch&lt;/a&gt; of our codebase.&lt;/p&gt;&lt;h4&gt;Why we chose these metrics&lt;/h4&gt;&lt;p&gt;Our task is fairly novel given the way we’re approaching it, so currently no dataset exists that pairs domain-specific words with definitions that are to the caliber of research technicality. Because of this, metrics that depend on gold standards such as ROUGE and BLEU are currently out of reach at this time.&lt;/p&gt;&lt;p&gt;There’s a chance we’ll experiment with these metrics if we can find a labeled dataset to supplement Semantic Scholar’s Open Research Corpus. We are also considering using the publications themselves as the gold standards, which may be helpful since a desirable trait of our model would be its ability to produce language similar to that of the corpus.&lt;/p&gt;&lt;h3&gt;Challenges Encountered while Baselining&lt;/h3&gt;&lt;p&gt;In establishing our baselines and metrics, there were several issues we ran into, both in training.&lt;/p&gt;&lt;h4&gt;Dealing with a larger corpus&lt;/h4&gt;&lt;p&gt;Given that our baselines are language models, and that our later prototype models will most likely contain an LM component, we have to deal with efficient learning given there are several million documents to process.&lt;/p&gt;&lt;p&gt;For efficient backpropagation, we opted to introduce a “backpropagation through time” as a hyperparameter defaulted at 50, which specifies the number of words we allow the model to see before updating our parameters.&lt;/p&gt;&lt;p&gt;Currently, batching is supported by our codebase but was not used in our initial experiments. Given that it takes roughly one minute for the GPUs on the cloud to process a single publication, we plan to concatenate document vectors and reshape into batch-by-length tensors in the future.&lt;/p&gt;&lt;h4&gt;Sorting By Domain&lt;/h4&gt;&lt;p&gt;We’d like our model to be trained on a single domain/field of study for our future case studies comparing dictionaries built on one domain versus others. This is further motivated by some of our experiment results discussed later, how loss tends to spike between documents.&lt;/p&gt;&lt;p&gt;Currently, the Semantic Scholar Open Research Corpus doesn’t include anything in the set of JSON fields that we could find for filtering the data. However, we’ve been assured by AllenNLP researchers that its possible to sort the data by research domain. We may performing another round of baseline experiments once we’ve sorted the data, but for now the results below are on publications of mixed domains.&lt;/p&gt;&lt;h3&gt;Experimental Results&lt;/h3&gt;&lt;p&gt;The Semantic Scholar Open Research Corpus provides a sample subset of its dataset: a JSON file containing 4000 entries. Within each entry, the URL of the publication’s online PDF is provided. We use a GET request to &lt;a href=&quot;https://github.com/allenai/science-parse&quot;&gt;AI2’s Science Parse&lt;/a&gt; service to extract the PDF contents.&lt;/p&gt;&lt;p&gt;From there, we run our experiments on 121 of the 4000 extracted documents, stopping training early at 15 documents and calculated perplexity on a validation set of 30 documents. The total vocabulary used was 11,330 words. Words outside of this vocabulary are replaced with an unknown token at training and test time.&lt;/p&gt;&lt;p&gt;Loss is calculated and normalized on the last 50 words the model is trained on.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Elman RNN:&lt;/strong&gt; Perplexity of 250.25 with an average loss of 7.908 over the last 50 words.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;GRU:&lt;/strong&gt; Perplexity of 265.50 with an average loss of 7.085 over the last 50 words&lt;/li&gt;&lt;li&gt;&lt;strong&gt;LSTM:&lt;/strong&gt; Perplexity of 261.95 with an average loss of 6.588 over the last 50 words&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Perplexities calculated using untrained models (with randomized parameters) were several orders of magnitude larger than the ones listed above, so it’s good to know that our baseline models can learn a significant amount of surface-level patterns with such a small subset of the corpus.&lt;/p&gt;&lt;p&gt;In terms of feature extraction and classification, the framework has been implemented but the data for this has not been created yet. We plan on evaluating our baselines along with our final model on this metric using publications from Semantic Scholar after establishing a vocabulary of semantically significant technical terms and creating the dataset using those terms. This will involve iterating over documents and replacing occurrences of these technical terms with a specialized, unknown token that won’t aid in inference.&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Our methods helped us familiarize ourselves with the data as well as handle significant amounts of overhead in terms of processing the data, toggling between different models, and integration of our evaluation metrics in an organized fashion.&lt;/p&gt;&lt;p&gt;We are excited to see how novel architectures tailored to the task perform on these metrics!&lt;/p&gt;&lt;p&gt;To keep up to date with our progress in baselining, evaluation, and other things, you can watch &lt;a href=&quot;https://github.com/NLP-Capstone-Project/&quot;&gt;this repository&lt;/a&gt;.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=15357a82fe06&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/first-impressions-baselines-and-the-evaluation-framework-15357a82fe06&quot;&gt;First Impressions: Baselines and the Evaluation Framework&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-18T05:40:07+00:00</dc:date>
	<dc:creator>Tam Dang</dc:creator>
</item>
<item rdf:about="http://mathstoc.wordpress.com/?p=314">
	<title>Kuikui Liu, Nicholas Ruhland &lt;br/&gt; Team INLP: NLP Capstone Post #4: Baseline and MIDI Frustration</title>
	<link>https://mathstoc.wordpress.com/2018/04/18/nlp-capstone-post-3-baseline-and-midi-frustration/</link>
	<content:encoded>&lt;h1&gt;&lt;b&gt;Baseline model&lt;/b&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our baseline approach is taken from Daniil Pakhomov’s excellent blog post[1]. In this post, two separate RNNs are trained as generators: one for lyrical content and one for music content (in piano roll format). We will begin by using his trained lyrical model, and attempt to do conditional sampled character generation given a starting sequence of characters. We loaded the already trained models from the blog post and generated lyrics according to the style of a given songwriter and with a given seed word. The lyrics are generated via a character-level LSTM and generates the next character conditioned on the preceding characters and choice of songwriter. The model is trained on a corpus of song lyrics, where naturally the “correct” character to generate is the next character in the lyrics. Essentially the same mechanism is applied to the musical note generation.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The characters are encoded as a one-hot vector over all letters in the English alphabet plus space, comma, etc. Piano rolls already come in an encoding amenable to feeding into RNNs, modulo additional zero padding to ensure every time slice of every piano role has the same dimension. In particular, at each time step (discretized in an appropriately fine-grained way), we have an indicator 0-1 vector on which notes are currently activated.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Here is a song generated in the style of “Queen” with the starting seed sequence of characters “Music”:&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Music savor valerite – yah  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Imabribot, bind me – I – well  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;All going down to L&lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;At the eyes of the universe  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Agree, five to the Slim  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;I just want to convincide  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;We wash stars and quiet Ich  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;You had a dirty old baby  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;We won’t  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;We does nothing no one ezy? follohin?  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Sometimes we get down and ooh  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Nothing do you see all night  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;This is my pries  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Joyful the world  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Does their beams  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Surgeon makes the scule la beat  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Walking out on my pocket ride  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;My faulty power  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;I wear from the ston&lt;/i&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Eventually, since we are actually interested in converting the musical information into plausible lyrics, we will need to modify this baseline in the natural way to take as input time slices of the musical instrumentation in piano roll format and predict characters (or syllables) that are to be enunciated simultaneously with the played notes. In this manner, the lyrics come already aligned in a natural way, and the words can be extracted by compressing the letters occurring between spaces.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;&lt;b&gt;Dataset parsing&lt;/b&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The MIDI format has an unfortunate number of unexpected caveats. We have spent a majority of our time so far cleaning the data and attempting to use it in existing Python libraries that handle MIDI. A brief description of MIDI[2] covers some of the challenges:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;MIDI is made up of a series of messages, such as notes, instruments, and tempo changes. Additional metadata messages exist called meta messages, which can contain text content such as the song title (and lyrics!). In our dataset, lyrics are provided either as “text” messages or as “lyrics” messages.&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Messages are grouped into different tracks, often representing separate instruments. Metadata sometimes is located in its own track, and lyrics are sometimes found in a different track from the rest of the metadata.&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Durations in the MIDI format are specified as a delta-time relative to the most recent frame. Delta times are in a unit called a tick. Ticks are defined in the file header as a division of the quarter note. The header also defines the number of ticks per frame, which is what the deltas are relative to. Beats per minute (bpm) messages adjust the speed of playback throughout the song.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The most promising library so far is PrettyMIDI[3], which handles most of the unexpected behavior of the basic MIDI format. This library wraps MIDI messages into structured python objects, and provides a conversion from MIDI into piano roll format. Piano roll in this case is a numpy array of shape (num_notes, num_frames). This allows us to input the musical data directly into an RNN. The units are also converted into absolute seconds, as opposed to relative durations. PrettyMIDI can additionally handle embedded lyrics, but this has proven to be a challenge due to the variety of annotation styles in our dataset. About 200 of our 900 files have parsed lyric data properly, so continuing to clean our data is a high priority.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;&lt;strong&gt;U&lt;/strong&gt;pd&lt;strong&gt;ate&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Unfortunately, we have found the Kara1k dataset[4] to be inapplicable to our project, as the raw sequence of musical notes and lyrical content are not provided, only metadata that the dataset developers have extracted.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[1] &lt;/span&gt;&lt;a href=&quot;http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[2] &lt;/span&gt;&lt;a href=&quot;http://www.music.mcgill.ca/~ich/classes/mumt306/StandardMIDIfileformat.html&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;http://www.music.mcgill.ca/~ich/classes/mumt306/StandardMIDIfileformat.html&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[3] &lt;/span&gt;&lt;a href=&quot;http://craffel.github.io/pretty-midi/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;http://craffel.github.io/pretty-midi/&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[4] &lt;/span&gt;&lt;a href=&quot;http://yannbayle.fr/karamir/kara1k.php&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;http://yannbayle.fr/karamir/kara1k.php&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-18T04:58:56+00:00</dc:date>
	<dc:creator>Nicholas Ruhland</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-9203775015655831448.post-1250926726356516395">
	<title>Pinyi Wang, Dawei Shen, Xukai Liu &lt;br/&gt; Team Overfit: #4 Milestone: Strawman/Baseline I</title>
	<link>https://teamoverfit.blogspot.com/2018/04/4-milestone-strawmanbaseline-i.html</link>
	<content:encoded>&lt;h2 style=&quot;height: 0px;&quot;&gt;&lt;span&gt;Team Overfit&lt;/span&gt;&lt;/h2&gt;&lt;h3&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3&gt;&lt;span&gt;Project repo: &lt;span style=&quot;font-size: 18.72px;&quot;&gt;&lt;a href=&quot;https://github.com/pinyiw/nlpcapstone-teamoverfit&quot;&gt;https://github.com/pinyiw/nlpcapstone-teamoverfit&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h4&gt;&lt;span&gt;Team members: Dawei Shen, Pinyi Wang, Xukai Liu&lt;/span&gt;&lt;/h4&gt;&lt;div style=&quot;text-align: start; text-indent: 0px;&quot;&gt;&lt;div style=&quot;margin: 0px;&quot;&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;Blog Post: #4: 04/17/2018&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;span&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Social Media Predicts Stock Price (StartUp Mode)&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;This week, we experimented with crawling data by querying from Twitter website, but we had some struggles finding the meaningful data set. Firstly, the Twitter API does not support fetching tweets prior than 7 days, so we have to write bash scripts and use a crawler to strip data from website, which is comparatively slower.&lt;/span&gt;&lt;br /&gt;&lt;span id=&quot;docs-internal-guid-945a652a-d716-7ba2-4653-bdd5ec706d01&quot;&gt;&lt;a href=&quot;https://github.com/Jefferson-Henrique/GetOldTweets-python&quot;&gt;&lt;span&gt;https://github.com/Jefferson-Henrique/GetOldTweets-python&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;We tried to query keywords like Apple, iPhone, iPad. Lots of tweets we got are not in English, so we used language detection tools to filter out tweets in other languages. Unfortunately, there are too much giveaway and advertisement flooding on the platform. Some examples are:&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&quot;Microsoft planning to launch Surface Pro 6 in first quarter of 2017&quot;&lt;/span&gt;&lt;/blockquote&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&quot;KFire TV Giveaway: Win a Microsoft Bluetooth Mouse. http://kodifiretvstick.com&quot;&lt;/span&gt;&lt;/blockquote&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&quot;Microsoft Releases The 'Studio', Its First Desktop Computer&quot;&lt;/span&gt;&lt;/blockquote&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&quot;Like - free microsoft points world http://freemicrosoftpointsworld.weebly.com/&quot;&lt;/span&gt;&lt;/blockquote&gt;&lt;span&gt;Therefore, for out strawman model #1 we decide to use the relative news headlines under twitter search query.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;We searched companies' names like Google, Microsoft, Tesla to find relative new headlines and use unigram (bag of words) to put it in a decision tree model with random forest mechanism to predict the price go UP or DOWN. We had mixed results using this model. This is just our first attempt to see whether the twitter data is useful to predict the movement of future stock prices.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;AAPL GOOG MSFT AMZN&lt;/span&gt;&lt;br /&gt;&lt;span&gt;53.66% 58.53% 56.10% 37.50%&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Next week, we'll try to use deep neural network as predictive model and expand our input to the user feedback of the product from general users.&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;/blockquote&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;/blockquote&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;/blockquote&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;/blockquote&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-04-18T04:47:00+00:00</dc:date>
	<dc:creator>Team Overfit</dc:creator>
</item>
<item rdf:about="http://sarahyu.weebly.com/cse-481n/first-strawman-attempt">
	<title>Sarah Yu &lt;br/&gt; Team Jekyll-Hyde: First Strawman Attempt</title>
	<link>http://sarahyu.weebly.com/cse-481n/first-strawman-attempt</link>
	<content:encoded>&lt;div class=&quot;paragraph&quot;&gt;Well...here's the first hiccup: the Reddit data I'd like to use is much too large (like 8GB compressed and unknown uncompressed for one month of posts). Since my last post, I had been working on trying to scrape the data manually through Reddit API requests, but I was running into some issues and it was taking quite a while because of the request restrictions.&lt;br /&gt;&lt;br /&gt;I decided, maybe a little later than I should have, to use the Reddit Data dumps provided by John Baumgartner at pushshift.io instead, but much of the recent data is too large for my computer, attu, as well as my Azure instance. I am working with a grad student to get access to more resources so that I can work with (or even open) some of these files! However, in the meantime I have committed some files that &lt;em&gt;would &lt;/em&gt;be my strawman. I have a list of 124 subreddits labeled as the neurodivergent subreddit subset and currently have a model that builds a set of users that post to these subsets, finds the other neurotypical subreddits those users also post to, and aggregates a set of those subreddits. The strawman compares the basic n-grams in each subset of the subreddits to see basic language model differences. I also cloned the vennclouds github project to try and visualize the n-grams (uni, bi, and tri) that are used in these two subsets as well as their overlap however, the current version seems to have a basic bug.&lt;br /&gt;&lt;br /&gt;I will get the resources hopefully in the next couple of days and update this post with the actual data results! &lt;br /&gt;&lt;br /&gt;&lt;em&gt;To be continued...&lt;/em&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-04-18T03:47:19+00:00</dc:date>
</item>
<item rdf:about="http://cse481n-capstone.azurewebsites.net/?p=41">
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: Data Collection and First Baseline</title>
	<link>http://cse481n-capstone.azurewebsites.net/2018/04/17/data-collection-and-first-baseline/</link>
	<content:encoded>&lt;h3&gt;Data Collection&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In a previous blog, we mentioned using RAKE to collect content phrases from posts in order to compare their similarity. However, we decided since then that it would be easier to just look at ngrams, since they can capture the same information as the content phrases. So, after removing stopwords we collect all of the unigrams, bigrams, and trigrams on each MeanJokes post and on each general Reddit post. However, this creates a ton of data compared to the posts or even their content phrases. So, we must do some cleaning before we make use of this data.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For data cleaning, we decided to use document frequency to further clean content ngrams gathered from both mean-jokes and other Reddit posts. The reason why we choose df is:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;First, we are using content ngrams, so it does not make sense to compute ngram term back on the original post since bigram, trigram term frequency is likely to be 1.&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Second, df provides us a good way to remove too frequent and too infrequent ngrams. If an ngram is too frequent or too infrequent, it won’t be very informative about the pattern of the sentence.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;By using document frequency, we achieved the following purpose:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Much shorter, cleaner content ngrams&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Example:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[(‘took’, ‘family’, ‘camping’), (‘camping’, ‘concentration’), (‘took’, ‘family’), (‘family’, ‘camping’), (‘took’,), (‘family’,), (‘concentration’, ‘camping’), (‘family’, ‘camping’, ‘concentration’), (‘camping’,), (‘concentration’,), (‘camping’, ‘concentration’, ‘camping’)]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;After filtering:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[(‘took’,), (‘family’,), (‘camping’,), (‘concentration’,)]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Filtered out posts written in a different language (like written in Spanish or Germany)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[(‘adan’,), (‘zapata-con’,), (‘carino’,), (‘para’,), (‘ti’,), (‘letra’,), (‘encanta’,), (‘sii’,), (‘adan’, ‘zapata-con’), (‘zapata-con’, ‘carino’), (‘carino’, ‘para’), (‘para’, ‘ti’), (‘ti’, ‘letra’), (‘letra’, ‘encanta’), (‘encanta’, ‘sii’), (‘adan’, ‘zapata-con’, ‘carino’), (‘zapata-con’, ‘carino’, ‘para’), (‘carino’, ‘para’, ‘ti’), (‘para’, ‘ti’, ‘letra’), (‘ti’, ‘letra’, ‘encanta’), (‘letra’, ‘encanta’, ‘sii’)]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;However, we sometimes encounter a problem that after filtering, the content ngrams becomes empty. We decide to skip these lines when we do set similarity because by looking back to the original posts, posts that end up with empty content ngram are generally &lt;/span&gt;&lt;b&gt;very short&lt;/b&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; and &lt;/span&gt;&lt;b&gt;non-informative&lt;/b&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;. For example, the post “Bolivian coastline MeanJokes” ended up with an empty content ngrams. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;To determine our final dataset, we want to find Reddit posts that use similar language to the MeanJokes posts. To do this, we use &lt;/span&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Jaccard index&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; to determine similarity. Due to the nature of MeanJokes posts being rather short compared to most Reddit posts, the Jaccard Index of most posts compared to these posts would be very low. By filtering as described above, Jaccard index becomes much more useful because the number of n-grams we consider for each post is greatly reduced, down to those we consider most identifying of those posts. However, Reddit posts are very diverse so there is a great sparsity of similar posts. Very few posts have Jaccard Indices greater than .2 even after this filtering. Before filtering, very few posts would even get close to .1. For context, Jaccard similarity could be thought of as a percentage from 0 to 1, where 1 means the posts are identical and 0 means they have no intersection. We have over 3 million Reddit posts just in one month though, so we are not worried about not getting a big enough dataset if we have a high threshold for similarity.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Here’s an example of what this data looks like:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Intersection: &lt;/b&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;{(‘ca’, “n’t”, ‘spell’), (‘ca’, “n’t”), (“n’t”, ‘spell’), (‘spell’,), (‘ca’,)}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;MeanJokes Post:&lt;/b&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; {(‘ca’, “n’t”), (‘therapist’,), (‘without’,), (‘ca’, “n’t”, ‘spell’), (‘rapist’,), (“n’t”, ‘spell’), (‘spell’,), (‘remember’,), (‘ca’,)}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Reddit Post:&lt;/b&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; {(‘ca’, “n’t”), (‘ca’, “n’t”, ‘spell’), (“n’t”, ‘spell’), (‘spell’,), (‘crisis’,), (‘ca’,)}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Jaccard index:&lt;/b&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; 0.5&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;5 / (9 + 6 – 5) -&amp;gt; 5/10 -&amp;gt; .5&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Baseline Model&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Because we are in the process of data collection, we want to build baseline models on similar datasets and later port the model over to our own dataset. The dataset we chose for this purpose is the &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Twitter Hate Speech dataset&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; created by&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; Zeerak Waseem and Dirk Hovy&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;. In this dataset, each tweet is labeled as “racism”, “sexism”, or “none”. Since our goal is to build tools that can not only detect if the text is offensive or non-offensive but also detect towards which group the text is offensive, the Twitter Hate Speech dataset serves as a good starting point. Here, we built a logistic regression model as a baseline for offensiveness detection. We consider both “racism” and “sexism” as offensive, and “none” non-offensive. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The each example in the dataset is of the format &amp;lt;tweet_id&amp;gt;, &amp;lt;label&amp;gt;. We first used &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;python-twitter &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(a python wrapper around Twitter API) to collect the original tweets by the tweet_ids given. In this process, we noticed that 1133 tweets from this dataset are already removed from Twitter. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;To make the results more reproducible, we made a train, dev, test dataset split (80%, 10%, 10%) using sklearn train_test_split function. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For data preprocessing, we used an existing &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;text preprocessor made by Zhang et. al. to clean each tweet down to plain text, removing extra space, URLs, hashtags, special characters, etc. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For text vectorization, we experimented with two different sklearn vectorizers: tf-idf vectorizer and count vectorizer. Count vectorizer converts a corpus to a document-term matrix. TF-IDF vectorizer converts a corpus to a tf-idf weighted document-term matrix. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;After that, we trained a logistic regression model on training data and tuned epoch on dev data. Both the trained model and trained text vectorizer would be saved. We then loaded in the saved model and text vectorizer to make predictions and evaluate on test data. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The metrics we chose for performance evaluation are accuracy, precision, recall, and f1 score. Here we decided to be more conservative and paid more attention to precision because the expected downstream application (a.k.a. the targeting group detector)  relies on text predicted “offensive” to be actually offensive. We also focused on the f1 score to evaluate the overall performance of this baseline model.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We ran experiments with this baseline model with the different types of vectorizers and different vectorizer setups. We decided to keep follow ngram_range (1, 4) chosen by &lt;/span&gt;Zeerak Waseem and Dirk Hovy.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set-up1:
&lt;ul&gt;
&lt;li&gt;Vectorizer:&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;type&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;strip_accents&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;analyzer&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;ngram_range&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;max_features&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;tf-idf&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;unicode&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;word&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(1, 4)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;10000&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;Model: &lt;span style=&quot;font-weight: 400;&quot;&gt;Logistic regression, epoch_chosen=5&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;dev accuracy: 0.8162&lt;/td&gt;
&lt;td&gt;test accuracy: 0.8131&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dev precision: 0.8170&lt;/td&gt;
&lt;td&gt;test precision: 0.8203&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dev recall: 0.7336&lt;/td&gt;
&lt;td&gt;test recall: 0.7333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dev f1: 0.7560&lt;/td&gt;
&lt;td&gt;test f1: 0.7552&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Set-up2:
&lt;ul&gt;
&lt;li&gt;Vectorizer:&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;type&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;strip_accents&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;analyzer&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;ngram_range&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;max_features&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;tf-idf&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;unicode&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;char&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(1, 4)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;10000&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;Model: &lt;span style=&quot;font-weight: 400;&quot;&gt;Logistic regression, epoch_chosen=18&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev accuracy: 0.8175&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test accuracy: 0.8067&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev precision: 0.8040&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test precision: 0.7951&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev recall: 0.7482&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test recall: 0.7388&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev f1: 0.7664&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test f1: 0.7561&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;Comparison: &lt;span style=&quot;font-weight: 400;&quot;&gt;By taking character level ngrams, test f1 actually improves, but test precision drops drastically. &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Set-up3:
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Vectorizer:&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;type&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;strip_accents&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;analyzer&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;ngram_range&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;max_features&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;tf-idf&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;unicode&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;char&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(1, 4)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;30000&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model: Logistic regression, epoch_chosen=10&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev accuracy: 0.8194&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test accuracy: 0.8105&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev precision: 0.8099&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test precision: 0.8021&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev recall: 0.7473&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test recall: 0.7416&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev f1: 0.7669&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test f1: 0.7598&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Comparison: By increasing the max number of features a document-term matrix can have, both test precision and test f1 improved compared with those of set-up2. &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Set-up4:
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Vectorizer:&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;type&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;strip_accents&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;analyzer&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;ngram_range&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;max_features&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;count&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;unicode&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;word&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(1, 4)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;30000&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model: Logistic regression, epoch_chosen=100&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev accuracy: 0.8321&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test accuracy: 0.8213&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev precision: 0.8213&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test precision: 0.8097&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev recall: 0.7690&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test recall: 0.7618&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev f1: 0.7871&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test f1: 0.7781&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Comparison: Vectorizer is now changed to count vectorizer. With this set-up, test f1 is the highest among all set-ups, while test precision is higher than that of set-up3 and lower of that of set-up1. &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Set-up5:
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Vectorizer:&lt;/span&gt;&lt;br /&gt;
&lt;table style=&quot;height: 96px;&quot; width=&quot;328&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;type&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;strip_accents&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;analyzer&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;ngram_range&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;max_features&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;count&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;unicode&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;char&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(1, 4)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;30000&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model: Logistic regression, epoch_chosen=30&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;
&lt;table style=&quot;height: 291px;&quot; width=&quot;388&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev accuracy: 0.8226&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test accuracy: 0.8093&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev precision: 0.8001&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test precision: 0.7860&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev recall: 0.7690&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test recall: 0.7594&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev f1: 0.7812&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test f1: 0.7698&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Comparison: Test precision is the lowest of all the set-ups, and test f1 is higher than the first 3 set-ups, but lower than set-up4.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Although &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Zeerak Waseem and Dirk Hovy’s paper stated that &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;character n-grams have better performance than work n-grams as features, our experiment results suggested otherwise. All of our set-ups have higher test precision and f1 score but lower test recall that the best model of the paper. These differences might be because of the removal of tweets from the original dataset. The train, dev, test data split method might also contribute to the differences. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Next Steps: We would like to experiment with other baseline models, especially neural networks and perform error analysis on these baseline models.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Work Cited:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/publication/323723283_Detecting_hate_speech_on_Twitter_using_a_convolution-GRU_based_deep_neural_network&quot;&gt;Zhang, Ziqi &amp;amp; Robinson, D &amp;amp; Tepper, J. (2018). Detecting hate speech on Twitter using a convolution-GRU based deep neural network.&lt;/a&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-18T02:36:38+00:00</dc:date>
	<dc:creator>Team Watch Your Language!</dc:creator>
</item>
<item rdf:about="https://medium.com/p/d1d1b2d1f34c">
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Image Annotation Model Baseline, Dataset and Evaluation Framework</title>
	<link>https://medium.com/@hongnin1/image-annotation-model-baseline-dataset-and-evaluation-framework-d1d1b2d1f34c?source=rss-c450eb982161------2</link>
	<content:encoded>&lt;h3&gt;Baseline approach:&lt;/h3&gt;&lt;h4&gt;Overview:&lt;/h4&gt;&lt;p&gt;We are using a deep neural network that learns how to output the description of a general image. Example:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*RWtQGi8MpUzCsQTmWpR0Og.png&quot; /&gt;Image from paper &lt;a href=&quot;https://arxiv.org/pdf/1412.6632.pdf&quot;&gt;https://arxiv.org/pdf/1412.6632.pdf&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;Our baseline model is an encoder-decoder neural network. First it encodes a image into a vector using a deep convolutional neural network (CNN), our baseline choice of network is &lt;a href=&quot;https://arxiv.org/abs/1512.00567&quot;&gt;Inception v3&lt;/a&gt; image recognition model pre-trained on the &lt;a href=&quot;http://www.image-net.org/challenges/LSVRC/2012/&quot;&gt;ILSVRC-2012-CLS&lt;/a&gt; image classification dataset. Then decode the vector into a paragraph of description using a long short-term memory network (LSTM). Words in the output description are represented with an embedding model with each word in the dictionary represented by a fixed-length vector (learned during training).&lt;/p&gt;&lt;h4&gt;Architecture:&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*na7zdPNZQG66QVCJK1Kqmg.png&quot; /&gt;Baseline Model Architecture&lt;/figure&gt;&lt;p&gt;Above architecture outputs the log likelihoods of the correct words at each step (logp1(S1)); later we use beam search to generate the description for given image.&lt;/p&gt;&lt;h4&gt;Running the model/Experiment:&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;we downloaded Cuda, Bazel, TensorFlow, Numpy, Natural Language ToolKit(NLTK).&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;2. Download necessary data from Microsoft COCO:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Ap25lgpLRu8lkr0omWUdqQ.png&quot; /&gt;Bash screenshot of downloading MSCOCO data.&lt;/figure&gt;&lt;p&gt;3. Prepare all the necessary data we just downloaded for training:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*KBg_dBwZX6w3gIw2XzJDeA.png&quot; /&gt;Last few lines of bash logs for preparing data.&lt;/figure&gt;&lt;p&gt;4. Run training scripts and require: We only trained for about ~3 minutes.&lt;/p&gt;&lt;p&gt;5. After acquiring the model, we tried it out by input following images:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*a7WVZZT-k0fYKq82WUFLhA.png&quot; /&gt;input image: Surfer Guy&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/623/1*H89koub2tfhDoxN4WPCYQA.png&quot; /&gt;input image 2: Rowing&lt;/figure&gt;&lt;p&gt;Our model’s output is not idea, for “Surfer Guy” image, our output is:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/784/1*Wl_AHUOJ8inu91LYEIKVZw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;For “Rowing” image, our output is:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/780/1*mHCnFYULsaI80jZWjfav5Q.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;6. Then by using TensorBoard, we plotted the log loss for our model in training:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*kLU6ytNXYM9ytsKUS9bUrQ.png&quot; /&gt;TensorBoard Log Loss Graph&lt;/figure&gt;&lt;p&gt;As well as our model’s architecture:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Coh87C_W_jfo4Pg4aObPwA.png&quot; /&gt;Image Captioning Baseline Model Architecture on TensorBoard.&lt;/figure&gt;&lt;h3&gt;Evaluation Framework:&lt;/h3&gt;&lt;p&gt;The model we are referencing on &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/im2txt#prepare-the-training-data&quot;&gt;git&lt;/a&gt; hub has a built in evaluation framework, so we were using their evaluation script. It logs evaluation metrics to TensorBoard which allows training progress to be monitored in real-time (refer to the image “TensorBoard Log Loss Graph”).&lt;/p&gt;&lt;h3&gt;Resources:&lt;/h3&gt;&lt;p&gt;paper referenced: &lt;a href=&quot;https://arxiv.org/pdf/1411.4555.pdf&quot;&gt;https://arxiv.org/pdf/1411.4555.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;RMSProp referenced: &lt;a href=&quot;https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop&quot;&gt;https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=d1d1b2d1f34c&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-17T07:30:16+00:00</dc:date>
	<dc:creator>Ning Hong</dc:creator>
</item>
<item rdf:about="https://medium.com/p/7de5277b5be">
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #4: Baseline Model I</title>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-4-baseline-model-i-7de5277b5be?source=rss-fad49d942bf3------2</link>
	<content:encoded>&lt;p&gt;This week, I implemented the baseline neural model from my minimal viable plan. Specifically, my model was an attentive biLSTM, which took as input concatenated word, polarity, and holder/target embeddings.&lt;/p&gt;&lt;p&gt;As for evaluation frameworks, I’ve implemented f1 scoring. I also kept track of other metrics like accuracy and loss, in addition to f1 score, to get a better understanding of my model’s performance.&lt;/p&gt;&lt;p&gt;The following is a summary of my preliminary results. I’ve done very little hyper-parameter tuning thus far, but of the tuning that I did do, I found that changing the optimizer had the greatest benefit, specifically, using the Adam optimizer over the SGD optimizer. I trained the following two hyper-parameter settings:&lt;/p&gt;&lt;pre&gt;╔═════════════════════╦═════════╦═════════╗&lt;br /&gt;║   Hyperparameters   ║    1    ║    2    ║&lt;br /&gt;╠═════════════════════╬═════════╬═════════╣&lt;br /&gt;║ Optimizer           ║ Adam    ║ SGD     ║&lt;br /&gt;║ Learning Rate       ║ 0.01    ║ 0.05    ║&lt;br /&gt;║ # Epochs            ║ 20      ║ 20      ║&lt;br /&gt;║ Loss Function       ║ NLLLoss ║ NLLLoss ║&lt;br /&gt;║ Hidden Dimension    ║ 150     ║ 150     ║&lt;br /&gt;║ Dropout Rate        ║ 0.20    ║ 0.20    ║&lt;br /&gt;║ Batch Size          ║ 10      ║ 10      ║&lt;br /&gt;║ Embedding Dimension ║ 50      ║ 50      ║&lt;br /&gt;║    (all 3)          ║         ║         ║&lt;br /&gt;╚═════════════════════╩═════════╩═════════╝&lt;/pre&gt;&lt;p&gt;As you can see, the only differences are in the optimizer and learning rate. Note I did experiment a little with learning rate for each optimizer, however, the amount of tuning I did for this is definitely not sufficiently.&lt;/p&gt;&lt;p&gt;Also note hyper-parameter configuration 1 (Adam optimizer) performed much better than 2 (SGD optimizer).&lt;/p&gt;&lt;h3&gt;Preliminary Results&lt;/h3&gt;&lt;h4&gt;Adam Optimizer&lt;/h4&gt;&lt;p&gt;Graph of the &lt;strong&gt;loss&lt;/strong&gt; over time:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/640/1*sYny1MXCplPx-XQ_ToYN4Q.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Graph of &lt;strong&gt;f1 scores&lt;/strong&gt; over time: (Note there are 3 types of sentiments two pairs of entities can share: positive sentiment, negative sentiment, or no sentiment)&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*iPBZ5X-TctKEGWXub_8Klw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Graph of &lt;strong&gt;accuracies&lt;/strong&gt; over time:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/640/1*5TrbatjcthnZDlaz4eNxXg.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;SGD Optimizer&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;For SGD, I experimented with a few values for learning rate (0.01, 0.05, and 0.1), and found 0.05 to fairly optimal.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Graph of the &lt;strong&gt;loss&lt;/strong&gt; over time:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/640/1*nweK1MYnyudak8XGAHMnng.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Graph of &lt;strong&gt;f1 scores&lt;/strong&gt; over time:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*VPN0B5UEkH1tWDShzrcSLQ.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Graph of &lt;strong&gt;accuracies&lt;/strong&gt; over time:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/640/1*9BmjOwEAcPQbHR-a1ZINLg.png&quot; /&gt;&lt;/figure&gt;&lt;h3&gt;Key Takeaways&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;There’s a huge discrepancy between the performance on the train set and the performance on the dev set, indicating that the development data set and training data set are very different.&lt;/li&gt;&lt;li&gt;Analysis of the datasets, as you can see, the training set is very different in composition from the other sets:&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;╔══════════════╦══════════╦══════════╦══════════╦══════════╦═══════╗&lt;br /&gt;║              ║          ║ average  ║    #     ║    #     ║   #   ║&lt;br /&gt;║              ║    #     ║ entities ║ positive ║ negative ║ none  ║&lt;br /&gt;║              ║ articles ║ /article ║  pairs   ║  pairs   ║ pairs ║&lt;br /&gt;╠══════════════╬══════════╬══════════╬══════════╬══════════╬═══════╣&lt;br /&gt;║ train-ACL    ║ 897      ║ 2.63     ║ 648      ║ 815      ║ 355   ║&lt;br /&gt;║ dev-ACL-tune ║ 38       ║ 8.82     ║ 257      ║ 95       ║ 2520  ║&lt;br /&gt;║ dev-ACL-test ║ 36       ║ 9        ║ 237      ║ 118      ║ 2547  ║&lt;br /&gt;║ test-ACL     ║ 79       ║ 9.25     ║ 379      ║ 198      ║ 6037  ║&lt;br /&gt;║ test-MPQA    ║ 54       ║ 11.72    ║ 435      ║ 362      ║ 6813  ║&lt;br /&gt;╚══════════════╩══════════╩══════════╩══════════╩══════════╩═══════╝&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;For the Adam optimizer, the loss is still decreasing at the 20th epoch, meaning that I could probably keep seeing an improvement in performance if I continued running many more epochs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Plan for Next Time&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;Deal with the discrepancy between train/dev/test data.&lt;/li&gt;&lt;li&gt;Do some more hyper-parameter tuning to try and improve scores.&lt;/li&gt;&lt;li&gt;Implement the end-to-end model architecture based on the &lt;a href=&quot;https://homes.cs.washington.edu/~luheng/files/emnlp2017_lhlz.pdf&quot;&gt;Lee et al. paper&lt;/a&gt;, i.e. scoring holder/target sentiment using a FFNN in a second step. This is point 1 of my MVP/Stretch Goal from my last blog post. If time suffices, also implement bi-affine scoring (&lt;a href=&quot;https://arxiv.org/pdf/1802.10569.pdf&quot;&gt;Verga et al., 2018&lt;/a&gt;, or point 2 in my MVP/Stretch Goal from my last blog post) and compare the results with the baseline model.&lt;/li&gt;&lt;li&gt;Experiment with attention mechanism, instead of calculating attention weights through a linear mapping from hidden state → weight, perhaps use a FFNN and pass in the hidden state.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;All code I’ve written for this post can be found on Github: &lt;a href=&quot;https://github.com/eunsol/document-e2e-sent&quot;&gt;https://github.com/eunsol/document-e2e-sent&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=7de5277b5be&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-17T06:51:48+00:00</dc:date>
	<dc:creator>Belinda Zou Li</dc:creator>
</item>
<item rdf:about="https://medium.com/p/613328b9a85e">
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Further Data Simplification and Baseline Exploration</title>
	<link>https://medium.com/@ryanp97/further-data-simplification-and-baseline-exploration-613328b9a85e?source=rss-6378d85d3a9b------2</link>
	<content:encoded>&lt;p&gt;As a side note, after meeting with Michael (the linguistics grad student mentioned in the first post), it seems that the “final” check of the Kyoto corpus is the most semantically accurate translation despite not being the most fluent. So it’s now feasible, and makes sense to, to get the Kyoto corpus incorporated into training at some point. He also suggested that I could use the top 5 statistically “best” parses from ERG/Jacy rather than just the best to increase the size of my dataset. These are both things that I will look into later down the line.&lt;/p&gt;&lt;h3&gt;Data Simplifications&lt;/h3&gt;&lt;p&gt;Jan and I decided to make a couple extra modifications to the input data in hopes of simplifying the model’s output. Ideally, these changes should allow the model to have a smaller output vocabulary size, and hopefully reduce the number of errors when generating a target tree.&lt;/p&gt;&lt;h4&gt;Simplifying Parentheses&lt;/h4&gt;&lt;p&gt;The first basic change was to separate closing parens from each other such that ) is the only way token that contains a closing paren. Originally, the parser would give spit out the closing parens as a single token, which most likely would confuse the model and result in non-wellformed output graphs.&lt;/p&gt;&lt;p&gt;The second step was to move the opening paren from the node label and attach it to the argument. Since the opening paren should only ever occur with an argument, this simplification removes the responsibility of having to figure out when to output an opening paren. We believe that this should not cause any sparsity issues since there is not very many different types of arguments.&lt;/p&gt;&lt;h4&gt;Simplifying Features&lt;/h4&gt;&lt;p&gt;Next, we concatenate argument features into a single token. The statistical parser is deterministic in its output, so the ordering of the features is always the same. Since there are only a few values each feature take on, the number of possible combinations is reasonable.&lt;/p&gt;&lt;p&gt;This is less than ideal since the model may never output sparse feature combinations. However, this is just a simplification to see the viability of the concept. Once we get a model that performs reasonably well, we can use the normal feature format as input and output.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*cFt-96vfvxtMvURKdfNttw.png&quot; /&gt;Simplified Japanese DMRS graph with squashed features.&lt;/figure&gt;&lt;h4&gt;Removing Features&lt;/h4&gt;&lt;p&gt;As a sanity check dataset, we created a dataset that has no features. This way, the model only has to predict arguments and argument labels. Though the features provide a lot of context, we are just using this dataset for preliminary testing and exploration.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*O5_KxoX1_gH9YURuqOu4nQ.png&quot; /&gt;Simplified, featureless, Japanese DMRS graph. Note that this is the featureless version of the above graph.&lt;/figure&gt;&lt;h4&gt;Removing `cargs`&lt;/h4&gt;&lt;p&gt;When parsing the graph, cargs shows up as one of the features of certain nodes. These represent the proper noun that the argument represents. In our scenario, we remove these carg features from the nodes since this should not affect semantic meaning and simplifies the model’s tasks and reduces sparsity issues (especially when we factor in squashing the features into a single token).&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/576/1*mIhOrdHdlF_R2_QCeXyxFQ.png&quot; /&gt;Example of English DMRS before simplification, with `carg` feature.&lt;/figure&gt;&lt;h3&gt;Baseline Exploration&lt;/h3&gt;&lt;p&gt;I’ve taken a look at both AllenNLP and OpenNMT for starting points of my code, but I haven’t quite made a full decision yet. AllenNLP seems much easier to extend and better documented, while OpenNMT is significantly faster but has a much less intuitive interface and codebase. For now, I’ve tested some models using AllenNLP, but I’m still considering OpenNMT. This week, I focused on experimenting on just the simplified, featureless dataset.&lt;/p&gt;&lt;p&gt;As for the AllenNLP model, it was trained with negative log likelihood as the objective function, 3 layer, BiLSTM with (very) small embedding and hidden sizes for 12 epochs. I wanted to see how much a small model could learn, and it learned how to match parens by epoch 5. It didn’t seem to learn much between the semantic structures, however. I assume this is caused mainly by two things: 1) the model was not trained for long enough or 2) the default attention is not particularly helpful for learning semantic structure. For changing the default attention, the structured attention presented in &lt;a href=&quot;https://arxiv.org/pdf/1705.09207.pdf&quot;&gt;&lt;em&gt;Learning Structured Text Representations&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;by&lt;em&gt; &lt;/em&gt;Liu et al. seems like an interesting and potentially useful change to the seq2seq model.&lt;/p&gt;&lt;p&gt;While I take a look at implementing the above attention, I’m going to train a similar model with 256 embedding and hidden sizes for 20 epochs, and see how that affects the ability of learning semantic representations. Ideally, I would like to get an implementation of the described attention in AllenNLP by next week, but depending on the difficult and how the next model goes, I may switch over to using OpenNMT as they already have it &lt;a href=&quot;https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/StructuredAttention.py&quot;&gt;implemented&lt;/a&gt;.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=613328b9a85e&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-17T05:49:19+00:00</dc:date>
	<dc:creator>Ryan Pham</dc:creator>
</item>
<item rdf:about="https://medium.com/p/7d8e9ec1a8e3">
	<title>Halden Lin &lt;br/&gt; Team undef.: NLP Capstone | 03: Project Proposal</title>
	<link>https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3?source=rss-2759d54493c0------2</link>
	<content:encoded>&lt;p&gt;&lt;em&gt;previous posts: &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;&lt;em&gt;01&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5&quot;&gt;&lt;em&gt;02&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;Towards a Better Understanding of Neural Networks: Visualizing Attention in Sequence-to-Sequence Models&lt;/h3&gt;&lt;h4&gt;A brief review of attention&lt;/h4&gt;&lt;p&gt;The idea of ‘attention’ was first introduced to the sphere of natural language processing by Bahdanau et al. (2014) in &lt;em&gt;Neural machine learning by jointly to align and translate&lt;/em&gt;. The idea is fairly straightforward: if we have an encoder-decoder model, at each decoding time-step we generate a vector of attention weights corresponding to each of the encoding units. That is to say, when generating each output token, we pay ‘attention’ to certain parts of the input sequence. Intuitively, this is much how we as humans fixate on parts of text to perform tasks such as summarization or question answering.&lt;/p&gt;&lt;h4&gt;Why visualization?&lt;/h4&gt;&lt;p&gt;In Machine Learning, neural networks have always been a sort of black box. We know they work incredibly well in certain contexts, but its often difficult to understand why they work so well. The following quote sums up the need for interpretability quite well.&lt;/p&gt;&lt;blockquote&gt;&lt;strong&gt;&lt;em&gt;“I believe the most important direction for future research is interpretability.&lt;/em&gt;&lt;/strong&gt;&lt;em&gt; The attention mechanism, by revealing what the network is “looking at”, shines some precious light into the black box of neural networks, helping us to debug problems like repetition and copying. To make further advances, we need greater insight into what RNNs are learning from text and how that knowledge is represented.”&lt;/em&gt;&lt;/blockquote&gt;&lt;blockquote&gt;- Abigail See, PhD - Stanford University, &lt;em&gt;‘So, is abstractive summarization solved?’&lt;/em&gt; from &lt;a href=&quot;http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html&quot;&gt;Taming Recurrent Neural Networks for Better Summarization&lt;/a&gt;&lt;/blockquote&gt;&lt;p&gt;Visualization provides an avenue for interpretability by mapping the behavior of the complex networks to easy-to-understand visual encodings.&lt;/p&gt;&lt;h4&gt;A survey of related work&lt;/h4&gt;&lt;p&gt;Although I am not aware of any papers dedicated to the visualization of attention, examples can be readily found in both published literature and online blogposts. For each example below, I’ll point out strengths and weaknesses. Ultimately, I hope to show that there are improvements we can make that can augment the interpretability of the workings of seq2seq attentional models.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Heat-maps&lt;/strong&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/404/0*9FCWt3AO5oGxLxtg.&quot; /&gt;Bahdanau et al. (2014). An attention visualization for a seq2seq problem (in this case, translation). Whiter cells represent higher attention.&lt;/figure&gt;&lt;p&gt;The encoding scheme used by Bahdanau et al. (2014) themselves, heat-maps were the most common encoding of attentional data I found. While making the task of relative correlation lookup efficient, these have a couple of weaknesses.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Hard to scale. With tasks involving large input or output (e.g. a hundred or more tokens) the size of the heat-map quickly gets out of hand. Scrolling greatly decreases the effectiveness of a visualization with respect to analysis tasks.&lt;/li&gt;&lt;li&gt;Difficult to read. We generally don’t read in a token-per-line format. Furthermore, source text is rarely in a token-per-line format — we lose insightful information that could be drawn from analyzing the original structure of the text.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;Flow-maps&lt;/strong&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*I3cdFcqDAcdKEwuCpAPHTA.png&quot; /&gt;Rikters et al (2017). The input sequence is seen on top — output on bottom. Thicker lines denote higher attention.&lt;/figure&gt;&lt;p&gt;Less common, but interesting nonetheless. This kind of flow-map suffers from problems similar to those of heat-maps. One could also argue that the thinness of the lines and their cross-hatch nature hinder interpretability.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Interaction&lt;/strong&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*3dTXvSI-L3X3M-MKXRraBA.gif&quot; /&gt;See et al. (2017). Interactive visualization of attention&lt;/figure&gt;&lt;p&gt;Interaction solves many of the issues of the static visualizations surveyed above. We retain the structure of both the input and output text, and lookup is quick and efficient. There is a trade-off, however. We are only able to view the attention of a single word at a time, and as a result it is hard to get a sense of the overall coverage or structure of attention.&lt;/p&gt;&lt;h4&gt;A case study: Summarization&lt;/h4&gt;&lt;p&gt;In particular,&lt;strong&gt; abstractive summarization&lt;/strong&gt;. Summarization is a particularly interesting use case of attention because of the requirement of the condensing of text. The hypothesis is that good abstractive models will be able to cover the majority of the original document. Here I note the difference between &lt;strong&gt;extractive &lt;/strong&gt;and&lt;strong&gt; abstractive &lt;/strong&gt;summarizations. The former involves selecting pieces of the original text, verbatim. The latter involves compressive paraphrasing.&lt;/p&gt;&lt;p&gt;Until recently, most of the work in text summarization has revolved around extractive summarization (See et al. 2017). However, the rising prevalence of recurrent neural networks has allowed for further focus in abstractive summarization. Attention has played an important role in improving results. Below is a brief list of relevant work.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1509.00685.pdf&quot;&gt;Rush, Alexander M. et al. “A Neural Attention Model for Abstractive Sentence Summarization.” &lt;em&gt;EMNLP&lt;/em&gt; (2015).&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1602.06023.pdf&quot;&gt;Nallapati, Ramesh et al. “Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.” &lt;em&gt;CoNLL&lt;/em&gt; (2016).&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1712.06100.pdf&quot;&gt;Hasselqvist, Johan et al. “Query-Based Abstractive Summarization Using Neural Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1712.06100 (2017): n. pag.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1705.04304.pdf&quot;&gt;Paulus, Romain et al. “A Deep Reinforced Model for Abstractive Summarization.” &lt;em&gt;CoRR&lt;/em&gt; abs/1705.04304 (2017): n. pag.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.04368.pdf&quot;&gt;See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” &lt;em&gt;ACL&lt;/em&gt; (2017).&lt;/a&gt;&lt;/p&gt;&lt;h4&gt;Summarization Specific Challenges&lt;/h4&gt;&lt;p&gt;While visualizations of attention are helpful in shedding light on the workings of seq2seq models, summarization models in particular have trouble leveraging this window.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;We care about &lt;strong&gt;where&lt;/strong&gt; attention falls just as much as what it falls on. We hope to maximize &lt;strong&gt;coverage&lt;/strong&gt;. This is not currently addressed in any interactive visualizations I am aware of.&lt;/li&gt;&lt;li&gt;We have large input sequences. As discussed in &lt;strong&gt;“A survey of related work,”&lt;/strong&gt; this is particularly problematic for static visualizations.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;With this in mind, I propose areas for improvement in both interactive and static visualizations.&lt;/p&gt;&lt;h4&gt;Where to?&lt;/h4&gt;&lt;p&gt;With interactive visualizations, two things.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;One. Coverage &lt;/strong&gt;is the aggregated attention over a sequence of output tokens. An example given by See et al. can be seen in the figure below. Perhaps allowing brushing to visualizing the aggregate attention over a phrase or sentence can help us understand attention in a more global context.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/956/0*vE-iXohphbWY6Nam.&quot; /&gt;See et al. (2017). Example of coverage.&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Two. Extraction vs Abstraction: &lt;/strong&gt;Ideally, we want our model to learn how to abstract rather than extract. 1:1 exact match attention is less interesting to see than seeing attention to groups of words. Perhaps emphasizing / de-emphasizing can this in visualizations can help aid understanding of models.&lt;/p&gt;&lt;p&gt;With static visualizations, there are two analysis tasks that we wish to optimize for.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Summary. &lt;/strong&gt;What is the overall structure of the attention (e.g. coverage).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Value. &lt;/strong&gt;Which input words are attended (i.e. focused on) by each output timestep?&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;It is difficult to design an &lt;em&gt;effective&lt;/em&gt; static visualization that lends itself well to both of these tasks. Perhaps we need a set of visualizations. For example, one visualization might afford better performance for summary analysis, while another might afford better performance for value analysis. In addition, these static visualizations can incorporate ideas described in previous section.&lt;/p&gt;&lt;p&gt;Additionally, attention visualizations thus far have been for &lt;strong&gt;specific examples&lt;/strong&gt;. Perhaps there a way we can look &lt;strong&gt;across examples&lt;/strong&gt; to better understand the behavior of these neural networks. Derived metrics for attention or coverage could be useful in better understanding and diagnosing these models.&lt;/p&gt;&lt;p&gt;My hope is that addressing these items in both interactive and static visualizations will allow us to better reason about neural networks. In particular, I hope the result can be used as a valuable tool for error analysis, &lt;strong&gt;even&lt;/strong&gt; &lt;strong&gt;beyond&lt;/strong&gt; hyperparameter tuning. Insights could be gleamed that motivate additions or constraints or mechanisms to optimize coverage (e.g. See et al. (2017)) or abstraction.&lt;/p&gt;&lt;h4&gt;The Plan&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;Minimum Viable Plan&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Develop a TensorBoard plugin that allows for the static and interactive visualizations described in &lt;strong&gt;Where to?&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;Acquire feedback from students / researchers in the Allen School.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;I intend to leverage existing models to retrieve data. For example, that &lt;a href=&quot;https://github.com/abisee/pointer-generator&quot;&gt;provided publicly&lt;/a&gt; by See et al. (2017). The dataset used by them is a &lt;a href=&quot;https://github.com/abisee/cnn-dailymail&quot;&gt;modified CNN/Daily Mail Dataset&lt;/a&gt; [Hermann et al. (2015), See et al. (2017)] — a collection of articles and bullet point summaries.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Stretch Goals&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Explore and implement aggregate, cross-example visualizations as described in &lt;strong&gt;Where to?&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;Release a beta of the TensorBoard plugin on github and acquire feedback there.&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;Works Cited&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.04368.pdf&quot;&gt;See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” &lt;em&gt;ACL&lt;/em&gt; (2017).&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1506.02078.pdf&quot;&gt;Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. “Visualizing and understanding recurrent networks.” &lt;em&gt;arXiv preprint arXiv:1506.02078&lt;/em&gt;(2015).&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1712.06100.pdf&quot;&gt;Hasselqvist, Johan et al. “Query-Based Abstractive Summarization Using Neural Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1712.06100 (2017): n. pag.&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1602.06023.pdf&quot;&gt;Nallapati, Ramesh et al. “Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.” &lt;em&gt;CoNLL&lt;/em&gt; (2016).&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1705.04304.pdf&quot;&gt;Paulus, Romain et al. “A Deep Reinforced Model for Abstractive Summarization.” &lt;em&gt;CoRR&lt;/em&gt; abs/1705.04304 (2017): n. pag.&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1509.00685.pdf&quot;&gt;Rush, Alexander M. et al. “A Neural Attention Model for Abstractive Sentence Summarization.” &lt;em&gt;EMNLP&lt;/em&gt; (2015).&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. “Neural machine translation by jointly learning to align and translate.” &lt;em&gt;arXiv preprint arXiv:1409.0473&lt;/em&gt; (2014).&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1506.03340.pdf&quot;&gt;Hermann, Karl Moritz et al. “Teaching Machines to Read and Comprehend.” &lt;em&gt;NIPS&lt;/em&gt;(2015).&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=7d8e9ec1a8e3&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-12T05:46:21+00:00</dc:date>
	<dc:creator>Halden Lin</dc:creator>
</item>
<item rdf:about="https://medium.com/p/b951950ad9a5">
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Image Annotation Papers</title>
	<link>https://medium.com/@hongnin1/image-summarization-b951950ad9a5?source=rss-c450eb982161------2</link>
	<content:encoded>&lt;p&gt;&lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/im2txt#prepare-the-training-data&quot;&gt;https://github.com/tensorflow/models/tree/master/research/im2txt#prepare-the-training-data&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.6632&quot;&gt;[1412.6632] Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1411.4555&quot;&gt;[1411.4555] Show and Tell: A Neural Image Caption Generator&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;a href=&quot;https://medium.com/media/3d1f87b3acd58d4b818d676b6a67c63a/href&quot;&gt;https://medium.com/media/3d1f87b3acd58d4b818d676b6a67c63a/href&lt;/a&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.03044&quot;&gt;[1502.03044] Show, Attend and Tell: Neural Image Caption Generation with Visual Attention&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Image Annotation for medical use:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1603.08486.pdf&quot;&gt;https://arxiv.org/pdf/1603.08486.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Image Annotation for visually impaired:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://cs224d.stanford.edu/reports/mcelamri.pdf&quot;&gt;https://cs224d.stanford.edu/reports/mcelamri.pdf&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=b951950ad9a5&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-12T04:21:59+00:00</dc:date>
	<dc:creator>Ning Hong</dc:creator>
</item>
<item rdf:about="http://sarahyu.weebly.com/cse-481n/formal-proposal">
	<title>Sarah Yu &lt;br/&gt; Team Jekyll-Hyde: Formal Proposal</title>
	<link>http://sarahyu.weebly.com/cse-481n/formal-proposal</link>
	<content:encoded>&lt;div class=&quot;paragraph&quot;&gt;Linguistic Accommodation for Self-Presentation as seen in Neurotypical vs. Neurodivergent Subreddits&lt;br /&gt;&lt;br /&gt;&lt;u&gt;Hypotheses:&lt;/u&gt; &lt;br /&gt;1) Users change their language depending on the community, represented by subreddits categorized as Neurotypical vs Neurodivergent&lt;br /&gt;      - The divergence from their own baseline is a&lt;span&gt; sign of assimilation through tuned self-presentation*&lt;/span&gt;&lt;br /&gt;2) Language Models, as used by the whole community, differ and not just in topic-specific jargon&lt;br /&gt;      - The language changes of the individual user and the change in delta from the community's language model is a sign of their attempt at assimilating language accommodation*&lt;br /&gt;          - Do certain users adapt better? If so, what is differentiating those users?&lt;br /&gt;&lt;font size=&quot;1&quot;&gt;*(subpoints are very similar, and I'm still working through if there is a nuance, or if they're the same)&lt;/font&gt;&lt;br /&gt;&lt;br /&gt;My &lt;strong&gt;objective&lt;/strong&gt;, then, is to address these hypotheses through the following approach:&lt;br /&gt;&lt;br /&gt;&lt;u&gt;Literature Survey:&lt;/u&gt;&lt;br /&gt;While this project is novel, mainly in the focus on neurotypical vs neurodivergent separation and the use of Reddit data, this project finds guidance from previous work done on similar questions. First, this project aims to extend upon the work of Danescu-Niculescu-Mizil et al. in&lt;em&gt; &lt;/em&gt;&lt;u&gt;&lt;em&gt;Mark My Words! Linguistic Style Accommodation in Social Media​.&lt;/em&gt;&lt;/u&gt; This was the first large-scale endeavor in identifying linguistic accommodation using social media. However, our project extends this work by taking advantage of the siloed nature of Reddit to identify linguistic accommodation employed by a single user across communities as opposed to the one-dimensional view of a user's linguistic accommodation to the general twittersphere in Danescu's paper. Also, this project is informed by Fast and Horvitz in &lt;u&gt;&lt;em&gt;Identifying Dogmatism in Social Media: Signals and Models&lt;/em&gt;&lt;/u&gt;, specifically in their methodologies and models; I attempt to extend upon these with more complex models. In this process, I also found several works that were similar in nature: Tamburrini et al. on language change based on social identity on Twitter, Nguyen and Rose on language socialization in online communities, and Michael and Otterbacher on herding in online review language. Two more relevant works for my project are De Choudhury et al.'s work on identifying the shift to Suicidal Ideation in social media and D&lt;span&gt;anescu-Niculescu-Mizil's work on the life-cycle of users in online communities. &lt;br /&gt;​&lt;br /&gt;&lt;u&gt;Proposed Methodologies&lt;/u&gt;: &lt;/span&gt;&lt;br /&gt;In it's most basic form, these questions can be explored with basic language models. First, we will identify a subset of neurotypical and neurodivergent subreddits to explore (100 or so respectively), chosen by a preliminary search on overlapping users posting between these. Based on this preliminary search, we will also gain a set of users who potentially post to both neurotypical and neurodivergent subreddits (we may need to look only at posts within a band of characters, but that is a parameter I'd like to explore). We will aggregate all of a user's posting history, not just in the subset aforementioned, to model the user's language use and do the same for the language of all posts made by any user (not just our set) to the subreddit to model the subreddit's language. I will supplement these models with the LIWC lexicon to characterize the differences between the communities and between users in different subreddits. (I may use a subset of the LIWC categories later on). A more complex model would be to use PPDB to find differences via paraphrasing. Yet another complex model would be to use a graphical model as inspired by Bamman et al's &lt;em&gt;&lt;u&gt;Learning Latent Personas of Film Characters&lt;/u&gt;&lt;/em&gt;. A stretch goal would be to train an RNN model for the language model of a neurotypical subreddit and that of a neurodivergent to see the probability of a post to belong to either of these categories. A stretch goal (not in complexity as in the RNN, but rather in interest) is to use the &lt;em&gt;Zelig Quotient&lt;/em&gt;, a proposed measure for normalizing linguistic accommodation by Jones et al and see how much this may affect our findings. &lt;br /&gt;One special consideration is the use of NSFW language. My only filter will be to disqualify the list of NSFW subreddits, as named by a reddit post (so meta) in being chosen for the subsets, but otherwise we will not do anything special for NSFW language in other subreddits. &lt;br /&gt;&lt;br /&gt;&lt;u&gt;Resources&lt;/u&gt;: Lots of Reddit fun!!!&lt;br /&gt;&lt;br /&gt;&lt;em&gt;​Here goes nothing...&lt;/em&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-04-11T07:00:00+00:00</dc:date>
</item>
<item rdf:about="http://deeplearningturingtest.wordpress.com/?p=14">
	<title>Ananth Gottumukkala &lt;br/&gt; Team Turing Test: Project Proposal: Question-Based Knowledge Representation</title>
	<link>https://deeplearningturingtest.wordpress.com/2018/04/11/project-proposal-question-based-knowledge-representation/</link>
	<content:encoded>&lt;p&gt;&lt;strong&gt;Project Objective:&lt;/strong&gt; The objective of this project is to create a model that can build a representation of the knowledge it’s gathered by reading the input text line by line and asking appropriate clarifying questions to a human for further insight.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Literature Survey:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;cite class=&quot;formatted-citation formatted-citation--style-mla&quot;&gt;Williams, Jason D. et al. “Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning.” &lt;em&gt;ACL&lt;/em&gt; (2017).&lt;/cite&gt;&lt;/li&gt;
&lt;li&gt;
&lt;div class=&quot;padded&quot;&gt;&lt;cite class=&quot;formatted-citation formatted-citation--style-mla&quot;&gt;Zhang, Qianqian et al. “A Review on Entity Relation Extraction.” &lt;em&gt;2017 Second International Conference on Mechanical, Control and Computer Engineering (ICMCCE)&lt;/em&gt; (2017): 178-183.&lt;/cite&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Arvind Neelakantan’s Doctoral Disseration: &lt;em&gt;Knowledge Representation and Reasoning with Deep Neural Networks (2017)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;
&lt;div class=&quot;padded&quot;&gt;&lt;cite class=&quot;formatted-citation formatted-citation--style-mla&quot;&gt;Zhao, Tiancheng and Maxine Eskénazi. “Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning.” &lt;em&gt;SIGDIAL Conference&lt;/em&gt; (2016).&lt;/cite&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Minimal Viable Action Plan:&lt;/strong&gt; My minimum action plan would be to implement and train an RL model to ask certain question template(s) as clarification as it processes input text line by line. Then a human (myself) would assign a reward to indicate the quality of the question and give a brief answer to the question. This brief answer would then be used to expand the model’s stored knowledge representation and the assigned reward would be used to train the RL model. This series of exchanges is treated like a one-on-one conversation even if it’s more like a one-sided lecture.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stretch Goals:&lt;/strong&gt; Stretch goals include extracting features from the existing knowledge base and feeding those into the model to further improve the relevancy of questions (for example not asking questions that the model should already know). Another stretch goal would be to translate the knowledge representation back into everyday English text for tasks such as answering reading comprehension questions. This can be done either by training a Machine Translation model, using a parser like ANTLR, or some other method.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proposed Methodologies:&lt;/strong&gt; My proposed methodologies follows a similar outline as given by Williams et al. in their paper on Hybrid Code Networks (HCNs).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The input text is read in line-by-line and goes through various preprocessing steps like entity extraction, word embeddings layer, sentiment analysis, bag of words, etc. These features are concatenated and fed into the model.&lt;/li&gt;
&lt;li&gt;In the HCN paper, the model was an RNN followed by a softmax layer (probability distribution over the various actions to take).
&lt;ul&gt;
&lt;li&gt;In this problem, the “actions” to take are the different types of question templates to ask so copying this model and substituting their action templates with my question templates would work.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;An alternative approach would be to build a Deep Q Network model with an LSTM (or GRU) layer at the end, so the network would update the Q values for each question type that can be asked (and the one with the highest Q value can be greedily selected). For clarification, each input sentence would represent a time step.&lt;/li&gt;
&lt;li&gt;After a question type is selected with either of the 2 approaches above, nouns need to be substituted in to form an actual question (as in the HCN paper). For example, a question template might be “Is there a relationship between ______ and ______?” and the two blanks need to be filled in with nouns. If the substitution results in a good question, a good reward is assigned and otherwise, a very negative reward is assigned. If it’s a good question, then an answer is given to update the knowledge representation.
&lt;ul&gt;
&lt;li&gt;Some possible ways to create this knowledge representation is to use lists, trees, semantic networks, production rules, logical propositions and/or other existing NLP knowledge representation models (I haven’t decided on one yet).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Available Resources/Databases:&lt;/strong&gt; To train the model, any reading comprehension dataset like MS Marco (Microsoft), SQuAD (Stanford), RACE, etc. can be used. If my project is successful enough, I can even use these datasets for evaluation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Evaluation Plan:&lt;/strong&gt; If I only finish my minimum viable action plan, I’m not sure how I can qualitatively assess my knowledge representation except by comparing the representation against the information I wanted to be recorded (or have other people judge what knowledge should be stored). On the other hand, if I finish my stretch goals (translating knowledge representation to English) I can try to use the knowledge representation to train on and respond to queries in a reading comprehension dataset (which would be a much more qualitative evaluation).&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-11T06:59:08+00:00</dc:date>
	<dc:creator>ananthgo</dc:creator>
</item>
<item rdf:about="https://medium.com/p/7b6d1a9ec67c">
	<title>Aaron Johnston, Lynsey Liu &lt;br/&gt; Team Viterbi Or Not To Be: Formal Project Proposal</title>
	<link>https://medium.com/@viterbi.or.not/formal-project-proposal-7b6d1a9ec67c?source=rss-c522ef075bb3------2</link>
	<content:encoded>&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*cr8jfKVLjgd9Y7YmxXWSYA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Introducing our project topic, &lt;strong&gt;automatic conversation summarization&lt;/strong&gt;! Our proposal will outline specific objectives, motivations, and plan we have for this project. We also cover here the resources we have gathered — related work, datasets (as promised), and evaluation frameworks, to demonstrate viability and give background on the topic.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Objective&lt;/strong&gt;&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*DmM6aXAhZi61Ec7Yhp6YBw.png&quot; /&gt;Example summarization of an email chain from the &lt;a href=&quot;https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/bc3.html&quot;&gt;W3C email threads corpus&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;In the field of automatic text summarization, there are numerous techniques that can be used to produce summaries of general text data. However, for our project, we seek to work on a more specific type of data by exploring and evaluating techniques for summarization of conversation logs. Therefore, the models we will attempt to implement and measure will take input in the form of natural text from conversations, such as email threads, chat logs, or transcribed spoken conversations, and output more concise summaries that capture the most important parts of the input.&lt;/p&gt;&lt;p&gt;While a document or paper typically sticks to a single topic at once and represents communication between an author and the reader, conversational data is characterized by a mixing of sub-topics and, in many cases, contributions from multiple different authors before one topic is finished. As a result, the objective of conversational summarization includes identifying topics and threads among a potentially chaotic conversation in order to make a sensible summarization even without the benefit of a single, linear topic progression.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Extractive vs. Abstractive&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;When considering text summarization, a distinction has to be made between two different approaches to summarization that offer different levels of implementation difficulty and usefulness. An extractive summary is produced by identifying the most important sentences from the input text and combining them to form a summary that is the concatenation of those sentences.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*iZujvW2rP_HSVU2KXLtvew.png&quot; /&gt;Example of a short chatlog and its corresponding extractive summary&lt;/figure&gt;&lt;p&gt;Alternatively, an abstractive summary consists of new text generated from the topics and important aspects of the input text, but requires the model to create new summary sentences rather than simply re-using existing ones. As a result of this generation process, abstractive summaries are typically considered to be more useful, because they consist of natural-sounding text while still paraphrasing the concepts that are important. However, current techniques perform much better at generating extractive summaries, which are considered much easier to implement because they only require assigning scores to the sentences of the input.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*1oiGv-MrfdubjbKv-Vp9fA.png&quot; /&gt;Example of a short chatlog and its corresponding abstractive summary&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The &lt;strong&gt;academic&lt;/strong&gt; motivations behind our project are to pursue a topic in summarization that has been relatively less explored. This means tackling the challenges that come with the conversation summarization domain — annotated conversation datasets are typically smaller and more unpredictable, and whereas tasks like document summary usually involve one topic written by one author/voice, conversations and involve many participants and have less well-defined topic segmentation.&lt;/p&gt;&lt;p&gt;The project is also motivated by the possibly impactful &lt;strong&gt;applications &lt;/strong&gt;of conversation summarization. Being able to summarize long chains of emails or group IMs is an increasingly important task to tackle in today’s world and can be a useful augmentation to digital group conversations.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Related Work&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;In our literature survey, we will first discuss the two papers that have been most impactful in helping establish our project plan.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/0*U0i-moKx4s_YB3My.&quot; /&gt;Graph of contributions of general feature categories to the performance of logistic regression classifiers&lt;/figure&gt;&lt;p&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/D08-1081&quot;&gt;&lt;strong&gt;Summarizing Spoken and Written Conversations&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;¹ &lt;/strong&gt;uses meeting and email datasets. The authors of this paper approach extractive summarization with logistic regression classifiers and a mix of general summarization features as well as some basic conversation summarization features. The overall feature categories can be seen in the corresponding figure, which graphs the contributions of each feature category to the classifier’s performance. “Participant” category features were found to help achieve competitive results.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1010/0*wLl3u1tR-GKYuIql.&quot; /&gt;Table of performances of previously published models and the models used in the experiment (C4.5, NB, MLP, SVM)&lt;/figure&gt;&lt;p&gt;&lt;a href=&quot;http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf&quot;&gt;&lt;strong&gt;Summarizing Online Conversations: A Machine Learning Approach&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;²&lt;/strong&gt; uses chatlog and meeting datasets. The authors of this paper conduct experiments with Decision Tree, Naive Bayes classifier, Multilayer Perceptron (MLP) and Support Vector Machine (SVM) summarizers to create extractive summaries. The input feature vectors to these trainable summarizers use both general summarization features (sentence length, sentence position, similarity to title, etc.) as well as conversation specific summarization features that are more specialized than those mentioned in the previously discussed paper (is question, sentiment score, discourse markers). The paper overall found that the Naive Bayes classifier and MLP performed the best.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The datasets we found cover a variety of conversation domains and are all human-annotated with summaries.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Email: &lt;/strong&gt;&lt;a href=&quot;https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/bc3.html&quot;&gt;&lt;strong&gt;W3C Email Threads&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;40 email threads of ~80 lines each&lt;/li&gt;&lt;li&gt;Extractive and Abstractive Summaries&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Chat: &lt;/strong&gt;&lt;a href=&quot;https://flossmole.org/content/software-archaeology-gnue-irc-data-summaries&quot;&gt;&lt;strong&gt;GNU Enterprise Chatlogs&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;~120 chats of ~1200 lines each&lt;/li&gt;&lt;li&gt;Abstractive Summaries&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Spoken Conversation: &lt;/strong&gt;&lt;a href=&quot;http://groups.inf.ed.ac.uk/ami/corpus/&quot;&gt;&lt;strong&gt;AMI Meeting Transcripts&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;140 meeting transcriptions of ~45 minutes each&lt;/li&gt;&lt;li&gt;Extractive and Abstractive Summaries&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;In terms of &lt;strong&gt;automated&lt;/strong&gt; methods of evaluation for the summaries that we will generate, there are a few frameworks we can use:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;BLEU&lt;/strong&gt; (BIlingual Evaluation Understudy)&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Precision measure with some enhancements&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;METEOR&lt;/strong&gt; (Metric for Evaluation of Translation with Explicit ORdering)&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Improves on BLEU by adding recall, synonyms&lt;/li&gt;&lt;li&gt;Better at sentence-level evaluation&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;ROUGE&lt;/strong&gt; (Recall-Oriented Understudy for Gisting Evaluation)&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;ROUGE-L&lt;/strong&gt;: Longest common subsequence&lt;/li&gt;&lt;li&gt;&lt;strong&gt;ROUGE-N&lt;/strong&gt;: Overlap of N-Grams between passages&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;However, for summarization tasks, &lt;strong&gt;human evaluation&lt;/strong&gt; is possibly most ideal and it is unclear if any automatic metric can be as effective.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Minimum Viable Product&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Our plan is to start with two &lt;strong&gt;baseline models&lt;/strong&gt; that replicate the best performing models (Naive Bayes and MLP approaches) of “Summarizing Online Conversations: A Machine Learning Approach” using the non-conversation specific features.&lt;/p&gt;&lt;p&gt;We would then experiment with tweaks to the baseline models that use conversation-specific features, both based off of the ones described in the paper and also based on some of our ideas about what kinds of domain-specific features might benefit conversation summarization. We can also continue to explore alternative models and compare model approaches.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Stretch Goals&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;There are a number of ideas we have for stretch goals that can push our project further:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Neural Network implementation: &lt;/strong&gt;Applying general summarization techniques to conversation&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Model entity relationships: &lt;/strong&gt;Identifying the role of a contributor or named entity, such as a supervisor in an email thread&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Abstractive summarization: &lt;/strong&gt;Convert a previously extractive summary to be natural-sounding and abstractive&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Optimize performance for a specific domain: &lt;/strong&gt;For example, in chats about bugfixes, use that specific context to try outperforming the general case&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Relate parameters from different types of data: &lt;/strong&gt;For example, use data from spoken corpus to improve results on email&lt;/p&gt;&lt;p&gt;We hope you enjoyed reading our project proposal and we are excited to get working!&lt;/p&gt;&lt;p&gt;[1] Murray, G. &amp;amp; Carenini, G. (2008). Summarizing Spoken and Written Conversations. &lt;em&gt;Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, &lt;/em&gt;773–782.&lt;/p&gt;&lt;p&gt;[2] Sood, A. &amp;amp; Varma, V. (2012). Summarizing Online Conversations: A Machine Learning Approach. &lt;em&gt;Centre for Search and Information Extraction Lab International Institute of Information Technology Hyderabad,&lt;/em&gt; 500.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=7b6d1a9ec67c&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-11T06:56:31+00:00</dc:date>
	<dc:creator>Viterbi Or Not To Be</dc:creator>
</item>
<item rdf:about="https://medium.com/p/c8a12d3ae611">
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #3: Project Proposal</title>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-3-project-proposal-c8a12d3ae611?source=rss-fad49d942bf3------2</link>
	<content:encoded>&lt;p&gt;The project I’m working on is document-level entity-entity sentiment analysis.&lt;/p&gt;&lt;h4&gt;Objectives and Definition&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*mfDKzKS3wAEnJZ3gvLHwyw.png&quot; /&gt;Notice in the above example that the arc from Russia to Belarus is negative, which makes sense if our article has the sentence “Russia criticizes Belarus.”&lt;/figure&gt;&lt;p&gt;The goal of my project can be stated thusly: given a document, be able to figure out whether various entities within the document feel positively or negatively towards each other.&lt;/p&gt;&lt;p&gt;In particular, I’m looking to apply a neural model to &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;an existing paper&lt;/a&gt; in an attempt to improve its F1 scores.&lt;/p&gt;&lt;p&gt;Applications of this work include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Modeling social dynamics between entities&lt;/li&gt;&lt;li&gt;Applying sentiment analysis to problems beyond simply movie/product reviews&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A farther motivation for this project is that entity-entity sentiment analysis is a relatively novel task, with little existing work thus far. Also, even work in related fields is usually focused on the sentence level, rather than the document level.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Literature Survey&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Some work in related fields to this project. For each paper, I will be focusing especially on its model /methodologies, which could be of use to developing my own model —&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Targeted Sentiment Analysis&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12074/12065&quot;&gt;Zhang, Meishan, Zhang, Yue, and Vo, Duy-Tin, 2016. Gated Neural Networks for Targeted Sentiment Analysis.&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Introduced the use of gated neural networks to perform targeted sentiment analysis.&lt;/li&gt;&lt;li&gt;Inputs: Concatenation of 2 types of word embeddings: both embeddings incorporate sentiment information in some way&lt;/li&gt;&lt;li&gt;Model: GRNN + G3 model, where outputs of GRNN are pooled and fed into G3. The G3 was used to better model interaction between left and right context for an entity, as sentiment can be dominated by left or right context&lt;/li&gt;&lt;li&gt;Outputs: Targeted sentiment&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Coreference Resolution&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://homes.cs.washington.edu/~luheng/files/emnlp2017_lhlz.pdf&quot;&gt;Lee, Kenton, He, Luheng, Lewis, Mike, and Zettlemoyer, Luke, 2017. End-to-end Neural Coreference Resolution.&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Model: 2-step, 1st step being a biLSTM, and 2nd step being a FFNN&lt;/li&gt;&lt;li&gt;Inputs to FFNN: Something nice about this model is the fact that they were able to encode multi-word entities efficiently by concatenating LSTM outputs.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/227/1*7yoU47vuvU5CaBm-r3n3aw.png&quot; /&gt;ϕ used to encode size of span. x* are outputs of LSTMs for boundaries of span. x hat are outputs of attention mechanism over span.&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;FFNN architecture:&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/390/1*9KxdIWkYNSzMwDJeSI3fuw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Document-level Relation Extraction&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cs.jhu.edu/~npeng/papers/TACL_17_RelationExtraction.pdf&quot;&gt;Peng, Nanyun, Poon, Hoifung, Quirk, Chris, Toutanova, Kristina, Yih, and Wen-tau, 2017. Cross-Sentence N-ary Relation Extraction with Graph LSTMs.&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/600/1*-M0TCCGCpmYtYpVnYfhAHg.png&quot; /&gt;Example of a graph LSTM structure used in the paper. Note the additional links between non-adjacent cells encodes for syntactic dependencies and discourse relations.&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;Uses a graph LSTM with architecture following the structure of the document graph (introduced by &lt;a href=&quot;https://arxiv.org/pdf/1609.04873.pdf&quot;&gt;Quirk and Poon, 2017&lt;/a&gt;), which encodes various dependencies, including word adjacency, syntactic dependencies, and discourse relations&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.10569.pdf&quot;&gt;Verga, Patrick, Strubell, Emma, and McCallum, Andrew, 2018. Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction.&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/345/1*6ap---pVvwWNNdCgwhh16w.png&quot; /&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;Bi-affine neural model: First pass document through Transformer, then evaluate relations between pairs of tokens using a bi-affine operator and the LogSumExp function&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;&lt;strong&gt;Methodologies&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;Baseline Neural Model (Minimal Viable Plan)&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Inputs: for each token, concatenate word embeddings, polarity embeddings, and holder/target/none embeddings&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Word embeddings are pre-trained GloVe embeddings&lt;/li&gt;&lt;li&gt;EX: “the cat disliked milk”, holder = “cat”, target = “milk”&lt;/li&gt;&lt;li&gt;Input becomes: [the, 0, 0] [cat, 0, H] [disliked, NEG, 0] [milk, 0, T]&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Model: Attentive biLSTM&lt;/p&gt;&lt;p&gt;Output: Sentiment score for positivity and negativity&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Scoring Holder/Target Sentiment in a Second Step (Minimal Viable Plan / Stretch Goal, i.e. I think I can get this but I can’t guaruntee it)&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Feedforward Neural Network: Incorporating the FFNN from the Lee et al. paper (see above for details). To adapt the architecture for my needs, I will get rid of the mention score, and thus the white and grey cells will be the same. The grey cell, instead of outputting a coreference score, will output a sentiment (positivity and negativity) score.&lt;/li&gt;&lt;li&gt;Bi-affine Scoring: Incorporating the scoring mechanism from the Verga et al. paper (see above for details). Instead of a “Transformer,” I will use the LSTM from my baseline model, which gets fed in to two MLPs, the biaffine operator, and finally aggregated using the LogSumExp function.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;Experimenting with Different Ways of Aggregating across Different Mentions (Stretch Goal)&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;LogSumExp function from the Verga et al. paper (see above).&lt;/li&gt;&lt;li&gt;Encode inputs to second step in a way that incorporates information from all mentions.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;Experimenting with Graph LSTM Structure (Stretch Goal)&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Based off of ideas in Peng et al. paper (see above)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Incorporate Ideas from Targeted Sentiment Analysis (Stretch Goal)&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Add a G3 gate, based on Zhang et al. paper (see above), or even adapt the architecture of the G3 gate to better suit the task of sentiment analysis between entities&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;&lt;strong&gt;Available Resources&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Dataset from Choi et al. (paper I’m improving). The data has already been pre-processed using StanfordNLP. Some of the preprocessing that’s been done includes tokenization, named entity recognition, and co-reference resolution.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Evaluation Plan&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;As an evaluation metric, I’m going to use the F1 scores for both positive and negative sentiments.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=c8a12d3ae611&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-11T06:47:08+00:00</dc:date>
	<dc:creator>Belinda Zou Li</dc:creator>
</item>
<item rdf:about="http://mathstoc.wordpress.com/?p=309">
	<title>Kuikui Liu, Nicholas Ruhland &lt;br/&gt; Team INLP: NLP Capstone Post #3: Proposal</title>
	<link>https://mathstoc.wordpress.com/2018/04/11/nlp-capstone-post-3-proposal/</link>
	<content:encoded>&lt;p&gt;Here, we finally present our project proposal in full.&lt;/p&gt;
&lt;h3&gt;Project Objectives:&lt;/h3&gt;
&lt;p&gt;Our goal for this project is to engineer a model that, given the instrumental (“karaoke”) music for a song in English represented as MIDI data, output a coherent sequence of words corresponding to lyrics for the music. The model will produce timings along with the words to align it with the background instrumentals. Additionally, given the output of the model and the input music, we will automate their combination into a song complete with lyrics and supporting instrumentals. This combined output will be playable and we intend to do live demonstration.&lt;/p&gt;
&lt;h3&gt;Proposed Methodology:&lt;/h3&gt;
&lt;p&gt;Here, we outline the steps we will need to take in detail.&lt;/p&gt;
&lt;li&gt;Data collection (datasets of songs, preferably with instrumentals and lyrics already separated)&lt;/li&gt;
&lt;li&gt;Decide on vocabulary and how to handle uncommon words&lt;/li&gt;
&lt;li&gt;Decide and implement any required preprocessing of the raw MIDI data. Strip lyrics from MIDI data if not already provided in dataset.&lt;/li&gt;
&lt;li&gt;Decide and implement model (see Model Design)&lt;/li&gt;
&lt;li&gt;Implement model sanity checks&lt;/li&gt;
&lt;li&gt;Model tweaking (we expect this will take the majority of the time; see Model Design)&lt;/li&gt;
&lt;li&gt;Implement automated combination of model output (lyrics) and model input (instrumentals)&lt;/li&gt;
&lt;li&gt;Further testing&lt;/li&gt;
&lt;li&gt;Assuming preceding steps are completed satisfactorily, proceed to stretch goals&lt;/li&gt;
&lt;li&gt;Presentation and write-up&lt;/li&gt;
&lt;h3&gt;Model Design:&lt;/h3&gt;
&lt;p&gt;We will pursue a seq2seq RNN approach, taking in input MIDI data represented as a sequence, and outputting a sequence of words from a specified vocabulary. This model will be referred to as the generator. We will employ adversarial training, simultaneously training a many-to-one RNN discriminator that, given the input instrumentals and corresponding lyrics, output if the lyrics were produced by the generator or not. We will follow approaches taken in previous works such as SeqGAN [2] (and [3, 4]), namely using policy gradient ideas from reinforcement learning to obtain gradients that can be backpropagated from the discriminator network through the generator network. We note that syntactic correctness can be enforced in this manner, as malformed lyrical output can be assigned arbitrarily small reward.&lt;/p&gt;
&lt;h3&gt;Stretch Goals:&lt;/h3&gt;
&lt;p&gt;There are several stretch goals we will consider, time permitted. They are as follows, in no particular order.&lt;/p&gt;
&lt;li&gt;Handling multiple languages, particularly those with less available data&lt;/li&gt;
&lt;li&gt;Given a specific songwriter/band, produce the instrumentals along with lyrics for a new song that is in the style of that songwriter/band&lt;/li&gt;
&lt;li&gt;Lyrics generation for duets, or multi-singer songs&lt;/li&gt;
&lt;li&gt;Playing with phoneme-level generation&lt;/li&gt;
&lt;h3&gt;Core Challenges:&lt;/h3&gt;
&lt;p&gt;The core challenges we will need to overcome include alignment of lyrics with the music, and production of sensible lyrics. On the more technical side, it is well-known that ensuring convergence in adversarial training is difficult.&lt;/p&gt;
&lt;h3&gt;Available Resources:&lt;/h3&gt;
&lt;p&gt;Existing music datasets for machine learning tasks are made up of audio samples (such as .wav or .mp3), or MIDI data that specifies timing and notes. For karaoke, lyrics are also provided either as a separate text file (.LRC) specifying the timing of each word, or can be embedded into the MIDI file directly (.KAR). It may also be useful to train a lyric model on a larger corpus of song lyrics, since lyrics are easier to collect than fully time-annotated karaoke files.&lt;/p&gt;
&lt;p&gt;The MusicNet dataset [9] provides 330 classical instrumental audio files, each of which has associated timing provided for every note. Since we are primarily interested in lyrical generation and alignment, this dataset is not going to be useful for creating a language model.&lt;/p&gt;
&lt;p&gt;An existing karaoke dataset called Kara1k [1] provides many features computed from 1000 lyric-annotated songs. This provides lots of metadata about each song, including annotated chords for each timestep of the song. According to the KaraMIR website, these features are extracted from audio samples using Vamp Plugins, which estimates chords with accuracy up to 70%. &lt;/p&gt;
&lt;p&gt;We propose a new dataset (name not yet determined) of MIDI karaoke data with embedded lyrics (.KAR). This dataset contains over 700 files, scraped from a karaoke content aggregator [11]. Timed lyrical data has been extracted from these files, and the precise timing of each note is already available by nature of the MIDI format.&lt;/p&gt;
&lt;p&gt;Additional datasets for training a lyric model may be useful, and many are available. One such dataset is the 55000+ Song Lyrics on Kaggle [10]. This could help our model generalize its lyrical output beyond the limited set of vocabulary available within the 1000 or fewer annotated karaoke songs.&lt;/p&gt;
&lt;h3&gt;Evaluation Plan:&lt;/h3&gt;
&lt;p&gt;Evaluation of our model can be done several ways. The first is simply to listen to the music ourselves. This is the most direct method of evaluation but is not efficient, as likely we will need many iterations of tuning; furthermore, will likely need to listen to several songs to be confident of the model’s quality. Hence, we will also design basic “sanity check” tests for our models.&lt;/p&gt;
&lt;p&gt;Recall that in our proposed methodology, we intend to use adversarial training. The discriminator network itself gives a direct evaluation of the generator. As long as the discriminator is of vetted quality, and the discriminator is run on sufficiently many examples (with roughly even number of generated and true examples mixed in), the generator will be deemed also of sufficient quality (as a “sanity check”).&lt;/p&gt;
&lt;p&gt;Of course, this leaves the question of ensuring the discriminator is good. We can run the discriminator on instrumentals combined with randomly generated words (according to some distribution), or on instrumentals combined with the original lyrics, which are perturbed in some fashion. As an example, one can perturb the original lyrics temporally (making an utterance off-beat when it should be precisely on the down-beat of a bar) or replacing a few words with randomly selected ones (according to some distribution over the vocabulary). These “test inputs” to the discriminator can be generated before-hand.&lt;/p&gt;
&lt;h3&gt;Literature Survey:&lt;/h3&gt;
&lt;p&gt;Here are some relevant papers (most were already included in preceding posts).&lt;/p&gt;
&lt;p&gt;[1] Y. Bayle, L. Marsik, M. Rusek, M. Robine, P. Hanna, K. Slaninova, J. Martinovic, J. Pokorny. “Kara1k: A Karaoke Dataset for Cover Song Identification and Singing Voice Analysis”. IEEE International Symposium on Multimedia (ISM), 2017. &lt;a href=&quot;https://ieeexplore.ieee.org/document/8241597/&quot; rel=&quot;nofollow&quot;&gt;https://ieeexplore.ieee.org/document/8241597/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] L. Yu, W. Zhang, J. Wang, Y. Yu. “SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient”. Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, 2017. &lt;a href=&quot;https://arxiv.org/abs/1609.05473&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/abs/1609.05473&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] S. Lee, U. Hwang, S. Min, S. Yoon. “A SeqGAN for Polyphonic Music Generation”. 2017. &lt;a href=&quot;https://arxiv.org/abs/1710.11418&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/abs/1710.11418&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] H. W. Dong, W. Y. Hsiao, L. C. Yang, Y. H. Yang. “MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment”. 2017. &lt;a href=&quot;https://arxiv.org/abs/1709.06298&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/abs/1709.06298&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio. “Generative Adversarial Nets”. NIPS, 2014. &lt;a href=&quot;https://papers.nips.cc/paper/5423-generative-adversarial-nets&quot; rel=&quot;nofollow&quot;&gt;https://papers.nips.cc/paper/5423-generative-adversarial-nets&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, X. Chen. “Improved Techniques for Training GANs”. NIPS, 2016. &lt;a href=&quot;https://arxiv.org/abs/1606.03498&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/abs/1606.03498&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[7] M. Arjovsky,  S. Chintala, L. Bottou. “Wasserstein GAN”. 2017. &lt;a href=&quot;https://arxiv.org/abs/1701.07875&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/abs/1701.07875&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[8] J. Faille, Y. Wang. “Using Deep Learning to Annotate Karaoke Songs”. 2016. &lt;a href=&quot;https://www.semanticscholar.org/paper/Using-Deep-Learning-to-Annotate-Karaoke-Songs-Faille-Wang/521361762a7327f8fcc77bd9d76eaa2b503f845a&quot; rel=&quot;nofollow&quot;&gt;https://www.semanticscholar.org/paper/Using-Deep-Learning-to-Annotate-Karaoke-Songs-Faille-Wang/521361762a7327f8fcc77bd9d76eaa2b503f845a&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9] J. Thickstun, Z. Harchaoui, S. Kakade. “Learning Features of Music from Scratch”. 2017. &lt;a href=&quot;https://arxiv.org/abs/1611.09827&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/abs/1611.09827&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[10] Additional data &lt;a href=&quot;https://www.kaggle.com/mousehead/songlyrics&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[11] Even more additional data &lt;a href=&quot;http://vooch.narod.ru/midi/midi.htm&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-11T06:45:55+00:00</dc:date>
	<dc:creator>Kuikui Liu</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-3753031463594823927.post-4531878816260312232">
	<title>Ron Fan, Aditya Saraf &lt;br/&gt; Team PrimeapeNLP: Blog Post #3</title>
	<link>https://cse481n.blogspot.com/2018/04/blog-post-3.html</link>
	<content:encoded>&lt;h1 dir=&quot;ltr&quot; id=&quot;docs-internal-guid-ea0c9d97-b369-9237-6f13-3675807d7a60&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Project Objectives&lt;/span&gt;&lt;/h1&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Single document summarization (SDS) is one of the remaining challenging problems in natural language processing. Novel methods are presented frequently in new papers, but they often do not include specific code allowing for reproducibility and are evaluated on specific datasets that make comparisons between models meaningless and difficult.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;There are many approaches to SDS, but they can be broadly divided into combinatorial approaches and neural approaches. Neural approaches build a neural architecture, such as a seq2seq/encoder-decoder model or single sequence RNNs. Combinatorial approaches will either try to frame the problem as an optimization problem, and then use an ILP solver, or frame the problem as a classic NP-hard problem, like Knapsack or Maximum Coverage. We want to explore both approaches, and compare their performance on the same dataset.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;SDS is comprised of two tasks: extractive summarization and abstractive summarization. Extractive summarization compiles a summary by selecting sentences from the document’s text while abstractive summarization generates text for the summary (sentences that may not have been present in the document’s text). While abstractive summarization might have more intuitive appeal, our project will focus on extractive summarization to enable meaningful comparisons between neural and combinatorial approaches (combinatorial approaches often must be extractive).&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;In this project, we plan to implement at least one neural and one combinatorial model for extractive single document summarization. We hope to establish some meaningful ways to compare the differences between selections made by the different types of models. Our primary goal is to better understand the strengths and weaknesses of neural and combinatorial models for single document summarization - a particular important aspect of SDS given the general roughness of existing evaluation metrics. We will gauge our progress based on reaching acceptable performance on commonly-used evaluation metrics when we implement models.&lt;/span&gt;&lt;/div&gt;&lt;h1 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Methodology&lt;/span&gt;&lt;/h1&gt;&lt;h3 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Minimal Viable Action Plan&lt;/span&gt;&lt;/h3&gt;&lt;ol style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Build our data set using existing data. Specifically, convert data better suited for training abstractive summarization models into data that can be used for extractive summarization..&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Implement a simple combinatorial model (for example, we can do a simple maximum coverage problem, where we set up the “universe” to be the vocabulary of the document, and treat the sentences as sets of words).&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Implement a simple neural model (just treat the problem as a generic binary classification problem).&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Train models on identical data sets and do a baseline comparison -- how well does a simple neural model do vs. a simple combinatorial model? This doesn’t tell us much about the relative strengths of the two approaches (we can’t quantify “simple”), but with some error analysis, we might be able to see what sentences neural models are misidentifying vs. what sentences combinatorial models are misidentifying.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Build at least one state-of-the-art combinatorial model (adapting from a recent paper). We have two candidate papers: Hirao et al.’s Tree Knapsack approach and Durrett et al.’s Compression/Anaphoricity&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;h3 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Stretch Goals&lt;/span&gt;&lt;/h3&gt;&lt;ol style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Design our own model that is a combination of the strong points of the combinatorial and neural models. Ideally, our model would be as good as or better than the existing models we implemented on the quantitative and qualitative metrics we use.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Alternatively, we can use ideas from one domain to improve an aspect of a SOTA model in the other domain. For example, we might learn that neural models are great at dealing with named entities, and so incorporate a neural layer in a combinatorial model (perhaps by allowing the output of the neural layer to determine the weights of named entities).&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Design a common system for comparing performance of extractive summarization models. Rather than a differentiable evaluation metric, we think it may be useful to choose a set of “tough” documents to summarize and bundle them together with specific reasons for their difficulty, so that researchers may more easily identify weaknesses in models they are working on.&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h1 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Available Resources&lt;/span&gt;&lt;/h1&gt;&lt;h3 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Dataset/Evaluation&lt;/span&gt;&lt;/h3&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;For this problem, we will be using the DailyMail/CNN dataset. From our initial research, this seems to be the standard dataset for both document summarization as well as basic reading comprehension. The dataset has 400,000 articles, and includes both the full text of the article as well as bullet point “highlights”. For reading comprehension, an important word is omitted from the highlights and the machine is asked to fill in the blank. For text summarization, the bullet points are considered the “gold standard” summaries -- machine generated summaries are evaluated against the bullet points, typically&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;using ROUGE metrics&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt; &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;(Lin, 2004). While this works fine for abstractive summarization, this training corpus is not annotated enough for extractive summarization. More specifically, extractive summarization requires sentence level binary annotations, to indicate whether each sentence does or doesn’t belong in the summary. So we need to first convert the bullet points into more fine grained annotations.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;We’ve looked at two papers which briefly touched on this. Nallapati et. al. used a greedy approach, where they added one sentence at a time to the extractive summary while seeking to maximize the Rouge score with respect to the abstractive summary (the bullet points). They also tried to use an RNN decoder in combination with the abstractive summaries to train the extractive model without using sentence-level annotations. However, this approach was slightly less successful than estimating sentence-level annotations. Cheng and Lapata used a different approach - they created a “rule-based system that determines whether a given sentence matches a highlight...The rules take into account the position of the sentence in the document, the unigram and bigram overlap between document sentences and highlights, [and] the number of entities appearing in the highlight and in the document sentence”. It’s not 100% clear what rules the authors used, but according to Nallapati et. al., the rule-based approach found a better “ground-truth” than the greedy approach.&lt;/span&gt;&lt;/div&gt;&lt;h3 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;GitHub Repositories&lt;/span&gt;&lt;/h3&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Some researchers publish the code they used in their paper on GitHub. We can use repos for quick comparisons or to see how they design their code.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;a href=&quot;https://github.com/abisee/pointer-generator&quot; style=&quot;text-decoration: none;&quot;&gt;&lt;span&gt;https://github.com/abisee/pointer-generator&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;: This repo is for See et al.’s Pointer-Generator neural model.&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;a href=&quot;https://github.com/cheng6076/NeuralSum&quot; style=&quot;text-decoration: none;&quot;&gt;&lt;span&gt;https://github.com/cheng6076/NeuralSum&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;: This repo is for Cheng and Lapata’s neural model, that combines a sentence level RNN with a word level CNN.&lt;/span&gt;&lt;/div&gt;&lt;h1 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Related Work and References&lt;/span&gt;&lt;/h1&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Cheng, J., &amp;amp; Lapata, M. (2016). Neural Summarization by Extracting Sentences and Words. &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;arXiv:1603.07252 [Cs]&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;. Retrieved from http://arxiv.org/abs/1603.07252&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Durrett, G., Berg-Kirkpatrick, T., &amp;amp; Klein, D. (2016). Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints. &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;arXiv:1603.08887 [Cs]&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;. Retrieved from http://arxiv.org/abs/1603.08887&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Hirao, T., Yoshida, Y., Nishino, M., Yasuda, N., &amp;amp; Nagata, M. (2013). Single-Document Summarization as a Tree Knapsack Problem. In &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt; (pp. 1515–1520). Seattle, Washington, USA: Association for Computational Linguistics. Retrieved from http://www.aclweb.org/anthology/D13-1158&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. In S. S. Marie-Francine Moens (Ed.), &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Text Summarization Branches Out: Proceedings of the ACL-04 Workshop&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt; (pp. 74–81). Barcelona, Spain: Association for Computational Linguistics.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;Nallapati, R., Zhai, F., &amp;amp; Zhou, B. (2016). SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents. &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;arXiv:1611.04230 [Cs]&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;. Retrieved from http://arxiv.org/abs/1611.04230&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;See, A., Liu, P. J., &amp;amp; Manning, C. D. (2017). Get To The Point: Summarization with Pointer-Generator Networks. &lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;arXiv:1704.04368 [Cs]&lt;/span&gt;&lt;span style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;&quot;&gt;. Retrieved from http://arxiv.org/abs/1704.04368&lt;/span&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-04-11T06:33:00+00:00</dc:date>
	<dc:creator>Ron &amp; Aditya</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-9203775015655831448.post-5878905571398539101">
	<title>Pinyi Wang, Dawei Shen, Xukai Liu &lt;br/&gt; Team Overfit: #3 Project Proposal</title>
	<link>https://teamoverfit.blogspot.com/2018/04/3-project-proposal.html</link>
	<content:encoded>&lt;h2 style=&quot;height: 0px;&quot;&gt;&lt;span&gt;Team Overfit&lt;/span&gt;&lt;/h2&gt;&lt;h3&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3&gt;&lt;span&gt;Project repo: &lt;span style=&quot;font-size: 18.72px;&quot;&gt;&lt;a href=&quot;https://github.com/pinyiw/nlpcapstone-teamoverfit&quot;&gt;https://github.com/pinyiw/nlpcapstone-teamoverfit&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h4&gt;&lt;span&gt;Team members: Dawei Shen, Pinyi Wang, Xukai Liu&lt;/span&gt;&lt;/h4&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div style=&quot;text-align: start; text-indent: 0px;&quot;&gt;&lt;div style=&quot;margin: 0px;&quot;&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;Blog Post: #3: 04/10/2018&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;span&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Social Media Predicts Stock Price (StartUp Mode)&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;span&gt;&lt;b id=&quot;docs-internal-guid-213a19db-b353-3e4c-1df6-5dd289daeb8b&quot; style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;There are vast amount of new information related to companies listed on the stock market appears instantly, with immediate impact on stock prices. Our project is for monitoring those text on the social media platform and extract the key information that have impact on the stock prices and predict its future.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Background and Project objectives &lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;https://www.investopedia.com/terms/s/stockmarket.asp&quot;&gt;Stock Market&lt;/a&gt; refers to the collection of markets and exchanges where the issuing and trading of equities, bonds and other sorts of securities takes place.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Social media, such as Twitter, often reflects how people think about a company and therefore can be used as an indicator of the changes of stock price in the near future.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Traditionally, analytics use statistical model built on past stock prices and recent news to forecast stock prices. We would like apply Machine Learning and Natural Language Processing models on social media to see if it has enough information for us to make good prediction of future stock price.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;&lt;span&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;span&gt;&lt;b&gt;Proposed methodologies&lt;/b&gt;&lt;span style=&quot;white-space: pre;&quot;&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;b&gt;&lt;span&gt;Dataset:&lt;/span&gt;&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;background-color: white; color: black; vertical-align: baseline; white-space: pre;&quot;&gt;Twitter data: &lt;/span&gt;&lt;span style=&quot;background-color: white; color: #1155cc; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;a href=&quot;https://developer.twitter.com/en/docs&quot;&gt;https://developer.twitter.com/en/docs&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Preprocess twitter data:&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Tokenization&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Stemming&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Lemmatization&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;background-color: white; color: black; vertical-align: baseline; white-space: pre;&quot;&gt;Bloomberg financial news dataset: &lt;/span&gt;&lt;span style=&quot;background-color: white; color: #1155cc; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;a href=&quot;https://github.com/philipperemy/financial-news-dataset&quot;&gt;https://github.com/philipperemy/financial-news-dataset&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;span&gt;&lt;b&gt;Minimal viable action plan&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;span&gt;Forecast companies’ stock price changes (UP, DOWN, STAY) &lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;b&gt;&lt;span&gt;Model&lt;/span&gt;&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;span&gt;N-gram with appropriate smoothing as baseline&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;span&gt;Use RNN/LSTM/GRU as model&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;&lt;span&gt;User Interface&lt;/span&gt;&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;span&gt;Command line REPL&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;span&gt;&lt;b&gt;Stretch goals&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;span&gt;We could implement LSTM/GRU model to extract important information from the text in the preprocess&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;span&gt;Forecast the approximate future stock price for a company given a future date&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;span&gt;Auto trader bot that can take streaming tweets from twitter api and update the model prediction &lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;span&gt;Fusion with 8-K reports to elevate the accuracy&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;&lt;b&gt;Evaluation plan&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;span&gt;F-1 score for (UP/DOWN)&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Loss functions for comparing predictions and expectations.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Evaluate on time required to do a prediction.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div style=&quot;margin: 0px;&quot;&gt;&lt;span&gt;&lt;b style=&quot;white-space: pre;&quot;&gt;Reference&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;background-color: white; color: black; vertical-align: baseline; white-space: pre;&quot;&gt;On the Importance of Text Analysis for Stock Price Prediction: &lt;/span&gt;&lt;a href=&quot;https://nlp.stanford.edu/pubs/lrec2014-stock.pdf&quot;&gt;&lt;span style=&quot;background-color: white; color: #1155cc; vertical-align: baseline; white-space: pre;&quot;&gt;https://nlp.stanford.edu/pubs/lrec2014-stock.pdf&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;color: black; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;Stock Trend Prediction Using News Sentiment Analysis: &lt;/span&gt;&lt;span style=&quot;color: #1155cc; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1607.01958.pdf&quot;&gt;https://arxiv.org/pdf/1607.01958.pdf&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div style=&quot;font-family: times;&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-04-11T06:10:00+00:00</dc:date>
	<dc:creator>Team Overfit</dc:creator>
</item>
<item rdf:about="https://medium.com/p/43368563cf97">
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: Machine Dictionary</title>
	<link>https://medium.com/nlp-capstone-blog/machine-dictionary-43368563cf97?source=rss----9ba3897b6688---4</link>
	<content:encoded>&lt;p&gt;We have decided to call our project, the Machine Dictionary. Formally, the goal of this project is to use a large corpus of data to generate definitions for technical terms that are consistent with how those terms are explored in the corpus.&lt;/p&gt;&lt;h4&gt;Motivation&lt;/h4&gt;&lt;p&gt;Our aim is to explore novel text generation approaches and apply them to the specific task of generating definitions. One of these techniques includes a specific approach we have termed “Connecting the Dots”. This approach will allow us to loosely structure the definitions such that they contain meaningful content but also keep the model general enough to generate appropriate context. We will discuss this approach further down. Another large motivation is the amount of research paper data we have from the AI2 Semantic Scholar corpus. This will allow us to use domain specific corpora to support definitions.&lt;/p&gt;&lt;h4&gt;Prior Work&lt;/h4&gt;&lt;p&gt;Although text generation has been a hot topic in NLP research for a while, we did not discover any prior attempts to generate definitions for technical terms. That said, the text generation task has itself been explored in great detail by many others.&lt;/p&gt;&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/pdf/1707.05501.pdf&quot;&gt;&lt;strong&gt;this paper&lt;/strong&gt; (Jain et al., 2017)&lt;/a&gt;, the authors explore the task of generating short stories given a sequence of independent short descriptions. They approach the problem by using an Encoder-Decoder model to connect the descriptions and the short stories. This method might help us generate short text, but in our case, the input would be a large amount of data in the domain.&lt;/p&gt;&lt;p&gt;In this &lt;a href=&quot;https://pdfs.semanticscholar.org/9dad/f5bb0a2182b1509c5ea60d434bb35d4701c1.pdf?_ga=2.15851958.1083977791.1523309085-1136887644.1523309085&quot;&gt;other paper (Ghazvininejad et. al, 2016)&lt;/a&gt;, the authors explore generating poetry based on topics. This paper is relevant to our project for several reasons. For instance, the paper generates poems based on a given topic, much like our goal which is to generate definitions based on a given term. In addition, the authors generate poetry by taking advantage of the structure of Shakespearean sonnets such as the unique rhyme scheme and the iambic pentameter cadence. We can use the techniques proposed in the paper to selectively choose information from the training corpus, based on the term we are asked to define and how we believe definitions should be structured.&lt;/p&gt;&lt;h4&gt;Minimum Viable Plan&lt;/h4&gt;&lt;p&gt;To reiterate, our model should be able to generate definitions that are based on context received from a large corpus. In this MVP, we can make the simplification that our model should generate definitions of a fixed length (for example, 5 sentences). These definitions should be grammatical and technically correct.&lt;/p&gt;&lt;h4&gt;Baseline Approach and Evaluation&lt;/h4&gt;&lt;p&gt;Andrej Karpathy, a former PhD student at Stanford, explores the incredible effectiveness of RNNs in in his blog. The article, &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;The Unreasonable Effectiveness of RNNs&lt;/a&gt;, essentially claims that RNNs have an uncanny ability to learn the structure of training data, such as Wikipedia Articles, Shakespeare, and even code. In our baseline approach, we take advantage of this ability by training an RNN language model on the training data. At testing time, we can provide the term we seek to define as a seed to the RNN, which can then generate text until we hit a word limit or the RNN generates the &amp;lt;STOP&amp;gt; character. We can evaluate this approach by using perplexity, to ensure that we have a good language model. We can also cross-check definitions with other sources like Wikipedia articles. Finally, it might also be prudent to have humans evaluate the definitions.&lt;/p&gt;&lt;h4&gt;Target Approach I and Evaluation&lt;/h4&gt;&lt;p&gt;We propose two different target approaches to this model. In this first approach, we utilize techniques from work done previously in abstractive summarization. Given a term, we could filter the input data on all sentences associated with that term. We could obtain all the sentences that contain the data and a few sentences in the nearby surroundings, which could capture the context for the data. We would then use these sentences to generate a summary, which we would call the definition of the term. In this approach, we might use an attention mechanism to focus on the most important parts of the input. In terms of evaluation, we would use the cross referencing method from above, where we take the produced definition and the “correct definition” as determined by an external source and compare the number of common words.&lt;/p&gt;&lt;h4&gt;Target Approach II and Evaluation&lt;/h4&gt;&lt;p&gt;In this second approach, we explore a concept we have chosen to call “Connecting the Dots”. In this approach, we structure the definition generation by using key words. To elaborate, each term might be closely connected to a certain number of other words which could influence the definition of the term greatly. Consider the term &lt;strong&gt;osteoporosis&lt;/strong&gt;. This term might be closely associated with the words &lt;strong&gt;bones, degrade, bone degradation, fractures, &lt;/strong&gt;and &lt;strong&gt;women. &lt;/strong&gt;We could use these words to structure a definition for &lt;strong&gt;osteoporosis &lt;/strong&gt;as a fill in the blank task.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Osteoporosis is … bones … degrade … bone degradation … fractures … women.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In the above structure, we would rely on the model to appropriately and grammatically fill in the context between each keyword. As a result, we might generate the following definition:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Osteoporosis &lt;/strong&gt;is a disease that cause &lt;strong&gt;bones &lt;/strong&gt;to &lt;strong&gt;degrade.&lt;/strong&gt; &lt;strong&gt;Bone degradation &lt;/strong&gt;often leads to &lt;strong&gt;fractures.&lt;/strong&gt; &lt;strong&gt;Osteoporosis &lt;/strong&gt;most commonly affects &lt;strong&gt;women.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;It’s important to note that when we fill in the context between keywords, we must condition on the original term that we are defining. For example, between the words &lt;strong&gt;fracture &lt;/strong&gt;and &lt;strong&gt;women&lt;/strong&gt;, there might be several sentences we could generate, but we must keep in mind how the keywords are related given that they are about &lt;strong&gt;osteoporosis.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In this approach, we will build on the neural network model and add task-specific architecture to capture relationships between words. We hope that the model can learn how to define keywords associated with terms and use those terms to structure a definition.&lt;/p&gt;&lt;p&gt;In terms of evaluation, we introduce another technique. In this evaluation method we take a paragraph in which the technical term appears and omit the term. If we rephrase the problem as a classification task and ask the model to predict the omitted term, we would be able to conclude whether the model has a contextual understanding of the technical term. This evaluation technique would be a good supplement to the human evaluation method where we request users to rank how correct and readable the definitions are.&lt;/p&gt;&lt;h4&gt;Stretch Goals&lt;/h4&gt;&lt;p&gt;The ideal goal would be to generate text without any constraints on length, order, or keyword usage. This would be a more “hands-off” approach to text generation and could also allow us to train on different domain based corpora. We might be able to achieve this stretch goal if we perform well on the goals outlined in the Minimum Viable Plan.&lt;/p&gt;&lt;p&gt;Another stretch goal has to do with ontology matching, whereby we compare two definitions to determine whether they describe the same concept. We could extend this example to generate definitions for all technical terms across a body of research papers, determine which terms are defined similarly in different papers, and unify the terminology across all papers. This goal is definitely a stretch goal, but if we can perfect the architecture for generating definitions, we see this as a future application of our project.&lt;/p&gt;&lt;h4&gt;Data&lt;/h4&gt;&lt;p&gt;We plan to use the Semantic Scholar Open Research Corpus for this project. This corpus consists of over 20 million research papers classified into two domains (Computer Science and Medicine). Depending on the approach we take to solve this task, we would filter the data and train accordingly.&lt;/p&gt;&lt;h4&gt;Resources and Literature Survey&lt;/h4&gt;&lt;p&gt;We have mentioned several resources above that were the most useful for formulating the project proposal. Here are those resources and a few other resources that we think might be useful in the future.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Text Generation Techniques&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/797d/7d968b88d5b5dd7c3271d08acd7296950d41.pdf?_ga=2.73597202.1083977791.1523309085-1136887644.1523309085&quot;&gt;Using Lexical Chains for Text Summarization (Barzilay et. al, 1997)&lt;br /&gt;&lt;/a&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/9dad/f5bb0a2182b1509c5ea60d434bb35d4701c1.pdf?_ga=2.15851958.1083977791.1523309085-1136887644.1523309085&quot;&gt;Generating Topical Poetry (Ghazvininejad et. al, 2016)&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;The Unreasonable Effectiveness of RNNs&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1707.05501.pdf&quot;&gt;Story Generation from Sequence of Independent Short Descriptions (Jain et al., 2017)&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=43368563cf97&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/machine-dictionary-43368563cf97&quot;&gt;Machine Dictionary&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-11T06:03:31+00:00</dc:date>
	<dc:creator>Karishma Mandyam</dc:creator>
</item>
<item rdf:about="http://cse481n-capstone.azurewebsites.net/?p=37">
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: Formal Proposal</title>
	<link>http://cse481n-capstone.azurewebsites.net/2018/04/10/formal-proposal/</link>
	<content:encoded>&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Motivations:&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We want to create novel models for determining if the text is offensive, and why that text is offensive. To do this we want to create a new dataset that makes this task easier. We hope that our dataset and models pave the way for further innovations by others, as well as better trained conversational agents that have a better understanding of what they should or should not say. We’re going to teach them how to watch their language!&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We would like to correctly classify sentences that keywords matching cannot achieve. For example:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;&lt;i&gt;What do you call an adult that has imaginary friends? Religious&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;And we would like to tell the reason why the sentence above is bad as well.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Minimal Viable Plan:&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;By comparing the similarity of content phrases found in r/MeanJokes posts and posts all over Reddit, we hope to create a large, high-quality dataset for training models to detect offensive text. We want to create this dataset and use crowdsourcing to label it. The labels should say if the text was offensive, and if it was then was it an attack against a particular group, what group that was, as well as the reasoning for why the labeler labeled the text this way. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;While we wait for our data to be labeled, we want to start by creating baseline models on existing datasets, such as Twitter Hate Speech, Wiki Detox, and Stanford Politeness. We think that these datasets are similar enough to begin work on classifiers that don’t make use of deep annotation. After this, we can start work on improving performance on these datasets up until our crowdsourcing completes. We will explore novel models on existing datasets and try to improve their performance. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Stretch Goals:&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Once our new Reddit dataset is fully labeled, we want to test the existing models that we made on the other datasets and continue improving them. We also want to use the new data to experiment with Q&amp;amp;A or Deep Annotation models for creating a model that knows why a particularly offensive post is offensive. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In case if we can’t receive labeled dataset on time, we will continue to make improvements to novel models on existing datasets.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Evaluation Plan: &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Classifier Models: Precision, Recall, F1 score&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Rationale Models: deeper comparison to crowdsourced label explanations&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Dataset: Random Sampling + Human Judgement&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Existing Work: &lt;/span&gt;&lt;/h3&gt;
&lt;h5&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Previous Capstone Project: &lt;/span&gt;&lt;/h5&gt;
&lt;h5&gt;&lt;a href=&quot;https://michael0x2a.github.io/nlp-capstone/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Team Inverted Cat&lt;/span&gt;&lt;/a&gt;&lt;/h5&gt;
&lt;h5&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Datasets: &lt;/span&gt;&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ZeerakW/hatespeech&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Hate Speech Twitter Annotations&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; (Waseem et al. 2016)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Hate Speech and Offensive language dataset &lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; (Davidson et al. 2017)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://meta.wikimedia.org/wiki/Research:Detox/Data_Release&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Wikipedia Talk Corpus &lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; (Wulczyn et al. 2017)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.cs.cornell.edu/~cristian//Politeness.html&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Stanford Politeness Corpus&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;A list of bad words&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Pre-trained word embeddings: GloVe, Facebook FastText, Google Word2Vec&lt;/span&gt;&lt;/p&gt;
&lt;h5&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Papers: &lt;/span&gt;&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Automated-Hate-Speech-Detection-and-the-Problem-of-Davidson-Warmsley/6ccfff0d7a10bf7046fbfd109b301323293b67da&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Davidson, Thomas J et al. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM (2017).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Hate-Speech-Detection-with-Comment-Embeddings-Djuric-Zhou/c9948f7213167d65db79b60381d01ea71d438f94&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Djuric, Nemanja et al. “Hate Speech Detection with Comment Embeddings.” &lt;/span&gt;&lt;i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;WWW&lt;/span&gt;&lt;/i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(2015).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Using-Convolutional-Neural-Networks-to-Classify-Gamb%C3%A4ck-Sikdar/0dca29b6a5ea2fe2b6373aba9fe0ab829c06fd78&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Gambäck, Björn and Utpal Kumar Sikdar. “Using Convolutional Neural Networks to Classify Hate-Speech.” (2017).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Abusive-Language-Detection-in-Online-User-Content-Nobata-Tetreault/e39b586e561b36a3b71fa3d9ee7cb15c35d84203&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Nobata, Chikashi et al. “Abusive Language Detection in Online User Content.” &lt;/span&gt;&lt;i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;WWW&lt;/span&gt;&lt;/i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(2016).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Ex-Machina%3A-Personal-Attacks-Seen-at-Scale-Wulczyn-Thain/4a7204431900338877c738c8f56b10a71a52e064&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Wulczyn, Ellery et al. “Ex Machina: Personal Attacks Seen at Scale.” &lt;/span&gt;&lt;i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;WWW&lt;/span&gt;&lt;/i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; (2017).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-10T22:37:31+00:00</dc:date>
	<dc:creator>Team Watch Your Language!</dc:creator>
</item>
<item rdf:about="https://medium.com/p/a1903faeadb7">
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Project Proposal — Neural Machine Translation with Semantic Transfer</title>
	<link>https://medium.com/@ryanp97/project-proposal-neural-machine-translation-with-semantic-transfer-a1903faeadb7?source=rss-6378d85d3a9b------2</link>
	<content:encoded>&lt;p&gt;State-of-the-art neural machine translation does not currently utilize much, if any, semantic information, meaning it misses out on a large amount of potentially useful information indirectly embedded in the sentence. This project aims to explore the benefits that semantic transfer could offer to neural machine translation.&lt;/p&gt;&lt;p&gt;In particular, this project will focus on Dependency Minimal Recursion Semantics and translation between English DMRS and Japanese DMRS. Currently, I’m working on this project alone under the supervision of Jan Buys.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*AlAZF1O2uYt4b12hCu7ajA.png&quot; /&gt;An example of English and Japanese DMRS graphs for the above sentence. Note the sentences have the same meaning.&lt;/figure&gt;&lt;h4&gt;Minimal Viable Action Plan&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;Obtain a parallel corpus.&lt;/li&gt;&lt;li&gt;Parse DMRS graphs from the parallel corpus&lt;/li&gt;&lt;li&gt;Simplify graphs (and be able to recover them in a robust manner to handle the model’s output)&lt;/li&gt;&lt;li&gt;Train a seq2seq model to predict a Japanese DMRS graph given an English DMRS graph as well as the required embeddings&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;I’ve chosen the &lt;a href=&quot;http://www.edrdg.org/wiki/index.php/Tanaka_Corpus&quot;&gt;Tanaka Corpus&lt;/a&gt; as my parallel corpus and cleaned it such that the only remaining sentences are English and Japanese sentences which belong to a translation pair. From there, I’ve parsed the corresponding DMRS graphs. For further details on how I cleaned the corpus and parsed the graphs, refer to my &lt;a href=&quot;https://medium.com/@ryanp97/project-logistics-and-package-exploration-3d3651220219&quot;&gt;last&lt;/a&gt; blog post.&lt;/p&gt;&lt;p&gt;Currently, I’m working on how to simplify and recover the graphs robustly. Simplifying the graph seems fairly easy; however, I made some incorrect assumptions about the graphs, so recovery is currently a work in progress. Furthermore, I have yet to handle the robustness aspect. In general, I need to handle the cases where the model does not output a valid “simplified” graph. This includes, but is not limited to: handling mismatched/missing parentheses.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*LdgExRT9YoJ-UwxYxwJfsA.png&quot; /&gt;On the left is the English DMRS and on the right is the Japanese DMRS. Each example is labeled with a sentence ID and the original sentence (prefixed with `#` which Smatch ignores). The model will not have to predict these comments. However, in the future, we’d like to extend the model such that it would be able to recover the original sentence after generating a DMRS graph in the target language.&lt;/figure&gt;&lt;p&gt;From then, the last step in the minimal viable action plan is to train and evaluate model. Ideally, we would train the embeddings for the tokens in the graphs and the seq2seq model end-to-end with &lt;a href=&quot;https://amr.isi.edu/eval/smatch/tutorial.html&quot;&gt;Smatch&lt;/a&gt; as the objective function, though I am still working with Jan on how I should piece this model together.&lt;/p&gt;&lt;h4&gt;Stretch Goals&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;Explore alternatives to standard seq2seq models (i.e. TreeLSTMs, custom architecture, etc.)&lt;/li&gt;&lt;li&gt;Expand/supplement dataset with &lt;a href=&quot;https://alaginrc.nict.go.jp/WikiCorpus/index_E.html&quot;&gt;Kyoto Corpus&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Since we are expecting the model to learn and generate graphs, it would be ideal if we were able to use a tree-like structure that would more accurately represent the data. If I am able to finish the minimal viable action plan with enough time left over, I would like to experiment and test out different architectures and see how that affects the model’s performance. There are a couple different packages for TreeLSTMs (such as &lt;a href=&quot;https://github.com/dasguptar/treelstm.pytorch&quot;&gt;this&lt;/a&gt; one) that would potentially make this task not too difficult. Though, eventually I would like to customize an architecture for this task.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/496/1*omSJj6AYg7TDPUW0XPY6ew.png&quot; /&gt;A figure of a TreeLSTM grabbed from the &lt;a href=&quot;https://arxiv.org/pdf/1503.00075.pdf&quot;&gt;original&lt;/a&gt; paper on TreeLSTMs.&lt;/figure&gt;&lt;p&gt;Each example in the Tanaka Corpus is a single sentence, making it easy to work with in terms of development and debugging. However, if I have extra time, I would like to supplement the Tanaka Corpus with the Kyoto Corpus which is comprised of translated Wikipedia articles. The corpus has much longer examples and provides a much more complex and realistic setting. An issue is that the documentation is in Japanese, and it is not entirely clear which of the different translations is “correct” as each example has a “primary,” “secondary,” and “check” translation. You can read more about it on the Kyoto Corpus website, hyperlinked above.&lt;/p&gt;&lt;h4&gt;Evaluation Plan&lt;/h4&gt;&lt;p&gt;There is not really a good statistical model, nor are there results that are very comparable due to the scope of the project. Since the scope of the project does not include recovering the sentence from a generated DMRS graph, comparisons between this project and papers that do tree-to-tree machine translations don’t really make much sense.&lt;/p&gt;&lt;p&gt;I expect to have some time leftover to explore different architectures, so I hope to make the seq2seq results the baseline for comparison for other architectures I have time to try.&lt;/p&gt;&lt;h4&gt;Related Work&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.08381.pdf&quot;&gt;Neural AMR: Sequence-to-Sequence Models for Parsing and Generation&lt;/a&gt; is fairly relevant to this project in the sense that it has similar motivations for working with semantics in NLP. The paper covers how they overcame a lack of labeled data, achieved competitive results for both parsing AMR graphs and generating text from AMR graphs, as well as extensive ablation studies and analysis. Their graph preprocessing steps are extremely relevant to this project, and I will likely be referencing their paper and &lt;a href=&quot;https://github.com/sinantie/NeuralAmr&quot;&gt;codebase&lt;/a&gt; often as I work on cleaning and preprocessing the Tanaka Corpus.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1503.00075.pdf&quot;&gt;Improved Semantic Representations From Tree-Structured LSTMs&lt;/a&gt; provides a good basis for my stretch goals. The Child-Sum TreeLSTM units seem like a good option to test for representing the DMRS graph as any given node can take any number of children. These children can be used to represent dependencies as outlined in the paper, but it seems it might be possible to have the properties of a head word as children of the corresponding node as well. There’s a couple other useful ideas that come from the paper, but for the sake of brevity, we’ll leave it at this.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1603.06075.pdf&quot;&gt;Tree-to-Sequence Attention Neural Machine Translation&lt;/a&gt; is a slight glimpse at the future of this project. If this project is successfully able to transfer semantic information between languages, the next step would make the model complete the entire cycle, sequence-to-graph-to-graph-to-sequence, in hopes of achieving better results than statistical machine translation.&lt;/p&gt;&lt;p&gt;Additionally, here is a short list of other relevant papers:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.07092.pdf&quot;&gt;Robust Incremental Neural Semantic Graph Parsing&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P13-2131&quot;&gt;Smatch: an Evaluation Metric for Semantic Feature Structures&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=a1903faeadb7&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-10T05:31:27+00:00</dc:date>
	<dc:creator>Ryan Pham</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-5600014144802012716.post-8898628104121215850">
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog 3: Formal Proposal</title>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/04/blog-3-formal-proposal.html</link>
	<content:encoded>&lt;b style=&quot;font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;Minimal viable action plan &lt;/b&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;Implement the model from the java paper mentioned in the previous blog posts. This includes the variable and method camel case encoding, the two step attention, the type constrained decoding, and many other tasks such as preprocessing, evaluation metrics etc.&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;Stretch goals&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;This depends on my error analysis on the mvp, but here are a couple ideas.&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;1. &lt;/b&gt;Incorporate implementation specific encoding. The encoder just uses the method names, but it'd be interesting to also include the method implementations.&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;&lt;b&gt;2. &lt;/b&gt;More type constraints on the decoder. Currently if the decoder wants to generate a variable, there's nothing to check that the variable was previously declared.&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;Project objectives&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;Reproduce the strong paper baseline. &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;Proposed methodologies&lt;/b&gt; &lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;Implement the model from the paper. Potentially experiment with other semantic parsing task architectures and see if they also perform competitively with the baseline, for example seq2seq.&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;Available resources&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;The dataset and allennlp.&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;Evaluation plan&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;Test on the test set and measure bleu and exact match metrics.&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;Test if this is useful by coming up with a couple of real classes that I've written and see if it generates the method.&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;Perhaps come up with a new metric, such as a binary executability metric (stretch goal).&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;Literature survey&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;The type constrained architecture was used in a number of recent papers such as &quot;&lt;/span&gt;&lt;span style=&quot;font-size: 14.44px;&quot;&gt;A syntactic neural model for parsing natural language to executable code&quot;. The dataset is novel in that previous ones haven't used programmatic contexts and have focused on nl2code pairs.&lt;/span&gt;&lt;/span&gt;</content:encoded>
	<dc:date>2018-04-10T03:21:00+00:00</dc:date>
	<dc:creator>nlpcapstone</dc:creator>
</item>
<item rdf:about="https://medium.com/p/45c89bec2c2e">
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Movie Sentiment Summarization — Project Proposal</title>
	<link>https://medium.com/@hongnin1/movie-sentiment-summarization-project-proposal-45c89bec2c2e?source=rss-c450eb982161------2</link>
	<content:encoded>&lt;p&gt;Team: Ning Hong, Zhuchun Liu, Sujie Zhou&lt;/p&gt;&lt;p&gt;Overview: our model will be able to summarize the reviews for the input movies. The summarization of the movie include how the audience feel about the movie and what is the overall rating for the movie, for example, given a movie title, our model should be able to produce something like this: &amp;lt;movie title&amp;gt; is violet but good, most people think this movie is 6/10.&lt;/p&gt;&lt;p&gt;If time permits, we would like to improve our model such that it can also output a more detailed overall review for the movie instead of simple sentences, for example, given a movie title as input, our model should output: “&amp;lt;movie title&amp;gt; got my full attention from beginning to end. I couldn’t turn away. I didn’t want to turn away. For me, that’s extremely rare, I would give it 8/10.”&lt;/p&gt;&lt;p&gt;Another stretch goal for our model is to be able to detect sentiment not only in the US market, but also in China market by using data from DouBan (one of the largest movie review site for China), and compare the sentiment between US and China for a certain movie, for example, given an input movie title, our model can output something like: &amp;lt;movie title&amp;gt; was generally perceived more positively in the US than in China, the Chinese audience mostly felt uncomfortable about its violent and explicit content whereas more American audience appreciated the bloodiness of the film.&lt;/p&gt;&lt;p&gt;Model: we are going to use basic encoder- decoder RNN that serves as our baseline and then propose several novel models for summarization, each addressing a specific weakness in the base- line such as encoder-decoder RNN with attention and large vocabulary trick, capturing keywords using feature-rich encoder, modeling rare/unseen words using switching generator-pointer, and capturing hierarchical document structure with hierarchical attention.&lt;/p&gt;&lt;p&gt;This project has a lot of potential uses. Not only can we use movie dataset to output movie sentiment summarization, if given restaurant review dataset (Yelp), we can also output summarization about how customers feel about a restaurant.&lt;/p&gt;&lt;p&gt;On the other hand, there might be some difficulties we will be facing: we are planning to use sequence2sequence model to generate summarization, it is hard to determine how good is the output we are generating; more research needs to be done for this problem.&lt;/p&gt;&lt;p&gt;Resources:&lt;/p&gt;&lt;p&gt;Data scraping blogpost:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.dataquest.io/blog/web-scraping-beautifulsoup/&quot;&gt;An intermediate tutorial&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Possible IMDb training data:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset&quot;&gt;SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Model paper:&lt;/p&gt;&lt;a href=&quot;https://medium.com/media/1ff2a893f361e9bda813a32d4e89c5f6/href&quot;&gt;https://medium.com/media/1ff2a893f361e9bda813a32d4e89c5f6/href&lt;/a&gt;&lt;p&gt;Model github:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/thunlp/TensorFlow-Summarization&quot;&gt;thunlp/TensorFlow-Summarization&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Movie sentiment paper:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/8076790/&quot;&gt;Movie review summarization and sentiment analysis using rapidminer - IEEE Conference Publication&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=45c89bec2c2e&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-10T02:54:02+00:00</dc:date>
	<dc:creator>Ning Hong</dc:creator>
</item>
<item rdf:about="http://deeplearningturingtest.wordpress.com/?p=12">
	<title>Ananth Gottumukkala &lt;br/&gt; Team Turing Test: Warm Up: Testing a Codebase</title>
	<link>https://deeplearningturingtest.wordpress.com/2018/04/06/warm-up-testing-a-codebase/</link>
	<content:encoded>&lt;p&gt;I installed both the Tensorflow and Pytorch API’s since I’m not sure which framework I will use yet. Then, I downloaded the following repository which implemented Hybrid Code Networks for Dialog State tracking in its respective research paper.&lt;/p&gt;
&lt;p&gt;Code base URL: &lt;a href=&quot;https://github.com/voicy-ai/DialogStateTracking&quot; rel=&quot;nofollow&quot;&gt;https://github.com/voicy-ai/DialogStateTracking&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Research Paper URL: &lt;a href=&quot;https://www.semanticscholar.org/paper/Hybrid-Code-Networks%3A-practical-and-efficient-with-Williams-Asadi/0645905d70caf180433145be09c9af266a85c863&quot; rel=&quot;nofollow&quot;&gt;https://www.semanticscholar.org/paper/Hybrid-Code-Networks%3A-practical-and-efficient-with-Williams-Asadi/0645905d70caf180433145be09c9af266a85c863&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Their implementation uses Keras (built on Tensorflow) to build the network. The model stores a predetermined set of action templates to execute based on what the user requests. By feeding in features like the previous action taken, a bag of words vector, an entity tracking feature vector, etc. their RNN outputs a softmax distribution over the possible action templates. The action taken is the one with the highest probability. Because the conversation is restricted to a particular domain such as searching for a restaurant, the model performed well when I ran and tested it. The model generally recognized the type of request I was making, but its responses were extremely robotic and towards the end, gave me yes or no questions to answer to narrow down what action it should take. My goal is to generalize the type of information the model can store between time steps to be able to provide responses for requests outside of a restricted domain (like searching for a restaurant in this case).&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-06T06:58:40+00:00</dc:date>
	<dc:creator>ananthgo</dc:creator>
</item>
<item rdf:about="https://medium.com/p/96fb908765f5">
	<title>Halden Lin &lt;br/&gt; Team undef.: NLP Capstone | 02: Getting Started</title>
	<link>https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5?source=rss-2759d54493c0------2</link>
	<content:encoded>&lt;p&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;previous post&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Alright, it’s been only 2 days since my last entry, so this will be a relatively short post. The direction I proposed in &lt;strong&gt;Option 1 &lt;/strong&gt;of that post was towards a more robust, interpretable, and informative visualization of attention, particularly in the context of text summarization. A quick recap:&lt;/p&gt;&lt;blockquote&gt;Perhaps interaction can be used to create a more insightful and interpretable visualization framework for understanding attention. For example, text heat-maps are already used widely to visualize sentiment analysis.&lt;/blockquote&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*lsgeuBXGGBog4YkuQNgJVw.png&quot; /&gt;Lin et al. (2017) [6]. Visualization of sentiment analysis on a token-by-token basis.&lt;/figure&gt;&lt;blockquote&gt;In a static context, using this method for attention would require repeat of the same input sequence for each word in the output sequence. Using interaction, however, a model creator could brush over single or sequences of words in the output sequence to view corresponding soft-alignment in the input sequence. Aggregate visualizations could be shown to supplement this view (either aggregates over a particular input / output sequence, or aggregates over all input / output sequences).&lt;/blockquote&gt;&lt;p&gt;I’m currently working on laying out the groundwork for such a project. Task 1: implement a model. Without one, there’s no data to visualize!&lt;/p&gt;&lt;p&gt;With that in mind, here’s what I’ve been up to:&lt;/p&gt;&lt;h4&gt;Finding a Text Summarization Dataset&lt;/h4&gt;&lt;p&gt;A quick survey of recent research papers [1–5] on text summarization points, as well as online forums, points to three commonly used datasets.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://cs.nyu.edu/~kcho/DMQA/&quot;&gt;CNN/Daily Mail Corpus&lt;/a&gt;. A collection of articles and their bullet point summaries, with each bullet split for Q/A purposes. &lt;a href=&quot;https://github.com/abisee/cnn-dailymail&quot;&gt;A script&lt;/a&gt; [1] can be ran over the original dataset to restore the original bullet point summaries, to be used as a summarization corpus.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www-nlpir.nist.gov/projects/duc/data.html&quot;&gt;DUC Corpus&lt;/a&gt;. In particular, DUC 2003 and DUC 2004. These contain a collection of documents, each accompanied by a short (~10 word) summary. There is also a longer summary for each cluster of documents.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://catalog.ldc.upenn.edu/ldc2003t05&quot;&gt;Gigaword Corpus&lt;/a&gt;. An annotated collection of millions of documents. The summarization task here would be to predict the headline of each [5]&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The accessibility of the &lt;strong&gt;CNN/Daily Mail Corpus&lt;/strong&gt; (a process is required for the other two), in addition to the prevalence of projects that used it as a primary dataset [1, 2, 4], made it the most attractive option. The relatively longer summaries (~4 bullet points as opposed a short blurb in the other two datasets) also lends itself conveniently to the case of an interactive visualization with multi-token selection (e.g. select a whole bullet point and see where it attended). For a baseline, this will be my dataset!&lt;/p&gt;&lt;h4&gt;Identifying a Baseline Model&lt;/h4&gt;&lt;p&gt;See et al. (2017) [1] lay out a seq2seq attentional model as their baseline (a bidirectional LSTM). I’ll be using this as a baseline model with which to obtain data.&lt;/p&gt;&lt;h4&gt;Getting Some Code Up&lt;/h4&gt;&lt;p&gt;I’ll be using &lt;a href=&quot;http://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; and the &lt;a href=&quot;http://allennlp.org/&quot;&gt;AllenNLP&lt;/a&gt; toolkit [7] to implement my NN models. These are both ready to go on both my machine and Azure. I’m currently in the process of writing a DatasetReader for the dataset described above.&lt;/p&gt;&lt;h3&gt;Next Steps&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finish writing the DatasetReader for the CNN/Daily Mail Corpus.&lt;/li&gt;&lt;li&gt;Begin work on a baseline seq2seq attentional model, as described in &lt;strong&gt;Identifying a Baseline Model&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Works Cited&lt;/h4&gt;&lt;p&gt;[1] &lt;a href=&quot;https://arxiv.org/pdf/1704.04368.pdf&quot;&gt;See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” &lt;em&gt;ACL&lt;/em&gt; (2017).&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[2] &lt;a href=&quot;https://arxiv.org/pdf/1712.06100.pdf&quot;&gt;Hasselqvist, Johan et al. “Query-Based Abstractive Summarization Using Neural Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1712.06100 (2017): n. pag.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[3] &lt;a href=&quot;https://arxiv.org/pdf/1602.06023.pdf&quot;&gt;Nallapati, Ramesh et al. “Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.” &lt;em&gt;CoNLL&lt;/em&gt; (2016).&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[4] &lt;a href=&quot;https://arxiv.org/pdf/1705.04304.pdf&quot;&gt;Paulus, Romain et al. “A Deep Reinforced Model for Abstractive Summarization.” &lt;em&gt;CoRR&lt;/em&gt; abs/1705.04304 (2017): n. pag.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[5] &lt;a href=&quot;https://arxiv.org/pdf/1509.00685.pdf&quot;&gt;Rush, Alexander M. et al. “A Neural Attention Model for Abstractive Sentence Summarization.” &lt;em&gt;EMNLP&lt;/em&gt; (2015).&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[6] &lt;a href=&quot;https://arxiv.org/pdf/1703.03130.pdf&quot;&gt;Lin, Zhouhan, &lt;em&gt;et al.&lt;/em&gt;, “A structured self-attentive sentence embedding.”&lt;em&gt;arXiv preprint arXiv:1703.03130&lt;/em&gt; (2017).&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[7] &lt;a href=&quot;https://pdfs.semanticscholar.org/a550/2187140cdd98d76ae711973dbcdaf1fef46d.pdf?_ga=2.150901366.1370831839.1522970228-1363309632.1522194596&quot;&gt;Gardner, Matt et al. “AllenNLP: A Deep Semantic Natural Language Processing Platform.” (2017).&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=96fb908765f5&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-06T06:53:32+00:00</dc:date>
	<dc:creator>Halden Lin</dc:creator>
</item>
<item rdf:about="https://medium.com/p/278789e4d04a">
	<title>Aaron Johnston, Lynsey Liu &lt;br/&gt; Team Viterbi Or Not To Be: Warming Up</title>
	<link>https://medium.com/@viterbi.or.not/warming-up-278789e4d04a?source=rss-c522ef075bb3------2</link>
	<content:encoded>&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*qc9b3NkzkWe1kNnqmlHukA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;In order to begin implementing our baseline model, for which we intend to duplicate the results presented in another research paper covering discussion summarization, we began by identifying software that was referenced by other papers as being useful for their implementations.&lt;/p&gt;&lt;p&gt;Using these leads, we decided to “warm up” by installing the software and gaining some familiarity with it. The main libraries that we identified are listed here:&lt;/p&gt;&lt;h4&gt;&lt;a href=&quot;https://www.nltk.org/&quot;&gt;&lt;strong&gt;Natural Language Toolkit (nltk)&lt;/strong&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Perhaps it is no surprise that this resource ended up first on our list, but it was a clear choice to familiarize ourselves with because of the sheer variety of tools it provides. While performing an initial survey of conversation summarization papers, we discovered a reference to the TextTiling algorithm, described in &lt;a href=&quot;http://www.aclweb.org/anthology/J97-1003&quot;&gt;this paper&lt;/a&gt; and referenced as a technique used in &lt;a href=&quot;http://www.aclweb.org/anthology/D08-1081&quot;&gt;another paper&lt;/a&gt; about summarization. Broadly, the algorithm detects boundaries between topics in text, so it was used by this summarization paper as part of a pipeline before assigning scores to those topics representing their importance. For our baseline model, one possibility is to implement a similar pipeline, so having access to an implementation of the TextTiling algorithm would allow us to quickly implement that component and spend more time on other design decisions and implementation details. The nltk library provides a &lt;a href=&quot;https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.texttiling&quot;&gt;TextTiling module&lt;/a&gt; with this functionality.&lt;/p&gt;&lt;h4&gt;&lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;&lt;strong&gt;SciKit-Learn&lt;/strong&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P05-1037&quot;&gt;Another research paper&lt;/a&gt; we found concerning the topic of conversation summarization had several sections dedicated to the task of identifying portions of a chatlog with direct relevance to one another — for example, a question asked by one contributor and answered by another contributor several messages down would be considered a pair of directly relevant sections. As part of their technique for identifying these pairs, the researchers used &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html&quot;&gt;Support Vector Machines&lt;/a&gt; and &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html&quot;&gt;Maximum Entropy&lt;/a&gt; models in order to determine the sections that most likely directly respond to previous portions of the conversation. SciKit-Learn provides these functionalities, and in order to familiarize ourselves with additional existing tools that might be useful in building a baseline model we have installed this tool and begun experimenting with it.&lt;/p&gt;&lt;h4&gt;&lt;a href=&quot;https://www.cs.waikato.ac.nz/ml/weka/&quot;&gt;&lt;strong&gt;Weka Toolkit&lt;/strong&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Another less commonly seen method for chat summarization can be found in a more recent (in comparison to the others we have referenced) &lt;a href=&quot;http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf&quot;&gt;research paper&lt;/a&gt; which explores the usage of Multilayer Perceptrons (MLP) for the task, among several other approaches. The MLP approach in the paper is broadly composed of a feedforward neural network with more layers between the input and output layers using backpropagation to train the network and built with the Weka toolkit, a collection of machine learning algorithms that can applied to a dataset and which contains tools for developing a variety of schemes for processing data. Although the paper finds an approach using Naive Bayes to be the most effective on the GNUe archives, their MLP implementation comes in relatively close second and we think the idea is worth pursuing further. For a baseline model, we could start by working on a similar MLP system to the one in the paper using the same Weka toolkit implementation and strive to improve from there.&lt;/p&gt;&lt;h4&gt;&lt;a href=&quot;http://pytorch.org/&quot;&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Though not referenced by any of the papers we have encountered so far, PyTorch would be a really helpful tool for us to further the explore the usage of neural network models in chat summarization, which we can use for the aforementioned MLP approach as well as apply to our stretch goal of working on a less commonly used deep learning based model.&lt;/p&gt;&lt;p&gt;In addition to researching libraries and tools that we might use in our project, we have begun the process of finding and enumerating datasets that might be useful for our project ideas — more to come on the datasets and other resources in our next blog post!&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=278789e4d04a&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-06T06:41:19+00:00</dc:date>
	<dc:creator>Viterbi Or Not To Be</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-3753031463594823927.post-2253300890173394060">
	<title>Ron Fan, Aditya Saraf &lt;br/&gt; Team PrimeapeNLP: Blog Post #2</title>
	<link>https://cse481n.blogspot.com/2018/04/blog-post-2.html</link>
	<content:encoded>&lt;div&gt;We’ve mostly settled on working on a single document summarization task. We want to pick a type of document to work on summarizing, although we haven’t decided on one specific category yet. &lt;/div&gt;&lt;br /&gt; &lt;div&gt;While we narrow down the details of the project, we have been reading a number of papers and other resources to become more familiar with the subject. We have setup PyTorch on our machines, which we are both familiar with, as well as Tensorflow, which we are still playing around with. We’ve found some interesting repositories on GitHub related to SDS that we are trying out: &lt;/div&gt; &lt;br /&gt; &lt;div&gt;&lt;a href=&quot;https://cse481n.blogspot.com/feeds/posts/default?alt=rss&quot;&gt;https://github.com/tensorflow/models/tree/master/research/textsum&lt;/a&gt;&lt;br /&gt; &lt;a href=&quot;https://cse481n.blogspot.com/feeds/posts/default?alt=rss&quot;&gt;https://github.com/gregdurrett/berkeley-doc-summarizer&lt;/a&gt;&lt;br /&gt; &lt;a href=&quot;https://cse481n.blogspot.com/feeds/posts/default?alt=rss&quot;&gt;https://github.com/chakki-works/sumeval&lt;/a&gt;&lt;br /&gt; &lt;a href=&quot;https://cse481n.blogspot.com/feeds/posts/default?alt=rss&quot;&gt;https://github.com/ceteri/pytextrank&lt;/a&gt;&lt;br /&gt; &lt;a href=&quot;https://cse481n.blogspot.com/feeds/posts/default?alt=rss&quot;&gt;https://github.com/adamfabish/Reduction&lt;/a&gt;&lt;br /&gt;&lt;/div&gt; &lt;br /&gt;&lt;div&gt;Not all of these tools use machine learning - many seem to be heuristic-based sentence extractors. Nonetheless, it is interesting to consider their ideas in the context of neural network approaches. &lt;/div&gt;&lt;br /&gt; &lt;div&gt;One of the reasons we chose to attack this problem is that there is a rich literature to consult; this problem has been worked on in one form or another since 1958 [1]. As one would imagine, this means that there have been many different approaches to this problem, to varying degrees of success. But unlike other problems, where all current approaches are based on deep learning, there is active research into non-neural solutions to SDS. &lt;/div&gt;&lt;br /&gt; &lt;div&gt;Many researchers have tried to solve SDS with combinatorial optimization, reducing it to the Knapsack problem, the Maximum Coverage problem, or the Budgeted Median problem. For example, the Maximum Coverage problem is: given a number k and a collection S, of m sets, choose less than k sets in S that maximize the number of covered elements. To frame SDS as a Maximum Coverage problem, you break the document into “conceptual units”. Conceptual units are supposed to represent a single concept - for example, “the man bought a book” and the “the man read a book”. But it’s not clear at what granularity these conceptual units should be defined. One easy (but not especially effective) solution is to simply make each word a conceptual unit. Then, the document = S, and each sentence is a set of words inside S. The problem is now to pick k sentences from the document that maximize the word coverage in the document [2]. &lt;/div&gt;&lt;br /&gt; &lt;div&gt;One example of a recent non-neural approach is from a paper published 5 years ago [3]. The paper solves SDS by reducing it to the so-called Tree Knapsack Problem. We’ve haven’t fully wrapped our heads around the Tree Knapsack problem (it’s actually not that easy to quickly state), but the researchers’ basically involved representing a document as a Rhetorical Structure Theory-based discourse tree (RST-DT) by “select[ing] textual units according to a preference ranking”. The researchers’ first transform the RST-DT into a dependency-based discourse tree (DEP-DT) in order to get a tree that contains textual units on all nodes (RST-DT only have textual units as leaves), and then trim the DEP-DT using the Tree Knapsack problem.  &lt;/div&gt;&lt;br /&gt; &lt;div&gt;We aim to find a suitable corpus, and implement multiple models directly from these papers as our baseline models. Hopefully, that will give us insight that’ll help us formulate the problem differently. We also want to explore some neural architectures for single document summarization.  &lt;/div&gt;&lt;br /&gt; &lt;div&gt;We also have to consider whether we want to build an extractive or abstractive text summarization - the former collects a set of sentences or phrases that summarize the document while the latter tries to “learn the internal language representation to generate more human-like summaries, paraphrasing the intent of the original text” [4]. We’re leaning towards an extractive model, although we may try both. &lt;/div&gt;&lt;br /&gt; &lt;div&gt;[1] = &lt;a target=&quot;&quot;&gt;https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119004752.ch3&lt;/a&gt; &lt;br /&gt;[2] = &lt;a target=&quot;&quot;&gt;http://www.anthology.aclweb.org/E/E09/E09-1089.pdf&lt;/a&gt; &lt;br /&gt; [3] = &lt;a target=&quot;&quot;&gt;https://www.semanticscholar.org/paper/Single-Document-Summarization-as-a-Tree-Knapsack-Hirao-Yoshida/ed0c8a7ab911cdb30b7e95edada3a55c01eb22c5&lt;/a&gt;&lt;br /&gt; [4] = &lt;a target=&quot;&quot;&gt;https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/&lt;/a&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-04-06T06:31:00+00:00</dc:date>
	<dc:creator>Ron &amp; Aditya</dc:creator>
</item>
<item rdf:about="http://mathstoc.wordpress.com/?p=304">
	<title>Kuikui Liu, Nicholas Ruhland &lt;br/&gt; Team INLP: Milestone #2: Music as a Natural Language Task</title>
	<link>https://mathstoc.wordpress.com/2018/04/06/milestone-2-music-as-a-natural-language-task/</link>
	<content:encoded>&lt;h3&gt;Framing the problem&lt;/h3&gt;
&lt;p&gt;The focus of Natural Language Processing relies on patterns in the structure of language and models that find ways to encode the complexities of these structures. Many forms of music also have large amounts of structure which could potentially be discovered using similar models as a standard natural language.&lt;/p&gt;
&lt;p&gt;Music datasets for machine learning purposes have recently become available through projects like MusicNet in 2016 [1]. This music is primarily classical, and provided as both audio and MIDI.&lt;/p&gt;
&lt;h3&gt;Project ideas&lt;/h3&gt;
&lt;p&gt;For our project we are interested in music with lyrical content – both for the potential to create a creative demo and for the interest of making this a language task. The current direction we are most interested in is the generation of lyrics for a song, given its nonlyrical content. This will be broken up into subtasks depending on the feasible scale of the project. Not all of the following points will necessarily be parts of our project, but we will use them as as starting point as we see the success of our models.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Creating a machine learning model for MIDI music&lt;/li&gt;
&lt;li&gt;Translating MIDI into specific artists or styles&lt;/li&gt;
&lt;li&gt;Creating models for the lyrical content of specific artists or styles of music&lt;/li&gt;
&lt;li&gt;Generating lyrics given an artist or style&lt;/li&gt;
&lt;li&gt;Seq2seq conversion of MIDI into lyrical content&lt;/li&gt;
&lt;li&gt;GANs for either side of the conversion – MIDI encoding or lyrical generating&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Using MIDIs in RNNs&lt;/h3&gt;
&lt;p&gt;Work by Pakhomov [2] has already used RNNs to create models for lyrics. In his &lt;a href=&quot;http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/&quot;&gt;blog post&lt;/a&gt; he additionally discusses a method for forming any MIDI into piano roll format. This is essentially a matrix where each column represents a different time step, and each row represents a different note. Having a 1 corresponds to that note sounding at that time. The individual time vectors can be used as the inputs to an RNN at each time step to create a model representing the various songs.&lt;/p&gt;
&lt;p&gt;One possible data source for our project is karaoke data available from various sources online. If available in large enough quantities this could be extremely convenient because it already contains many pairings of MIDI music to their lyrics.&lt;/p&gt;
&lt;h3&gt;Azure&lt;/h3&gt;
&lt;p&gt;We intend to use PyTorch to train our models, and have begun setting up an instance on Microsoft Azure.&lt;/p&gt;
&lt;h3&gt;Relevant work&lt;/h3&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://homes.cs.washington.edu/~thickstn/musicnet.html&quot; rel=&quot;nofollow&quot;&gt;https://homes.cs.washington.edu/~thickstn/musicnet.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/&quot; rel=&quot;nofollow&quot;&gt;http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] Dong, Hao-Wen. 2017. MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment. &lt;a href=&quot;https://arxiv.org/pdf/1709.06298&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/pdf/1709.06298&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] Yu, Lantao. 2016. SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. &lt;a href=&quot;https://arxiv.org/abs/1609.05473&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/abs/1609.05473&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] Lee, Sang-gil. 2017. A SeqGAN for Polyphonic Music Generation. &lt;a href=&quot;https://arxiv.org/abs/1710.11418&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/abs/1710.11418&lt;/a&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-06T06:30:14+00:00</dc:date>
	<dc:creator>Nicholas Ruhland</dc:creator>
</item>
<item rdf:about="https://medium.com/p/3a2f40b355a5">
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: Getting Started for the Capstone: Software Installation &amp; Pipeline Brainstorming</title>
	<link>https://medium.com/nlp-capstone-blog/getting-started-for-the-capstone-software-installation-pipeline-brainstorming-3a2f40b355a5?source=rss----9ba3897b6688---4</link>
	<content:encoded>&lt;p&gt;Currently, our top two choices for the capstone is&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Machine Dictionary: &lt;/strong&gt;learning definitions of technical terms whose semantics are averaged over all places it is mentioned in training (in this case, research publications in the given field of study)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Visual Reasoning: &lt;/strong&gt;Given three windows, each of which containing a random arrangement of colored, geometric shapes, and a statement about the image, predict whether the statement is true or false.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Despite being problems with very different needs and challenges, the bulk of the tools and frameworks we’ll be using overlap for both tasks. Here, we discuss those tools and frameworks, followed by things we need specific to &lt;strong&gt;Machine Dictionary &lt;/strong&gt;and &lt;strong&gt;Visual Reasoning &lt;/strong&gt;separately.&lt;/p&gt;&lt;h3&gt;Resources Used for Both Tasks&lt;/h3&gt;&lt;h4&gt;PyTorch&lt;/h4&gt;&lt;p&gt;Given the limited time that we have, a neural-based approach using an established framework is preferred over implementing all of the model architecture from scratch, and to help avoid complications that can accompany other methods such as deriving parameter updates for bayesian models. PyTorch is an excellent framework that abstracts away differentiation and tensor arithmetic while still allowing a healthy amount of flexibility with it’s ability to dynamically produce computation graphs.&lt;/p&gt;&lt;h4&gt;AllenNLP&lt;/h4&gt;&lt;p&gt;After the crash course on the framework provided by AI2 in class, along with our experience from using it in the undergraduate NLP class, we’re convinced that the integration of AllenNLP with PyTorch is the best way to be as productive as possible. We plan to use the libraries it provides to make training more streamline and organized.&lt;/p&gt;&lt;p&gt;Since we’ve taken the undergraduate NLP class, we’ve already installed PyTorch and AllenNLP. We installed PyTorch through conda and AllenNLP through pip. Deep Learning projects also tend to involve complicated models which might require more computing resources, so we will also utilize the Azure credits available through the capstone. This process involved installing PyTorch and AllenNLP on Ubuntu VMs on Azure configured into include NVIDIA GPUs so that we can take advantage of PyTorch’s .&lt;/p&gt;&lt;h3&gt;Resources Specific to Machine Dictionary&lt;/h3&gt;&lt;h4&gt;Semantic Scholar Open Research Corpus&lt;/h4&gt;&lt;p&gt;The best dataset we’ve seen so far for this task is the &lt;a href=&quot;http://labs.semanticscholar.org/corpus/&quot;&gt;Semantic Scholar Open Research Corpus&lt;/a&gt; provided by &lt;a href=&quot;http://allenai.org/&quot;&gt;AI2&lt;/a&gt;. The dataset specifically consists of JSON files with metadata for each publication. The most relevant files will be &lt;em&gt;title, pdfUrls, &lt;/em&gt;and &lt;em&gt;year&lt;/em&gt;. Given all of the content besides the paper abstract (which is included as a field called &lt;em&gt;paperAbstract&lt;/em&gt;), we resort to using the &lt;em&gt;pdfUrls&lt;/em&gt; and extracting the text from each.&lt;/p&gt;&lt;h4&gt;Textract&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/deanmalmgren/textract&quot;&gt;Textract&lt;/a&gt; is a Python package that allows the extraction of text from PDFs. We plan to rely on this package given that it’s robust to both compiled and scanned PDFs.&lt;/p&gt;&lt;p&gt;We downloaded Textract to the Azure Linux VM using the Ubuntu installation directions:&lt;/p&gt;&lt;pre&gt;apt-get install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr \&lt;br /&gt;flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig&lt;br /&gt;pip install textract&lt;/pre&gt;&lt;p&gt;but ran into an issue that the developers haven’t dealt with yet. In the first line, they are missing a dependency to the libpulse-dev package, which causes the build to fail when downloading Textract. Doing a pip install libpulse-dev takes care of it.&lt;/p&gt;&lt;p&gt;Grabbing the text from a PDF file is then fairly convenient. We tested this on a &lt;a href=&quot;https://www.semanticscholar.org/paper/Effects-of-anthocyanins-on-the-prevention-and-of-Lin-Gong/1bcf9ae84d4ec5c0aba7918e6784dbfd0e8514b6&quot;&gt;cancer research paper&lt;/a&gt; taken from the Semantic Scholar dataset:&lt;/p&gt;&lt;pre&gt;&lt;strong&gt;import&lt;/strong&gt; textract&lt;br /&gt;text = textract.process(&quot;path-to-doc.pdf&quot;, encoding=&quot;ascii&quot;)&lt;/pre&gt;&lt;p&gt;which produced an excellent parse of the PDF.&lt;/p&gt;&lt;blockquote&gt;b’BJP\n\nBritish Journal of\nPharmacology\n\nBritish Journal of Pharmacology (2016) \n\n1\n\nREVIEW ARTICLE THEMED ISSUE\nEffects of anthocyanins on the prevention and\ntreatment of cancer\nCorrespondence Ying-Yu Cui, Department of Regenerative Medicine, Tongji University School of Medicine, Shanghai 200092,\nChina. E-mail: yycui@tongji.edu.cn\n\nReceived 13 June 2016; Revised 17 August 2016; Accepted 13 September 2016\n\nBo-Wen Lin1, Cheng-Chen Gong1, Hai-Fei Song1 and Ying-Yu Cui1,2,3\n1\n\nDepartment of Regenerative Medicine, Tongji University School of Medicine, Shanghai, China, 2Key Laboratory of Arrhythmias, Ministry of\n\nEducation (Tongji University), Shanghai, China, and 3Institute of Medical Genetics, Tongji University School of Medicine, Shanghai, China\n\nAnthocyanins are a class of water-soluble avonoids, which show a range of pharmacological effects, such as prevention of\ncardiovascular disease, obesity control and antitumour activity.&lt;/blockquote&gt;&lt;h3&gt;Resources Specific to Visual Reasoning&lt;/h3&gt;&lt;h4&gt;Cornell NLVR Dataset&lt;/h4&gt;&lt;p&gt;The dataset for the Visual Reasoning task is easily available through &lt;a href=&quot;https://github.com/clic-lab/nlvr&quot;&gt;github&lt;/a&gt;. This dataset was also very conveniently organized into three folders: testing data, development data, and training data. Each folder contains the actual images which are provided for us to train on.&lt;/p&gt;&lt;p&gt;In addition to the images, each folder also includes a JSON file which contains basic JSON representations of each image and the sentence that needs to be validated in each data point. This JSON representation is very useful because we do not need to parse the image to retrieve the raw elements (shape, color, location).&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=3a2f40b355a5&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/getting-started-for-the-capstone-software-installation-pipeline-brainstorming-3a2f40b355a5&quot;&gt;Getting Started for the Capstone: Software Installation &amp;amp; Pipeline Brainstorming&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-06T06:06:13+00:00</dc:date>
	<dc:creator>Tam Dang</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-9203775015655831448.post-34377626932024049">
	<title>Pinyi Wang, Dawei Shen, Xukai Liu &lt;br/&gt; Team Overfit: #2 Milestone: Warm up</title>
	<link>https://teamoverfit.blogspot.com/2018/04/2-milestone-warm-up.html</link>
	<content:encoded>&lt;h2 style=&quot;height: 0px;&quot;&gt;&lt;span&gt;Team Overfit&lt;/span&gt;&lt;/h2&gt;&lt;h3&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h3&gt;&lt;span&gt;Project repo: &lt;span style=&quot;font-size: 18.72px;&quot;&gt;&lt;a href=&quot;https://github.com/pinyiw/nlpcapstone-teamoverfit&quot;&gt;https://github.com/pinyiw/nlpcapstone-teamoverfit&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt;&lt;h4&gt;&lt;span&gt;Team members: Dawei Shen, Pinyi Wang, Xukai Liu&lt;/span&gt;&lt;/h4&gt;&lt;br /&gt;&lt;div&gt;&lt;/div&gt;&lt;br /&gt;&lt;div style=&quot;text-align: start; text-indent: 0px;&quot;&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;Blog Post: #2: 04/05/2018&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;div style=&quot;margin: 0px;&quot;&gt;&lt;/div&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;We first installed Pytorch 3.6 and we tried to run small programs on our local machines.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;We then explored the usage of the RNN and seq2seq APIs, which we are going to use for most of our projects ideas.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;We looked through the tutorial of RNNs/LSTMs/GRUs from the previous 447 class.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;span id=&quot;docs-internal-guid-97b5af9d-9943-133b-4f16-5d4414eefd5d&quot;&gt;&lt;span&gt;&lt;a href=&quot;https://colab.research.google.com/drive/11iLtGFDpnIuHj5B0rQDGG5lqq6BQ8FRh&quot;&gt;https://colab.research.google.com/drive/11iLtGFDpnIuHj5B0rQDGG5lqq6BQ8FRh&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;We tried to set up an Azure instance for GPU computation&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;We installed cuda support for the Pytorch package and it ran successfully with Tesla K80&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;&lt;span style=&quot;white-space: pre-wrap;&quot;&gt;We revisited the Recurrent Neural Networks, Attention and Reading Comprehension projects from the last quarter and experimented with other Pytorch features related to our project.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-04-06T04:48:00+00:00</dc:date>
	<dc:creator>Team Overfit</dc:creator>
</item>
<item rdf:about="https://medium.com/p/3d3651220219">
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Project Logistics and Package Exploration</title>
	<link>https://medium.com/@ryanp97/project-logistics-and-package-exploration-3d3651220219?source=rss-6378d85d3a9b------2</link>
	<content:encoded>&lt;p&gt;Jan Buys has agreed to advise me while I pursue Neural Machine Translation with Semantic Transfer, so this post will mainly focus on the packages and resources available for completing and exploring the minimal viable action plan as described &lt;a href=&quot;https://medium.com/@ryanp97/project-ideas-ab3d796c422e&quot;&gt;previously&lt;/a&gt;.&lt;/p&gt;&lt;h4&gt;Dataset&lt;/h4&gt;&lt;p&gt;I’m currently working on cleaning the &lt;a href=&quot;http://www.edrdg.org/wiki/index.php/Tanaka_Corpus&quot;&gt;Tanaka Corpus&lt;/a&gt; so that I can segment the sentences and then parse the graphs. This corpus is small and the sentences are short, so it seems like a good option for development. I’ve cleaned the corpus by removing any sentences that are not in English or Japanese as well as removing any sentences that do not belong to a pair of translations. Each sentence has at least one translation; note that some sentences have more than one translation.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/753/1*vajUkTbO551F3mjGQjyHQA.png&quot; /&gt;The Japanese sentence can be translated into the two English sentences. Note that the first translation is more direct, while the second translation seems to be drawing on the author’s bias or some other context.&lt;/figure&gt;&lt;p&gt;Also, in the Tanaka corpus, I noticed some things of interest. The first thing is that some hard to read Kanji are annotated with pronunciations in Katakana. The next were a couple typos in the translation (i.e. the proper noun “Tatoeba” was spelled “Tatoeb” in the translation). Also, on one line (the only one that I could find), the Japanese sentence had the romanization in parentheses following the translation. For the sake of consistency, I’ve removed this from the dataset for the sake of consistency. Aside from this single change, everything else was left untouched.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/838/1*xhYvt3b5aClaW0-pDdK8Cw.png&quot; /&gt;Example of a translation pair from the Tanaka corpus. Note the translated smiley face!&lt;/figure&gt;&lt;p&gt;There is also the &lt;a href=&quot;https://alaginrc.nict.go.jp/WikiCorpus/index_E.html&quot;&gt;Kyoto Corpus&lt;/a&gt; which is larger, more diverse, and has a more accurate representation of ‘real’ sentences. However, this dataset is formatted in a slightly more complex way, so I’m still figuring out how to tackle cleaning this corpus. This corpus also provides at least one translation.&lt;/p&gt;&lt;h4&gt;Word Segmentation&lt;/h4&gt;&lt;p&gt;There were 3 main programs that I considered and experimented with: &lt;a href=&quot;https://github.com/neubig/kytea&quot;&gt;KyTea&lt;/a&gt; (pronounced “cutie”), &lt;a href=&quot;http://www.atilika.org/&quot;&gt;Kuromoji&lt;/a&gt;, and &lt;a href=&quot;http://taku910.github.io/mecab/&quot;&gt;MeCab&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;After comparing the segmentation outputs on some small samples, it seemed that KyTea’s outputs often had significant differences between the outputs from Kuromoji and MeCab. After asking a lecturer from the UW Japanese department for their opinion, it seemed that the outputs from Kuromoji and MeCab were closer to the gold standard than the KyTea’s. As implied by the previous sentiment: the data that I will be working with is NOT gold standard, but it is the best available.&lt;/p&gt;&lt;p&gt;I decided on using MeCab as it had a very easy to use Python interface while Kuromoji did not have a easily accessible Python interface. Following &lt;a href=&quot;http://www.robfahey.co.uk/blog/japanese-text-analysis-in-python/&quot;&gt;this&lt;/a&gt; blog post, I’ve setup MeCab to be able to handle more robust input such as slang and neologisms, though I’m unsure if this will work well with the Jacy grammar mentioned in the Graph Parsing section.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1410.0291.pdf&quot;&gt;Here&lt;/a&gt; is a paper on the accuracy of MeCab when performing segmentation. The paper also gives a nice, brief overview on the differences between English and Japanese. Note the size of the Tanaka corpus is smaller than the current version as translations are added semi-regularly.&lt;/p&gt;&lt;h4&gt;Graph Parsing&lt;/h4&gt;&lt;p&gt;For parsing the MRS graphs and converting MRS to DMRS, I used the &lt;a href=&quot;https://github.com/goodmami/mrs-to-penman&quot;&gt;mrs-to-penmen&lt;/a&gt;. It uses the &lt;a href=&quot;https://github.com/delph-in/pydelphin&quot;&gt;PyDelphin&lt;/a&gt; interface which provides a wrapper for the &lt;a href=&quot;http://sweaglesw.org/linguistics/ace/&quot;&gt;ACE&lt;/a&gt; parser. The ACE parser parses the MRS graph, and then mrs-to-penmen should convert the parsed MRS graph to penmen format.&lt;/p&gt;&lt;p&gt;As for the grammar that ACE uses to parse, I’m using the &lt;a href=&quot;http://www.delph-in.net/erg/&quot;&gt;English Resource Grammar&lt;/a&gt; and &lt;a href=&quot;http://moin.delph-in.net/JacyTop&quot;&gt;Jacy&lt;/a&gt; for English and Japanese respectively.&lt;/p&gt;&lt;p&gt;After getting the parsed penman format, there is still some cleaning that I have to do and some simplification of the penman format to simplify what the seq2seq model is expected to output.&lt;/p&gt;&lt;h4&gt;Deep Learning Packages&lt;/h4&gt;&lt;p&gt;Additionally, since this project will rely on Deep Learning, I’ve installed AllenNLP and PyTorch in anticipation of the seq2seq model that will be trained after linearizing the DMRS graph as well as the TreeLSTM model (stretch goal).&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=3d3651220219&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-06T01:20:18+00:00</dc:date>
	<dc:creator>Ryan Pham</dc:creator>
</item>
<item rdf:about="http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2">
	<title>Sarah Yu &lt;br/&gt; Team Jekyll-Hyde: Technical Details (Blog Post #2)</title>
	<link>http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2</link>
	<content:encoded>&lt;div class=&quot;paragraph&quot;&gt;&lt;span style=&quot;color: rgb(0, 0, 0);&quot;&gt;For my project I am planning to do some deep learning at the end if I have time and if the results up to that point lead to that track. (I have pytorch installed from NLP so that's nice to have). &lt;br /&gt;&lt;br /&gt;With that said, I've been working with the Reddit API's and Reddit datadumps to get started on gathering the necessary data for pursuing the Language Accommodation project. I've been trying to figure out if the best approach is to work with the limited requests, the direct json files, or if some of the data dumps will suffice. I hope to have most of that and some basic data visualizations ready in the next couple of days to inform some of the choices I should make regarding the data (i.e. what time period to gather data from, what subreddits to pull from, etc.)&lt;/span&gt;&lt;br /&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-04-06T00:10:50+00:00</dc:date>
</item>

</rdf:RDF>
