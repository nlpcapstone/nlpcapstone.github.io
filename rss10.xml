<?xml version="1.0"?>
<rdf:RDF
	xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:foaf="http://xmlns.com/foaf/0.1/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns="http://purl.org/rss/1.0/"
>
<channel rdf:about="https://nlpcapstone.github.io/">
	<title>NLP Capstone Spring 2018</title>
	<link>https://nlpcapstone.github.io/</link>
	<description>NLP Capstone Spring 2018 - https://nlpcapstone.github.io/</description>
	<atom:link rel="self" href="https://nlpcapstone.github.io/rss10.xml" type="application/rss+xml"/>

	<items>
		<rdf:Seq>
			<rdf:li rdf:resource="https://medium.com/p/5258ddd9eedd" />
			<rdf:li rdf:resource="https://medium.com/p/ab3d796c422e" />
			<rdf:li rdf:resource="http://cse481n-capstone.azurewebsites.net/?p=15" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-5600014144802012716.post-6768023392170538237" />
		</rdf:Seq>
	</items>
</channel>

<item rdf:about="https://medium.com/p/5258ddd9eedd">
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: NLP</title>
	<link>https://medium.com/@hongnin1/nlp-5258ddd9eedd?source=rss-c450eb982161------2</link>
	<content:encoded>&lt;p&gt;This is a blog about Natural Language Processing.&lt;/p&gt;&lt;p&gt;Group member: Ning Hong, Zichun Liu, Zhou Sujie&lt;/p&gt;&lt;p&gt;Team name: The Bugless&lt;/p&gt;&lt;p&gt;Three topics our team is considering doing (All start-up mode):&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Movie summarization (imdb)/Food review summarization (yelp)&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;minimal viable action plan: scrap data from twitter/imdb/rotten tomato/yelp and train our model to be able to summarize the reviews for movies (or food if we are scrapping data from yelp). The summarization of the movie is how the audience feel about the movie in general, for example, given a movie title, our model should be able to produce something like this: It is violent but good.&lt;/p&gt;&lt;p&gt;stretch goals: Output a overall review for the movie instead of simple sentences, for example, given a movie title as input, our model should output: “The Terror got my full attention from beginning to end. I couldn’t turn away. I didn’t want to turn away. For me, that’s extremely rare.”&lt;/p&gt;&lt;p&gt;Another stretch goal is to be able to detect sentiment not only in the US market, but also in China market by using data from DouBan (one of the largest movie review site for China), and compare the sentiment between US and China for a certain movie.&lt;/p&gt;&lt;p&gt;2.Chinese Phoneticization mapping:&lt;/p&gt;&lt;p&gt;The way input Chinese to machine is by typing Pinyin, a kind of phoneticization for Chinese sentence and words, which almost every boy in China know about. However, one Chinese character may have many phoneticization can one Chinese phoneticization sequence may map to many Chinese sentences. When typing Chinese to a machine by Pinyin, the machine will rank the potential Chinese sentences by preferences. However, the ranking may not be so consistent to the context. Therefore, we want to generate a language model that map from Pinyin (English character) to Chinese words and sentences depend on context and speaking habit of this person. In addition, this can be potentially turned into an online learn algorithm.&lt;/p&gt;&lt;p&gt;First step: finding data, where input is Chinese phoneticization (pinyin) and output is Chinese sentences and words. Also, we need to find a good algorithm to do that, and onlint learning algorithm will be better.&lt;/p&gt;&lt;p&gt;Ideas come from Neural Input Method Engine of last quarter: &lt;a href=&quot;https://www.dropbox.com/sh/z3idncggfpwm8rs/AAAnHVvHIPvt_CxTXsaDVqvda?dl=0&amp;amp;preview=teamverynatural_3417269_43042970_Very+Natural+Final+Report.pdf&quot;&gt;https://www.dropbox.com/sh/z3idncggfpwm8rs/AAAnHVvHIPvt_CxTXsaDVqvda?dl=0&amp;amp;preview=teamverynatural_3417269_43042970_Very+Natural+Final+Report.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;3. Flirt tutor:&lt;/p&gt;&lt;p&gt;When you chat with your beloved boy/girl, you must have some hard time picking interesting and flirting word to response. Then let the machine teach you! Specifically, we want to build a model that can generate cute response giving the context of chatting. This model is neural based.&lt;/p&gt;&lt;p&gt;First step: figuring out what is the right dataset and find such. Meanwhile, consider the right model to use and find paper/resources about is.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=5258ddd9eedd&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-03T02:48:17+00:00</dc:date>
	<dc:creator>Ning Hong</dc:creator>
</item>
<item rdf:about="https://medium.com/p/ab3d796c422e">
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Project Ideas</title>
	<link>https://medium.com/@ryanp97/project-ideas-ab3d796c422e?source=rss-6378d85d3a9b------2</link>
	<content:encoded>&lt;p&gt;I plan on following a research track and hope to pursue one of the following ideas:&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Neural Machine Translation with Semantic Transfer&lt;/strong&gt; (as outlined by Jan Buys)&lt;/h4&gt;&lt;p&gt;&lt;em&gt;Minimal Viable Action Plan:&lt;/em&gt; &lt;br /&gt;1) Use statistical parser (&lt;a href=&quot;http://sweaglesw.org/linguistics/ace/&quot;&gt;ACE&lt;/a&gt;) to get MRS graphs of English/Japanese sentences and convert to DMRS graph (using &lt;a href=&quot;https://github.com/delph-in/pydelphin&quot;&gt;PyDelphin&lt;/a&gt; interface to do parsing and conversion)&lt;br /&gt;2) Linearize DMRS graph&lt;br /&gt;3) Train a seq2seq that takes linearized English DMRS graph and outputs linearized Japanese DMRS graph&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*J3l_TjWr3A-jOAcrCznqvA.png&quot; /&gt;Example of penmen format that DMRS can be represented with (non-linearized). Note that the representation shown is an AMR graph, not a DMRS graph. Figure taken from this &lt;a href=&quot;https://arxiv.org/pdf/1704.08381.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Stretch Goals:&lt;br /&gt;&lt;/em&gt;1) Explore different architectures for semantic transfer (e.g. TreeLSTM as opposed to seq2seq)&lt;br /&gt;2) Explore ways to learn correspondences between semantic concepts in the two languages&lt;/p&gt;&lt;h4&gt;DMRS to Text Generation (as outlined by Jan Buys)&lt;/h4&gt;&lt;p&gt;&lt;em&gt;Minimal Viable Action Plan:&lt;br /&gt;&lt;/em&gt;1) Generate DMRS graph serialization similar to what was outlined above&lt;br /&gt;2) Train seq2seq model to generate text directly from graph serialization&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Bndfdhs3ixwG6-JUQPC72A.png&quot; /&gt;Example of different graph representations. Figure taken from this &lt;a href=&quot;http://www.lrec-conf.org/proceedings/lrec2016/pdf/634_Paper.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Stretch Goals:&lt;/em&gt;&lt;br /&gt;1) Experiment with different seq2seq architectures&lt;br /&gt;2) Attempt semi-supervised training using a high-precision grammar-based parser&lt;/p&gt;&lt;h4&gt;Reproduce results / expand on a Paper&lt;/h4&gt;&lt;p&gt;In this option, I would be attempting to expand on this &lt;a href=&quot;https://arxiv.org/pdf/1704.04859.pdf&quot;&gt;paper&lt;/a&gt; regarding hybrid models. The paper attempts to alleviate and/or solve the issue of out-of-vocabulary words and characters, specifically in Chinese and Japanese. It describes using a CNN in conjunction with an RNN to do so; the CNN learns the radicals of characters and attempts to choose characters with similar radicals since the meaning of radicals remains constant between characters.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/822/1*5GyInYVxc8ifP_YRCHiHHA.png&quot; /&gt;Image taken from the linked paper.&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Minimal Viable Action Plan:&lt;br /&gt;&lt;/em&gt;1) Obtain/create a visual dataset of the characters in order to train the CNN&lt;br /&gt;2) Design and explore different methods for joining the two models such as described in the paper&lt;br /&gt;3) Train the model end-to-end&lt;/p&gt;&lt;p&gt;&lt;em&gt;Stretch Goals:&lt;br /&gt;&lt;/em&gt;1) Error analysis on different methods for joining and potential points of error&lt;br /&gt;2) Exploration of model variations and architecture (incremental changes similar to this &lt;a href=&quot;https://arxiv.org/pdf/1708.04755.pdf&quot;&gt;paper&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;For current progress, visit the &lt;a href=&quot;https://github.com/ryanp97/NeuralEmpty&quot;&gt;repo&lt;/a&gt;.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=ab3d796c422e&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-03T00:55:49+00:00</dc:date>
	<dc:creator>Ryan Pham</dc:creator>
</item>
<item rdf:about="http://cse481n-capstone.azurewebsites.net/?p=15">
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: First Blog Post!</title>
	<link>http://cse481n-capstone.azurewebsites.net/2018/04/02/first-blog-post/</link>
	<content:encoded>&lt;h3&gt;&lt;strong&gt;Team Name: &lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Team Watch Your Language!&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Three Project Ideas:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Offensive Text Recognition&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our minimal viable plan is to create two models, one which determines if text is offensive or not, and another which determines if any particular group is targeted by the text, such as a racial, religious, or political grouping. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our stretch goal is to use these models to make a third model which can use the first two outputs as assumptions to then provide human-readable explanations as to why that particular text was labeled the way it was. This way we can determine if text is offensive or not and provide reasons for that labeling. This can be used to assist in teaching conversational agents common sense about what to say.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Domain-Specific Conversational Agent&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our minimal viable plan is to create a conversational agent which can provide information and hold a conversation with a well-intentioned user about a particular domain. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The stretch goal here would be to just continually make it better at conversing, at least in the particular domain it is trained to be good at talking about.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Image Description&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our minimal viable plan is to create a model that can describe a simple image correctly, like generating a sentence describing the spatial relation between a box and a cylinder.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For the stretch goal, we would like to improve our model that can describe a unique pattern of an image among 3 (or multiple) other images. For example, if we have 3 images A, B, and C, we would like to come up with a model to generate a sentence that describes image A but not image B or C.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;GitLab Repo:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; &lt;/span&gt;&lt;a href=&quot;https://gitlab.cs.washington.edu/danielby/nlp-capstone&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;https://gitlab.cs.washington.edu/danielby/nlp-capstone&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Mode:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We will be tackling the Offensive Text Recognition task in&lt;/span&gt; &lt;em&gt;research mode&lt;/em&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;!&lt;/span&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-02T06:58:42+00:00</dc:date>
	<dc:creator>Team Watch Your Language!</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-5600014144802012716.post-6768023392170538237">
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog Post 1</title>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/03/blog-post-1.html</link>
	<content:encoded>&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;font-family: arial;&quot;&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt;I am interested in working in the code generation/semantic parsing space on the research track. My code &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: arial;&quot;&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt;will be in various branches of my fork of allennlp (https://github.com/rajasagashe/allennlp). I will keep you&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: arial;&quot;&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt; updated on which branch/commits I worked on during each blog post. Also note that project idea 1 has &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: arial;&quot;&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt;the most detail since I have picked it as my project!&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;h2 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;&quot;&gt;&lt;span&gt;Project Idea 1&lt;/span&gt;&lt;/h2&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Minimum Viable Plan:&lt;/span&gt;&lt;span&gt; Implement the model in the recent UW paper which introduces the task of &lt;/span&gt;&lt;br /&gt;&lt;span&gt;generating the code for a java function from a natural language description. To further aid code &lt;/span&gt;&lt;br /&gt;&lt;span&gt;generation, the class in which the generated function is to reside is provided, i.e. the class variables &lt;/span&gt;&lt;br /&gt;&lt;span&gt;and methods. Thus the encoder encodes the class as well as the utterance and the decoder uses a &lt;/span&gt;&lt;br /&gt;&lt;span&gt;two step attention mechanism and decodes through the java grammar production rules. &lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Stretch Goals:&lt;/span&gt;&lt;span&gt; Reproduce the state of the art results in the paper. I’m putting this in the stretch goals &lt;/span&gt;&lt;br /&gt;&lt;span&gt;since successfully implementing a neural semantic parser with type constraints is pretty challenging. &lt;/span&gt;&lt;br /&gt;&lt;span&gt;In addition, I hope to experiment with other improvements like encoding the entire class method body&lt;/span&gt;&lt;br /&gt;&lt;span&gt; which wasn’t done.&lt;/span&gt;&lt;/div&gt;&lt;h2 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;&quot;&gt;&lt;span&gt;Project Idea 2&lt;/span&gt;&lt;/h2&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Minimum Viable Plan: &lt;/span&gt;&lt;span&gt;Implement this paper: &lt;/span&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.01696.pdf&quot; style=&quot;text-decoration: none;&quot;&gt;&lt;span&gt;https://arxiv.org/pdf/1704.01696.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;. The model is &lt;/span&gt;&lt;br /&gt;&lt;span&gt;similar to that of the previous idea, but the datasets are for python and ifttt instead.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Stretch Goals: &lt;/span&gt;&lt;span&gt;Improve the paper’s result.&lt;/span&gt;&lt;/div&gt;&lt;h2 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;&quot;&gt;&lt;span&gt;Project Idea 3&lt;/span&gt;&lt;/h2&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Minimum Viable Plan: &lt;/span&gt;&lt;span&gt;Perform transfer learning across several code generation tasks by using the &lt;/span&gt;&lt;br /&gt;&lt;span&gt;same encoder for them all. This technique would be similar to what was used in the Cove paper &lt;/span&gt;&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.00107.pdf&quot; style=&quot;text-decoration: none;&quot;&gt;&lt;span&gt;https://arxiv.org/pdf/1708.00107.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;. &lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Stretch Goals: &lt;/span&gt;&lt;span&gt;Improve the individual paper results with this technique.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;</content:encoded>
	<dc:date>2018-03-30T22:41:00+00:00</dc:date>
	<dc:creator>nlpcapstone</dc:creator>
</item>

</rdf:RDF>
