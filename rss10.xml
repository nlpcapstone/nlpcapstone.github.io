<?xml version="1.0"?>
<rdf:RDF
	xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:foaf="http://xmlns.com/foaf/0.1/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns="http://purl.org/rss/1.0/"
>
<channel rdf:about="https://nlpcapstone.github.io/">
	<title>NLP Capstone Spring 2018</title>
	<link>https://nlpcapstone.github.io/</link>
	<description>NLP Capstone Spring 2018 - https://nlpcapstone.github.io/</description>
	<atom:link rel="self" href="https://nlpcapstone.github.io/rss10.xml" type="application/rss+xml"/>

	<items>
		<rdf:Seq>
			<rdf:li rdf:resource="https://medium.com/p/b7c31ac45ecc" />
			<rdf:li rdf:resource="http://mathstoc.wordpress.com/?p=339" />
			<rdf:li rdf:resource="https://medium.com/p/33072535817f" />
			<rdf:li rdf:resource="http://mathstoc.wordpress.com/?p=335" />
			<rdf:li rdf:resource="https://medium.com/p/a2d837ecf66b" />
			<rdf:li rdf:resource="https://medium.com/p/6f773ae418d0" />
			<rdf:li rdf:resource="https://medium.com/p/306dca636d3a" />
			<rdf:li rdf:resource="http://mathstoc.wordpress.com/?p=323" />
			<rdf:li rdf:resource="http://mathstoc.wordpress.com/?p=314" />
			<rdf:li rdf:resource="https://medium.com/p/96fb908765f5" />
			<rdf:li rdf:resource="http://mathstoc.wordpress.com/?p=304" />
			<rdf:li rdf:resource="http://mathstoc.wordpress.com/?p=277" />
		</rdf:Seq>
	</items>
</channel>

<item rdf:about="https://medium.com/p/b7c31ac45ecc">
	<title>Halden Lin &lt;br/&gt; Team undef.: NLP Capstone | 09: Any Summary</title>
	<link>https://medium.com/@halden.lin/nlp-capstone-09-any-summary-b7c31ac45ecc?source=rss-2759d54493c0------2</link>
	<content:encoded>&lt;p&gt;&lt;em&gt;previous posts: &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;&lt;em&gt;01&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5&quot;&gt;&lt;em&gt;02&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3&quot;&gt;&lt;em&gt;03&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7&quot;&gt;&lt;em&gt;04&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a&quot;&gt;&lt;em&gt;05&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0&quot;&gt;&lt;em&gt;06&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-07-formalizing-a2d837ecf66b&quot;&gt;&lt;em&gt;07&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-08-human-summaries-33072535817f&quot;&gt;&lt;em&gt;08&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In my last post, I stated a main goal of mine was to visualization &lt;strong&gt;human summaries&lt;/strong&gt;. After talking with Prof. Jeff Heer this past week, I’ve developed a more concrete goal for this segment of my project.&lt;/p&gt;&lt;p&gt;If we are able to develop a method for approximating human ‘attention’ between source and summary, we can use it in the following ways.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Evaluation tool.&lt;/strong&gt; Current evaluation requires reading article, summary, and thinking critically to map between the two in order to determine whether or not the summary is ‘good’.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Enable cross-model comparison and analysis.&lt;/strong&gt; How do different models produce summaries for the same article? Automatic measures, such as Rouge and Meteor, are generally poor indicators of proper quality. Currently, one may read summaries and source text and attempt to qualify proper coverage of key ideas. By introducing a visualization that can be generated from &lt;strong&gt;any&lt;/strong&gt; source-summary pair, we can enable more principled analysis.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Enable model to human comparison and analysis.&lt;/strong&gt; This I discussed in the previous post. What do human summaries have that our models are missing? Missing coverage? Missing entities? This visualization tool could answer these questions.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;In general, this tool would allow researchers to gain insights about both human and machine summaries.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;With this in mind, I’ll go into the approaches I’ve been experimenting with in the past week.&lt;/p&gt;&lt;h4&gt;Hierarchical Similarity&lt;/h4&gt;&lt;p&gt;Last week, I attempted token-on-token similarity. The results can be seen the gif below. The weight between input and output token &lt;em&gt;x &lt;/em&gt;and &lt;em&gt;y&lt;/em&gt;, respectively, can be described as so:&lt;/p&gt;&lt;p&gt;&lt;em&gt;a(x, y) = similarity(x, y)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Where similarity is calculated using a standard word embedding API (in this case, &lt;a href=&quot;https://spacy.io/&quot;&gt;spaCy&lt;/a&gt;). The issue with this approach was that context is lost, and so a word will often attend to nearly the entire document with no regard to the ideas coming out of each portion (in summaries, we expect a sentence or phrase to summarize a specific part or few parts of the original document).&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*-ganHe0RsisBYzaPKOuHhA.gif&quot; /&gt;Token-on-token similarity pays no heed to context — problematic.&lt;/figure&gt;&lt;p&gt;In attempt to remedy this, I added a factor to each weight that represents the similarity of the tokens’ respective sentences. That is, the weight of a given &lt;em&gt;x, y&lt;/em&gt; pair is determined by the similarity of the sentence of &lt;em&gt;x &lt;/em&gt;and the sentence of &lt;em&gt;y&lt;/em&gt;, multiplied by the similarity of the tokens themselves. To both normalize weights (over output token) and exaggerate salient pairs, I also add a soft-max transformation for each similarity score. The equation below describes this formula.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/0*WE0z9rPYF_Nb_X4Y.&quot; /&gt;&lt;/figure&gt;&lt;p&gt;The &lt;em&gt;theta&lt;/em&gt; terms here are important in properly exaggerating salient pairs, and so require some tuning.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*QJFNJ0ty3NimB1nBAnTdXg.gif&quot; /&gt;Hierarchical Similarity shows some promise, but has a few issues.&lt;/figure&gt;&lt;p&gt;This approach shows some promise. Context is taken into account, at least at a sentence-by-sentence level. However, there are a few shortcomings that become apparent with more abstractive summaries. In particular:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Context is better but far from perfect.&lt;/strong&gt; Sometimes ideas span multiple sentences, difficult to model. Additionally, repeating words in a sentence get equal ‘attention’ even though one may make more sense from a token-by-token generation standpoint.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I’ll be exploring this approach further in the next week, but I have concerns about its ability to generalize well, per issues described above.&lt;/p&gt;&lt;h4&gt;Hidden Markov Model&lt;/h4&gt;&lt;p&gt;At a high level, we can imagine ‘attention’ as the words and phrases from the source text to that one would draw from to write a portion of a summary. This makes sense: we tend to focus on specific areas of a document at a time when writing summaries. Breaking this into token-by-token time-steps, summary token is &lt;strong&gt;conditioned&lt;/strong&gt; on the ‘attention’ vector for that time-step.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/151/1*6wNP-KSn13tGSSwDjmDXxw.png&quot; /&gt;Summary tokens are conditioned on attention vectors over the source text.&lt;/figure&gt;&lt;p&gt;Further, we can reason that attention vectors change from time-step to time-step, dependent on the previous attention vector.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/361/1*krjZPlauErnawaKymE5LAA.png&quot; /&gt;Attention vectors are conditioned on each other.&lt;/figure&gt;&lt;p&gt;This of course is an simplification — the way our minds work is likely far more complex — but it allows us to model the ‘attention’ between source and summary as a Hidden Markov Model (HMM).&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/611/1*3wfXRV8pMJZQ74Ux1OGjDA.png&quot; /&gt;Source to summary modeled as a Hidden Markov Model.&lt;/figure&gt;&lt;p&gt;We can then use this model to predict attention vectors at each time-step (e.g. Viterbi, Forward-Backward). This is similar to how HMMs are used to predict part-of-speech tags (where POS tags are conditioned on each other and tokens are conditioned on those tags). Emissions (the edge weight going from distribution to summary token) can be defined by token similarity, but there are still a challenges here.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;How to define transition probabilities?&lt;/li&gt;&lt;li&gt;Treat attention states as distributions or single tokens (e.g. argmax in vector)?&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;I’ll need to consider this approach further to see if I can work out these kinks.&lt;/p&gt;&lt;h4&gt;POS Tags&lt;/h4&gt;&lt;p&gt;I’ve also been slowly improving the visualization tool itself. I’ll briefly describe my progress on this front.&lt;/p&gt;&lt;p&gt;Using &lt;a href=&quot;https://www.nltk.org/&quot;&gt;NLTK&lt;/a&gt;, I was able to part-of-speech tag machine-generated summaries. At the top right of the visualization, users are presented a panel of the POS Tags used by the &lt;a href=&quot;https://catalog.ldc.upenn.edu/ldc99t42&quot;&gt;Penn Tree Bank&lt;/a&gt;, which NLTK sources from. Non-present tags are greyed-out.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1000/1*i87AaOJBtCzk4h_G5b5z9A.gif&quot; /&gt;Users can highlight tokens to view the corresponding tag, or mouse over tags to highlight all corresponding tokens.&lt;/figure&gt;&lt;p&gt;This should allow more in-depth analysis of the attention vectors produced by the machine. Eventually I’d like to work towards highlighting named entities in the source / summary to allow users to identify present / missing ideas centered on important entities.&lt;/p&gt;&lt;h4&gt;Upcoming Work&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;Continue working on visualizing source-summary alignment.&lt;/li&gt;&lt;li&gt;Continue improving visualization.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;I have lots, lots, lots to do. Until next time!&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=b7c31ac45ecc&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-05-23T06:43:01+00:00</dc:date>
	<dc:creator>Halden Lin</dc:creator>
</item>
<item rdf:about="http://mathstoc.wordpress.com/?p=339">
	<title>Kuikui Liu, Nicholas Ruhland &lt;br/&gt; Team INLP: NLP Capstone Post #8: Training challenges</title>
	<link>https://mathstoc.wordpress.com/2018/05/17/nlp-capstone-post-8-training-challenges/</link>
	<content:encoded>&lt;h1&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Challenges with training TSL model&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In our previous post we proposed a three-model system that would allow us to take advantage of a larger corpus of higher quality lyrics data for the production of lyrics. We also finally tackle the alignment task with a simple approach of determining whether a lyric token should be produced at each timestep. This seems sensible since we have begun dividing the MIDIs into pianorolls with a constant frequency.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Unfortunately, even after several bugs bashed, we’ve been still unable to produce even sensible timings. We find the RNN collapses to repeatedly generating 0 (for no lyric event), even though a randomly initialized RNN will repeatedly generate 1 (and perform better with respect to classification accuracy).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Future direction&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;If we are able to produce something reasonable from our existing architecture, we would like to move on to a second model that structures the problem as machine translation. We have decided to focus on the paper Attention is All You Need by Vaswani et al. [1] for our presentation in two weeks. The structure of our problem is straightforward to apply to translation as converting pianoroll format into english sentences. Incorporating attention has shown promising results in the literature, though that is no guarantee that our noisy dataset would be able to take advantage of this proposed architecture.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;References&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[1] &lt;/span&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;https://arxiv.org/abs/1706.03762&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2018-05-17T05:55:03+00:00</dc:date>
	<dc:creator>Nicholas Ruhland</dc:creator>
</item>
<item rdf:about="https://medium.com/p/33072535817f">
	<title>Halden Lin &lt;br/&gt; Team undef.: NLP Capstone | 08: Human Summaries</title>
	<link>https://medium.com/@halden.lin/nlp-capstone-08-human-summaries-33072535817f?source=rss-2759d54493c0------2</link>
	<content:encoded>&lt;p&gt;&lt;em&gt;previous posts: &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;&lt;em&gt;01&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5&quot;&gt;&lt;em&gt;02&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3&quot;&gt;&lt;em&gt;03&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7&quot;&gt;&lt;em&gt;04&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a&quot;&gt;&lt;em&gt;05&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0&quot;&gt;&lt;em&gt;06&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-07-formalizing-a2d837ecf66b&quot;&gt;&lt;em&gt;07&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;I’ll keep this blogpost short — my current undertakings are in-progress and it might be a week or more before they are realized. To preface:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Visualizing human summaries&lt;/li&gt;&lt;li&gt;Improving the visualization.&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Visualizing Human Summaries&lt;/h3&gt;&lt;p&gt;When I was presenting my project update last week, Prof. Choi asked a very interesting question. What if we could use this visualization tool to not only understand how the model is generating summaries, but also how human summaries are produced and how the two compare?&lt;/p&gt;&lt;p&gt;I’ll briefly explain the thought behind this.&lt;/p&gt;&lt;p&gt;Visualizations provide a mapping from raw data (in this case, attention weights and input / output tokens) to visual encodings. These visual encodings are valuable in that they allow us as humans to perceive the data in a meaningful way. For example, the attention visualizer I am working on allows us to identify overall patterns in the attention of the model.&lt;/p&gt;&lt;p&gt;How do we interpret human written summaries, i.e. their relation to the source text? Perhaps we scan the document and attempt to match paragraphs or sentences to sentences in the summary. This can be compared to the ‘attention’ our minds use to generate the summary. If we can visualize this mapping, perhaps in a more refined and detailed manner (i.e. token by token) then we should be able to compare the human summaries with the machine generated summaries, right? And then one may be able to identify what the human summaries have that the machine summaries do not, or visa versa. The hope is that if we can enable this kind of comparison, researchers may be better equipped to improve their models by using these insights.&lt;/p&gt;&lt;p&gt;So far, I’ve experimented with two methods for generating this ‘attention’ from human summary to source text.&lt;/p&gt;&lt;h4&gt;Word Similarity&lt;/h4&gt;&lt;p&gt;The first approach that sprung to mind was to use word similarity as a proxy for ‘attention’. To do this, I used &lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot;&gt;pre-trained GloVe embeddings&lt;/a&gt; and the &lt;a href=&quot;https://radimrehurek.com/gensim/&quot;&gt;Gensim API&lt;/a&gt; to calculate word similarities between each input token / output token pair. &lt;a href=&quot;https://spacy.io/&quot;&gt;SpaCy&lt;/a&gt; was used to tokenize the sequences. The result is a matrix of weights, similar to attention distribution, albeit not normalized per output token (with attention, for a given output token the aggregate over all input tokens is 1). As the example below shows, this method falls flat, as output tokens are matched to input tokens regardless of context. This means it makes little sense to compare these weights to attention.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*D6pMsnZjlNiz1XJyN5xcBQ.png&quot; /&gt;The word ‘the’ is matched with nearly every token in the input sequence, likely a result of its extreme commonness and proximity to most words in embedding space.&lt;/figure&gt;&lt;h4&gt;Summarization Model&lt;/h4&gt;&lt;p&gt;The second method I considered was one suggested by Ari. Here, we use the same model used to generate the machine summaries. The difference is that at each decoder time-step, instead of feeding in the previous &lt;strong&gt;predicted&lt;/strong&gt; token, we feed in the previous &lt;strong&gt;actual&lt;/strong&gt; token. This means that instead of having a decoding pipeline that looks like this:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*N-til0kZzQeAVI3zkoDKVg.jpeg&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Where y-hats represent predicted output tokens, we have one that looks like this:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*H7nMlelojnzNkfthS2ljAg.jpeg&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Where y (no hat) represents true output token (the token from the human summary). This is a process similar to the one taken during training of generative encoder-decoder models.&lt;/p&gt;&lt;p&gt;By grabbing attention distributions just as we had with the machine summaries, we hope to get an approximation of the attention distributions for the human summaries — we are essentially feeding the model the answer. However, there is a catch, and an important one. Because attention weights are used to create a context vector that is then fed into the &lt;strong&gt;next&lt;/strong&gt; decoder unit to predict the &lt;strong&gt;next &lt;/strong&gt;word, we run into an issue when the next word is predicted incorrectly. Turns out, this happens often under the model being used (from &lt;a href=&quot;https://github.com/abisee/pointer-generator&quot;&gt;See et al.&lt;/a&gt;). This actually makes sense, as the model has been shown to produce largely &lt;strong&gt;extractive &lt;/strong&gt;summaries, and so one would expect the model to, at each time-step, attempt to produce the next word in the source text that follows the word fed to it. With &lt;strong&gt;abstractive&lt;/strong&gt; summaries, this is not often the optimal choice. This results in attention weights that make little sense, as seen below.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*c5KuVp0qzjkcQxNAJSfYLw.png&quot; /&gt;According to this, ‘manager’ was the focal point of attention when producing the word ‘were’. It would seem that the model intended to predict ‘manager’, and so when we map this attention weight to the human token, we run into an issue.&lt;/figure&gt;&lt;p&gt;Here’s the problem. The model is trained to use attention weights to &lt;strong&gt;generate &lt;/strong&gt;an output token. What we want is the opposite. We want the attention &lt;strong&gt;given&lt;/strong&gt; an output token — use the output token to generate the attention weights. This poses a significant challenge.&lt;/p&gt;&lt;h4&gt;So How Else?&lt;/h4&gt;&lt;p&gt;So neither of these methods seem to produce anything meaningful. I’m not ready to give up though — this is an intriguing problem. In the next week I’ll be brainstorming other methods. One that might have some traction is to use a few heuristics to approximate ‘attention’ using word similarities in conjunction with context. For example, by imposing a penalty on the weight if the context of the output token is dissimilar to the word ‘attended’ to in the source text. Much more work to be done here.&lt;/p&gt;&lt;h3&gt;Improving the visualization&lt;/h3&gt;&lt;p&gt;This section will be short. There are a few problems I looked into in the past week.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Implement divided edge bundling, as produced by &lt;a href=&quot;http://vis.stanford.edu/files/2011-DividedEdgeBundling-InfoVis.pdf&quot;&gt;Selassie et al. (2011)&lt;/a&gt;. I described this briefly in my previous blog post. The obstacle here is that there is not available d3 implementation of the algorithm. In fact, the only implementation I could find available was &lt;a href=&quot;https://github.com/kakearney/divedgebundle-pkg&quot;&gt;one for Matlab&lt;/a&gt;, produced by &lt;a href=&quot;http://kellyakearney.net/&quot;&gt;Kelly Kearny (University of Washington)&lt;/a&gt;. This might prove more difficult than the remaining time in this quarter allows, but I’ve started the process anyways and will see where it takes me.&lt;/li&gt;&lt;li&gt;Highlighting extraction. That is, making it apparent in the visualization when the model is simply copying. I’ve been playing around with things such as color to encode this, but haven’t settled on anything I like.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;More work to come!&lt;/p&gt;&lt;h4&gt;References&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.04368.pdf&quot;&gt;See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” &lt;em&gt;ACL&lt;/em&gt; (2017).&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://vis.stanford.edu/files/2011-DividedEdgeBundling-InfoVis.pdf&quot;&gt;Selassie, David et al. “Divided Edge Bundling for Directional Network Data.” &lt;em&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/em&gt; 17 (2011): 2354–2363.&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=33072535817f&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-05-16T06:58:49+00:00</dc:date>
	<dc:creator>Halden Lin</dc:creator>
</item>
<item rdf:about="http://mathstoc.wordpress.com/?p=335">
	<title>Kuikui Liu, Nicholas Ruhland &lt;br/&gt; Team INLP: NLP Capstone Post #7: TSL Pipeline</title>
	<link>https://mathstoc.wordpress.com/2018/05/10/nlp-capstone-post-7-tsl-pipeline/</link>
	<content:encoded>&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Modeling issues&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;As seen in our results last week, the RNN architecture we have been training has not been able to produce any coherent series of tokens based on the music data provided in the clean Lakh dataset. To analyze the poor results of this model, we have considered various features of the quality of the data. To simplify the issue of timing the lyric tokens, this model attempts to predict a lyric token at every timestep. Between each token we have summed all the musical data, producing a piano roll that looks approximately like the following image.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;Screen Shot 2018-05-09 at 4.31.53 PM&quot; class=&quot;  wp-image-331 aligncenter&quot; height=&quot;303&quot; src=&quot;https://mathstoc.files.wordpress.com/2018/05/screen-shot-2018-05-09-at-4-31-53-pm.png?w=501&amp;amp;h=303&quot; width=&quot;501&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In the event that two lyrics occur at exactly the same time step, we end up with a gap in the notes, here highlighted in red.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;Screen Shot 2018-05-09 at 4.31.53 PM&quot; class=&quot;  wp-image-334 aligncenter&quot; height=&quot;302&quot; src=&quot;https://mathstoc.files.wordpress.com/2018/05/screen-shot-2018-05-09-at-4-31-53-pm1.png?w=500&amp;amp;h=302&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;At first we expected this problem to occur in only a small number of cases, but it is often the result of the newline character appearing in a message simultaneously with the first lyric of the next sentence. This processing poses several problems to the task of learning the lyrical content based on the structure of the music. First, the large number of musical gaps may be confounding the model due to the large variety in lyrics that will be seen at those time steps. Additionally, we lose all information about the song timing since all regions without lyrics are compressed into a single time step. In theory, gaps in lyrics could hint to the model that the next section should start a new verse or chorus.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The TSL Pipeline&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;As suggested in the previous blogpost, we would like to be able to augment the results of the musical model with a higher quality lyrical dataset. The Kaggle lyrics dataset has shown promising results in previous blogposts at the quality of the lyric sequences it has been able to produce.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The TSL Pipeline is a combination of three models: Timing, Seed, and Lyrics. The architecture may look something like the following diagram:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;Training&quot; class=&quot;  wp-image-333 aligncenter&quot; height=&quot;221&quot; src=&quot;https://mathstoc.files.wordpress.com/2018/05/training.png?w=531&amp;amp;h=221&quot; width=&quot;531&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;During training, each pianoroll will be separated into data representing the timing, notes and lyrics. These get passed into respective models to learn timing and “seed” information. Additional lyrics information from the Kaggle dataset is used to train a lyrical model.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;Evaluation&quot; class=&quot;  wp-image-332 aligncenter&quot; height=&quot;220&quot; src=&quot;https://mathstoc.files.wordpress.com/2018/05/evaluation.png?w=538&amp;amp;h=220&quot; width=&quot;538&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;At evaluation time, the lyrics from the original pianoroll are not passed into the Seed model. Instead, the Seed model attempts to predict some seed based on the musical content, and will pass its result into the lyrics model. The combination of these lyrics and timing information constitute the complete description of our karaoke output.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Timing Model&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In all previous posts we ignored the issue of lyrics timing in the interest of creating a reasonable lyrical model. Our current timing model is similar to our previous model attempt, but the data is generated differently. Instead of computing a pianoroll sample at each lyrical timestep, we us a constant sampling frequency of 10 timesteps per second. We then annotate each timestep with a 1 or 0 based on if a lyric was annotated at that step. The model will then attempt to predict for each step of a given pianoroll the probability there should be a lyric at that time.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Seed Model&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The seed model will be a simplified version of the poorly performing model from before. Instead of predicting all lyrics, it will attempt to predict a small subset of the initial lyrics. This would also allow us to create a dataset with more training examples by splitting each song into smaller samples.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Lyrics Model&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The lyrics model will be similar to the one described in the second blog post, which is a character level RNN for generating lyrics. This will take the first few words predicted by the seed model and generate the remainder of the lyrics. Since it’s trained on the large Kaggle dataset the quality seems to be much higher than what our MIDI training has produced.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;As of this blog post, we are still testing various hyperparameters and waiting for models to converge.  Additional results will follow once we can examine the various output.&lt;/p&gt;</content:encoded>
	<dc:date>2018-05-10T06:56:01+00:00</dc:date>
	<dc:creator>Nicholas Ruhland</dc:creator>
</item>
<item rdf:about="https://medium.com/p/a2d837ecf66b">
	<title>Halden Lin &lt;br/&gt; Team undef.: NLP Capstone | 07: Formalizing</title>
	<link>https://medium.com/@halden.lin/nlp-capstone-07-formalizing-a2d837ecf66b?source=rss-2759d54493c0------2</link>
	<content:encoded>&lt;p&gt;&lt;em&gt;previous posts: &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;&lt;em&gt;01&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5&quot;&gt;&lt;em&gt;02&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3&quot;&gt;&lt;em&gt;03&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7&quot;&gt;&lt;em&gt;04&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a&quot;&gt;&lt;em&gt;05&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0&quot;&gt;&lt;em&gt;06&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A couple of developments since I last posted:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;I’m now more formally receiving guidance from Kanit (Ham) Wongsuphasawat and Tongshuang (Sherry) Wu of the Interactive Data Lab. Special thanks to them for helping me thus far!&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0&quot;&gt;Last week&lt;/a&gt; I was uncertain as to the future direction of this project. After much deliberation and several conversations, I’ve decided to remain on the tool-based approach originally conceived. There are a couple reasons for this. First, the user study I proposed has, at least from an NLP perspective, limited novelty. What’s more, learning from the study by modifying the underlying model would require time that would likely fall outside of the quarter. Second, carrying out this study would involve significant logistical work (again, a time constraint). Finally, in beginning to formalize this visualization tool, I’ve become more excited in its potential as a useful part of a researcher’s debugging pipeline. In any case, any model modifications I may make as part of the user study would difficult without a similar tool.&lt;/li&gt;&lt;li&gt;As mentioned, I’ve been formalizing this tool in a React application, iterating on the exploration I’ve done with prototypes in the weeks previous. The rest of this post will describe my work here.&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;Starting with Text&lt;/h4&gt;&lt;p&gt;First things first: text brushing is critical in understanding the attention each output token, or a series of output tokens, pays to the source text. In the gif below, the left side holds the source text (along with a mini-map to prevent the need of scrolling to understand the overall distribution of attention), while the right holds the summary. Selection and brushing have been implemented, as in the prototypes of last week, albeit cleaned up.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/999/1*LKOEMB1cwvp2qXRy58_-4w.gif&quot; /&gt;Cleaned up selection and brushing, in addition to a minimap for long input sequences.&lt;/figure&gt;&lt;h4&gt;&lt;strong&gt;Challenges with Text (Future Work):&lt;/strong&gt;&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;Sentence / paragraph level structure is lost via tokenizing. This is an artifact of the tokenization process performed prior to feeding text into models.&lt;/li&gt;&lt;li&gt;Lowercase and always-on spacing between tokens makes text difficult to read. For now, I’ve been dealing this with a few hand-coded rules. For example, removing spaces before punctuation, and capitalizing the first word after end of sentence punctuation.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;I’ll need to keep brainstorming to find methods for addressing these issues.&lt;/p&gt;&lt;h4&gt;How do we enable identification of patterns?&lt;/h4&gt;&lt;p&gt;While selection and brushing over the text is valuable in allowing users to understand attention for specific words or phrases, it falls short in enabling big-picture identification of patterns. Without brushing over each token and / or sentence (and memorizing coverage along the way), the closest users may get is the aggregate view (when nothing is selected) in which &lt;strong&gt;what&lt;/strong&gt; is being attended to is apparent, but not &lt;strong&gt;how.&lt;/strong&gt; That is to say, it is not apparent which tokens / phrases in the summary attend to which tokens / phrases in the source text. In particular, below is a growing list of goals for this visualization.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Enable identification of coverage&lt;/strong&gt;. For words / phrases / sentences, where is the attention being paid, and by what?&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Enable identification of missing coverage&lt;/strong&gt;. What is being unattended to that should be?&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Enable identification of extraction vs abstraction&lt;/strong&gt;. Where is copying occuring? Where is true abstraction occuring?&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;If this tool can accomplish these items, I believe it will be a good start in proving its value.&lt;/p&gt;&lt;p&gt;With this in mind, we need some sort of visualization to accompany the two blocks of text. As previously mentioned, heat-maps may not be the best solution.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Kd4lr3ZZmOZ6ffMdp82PJg.gif&quot; /&gt;Interactive heat-map, as prototyped previously.&lt;/figure&gt;&lt;p&gt;Lag is apparent (likely a result of the large number of elements drawn), and even ignoring this, the tiles become extremely small as the input / output sequences grow, making it difficult to pick out even high attention weights. More visual weight is needed for significant attention weights, which is difficult to accomplish as x and y space is already taken by the input / output token position.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;So where else can we look?&lt;/strong&gt;&lt;/p&gt;&lt;h4&gt;Flowmaps&lt;/h4&gt;&lt;p&gt;Looking back at my project proposal, I saw this visualization made by Rikters et al. (2017).&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/0*uP_-gGiPkv6YMFYz.&quot; /&gt;An example of a flow-map from machine translation [Rikters et al. 2017].&lt;/figure&gt;&lt;p&gt;I thought this might be worth exploring, so I attempted to create something similar.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/0*SxZUaHFYssV5JtXo.&quot; /&gt;A flowmap displaying all edges is problematic. This doesn’t scale either.&lt;/figure&gt;&lt;p&gt;Unsurprisingly, there are far too many edges to display without significant overlap and occlusion. Summarization rears its head again as a challenge with its large input sequences.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;How do we remedy this?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;One observation is that high attention weights are fairly sparse (as evident by the aggregate on the source text above). What if we filtered out these insignificant weights? A naive approach is to take the top &lt;em&gt;k &lt;/em&gt;percent of weights and display only those.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/0*I4jiiM98R88ZfMKz.&quot; /&gt;A flowmap showing only the top 1% of weights.&lt;/figure&gt;&lt;p&gt;The above example is displaying only the top 1% of attention weights. Edges have both their width and opacity scaled by their weight within this 1% domain. Significant (wide) weights are clearly identifiable. Thinner lines, faint (or nearly invisible) lines can also be seen, indicating that the much more significant weights have been preserved. Four distinct ‘rays’ can be seen, seemingly corresponding to the four sentence of the summary.&lt;/p&gt;&lt;p&gt;Selection and brushing seemed like intuitive follow-ups for interaction to enable more detailed / accurate pattern identification.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Selection&lt;/strong&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1000/1*U8_42zGX24nN8iB7Wo-zPw.gif&quot; /&gt;Selection over the flowmap, both over the output nodes as well as the output text.&lt;/figure&gt;&lt;p&gt;Selection allows users to orient themselves in the flowmap, picking out which edges correspond to which input / output tokens.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Brushing&lt;/strong&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1000/1*1t7BRJX9g2k80B67IFQ4lw.gif&quot; /&gt;Brushing over the flowmap allows for windowed pattern identification.&lt;/figure&gt;&lt;p&gt;Brushing enables accurate pattern identification. In the example above, brushing over the distinct rays allows us to see the almost entirely extractive nature of the summary — there is a clear 1:1 mapping from input to output that is implied by the clean structure of the rays, and confirmed upon inspecting the corresponding input / output tokens for these rays.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Challenges with Flowmaps (Future Work):&lt;/strong&gt;&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Selecting a top k% is not ideal — it does not generalize well.&lt;/strong&gt; In the pathological case, where attention is evenly distributed for all tokens, we lose a lot of valuable information. A potential band-aid to this is to allow users to select the percentage of weights displayed, but this may be dangerous as high percentages can crash the browser. Perhaps a more elegant solution would be to perform clustering on the weights. I’ll need to do more research here.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Overlapping paths can make reading the flowmap difficult.&lt;/strong&gt; The example shown above is fairly clean, direction flows, for the most part, in a single direction. You could imagine, however that if a summary is extremely abstractive, pulling from all over the source, there might be significant overlap in edges, decreasing legibility. A potential solution to this is edge bundling, where edges going in similar directions are pulled together to preserve pattern recognition.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;A good starting point for exploration here is the paper &lt;a href=&quot;http://vis.stanford.edu/files/2011-DividedEdgeBundling-InfoVis.pdf&quot;&gt;&lt;em&gt;Divided Edge Bundling for Directional Network Data&lt;/em&gt;&lt;/a&gt; by Selassie, Heller, &amp;amp; Heer (2011). In this, the authors describe a method for bundling, divided edge bundling, that holds characteristics I believe are important for my own visualization.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/0*Ma6OlxvATqGNHbyu.&quot; /&gt;Different techniques applied to a network of GitHub contributions along the west coast of the United States [Selassie, Heller, &amp;amp; Heer 2011].&lt;/figure&gt;&lt;p&gt;In the coming weeks I hope to dive into these techniques and explore their impact on the flowmap I’ve developed thus far.&lt;/p&gt;&lt;h4&gt;In Summary&lt;/h4&gt;&lt;p&gt;Still a lot of work to do! Here are my goals for the next week.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Improve flowmap visualization.&lt;/li&gt;&lt;li&gt;Find examples that cover the problem space (e.g. low coverage, abstraction, extraction).&lt;/li&gt;&lt;li&gt;Keep brainstorming.&lt;/li&gt;&lt;li&gt;Optimize code.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Things are getting interesting!&lt;/p&gt;&lt;h4&gt;References&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://ufal.mff.cuni.cz/pbml/109/art-rikters-fishel-bojar.pdf&quot;&gt;Rikters, Matīss, Mark Fishel, and Ondřej Bojar. “Visualizing neural machine translation attention and confidence.” &lt;em&gt;The Prague Bulletin of Mathematical Linguistics&lt;/em&gt; 109.1 (2017): 39–50.&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://vis.stanford.edu/files/2011-DividedEdgeBundling-InfoVis.pdf&quot;&gt;Selassie, David et al. “Divided Edge Bundling for Directional Network Data.” &lt;em&gt;IEEE Transactions on Visualization and Computer Graphics&lt;/em&gt; 17 (2011): 2354–2363.&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=a2d837ecf66b&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-05-09T06:48:30+00:00</dc:date>
	<dc:creator>Halden Lin</dc:creator>
</item>
<item rdf:about="https://medium.com/p/6f773ae418d0">
	<title>Halden Lin &lt;br/&gt; Team undef.: NLP Capstone | 06: Uncertainty</title>
	<link>https://medium.com/@halden.lin/nlp-capstone-06-uncertainty-6f773ae418d0?source=rss-2759d54493c0------2</link>
	<content:encoded>&lt;p&gt;previous posts: &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;01&lt;/a&gt; &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5&quot;&gt;02&lt;/a&gt; &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3&quot;&gt;03&lt;/a&gt; &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7&quot;&gt;04&lt;/a&gt; &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a&quot;&gt;05&lt;/a&gt;&lt;/p&gt;&lt;p&gt;I’ve begun to realize I may not be getting as much out of the project I chose I had hoped. My initial motivation for my project was a hope of expanding my knowledge and developing insights on the NLP front by leveraging the familiarity of Visualization. While I am certainly learning a lot by reading papers on Attention and Neural Networks as a whole (especially through my in-class paper presentation), I feel the work I am doing in building a tool for visualizing and debugging attention models may not be providing me the space to explore NLP that I had hoped for. While the tool will certainly &lt;strong&gt;enable &lt;/strong&gt;exploration, my concern is that this exploration will not occur until after the tool is completed at the end of the quarter.&lt;/p&gt;&lt;p&gt;The good news is that there have been two recent developments that, while increasing my uncertainty, offer potential for greater depth in exploration along the NLP front.&lt;/p&gt;&lt;h4&gt;1. Potential Pivot&lt;/h4&gt;&lt;p&gt;I voiced these concerns with Prof. Choi this past week and was given a good amount of valuable advice. Per her suggestion, the beginning of my last cycle began with three tasks.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Read &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=2470718&quot;&gt;&lt;em&gt;The Efficacy of Human Post-editing for Language Translation&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;authored by&lt;em&gt; &lt;/em&gt;Spence Green, Jeff Heer, and Christopher Manning. This paper is unique in that it presents the value of Visualization and HCI within Natural Language Processing, but not as a window into a model. Rather, the authors explore a specific task integral to the Language Translation pipeline and present suggestions for future work in improving Language Translation.&lt;/li&gt;&lt;li&gt;Do in-depth human error-analysis of existing summarization models. I used examples from See et al.’s paper &lt;em&gt;Get To The Point: Summarization with Pointer-Generator Networks &lt;/em&gt;(2017).This was helpful gaining a better intuition as to the problem space and the challenges currently posed by machine summarization.&lt;/li&gt;&lt;li&gt;Think about how summarization as a task, whether that be the development of models, the model’s task itself, or end-user tasks that use the model, can be re-framed in order to leverage Visualization. This was especially time consuming, as it was difficult for me, but it helped immensely in taking a step back to understand the purpose of these models. This, in turn, helped me understand how my work can fit into this purpose.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The next few days consisted largely of brainstorming pivots for my project. The most promising direction that came out of these sessions is very briefly outlined below.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Assisted Cognitive Document Abstraction&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Machine-generated document summaries, even the state-of-the-art, are infrequently used in practice because their summaries are quite poor. Perhaps we can leverage existing models to, rather than produce definite summaries which may be error-prone or difficult to understand, create visualizations over the source text in order to assist humans in comprehension and abstraction.&lt;/p&gt;&lt;p&gt;For example, instead of treating attention weights as input for an output of text, as we do in summarization models, we can view them as output for human interpretation. Aggregate attention distributions (in summarization) highlight areas of the input text that are salient for the summary produced. Note that this is potentially more valuable than highlighting extractive summaries in the text because attention could potentially point towards different areas of the text that relate to a summary sequence. In this way, generation of summaries becomes a proxy task for creating salient highlights for text. We could then use this as a starting point from which ‘related’ sections in an article may be highlighted for users upon interaction (e.g. mousing over an attended-to sequence).&lt;/p&gt;&lt;p&gt;The hope is that these visualizations will increase the speed (over no summarization) or accuracy (over machine summarization) at which readers can abstract / understand key ideas in a document.&lt;/p&gt;&lt;p&gt;Most excitingly, with this re-framing of the task for these models, from sequence output to highlighting, perhaps the models can be modified by adding or removing constraints and mechanisms in order to improve performance for this new task.&lt;/p&gt;&lt;p&gt;Upon presenting this idea (in longer form) to Prof. Choi, I was encouraged to (1) think more about weaknesses of removing summaries altogether and (2) push for more novelty in the approach — is there any meaningful insight about attention models or summarization as a task that can be gleamed from this pivot, and if not, how can I work towards that. While I do not yet have answers to these concerns, the next development may result in a few.&lt;/p&gt;&lt;h4&gt;2. Related work, here at the Allen School&lt;/h4&gt;&lt;p&gt;It was just recently brought to my attention that a Tongshuang (Sherry) Wu, a PhD student in the Interactive Data Lab (in which I am currently working), is also working on visualizations for understanding attention models in NLP. As a part of her project, she and a few of her peers have developed a preliminary visualization tool for an attentive QA model (on the SQuAD dataset).She and my mentor, Kanit (Ham) Wongsuphasawat (whom I have been bouncing ideas off recently), have kindly offered to meet and discuss her work and insights on the problem space. Perhaps collaboration is a possibility — this is exciting! In any case, I suspect talking with Sherry and Ham will provide me the insight and guidance to make a decision on the direction of my project.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Future Work&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;I hope to make another blog post in the coming few days as I iron out a future direction. Before this, however, future work is unclear. Until next time!&lt;/p&gt;&lt;h4&gt;In the meanwhile (supplementary material)&lt;/h4&gt;&lt;p&gt;I’ve also been playing around with my visualization prototypes, even as I am uncertain as to whether or not they will be relevant to my project after this week. Here’s what I’ve discovered and implemented in that time.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Interactive heat-maps likely won’t work.&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*_bVKvDPn6jBADKwFU9VG3g.gif&quot; /&gt;Interactive heat-maps result in a large degree of lag between input and visual update. This is likely due to the extremely large size of the attention matrix in summarization (24,000 individual squares in the heat-map).&lt;/figure&gt;&lt;p&gt;This is unfortunate, but browser limitations are limitations that must be worked around.&lt;/p&gt;&lt;p&gt;2. Selection over output text.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/999/1*KfMQd6YnDzdH9dZOyjVEIw.gif&quot; /&gt;Mousing over words in the summary results in a view of the attention distribution over the article for that decoder time-step.&lt;/figure&gt;&lt;p&gt;This is similar to the interactive visualizations presented by See et al. in their &lt;a href=&quot;http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html&quot;&gt;blogpost&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;3. Brushing over output text.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1000/1*nwGIF3VgNDeuFxRml8gGzg.gif&quot; /&gt;Brushing over the summary results in an aggregate attention distribution (i.e. coverage) over the article for the selected decoder time-steps.&lt;/figure&gt;&lt;p&gt;This is an interaction technique I have yet to see in work involving attention analysis, so this is exciting! It looks to be somewhat useful in identifying sections of input text that are salient to an &lt;strong&gt;idea&lt;/strong&gt; rather than a &lt;strong&gt;single word&lt;/strong&gt; in the output text.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=6f773ae418d0&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-05-02T06:59:16+00:00</dc:date>
	<dc:creator>Halden Lin</dc:creator>
</item>
<item rdf:about="https://medium.com/p/306dca636d3a">
	<title>Halden Lin &lt;br/&gt; Team undef.: NLP Capstone | 05: Experimenting</title>
	<link>https://medium.com/@halden.lin/nlp-capstone-05-experimenting-306dca636d3a?source=rss-2759d54493c0------2</link>
	<content:encoded>&lt;p&gt;&lt;em&gt;previous posts: &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;&lt;em&gt;01&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5\&quot;&gt;&lt;em&gt;02&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-03-project-proposal-7d8e9ec1a8e3&quot;&gt;&lt;em&gt;03&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-04-first-steps-be87c31976b7&quot;&gt;&lt;em&gt;04&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Hi! Here’s what I’ve been up to in the past week.&lt;/p&gt;&lt;h4&gt;Progress on the TensorBoard Plugin&lt;/h4&gt;&lt;p&gt;Real data collection and the backend are functioning!&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*l6M8uMcswVoaEtL0_HGt0w.png&quot; /&gt;Architecture of the Attention Plugin.&lt;/figure&gt;&lt;p&gt;At this point last week, I had implemented the nodes in green above. These were the operators / functions required to produce Summary protobufs that are in turn saved to disk.&lt;/p&gt;&lt;p&gt;This week, I completed a number of tasks to produce a bare-bones functioning plugin (sans visualizations).&lt;/p&gt;&lt;p&gt;First, I modified the source code for &lt;a href=&quot;https://github.com/abisee/pointer-generator&quot;&gt;See et al.’s (2017) attentional models&lt;/a&gt; to use the Attention Plugin API to save input text, output text, and attention distributions during evaluation.&lt;/p&gt;&lt;p&gt;Next, I implemented the Attention Plugin’s back-end, which is used to fulfill requests made by the front-end. This service currently offers two services:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;tags This route returns all tags associated for each run in the log. This should include 3 tags for each run: one for each of the input, output, and attention tensors.&lt;/li&gt;&lt;li&gt;attention This route returns a list values associated with the given tag (including time and step stamps). This can be used by the front-end to acquire each of the input, output, and attention lists (converted from tensors) by passing the corresponding tag (retrieved using the tags route).&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Finally, as a proof of concept, I modified the front-end provided in the &lt;a href=&quot;https://github.com/tensorflow/tensorboard-plugin-example&quot;&gt;TensorBoard Plugin Example&lt;/a&gt; to consume this back-end, showing it is able to retrieve summaries. Now we just need some visualizations to consume the data!&lt;/p&gt;&lt;h4&gt;Visualization Prototyping Begins&lt;/h4&gt;&lt;p&gt;While data collection and back-end development has been wrapping up, I’ve begun to prototype static visualizations for the plugin. To do this, I used data produced by &lt;a href=&quot;https://github.com/abisee/pointer-generator&quot;&gt;See et al.’s (2017) pre-trained attentional models&lt;/a&gt; (produced only at decode time without the Attention Plugin). Through this process, I hope to gain two things in particular.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;A idea of what will/won’t work as visualizations for summarization tasks.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;A better understanding of the behavior of attentive models&lt;/strong&gt;, and through that a better idea of how static and/or interactive visualizations can further interpretability and understanding.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The first idea I decided to pursue was that of a &lt;strong&gt;condensed heat-map&lt;/strong&gt;. You may recall the conventional heat-map used for attention visualizations described in &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;my first blog post&lt;/a&gt;.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/612/1*_sq2Vy_Py7hEXp2tWBBXxg.png&quot; /&gt;Rikters et al. (2017). A heat-map with relatively large cells, allowing for display of text along the axes.&lt;/figure&gt;&lt;p&gt;The issues I noted with this visualization pattern are as follows:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;It is difficult to fit the words (as seen above) on the x-axis, harming readability.&lt;/li&gt;&lt;li&gt;This does not scale well with large input or output (e.g. summarization)&lt;/li&gt;&lt;li&gt;We do not read single-tokens at a time (i.e. y-axis), and input and output are generally not in this format either.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;To address the point (2), scalability, I decided to try producing a heat-map with no text labels, and thus each cell could be as small as a single pixel.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*WOlVYgmeZ0DmutTZkDXn2g.png&quot; /&gt;A prototype of a condensed heat-map, where x-axis represents input and y-axis represents output.&lt;/figure&gt;&lt;p&gt;The color scale is a discrete scale, where each step is determined by the quantiles of the weight distribution. The x-axis represents the input text, and the y-axis represents the output text, with each cell representing the amount of attention paid for that pair (output paid to input). The good news here is that the attention distribution is relatively easy to understand at a quick glance. The downside is that cells that are not part of a larger trend (you may notice a lone red spot near the top of the heat-map, approximately a quarter of the way through the x-axis) are harder to make out, as the cells are so small. Further, the distribution is contextless — we don’t know the structure of the input text or what words these high weights are associated with. In the example above, we understand that the model focused primarily on the beginning of the article, but we can’t tell whether that is good or bad without seeing the text.&lt;/p&gt;&lt;p&gt;To remedy this, I decided to also display the input text, with the input text highlighted according to the maximum of the weights it received. This also solves concerns (1) and (3) for the conventional heat-map.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*dJXS9s_391dACPuUwAgPmA.png&quot; /&gt;The input text corresponding to the heat-map above, where each token is highlighted according to its max attention weight received.&lt;/figure&gt;&lt;p&gt;By putting these two together (along with the output text for reference), we can gain a better understanding of how the model arrived at its summary. A viewer can now map the attention distribution shown in the heat-map to text in the input sequence by looking for patches of similar color intensity.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*dMqdcl01Za5U4nYrXpye6g.png&quot; /&gt;A prototype static visualization including both heat-map and highlighted text.&lt;/figure&gt;&lt;p&gt;To get a better sense of how this visualization pattern would play out, I built a light web-page that allows users to cycle through different input / output examples. The gif below shows several of these.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*_ACE44hvUSrwfE3F04gslg.gif&quot; /&gt;The described visualization pattern over several input / output sequences.&lt;/figure&gt;&lt;p&gt;More exploration (inside and outside of this pattern) will need to occur, but this seems promising!&lt;/p&gt;&lt;h4&gt;What’s Next&lt;/h4&gt;&lt;p&gt;Lots to get done this next week. Here’s what’s in my plan:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Continue working on data collection and cleaning up the TensorBoard plug-in. Move beyond the proof-of-concept front-end and show that meaningful visualizations (perhaps extremely basic ones) can be generated using the plugin back-end as a data source.&lt;/li&gt;&lt;li&gt;Read more into the model provided by See et al. (2017), as well as related work, to gain a better understanding of the architecture and function/behavior of attention. A closer study of the works cited in my &lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5&quot;&gt;second blog post&lt;/a&gt; will be a good starting point. The better I understand this mechanism the more equipped I’ll be to create meaningful visualizations.&lt;/li&gt;&lt;li&gt;Continue prototyping static visualizations, move on to interactive visualizations. Acquire feedback from peers for both.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Thanks for reading!&lt;/p&gt;&lt;h4&gt;Works Cited&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.04368.pdf&quot;&gt;See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” &lt;em&gt;ACL&lt;/em&gt; (2017).&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://ufal.mff.cuni.cz/pbml/109/art-rikters-fishel-bojar.pdf&quot;&gt;Rikters, Matīss, Mark Fishel, and Ondřej Bojar. “Visualizing neural machine translation attention and confidence.” &lt;em&gt;The Prague Bulletin of Mathematical Linguistics&lt;/em&gt; 109.1 (2017): 39–50.&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=306dca636d3a&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-25T06:48:23+00:00</dc:date>
	<dc:creator>Halden Lin</dc:creator>
</item>
<item rdf:about="http://mathstoc.wordpress.com/?p=323">
	<title>Kuikui Liu, Nicholas Ruhland &lt;br/&gt; Team INLP: NLP Capstone Post #5: A New Hope</title>
	<link>https://mathstoc.wordpress.com/2018/04/25/nlp-capstone-post-5-a-new-hope/</link>
	<content:encoded>&lt;h1&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Dataset Improvements&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Last time, on Music NLP.&lt;/span&gt;&lt;/i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; We ran into many midi data parsing issues. Since then, we have discovered a new dataset called the Lakh MIDI Dataset (&lt;/span&gt;&lt;a href=&quot;http://colinraffel.com/projects/lmd/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;http://colinraffel.com/projects/lmd/&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;) that comes with reasonably well-formed midi files. Using the “Clean MIDI Subset”, we found thousands of midi files with their associated song names and songwriters. From these midi files, we extracted all with nonempty “lyric” fields when parsed via the pretty_midi package (which, incidentally, is also developed by Colin Raffel). After this step, we were left with ~1200 midi files that contain lyrics.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We currently clean the lyrical content by removing all unusual symbols and setting all characters to lowercase. We leave all lyrical tokens as is, which typically means syllable. Due to the inconsistent quality of the MIDI annotations, many songs are tokenized instead to characters, words, or even sentences. We will explore other methods for processing data if this is not sufficient for our results.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;It is unfortunate we did not find this dataset sooner, because most of our challenges up to this point have been dealing with the poor quality of the gathered data.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Alignment&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For our task of producing karaoke style output, there are two main tasks we have to solve. The first task is the generation of plausible lyrics, and the second is to align the lyrics to the proper time along the musical data. The alignment task has been studied extensively, but specifically aligning lyrical content to MIDI has not been covered in literature we have found. The most common alignment task is lyrics to audio data, as opposed to MIDI. The other common task is to align audio data to the notes defined in a MIDI file. In [1], they show a method that takes a MIDI file with annotated lyrics and uses this to align the lyrics to the raw audio. Unfortunately this is not our task, because we are trying to generate the annotated MIDI.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;This week, we have decided to ignore the alignment task and focus primarily on making a reasonable lyrical model. We will return to alignment next week.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The next step was to align the lyrics with pianoroll. Fortunately, well-formed midi data parsed into PrettyMIDI objects come with a “get_piano_roll” function that takes as input a list of “times” which correspond to where in time pretty_midi will attempt to sample the music. As each syllable in the lyrics comes with a start time for when the singer enunciates it, we can pass in these start times to produce pianoroll that is aligned (up to small error) with the lyrics.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For some implementation reasons that are difficult to explain in English, it is possible for “get_piano_roll” to produce NaN entries, which we have replaced with zeros. Due to this and the potential for other such problems, we have forked the pretty_midi package and will be able to modify the code for our needs. For example, as pointed out in [2], “in a given MIDI file there is no reliable way of determining which instrument is a transcription of the vocals in a song”. As such, there are many choices for how to do alignment; pretty_midi has implemented just one. It is an interesting task to see how different alignment methods help or hurt our models.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Lyric prediction&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Now that we have aligned pianoroll to lyrics data, we can begin engineering the model. Last time, we used an LSTM to generate lyrics given starting characters. Here, we will again use LSTMs, but instead, work at the syllable level and take as input the pianoroll of a song. As each column of a pianoroll is a time slice, each input vector to the LSTM is a single time slice. Each time slice is a 128-dimensional vector, with each entry representing the activation of an instrument; there are 128 midi recognized “instruments”.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;All that is left is to play with the architecture. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;RNN model&quot; class=&quot;alignnone size-full wp-image-322&quot; src=&quot;https://mathstoc.files.wordpress.com/2018/04/rnn-model1.png?w=676&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;At the moment, our pipeline looks like what is shown in the diagram. At each iteration, we take a song, extract the lyrics and the corresponding pianoroll data. We then feed each time slice of the pianoroll data through an encoder unit, then through an LSTM unit, then through a decoder unit, and finally through a softmax to produce the prediction. Our loss is the negative log-likelihood (negative logarithm of the RNN softmax probability of the true syllable).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We will compare our final model to this baseline with respect to the loss on a held-out validation set. We will also experiment with loss functions other than cross entropy to see how it affects the actual lyrical output.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model results&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We have so far only trained our model for a single iteration over the training set. For an empirical evaluation on the current model quality, we ran a single MIDI through the input and computed the argmax word for each output. This produced a result in which every predicted lyric was an empty message, which is the most common string in the training set. We will explore methods to handle this class imbalance as our next task.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[1] Müller, Meinard &amp;amp; Kurth, Frank &amp;amp; Damm, David &amp;amp; Fremerey, Christian &amp;amp; Clausen, Michael. (2007). Lyrics-Based Audio Retrieval and Multimodal Navigation in Music Collections. 4675. 112-123. 10.1007/978-3-540-74851-9_10.&lt;/span&gt;&lt;br /&gt;
&lt;span style=&quot;font-weight: 400;&quot;&gt;[2] &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Raffel, Colin and Daniel P. W. Ellis. “Extracting Ground-Truth Information from MIDI Files: A MIDIfesto.” &lt;/span&gt;&lt;i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;ISMIR&lt;/span&gt;&lt;/i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; (2016). &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; &lt;/span&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-25T04:44:57+00:00</dc:date>
	<dc:creator>Nicholas Ruhland</dc:creator>
</item>
<item rdf:about="http://mathstoc.wordpress.com/?p=314">
	<title>Kuikui Liu, Nicholas Ruhland &lt;br/&gt; Team INLP: NLP Capstone Post #4: Baseline and MIDI Frustration</title>
	<link>https://mathstoc.wordpress.com/2018/04/18/nlp-capstone-post-3-baseline-and-midi-frustration/</link>
	<content:encoded>&lt;h1&gt;&lt;b&gt;Baseline model&lt;/b&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our baseline approach is taken from Daniil Pakhomov’s excellent blog post[1]. In this post, two separate RNNs are trained as generators: one for lyrical content and one for music content (in piano roll format). We will begin by using his trained lyrical model, and attempt to do conditional sampled character generation given a starting sequence of characters. We loaded the already trained models from the blog post and generated lyrics according to the style of a given songwriter and with a given seed word. The lyrics are generated via a character-level LSTM and generates the next character conditioned on the preceding characters and choice of songwriter. The model is trained on a corpus of song lyrics, where naturally the “correct” character to generate is the next character in the lyrics. Essentially the same mechanism is applied to the musical note generation.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The characters are encoded as a one-hot vector over all letters in the English alphabet plus space, comma, etc. Piano rolls already come in an encoding amenable to feeding into RNNs, modulo additional zero padding to ensure every time slice of every piano role has the same dimension. In particular, at each time step (discretized in an appropriately fine-grained way), we have an indicator 0-1 vector on which notes are currently activated.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Here is a song generated in the style of “Queen” with the starting seed sequence of characters “Music”:&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Music savor valerite – yah  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Imabribot, bind me – I – well  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;All going down to L&lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;At the eyes of the universe  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Agree, five to the Slim  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;I just want to convincide  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;We wash stars and quiet Ich  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;You had a dirty old baby  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;We won’t  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;We does nothing no one ezy? follohin?  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Sometimes we get down and ooh  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Nothing do you see all night  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;This is my pries  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Joyful the world  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Does their beams  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Surgeon makes the scule la beat  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;Walking out on my pocket ride  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;My faulty power  &lt;/i&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;i&gt;I wear from the ston&lt;/i&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Eventually, since we are actually interested in converting the musical information into plausible lyrics, we will need to modify this baseline in the natural way to take as input time slices of the musical instrumentation in piano roll format and predict characters (or syllables) that are to be enunciated simultaneously with the played notes. In this manner, the lyrics come already aligned in a natural way, and the words can be extracted by compressing the letters occurring between spaces.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;&lt;b&gt;Dataset parsing&lt;/b&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The MIDI format has an unfortunate number of unexpected caveats. We have spent a majority of our time so far cleaning the data and attempting to use it in existing Python libraries that handle MIDI. A brief description of MIDI[2] covers some of the challenges:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;MIDI is made up of a series of messages, such as notes, instruments, and tempo changes. Additional metadata messages exist called meta messages, which can contain text content such as the song title (and lyrics!). In our dataset, lyrics are provided either as “text” messages or as “lyrics” messages.&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Messages are grouped into different tracks, often representing separate instruments. Metadata sometimes is located in its own track, and lyrics are sometimes found in a different track from the rest of the metadata.&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Durations in the MIDI format are specified as a delta-time relative to the most recent frame. Delta times are in a unit called a tick. Ticks are defined in the file header as a division of the quarter note. The header also defines the number of ticks per frame, which is what the deltas are relative to. Beats per minute (bpm) messages adjust the speed of playback throughout the song.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The most promising library so far is PrettyMIDI[3], which handles most of the unexpected behavior of the basic MIDI format. This library wraps MIDI messages into structured python objects, and provides a conversion from MIDI into piano roll format. Piano roll in this case is a numpy array of shape (num_notes, num_frames). This allows us to input the musical data directly into an RNN. The units are also converted into absolute seconds, as opposed to relative durations. PrettyMIDI can additionally handle embedded lyrics, but this has proven to be a challenge due to the variety of annotation styles in our dataset. About 200 of our 900 files have parsed lyric data properly, so continuing to clean our data is a high priority.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;&lt;strong&gt;U&lt;/strong&gt;pd&lt;strong&gt;ate&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Unfortunately, we have found the Kara1k dataset[4] to be inapplicable to our project, as the raw sequence of musical notes and lyrical content are not provided, only metadata that the dataset developers have extracted.&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[1] &lt;/span&gt;&lt;a href=&quot;http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[2] &lt;/span&gt;&lt;a href=&quot;http://www.music.mcgill.ca/~ich/classes/mumt306/StandardMIDIfileformat.html&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;http://www.music.mcgill.ca/~ich/classes/mumt306/StandardMIDIfileformat.html&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[3] &lt;/span&gt;&lt;a href=&quot;http://craffel.github.io/pretty-midi/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;http://craffel.github.io/pretty-midi/&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[4] &lt;/span&gt;&lt;a href=&quot;http://yannbayle.fr/karamir/kara1k.php&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;http://yannbayle.fr/karamir/kara1k.php&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-18T04:58:56+00:00</dc:date>
	<dc:creator>Nicholas Ruhland</dc:creator>
</item>
<item rdf:about="https://medium.com/p/96fb908765f5">
	<title>Halden Lin &lt;br/&gt; Team undef.: NLP Capstone | 02: Getting Started</title>
	<link>https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5?source=rss-2759d54493c0------2</link>
	<content:encoded>&lt;p&gt;&lt;a href=&quot;https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5&quot;&gt;previous post&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Alright, it’s been only 2 days since my last entry, so this will be a relatively short post. The direction I proposed in &lt;strong&gt;Option 1 &lt;/strong&gt;of that post was towards a more robust, interpretable, and informative visualization of attention, particularly in the context of text summarization. A quick recap:&lt;/p&gt;&lt;blockquote&gt;Perhaps interaction can be used to create a more insightful and interpretable visualization framework for understanding attention. For example, text heat-maps are already used widely to visualize sentiment analysis.&lt;/blockquote&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*lsgeuBXGGBog4YkuQNgJVw.png&quot; /&gt;Lin et al. (2017) [6]. Visualization of sentiment analysis on a token-by-token basis.&lt;/figure&gt;&lt;blockquote&gt;In a static context, using this method for attention would require repeat of the same input sequence for each word in the output sequence. Using interaction, however, a model creator could brush over single or sequences of words in the output sequence to view corresponding soft-alignment in the input sequence. Aggregate visualizations could be shown to supplement this view (either aggregates over a particular input / output sequence, or aggregates over all input / output sequences).&lt;/blockquote&gt;&lt;p&gt;I’m currently working on laying out the groundwork for such a project. Task 1: implement a model. Without one, there’s no data to visualize!&lt;/p&gt;&lt;p&gt;With that in mind, here’s what I’ve been up to:&lt;/p&gt;&lt;h4&gt;Finding a Text Summarization Dataset&lt;/h4&gt;&lt;p&gt;A quick survey of recent research papers [1–5] on text summarization points, as well as online forums, points to three commonly used datasets.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://cs.nyu.edu/~kcho/DMQA/&quot;&gt;CNN/Daily Mail Corpus&lt;/a&gt;. A collection of articles and their bullet point summaries, with each bullet split for Q/A purposes. &lt;a href=&quot;https://github.com/abisee/cnn-dailymail&quot;&gt;A script&lt;/a&gt; [1] can be ran over the original dataset to restore the original bullet point summaries, to be used as a summarization corpus.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www-nlpir.nist.gov/projects/duc/data.html&quot;&gt;DUC Corpus&lt;/a&gt;. In particular, DUC 2003 and DUC 2004. These contain a collection of documents, each accompanied by a short (~10 word) summary. There is also a longer summary for each cluster of documents.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://catalog.ldc.upenn.edu/ldc2003t05&quot;&gt;Gigaword Corpus&lt;/a&gt;. An annotated collection of millions of documents. The summarization task here would be to predict the headline of each [5]&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The accessibility of the &lt;strong&gt;CNN/Daily Mail Corpus&lt;/strong&gt; (a process is required for the other two), in addition to the prevalence of projects that used it as a primary dataset [1, 2, 4], made it the most attractive option. The relatively longer summaries (~4 bullet points as opposed a short blurb in the other two datasets) also lends itself conveniently to the case of an interactive visualization with multi-token selection (e.g. select a whole bullet point and see where it attended). For a baseline, this will be my dataset!&lt;/p&gt;&lt;h4&gt;Identifying a Baseline Model&lt;/h4&gt;&lt;p&gt;See et al. (2017) [1] lay out a seq2seq attentional model as their baseline (a bidirectional LSTM). I’ll be using this as a baseline model with which to obtain data.&lt;/p&gt;&lt;h4&gt;Getting Some Code Up&lt;/h4&gt;&lt;p&gt;I’ll be using &lt;a href=&quot;http://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; and the &lt;a href=&quot;http://allennlp.org/&quot;&gt;AllenNLP&lt;/a&gt; toolkit [7] to implement my NN models. These are both ready to go on both my machine and Azure. I’m currently in the process of writing a DatasetReader for the dataset described above.&lt;/p&gt;&lt;h3&gt;Next Steps&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Finish writing the DatasetReader for the CNN/Daily Mail Corpus.&lt;/li&gt;&lt;li&gt;Begin work on a baseline seq2seq attentional model, as described in &lt;strong&gt;Identifying a Baseline Model&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Works Cited&lt;/h4&gt;&lt;p&gt;[1] &lt;a href=&quot;https://arxiv.org/pdf/1704.04368.pdf&quot;&gt;See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” &lt;em&gt;ACL&lt;/em&gt; (2017).&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[2] &lt;a href=&quot;https://arxiv.org/pdf/1712.06100.pdf&quot;&gt;Hasselqvist, Johan et al. “Query-Based Abstractive Summarization Using Neural Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1712.06100 (2017): n. pag.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[3] &lt;a href=&quot;https://arxiv.org/pdf/1602.06023.pdf&quot;&gt;Nallapati, Ramesh et al. “Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.” &lt;em&gt;CoNLL&lt;/em&gt; (2016).&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[4] &lt;a href=&quot;https://arxiv.org/pdf/1705.04304.pdf&quot;&gt;Paulus, Romain et al. “A Deep Reinforced Model for Abstractive Summarization.” &lt;em&gt;CoRR&lt;/em&gt; abs/1705.04304 (2017): n. pag.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[5] &lt;a href=&quot;https://arxiv.org/pdf/1509.00685.pdf&quot;&gt;Rush, Alexander M. et al. “A Neural Attention Model for Abstractive Sentence Summarization.” &lt;em&gt;EMNLP&lt;/em&gt; (2015).&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[6] &lt;a href=&quot;https://arxiv.org/pdf/1703.03130.pdf&quot;&gt;Lin, Zhouhan, &lt;em&gt;et al.&lt;/em&gt;, “A structured self-attentive sentence embedding.”&lt;em&gt;arXiv preprint arXiv:1703.03130&lt;/em&gt; (2017).&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[7] &lt;a href=&quot;https://pdfs.semanticscholar.org/a550/2187140cdd98d76ae711973dbcdaf1fef46d.pdf?_ga=2.150901366.1370831839.1522970228-1363309632.1522194596&quot;&gt;Gardner, Matt et al. “AllenNLP: A Deep Semantic Natural Language Processing Platform.” (2017).&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=96fb908765f5&quot; width=&quot;1&quot; /&gt;</content:encoded>
	<dc:date>2018-04-06T06:53:32+00:00</dc:date>
	<dc:creator>Halden Lin</dc:creator>
</item>
<item rdf:about="http://mathstoc.wordpress.com/?p=304">
	<title>Kuikui Liu, Nicholas Ruhland &lt;br/&gt; Team INLP: Milestone #2: Music as a Natural Language Task</title>
	<link>https://mathstoc.wordpress.com/2018/04/06/milestone-2-music-as-a-natural-language-task/</link>
	<content:encoded>&lt;h3&gt;Framing the problem&lt;/h3&gt;
&lt;p&gt;The focus of Natural Language Processing relies on patterns in the structure of language and models that find ways to encode the complexities of these structures. Many forms of music also have large amounts of structure which could potentially be discovered using similar models as a standard natural language.&lt;/p&gt;
&lt;p&gt;Music datasets for machine learning purposes have recently become available through projects like MusicNet in 2016 [1]. This music is primarily classical, and provided as both audio and MIDI.&lt;/p&gt;
&lt;h3&gt;Project ideas&lt;/h3&gt;
&lt;p&gt;For our project we are interested in music with lyrical content – both for the potential to create a creative demo and for the interest of making this a language task. The current direction we are most interested in is the generation of lyrics for a song, given its nonlyrical content. This will be broken up into subtasks depending on the feasible scale of the project. Not all of the following points will necessarily be parts of our project, but we will use them as as starting point as we see the success of our models.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Creating a machine learning model for MIDI music&lt;/li&gt;
&lt;li&gt;Translating MIDI into specific artists or styles&lt;/li&gt;
&lt;li&gt;Creating models for the lyrical content of specific artists or styles of music&lt;/li&gt;
&lt;li&gt;Generating lyrics given an artist or style&lt;/li&gt;
&lt;li&gt;Seq2seq conversion of MIDI into lyrical content&lt;/li&gt;
&lt;li&gt;GANs for either side of the conversion – MIDI encoding or lyrical generating&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Using MIDIs in RNNs&lt;/h3&gt;
&lt;p&gt;Work by Pakhomov [2] has already used RNNs to create models for lyrics. In his &lt;a href=&quot;http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/&quot;&gt;blog post&lt;/a&gt; he additionally discusses a method for forming any MIDI into piano roll format. This is essentially a matrix where each column represents a different time step, and each row represents a different note. Having a 1 corresponds to that note sounding at that time. The individual time vectors can be used as the inputs to an RNN at each time step to create a model representing the various songs.&lt;/p&gt;
&lt;p&gt;One possible data source for our project is karaoke data available from various sources online. If available in large enough quantities this could be extremely convenient because it already contains many pairings of MIDI music to their lyrics.&lt;/p&gt;
&lt;h3&gt;Azure&lt;/h3&gt;
&lt;p&gt;We intend to use PyTorch to train our models, and have begun setting up an instance on Microsoft Azure.&lt;/p&gt;
&lt;h3&gt;Relevant work&lt;/h3&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://homes.cs.washington.edu/~thickstn/musicnet.html&quot; rel=&quot;nofollow&quot;&gt;https://homes.cs.washington.edu/~thickstn/musicnet.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/&quot; rel=&quot;nofollow&quot;&gt;http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] Dong, Hao-Wen. 2017. MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment. &lt;a href=&quot;https://arxiv.org/pdf/1709.06298&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/pdf/1709.06298&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] Yu, Lantao. 2016. SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. &lt;a href=&quot;https://arxiv.org/abs/1609.05473&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/abs/1609.05473&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] Lee, Sang-gil. 2017. A SeqGAN for Polyphonic Music Generation. &lt;a href=&quot;https://arxiv.org/abs/1710.11418&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/abs/1710.11418&lt;/a&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-06T06:30:14+00:00</dc:date>
	<dc:creator>Nicholas Ruhland</dc:creator>
</item>
<item rdf:about="http://mathstoc.wordpress.com/?p=277">
	<title>Kuikui Liu, Nicholas Ruhland &lt;br/&gt; Team INLP: NLP Capstone Post #1: Ideation</title>
	<link>https://mathstoc.wordpress.com/2018/04/04/nlp-capstone-post-1-ideation/</link>
	<content:encoded>&lt;p&gt;In this post, I’d like to briefly discuss three different ideas I have for my capstone project.&lt;/p&gt;
&lt;p&gt;UPDATE (04/05/2018): I am fortunate to be joined by a fellow student, Nicholas Ruhland, for this capstone project.&lt;/p&gt;
&lt;h1&gt;A Theoretical Analysis of RNNs (Research Mode):&lt;/h1&gt;
&lt;p&gt; A recent &lt;a href=&quot;https://arxiv.org/abs/1703.00810&quot;&gt;paper of Professor Naftali Tishby&lt;/a&gt; provided some useful observations on the behavior of feedforward neural networks, and proposed a promising approach to understanding their performance. Earlier empirical work done in the vision community showed that when a convolutional neural network is trained, layers closer to the input learn lower level features (such as edges and corners) and layers closer to the output learn higher level features (“this part of the image resembles a nose, and this other part resembles an eye”). One might expect similar behavior to occur with general feedforward neural networks: that earlier layers learn lower level features of the input and later levels learn higher level features of the input. The key insight here was to think of each layer of a neural network as a Markov chain, where each layer &lt;img alt=&quot;L_{i}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7Bi%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{i}&quot; /&gt; is a (vector-valued) random variable that is conditionally independent of &lt;img alt=&quot;L_{j}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7Bj%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{j}&quot; /&gt; for all &lt;img alt=&quot;j &amp;lt; i - 1&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=j+%3C+i+-+1&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;j &amp;lt; i - 1&quot; /&gt; given &lt;img alt=&quot;L_{i-1}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7Bi-1%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{i-1}&quot; /&gt;. In this way, information flowing forward in the network can be quantified via notions of entropy from traditional information theory.&lt;/p&gt;
&lt;p&gt;The paper contains some empirical work, observing that there are generally two phases to learning artificial neural networks via stochastic gradient descent: the fitting phase, and the compression phase. The fitting phase is the shorter phase, where the model is quickly tuning itself to minimize the empirical loss function. At the end of this phase, we don't necessarily have a model that will generalize to new data. The compression phase is where the model begins to learn the relevant features in the input, with the intuition that there are many irrelevant parts of the input (I don't need to know every atom in an object to identify it). &lt;/p&gt;
&lt;p&gt;The goal of this project would be to perform a similar theoretical analysis and empirical work for RNN architectures (whose &quot;natural&quot; Markov chain isn't as simple, as there are cycles) on some traditional NLP task, such as Machine Translation, with the goal of studying the flow of information in an RNN architecture, rather than performing comparably to state-of-the-art Machine Translation models (although this can be a stretch goal).&lt;/p&gt;
&lt;p&gt;The relevant steps in this project will likely look like the following:&lt;br /&gt;
1. Reading up on the relevant work by Tishby et. al. (and any other theoretical papers on deep learning).&lt;br /&gt;
2. Understand basic and traditional RNN architectures.&lt;br /&gt;
3. Learning PyTorch.&lt;br /&gt;
4. Implementing several of these architectures and testing (for example, to see if learning also comes in two distinct phases: fitting and compression)&lt;br /&gt;
5. Using these empirical observations, and information theory to analyze these architectures.&lt;br /&gt;
6. Time permitted, play around with new RNN architectures.&lt;/p&gt;
&lt;h1&gt;Musical Style Learning from Musical Scores (Research/Start-Up Mode):&lt;/h1&gt;
&lt;p&gt; This idea lies somewhat outside traditional NLP in that it tackles the language of music. While the alphabet of a musical score consist chiefly of the 12 musical notes, there is added challenge in that several notes may be played simultaneously, especially if there are several instruments involved or simply the two hands of a pianist. Furthermore, the exact timing of each note played matters, note merely the ordering of the notes.&lt;/p&gt;
&lt;p&gt;The idea here is simply to, given the score of a musical piece, represented as a sequence of notes at each time, predict the era (Baroque, Classical, Romantic, etc.) or even, the composer of the piece (Bach, Beethoven, Brahms, etc.) There are several problems to be solved step by step for this project.&lt;/p&gt;
&lt;p&gt;1. Data collection from a large library of musical scores (ex: &lt;a href=&quot;http://imslp.org/&quot;&gt;IMSLP&lt;/a&gt;)&lt;br /&gt;
2. Data formatting so as to be usable.&lt;br /&gt;
3. Model selection.&lt;br /&gt;
4. Model implementation (PyTorch).&lt;br /&gt;
5. Model testing.&lt;/p&gt;
&lt;p&gt;There are also several extensions that can be viewed as stretch goals. For these, the first two can be reused.&lt;/p&gt;
&lt;h3&gt;Musical Score Generation:&lt;/h3&gt;
&lt;p&gt; Now, we learn how to compose a piece that “sounds” similar to a given composer. This will involve learning from the pieces written by a given input composer, and outputting a new piece. One core challenge here is ensuring that the output is syntactically correct.&lt;/p&gt;
&lt;h1&gt;Story Illustration (Start-Up Mode):&lt;/h1&gt;
&lt;p&gt; Given a short story and a specific scene (or place in the text), produce an image that is representative of the scene. This project combines aspects of NLP and vision. This project may also explore generative adversarial methods. One well-known challenge here is convergence.&lt;/p&gt;
&lt;p&gt;Here are the general steps for this project:&lt;br /&gt;
1. Data collection (image captioning dataset can be helpful)&lt;br /&gt;
2. Model selection.&lt;br /&gt;
3. Model implementation (PyTorch).&lt;br /&gt;
4. Model testing.&lt;/p&gt;
&lt;p&gt;As an extension, one can also generate several frames to form a short “movie”. Another can be comic book pane generation.&lt;/p&gt;</content:encoded>
	<dc:date>2018-04-04T06:53:35+00:00</dc:date>
	<dc:creator>Kuikui Liu</dc:creator>
</item>

</rdf:RDF>
