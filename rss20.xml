<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">

<channel>
	<title>NLP Capstone Spring 2018</title>
	<link>https://nlpcapstone.github.io/</link>
	<language>en</language>
	<description>NLP Capstone Spring 2018 - https://nlpcapstone.github.io/</description>
	<atom:link rel="self" href="https://nlpcapstone.github.io/rss20.xml" type="application/rss+xml"/>

<item>
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Data Subsets, Parent Feeding, and Future Work</title>
	<guid isPermaLink="false">https://medium.com/p/81ba6be6634</guid>
	<link>https://medium.com/@ryanp97/data-subsets-parent-feeding-and-future-work-81ba6be6634?source=rss-6378d85d3a9b------2</link>
	<description>&lt;p&gt;In the last week, I’ve been training more models on different subsets of the data and began working on implementing parent feeding. Though I don’t think I will have parent feeding done in time for the final presentation, it’ll be a nice checkpoint to start at after this capstone finishes.&lt;/p&gt;&lt;h4&gt;Data Subsets&lt;/h4&gt;&lt;p&gt;The model trained on all of Michael Goodman’s data last week had an issue with SMATCH (which is why the SMATCH score was omitted in last week’s blogpost). It turns out that there were a couple translation pairs in which the unicode character U+3000 (ideographic/wide space) was used as a token causing it to appear as a surface predicate in some development examples. As a result, SMATCH had issues dealing with these and crashed. Considering how infrequently this surface predicate actually occurred, I decided to invalidate the graphs that contained them during post-processing. This model ‘achieved’ a SMATCH score of 0.54. For reference, when treating the predicates as a bag of words, the model had an F1 score of 0.52.&lt;/p&gt;&lt;p&gt;Considering the Kyoto Corpus suffers the issue of having many uncommon named entities, I decided to train a model on just the Japanese WordNet corpus. This dataset is significantly smaller with ~105,000 training examples compared to ~325,000 training examples for the combined corpus. I figured this dataset would have significantly fewer named entities and not mis-predict the named abstract predicate so often. This model did, in fact, achieve a better SMATCH score with a F1 of 0.57 and an F1 score of 0.54 when treating the predicates as a bag of words. Notably, the named abstract predicate was mis-predicted less often, though it was still in the top 10 mis-predicted predicates. This resulted in a higher abstract predicate precision, ~0.04 above. Surprisingly, however, the surface predicate precision drop ~0.04. I’m not entirely sure why quite yet, but it may be due to the ratio of number of surface predicates to number of abstract predicates in each dataset.&lt;/p&gt;&lt;p&gt;I’m currently training a model with all of the data from Michael Goodman and adding all the training examples I had parsed from the Tanaka Corpus. In theory this model should perform slightly better than the model trained with solely Michael Goodman’s dataset, though I won’t be able to tell until late tomorrow considering the time it takes to train a single epoch.&lt;/p&gt;&lt;h4&gt;Parent Feeding&lt;/h4&gt;&lt;p&gt;Working with OpenNMT’s codebase has been quite a pain. Though I’ve implemented a short method to calculate the parent indicies of a single graph, I have had lots of trouble figuring out where exactly they should be calculated and how they will be stored. For now I’ve placed it as a step in ShardedTextCorpusIterator. So the pipeline for generating the input to OpenNMT is still the same. The preprocess.py script takes the same inputs and outputs as usual. The only thing that is different now is the saved files will now also contain parent indicies for each example.&lt;/p&gt;&lt;p&gt;I haven’t been able to figure out how batching will work with this quite yet, so I’m meeting up with Jan later this week to discuss how we should do batching for this. Once Jan clears up how batching works on Wednesday, I should have a good enough understanding to attempt to modify/write a decoder that also uses parent feeding.&lt;/p&gt;&lt;h4&gt;ELMo Embeddings&lt;/h4&gt;&lt;p&gt;After discussing with Jan more, I likely would not see significantly improved results with ELMo embeddings unless I was able to scrape more data to train these embeddings with. Since the vocabulary the model is trying to predict are predicates and edges, I would have to generate graphs after scraping more data for both languages. Considering how little time I have left, I decided to leave this for future work.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=81ba6be6634&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 29 May 2018 00:20:11 +0000</pubDate>
</item>
<item>
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #9: Additions to Advanced Model</title>
	<guid isPermaLink="false">https://medium.com/p/db16f27c255d</guid>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-9-additions-to-advanced-model-db16f27c255d?source=rss-fad49d942bf3------2</link>
	<description>&lt;p&gt;This week, I took into account my error analysis from last week and made some modifications to my model to try and improve F1 scores.&lt;/p&gt;&lt;h3&gt;Modification to Architecture&lt;/h3&gt;&lt;p&gt;I modified the architecture of my model a little to better aggregate across mentions. Instead of taking the mean across all mentions for each entity as I had done previously, I paired up all mentions of the holder/target entity, concatenated them, and then aggregated each pair through an attentive sum.&lt;/p&gt;&lt;p&gt;More specifically, holder/target aggregation is computed as follows:&lt;/p&gt;&lt;p&gt;Let [&lt;em&gt;h_0, h_1, …, h_n&lt;/em&gt;] be the encoded holder mentions and [&lt;em&gt;t_0, t_1, …, t_m&lt;/em&gt;] be the encoded target mentions.&lt;/p&gt;&lt;p&gt;I took all pairwise combinations of the mentions and concatenated them, creating (&lt;em&gt;n&lt;/em&gt; x &lt;em&gt;m&lt;/em&gt;) pairs in total:&lt;/p&gt;&lt;p&gt;[&lt;em&gt;h_0, t_0&lt;/em&gt;], [&lt;em&gt;h_1, t_0&lt;/em&gt;], …, [&lt;em&gt;h_n, t_0&lt;/em&gt;]&lt;/p&gt;&lt;p&gt;[&lt;em&gt;h_0, t_1&lt;/em&gt;], [&lt;em&gt;h_1, t_1&lt;/em&gt;], …, [&lt;em&gt;h_n, t_1&lt;/em&gt;]&lt;/p&gt;&lt;p&gt;…&lt;/p&gt;&lt;p&gt;[&lt;em&gt;h_0, t_m&lt;/em&gt;], [&lt;em&gt;h_1, t_m&lt;/em&gt;], …, [&lt;em&gt;h_n, t_m&lt;/em&gt;]&lt;/p&gt;&lt;p&gt;I then passed each concatenated pair through a linear layer to compute attention&lt;/p&gt;&lt;p&gt;&lt;em&gt;α_ij = &lt;/em&gt;&lt;strong&gt;w&lt;/strong&gt;&lt;em&gt;_α &lt;/em&gt;*&lt;em&gt; &lt;/em&gt;[&lt;em&gt;h_i, t_j&lt;/em&gt;]&lt;em&gt; + b_α&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;a_ij&lt;/em&gt; = softmax(&lt;em&gt;α_ij&lt;/em&gt;)&lt;/p&gt;&lt;p&gt;…and computed the final representation &lt;em&gt;x &lt;/em&gt;of the holder/target pair through an attentive sum:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/199/1*kq036bF6zKqsTBN1OKL5uQ.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Afterwards, I append embeddings for any features &lt;em&gt;Φ(x)&lt;/em&gt;&lt;strong&gt; &lt;/strong&gt;to &lt;em&gt;x&lt;/em&gt;, and pass it through a final linear layer to compute the final scores.&lt;/p&gt;&lt;p&gt;&lt;em&gt;v = &lt;/em&gt;[&lt;em&gt;x&lt;/em&gt;, &lt;em&gt;Φ_1(x)&lt;/em&gt;,&lt;em&gt; Φ_2(x)&lt;/em&gt;, &lt;em&gt;Φ_3(x)&lt;/em&gt;, &lt;em&gt;Φ_4(x)&lt;/em&gt;, &lt;em&gt;Φ_5(x)&lt;/em&gt;]&lt;/p&gt;&lt;p&gt;scores = &lt;strong&gt;w&lt;/strong&gt;&lt;em&gt;_α &lt;/em&gt;*&lt;em&gt; v + b_α&lt;/em&gt;&lt;/p&gt;&lt;p&gt;The aggregation mechanism is still suboptimal, however, given that I’m basically doing the equivalent of just summing up weighted copies of the encoded holder and target entities — more experimentation is necessary on this point. Moreover, I’d like to take into account the proximity of the holder and target mentions into the attention mechanism — giving more attention to entity pairs that are closer or co-occur in a sentence.&lt;/p&gt;&lt;h3&gt;Addition of Features&lt;/h3&gt;&lt;p&gt;The second thing I did this week was add features to my model in accordance with my error analysis from last week. 50-dimensional embeddings for each features are appended to the entity pair representation right before the last step of the model, where the entire vector is then passed through a linear layer to compute the final scores.&lt;/p&gt;&lt;p&gt;I took into account 5 features total:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Co-occurrence feature&lt;/strong&gt;: a feature for the number of sentences in which the holder/target entity co-occurs&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Holder mention frequency feature&lt;/strong&gt;: a feature for the number of times the holder entity is mentioned in the document&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Target mention frequency feature&lt;/strong&gt;: a feature for the number of times the target entity is mentioned in the document&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Holder mention rank feature&lt;/strong&gt;: a feature for the rank of the holder entity, relative to all other entities, and its number of mentions in the document&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Target mention rank feature&lt;/strong&gt;: a feature for the rank of the target entity, relative to all other entities, and its number of mentions in the document&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;As my ablation studies show, all of the features are helpful in improving the score:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/722/1*qllhecRTNgittY0GTyBQmQ.png&quot; /&gt;Ablation studies, where I removed the one of the features each time and ran the model on the dev data.&lt;/figure&gt;&lt;h3&gt;Final Plan&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;Train separate classifiers for co-occurring and non-co-occurring entities&lt;/li&gt;&lt;li&gt;Experiment with aggregation functions&lt;/li&gt;&lt;li&gt;Encode restraints in the loss function (if time suffices)&lt;/li&gt;&lt;li&gt;Write the final paper and get a demo up and running&lt;/li&gt;&lt;/ol&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=db16f27c255d&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 23 May 2018 05:53:49 +0000</pubDate>
</item>
<item>
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: Advanced Model Update: From Definition Extraction to Entity Discovery</title>
	<guid isPermaLink="false">https://medium.com/p/ef93c2149aa2</guid>
	<link>https://medium.com/nlp-capstone-blog/advanced-model-update-from-definition-extraction-to-entity-discovery-ef93c2149aa2?source=rss----9ba3897b6688---4</link>
	<description>&lt;p&gt;Over the past few weeks, developing a dataset to test our model and flesh out this novel task has proven to be a difficult task in itself. Here, we discuss what worked, what didn’t work, and how the development of our dataset has influenced our perspective of the task, and ultimately, what we will now expect out of our advanced model.&lt;/p&gt;&lt;p&gt;To recap, we began making preliminary version of our dataset using ROUGE, cosine similarity, and skip-bigrams. In particular, given a definition-document pair, we aimed to extract sentences from the document that were most conducive to describing and re-creating the definition. From there, our model can compute latent representations of the term, sentences, and document as a whole, in order to learn how to extract these sentences we’ve chosen.&lt;/p&gt;&lt;h3&gt;The Heuristics that Failed&lt;/h3&gt;&lt;p&gt;Unfortunately, we can’t all be winners.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Cosine Similarity&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;We used &lt;a href=&quot;https://spacy.io/usage/vectors-similarity&quot;&gt;spaCy’s&lt;/a&gt; implementation of cosine similarity using context vectors. Admittedly, cosine similarity does a great job of ruling out sentences that have nothing to do with the current definition when extracting. However, there was much too little variation in the scores that sentence-definition pairs received. Often, they would range from 0.80 to 0.94, and tended to cluster around 0.83–0.87 and 0.90 to 0.93. Not only are so many sentence-definition pairs scoring so highly, it becomes a very fine line between what we should keep and what we shouldn’t.&lt;/p&gt;&lt;p&gt;We attempted to be extremely strict and only keep pairs that scored 0.94; but this often led to many great pairs being ruled out. We speculate that medical language in general tends to cluster together with respect to the rest of the vocabulary in which spaCy’s word vectors were trained. We also speculate that being a bag-of-words method in defining similarity, much of the richness in context and order that makes differences and similarity obvious at a glance are washed away. Despite the method clearly being able to separate contrived sentence-definition pairs, in the landscape of our data, it fails to draw the line the way we’d like it to.&lt;/p&gt;&lt;h4&gt;Skip-bigrams&lt;/h4&gt;&lt;p&gt;This heuristic in particular was troublesome in that, many UMLS definitions were 1–2 sentences long, while others were several paragraphs. So the idea of using the number of overlapping skip-bigrams between a sentence-definition pair will severely punish shorter glosses. Because of this, longer definitions having little relevance to a sentence may likely still match to it.&lt;/p&gt;&lt;h3&gt;The Heursitics that Worked&lt;/h3&gt;&lt;p&gt;After several attempts at tuning the above heuristics, we decided to look for more. The following heuristics are how our final dataset will be constructed.&lt;/p&gt;&lt;h4&gt;Google’s Top 10,000 Words&lt;/h4&gt;&lt;p&gt;There’s currently a &lt;a href=&quot;https://github.com/first20hours/google-10000-english&quot;&gt;repository&lt;/a&gt; containing the top 1000 and top 10000 words according to n-gram frequency analysis of Google’s Trillion Word Corpus. In particular we are using the &lt;strong&gt;no swears &lt;/strong&gt;list.&lt;/p&gt;&lt;p&gt;Given the roughly 800,000 glosses that UMLS provides us, we shave this down to roughly 165,000 by removing all definitions that contain an synonym that is also contained within the Google no-swears top 10k list. This drastically reduces our search space when creating examples, and ultimately we are okay with it since common words are trivial to define.&lt;/p&gt;&lt;h4&gt;First 15%&lt;/h4&gt;&lt;p&gt;Given a definition-document pair, only attempt extraction if at least one of the aliases (synonyms) that the definition defines occurs within the first 15% of the sentences.&lt;/p&gt;&lt;h4&gt;Word Embeddings&lt;/h4&gt;&lt;p&gt;Following the first two heuristics, given definition-document pairs that make it through these filters we then extract &lt;strong&gt;all&lt;/strong&gt; sentences containing &lt;strong&gt;any &lt;/strong&gt;of the alias for the document.&lt;/p&gt;&lt;p&gt;We then calculate a similarity score between each sentence and the gold standard definition of the term. We do so by using pre-trained word vectors, namely Glove vectors, to better represent sentences. Each sentence is represented as the average of all its word vectors and similarity is defined as the Euclidean distance between the gold standard vector and the sentence vector. Given these distances, we sort them and choose the smallest 5 sentences if there are that many. We believe that through this, we are using better representations of sentences as opposed to the heuristics we tried previously. Although we did consider training our own set of word vectors (the large size of the Semantic Scholar corpus would allow us to do this), we felt that given the time constraints, Glove vectors were sufficient for now.&lt;/p&gt;&lt;p&gt;We then filter out the document to include only mentions of the entity that we are trying to extract (or its aliases). This approach is made possible by UMLS pairing all definitions with all of the aliases that it defines.&lt;/p&gt;&lt;p&gt;After choosing the sentences for each term-document pair, we then incorporate aliases when creating the final training examples. Recall that each training example includes a term, a gold standard definition, sentences within the document, and the target vector. In order to encourage the model to associate synonyms with each other, we can swap out the terms and its aliases in the target sentences, randomly inserting an alias or the term in places that another alias or term might be. This will not only give us a way to produce more training examples, it will also help the model understand the contexts of similar words, which might help it discover entities.&lt;/p&gt;&lt;h3&gt;Reframing the Problem to Entity Discovery&lt;/h3&gt;&lt;p&gt;Originally, our task was to generate definitions of entities consistent with our corpus. We then reframed the task as an extractive process.&lt;/p&gt;&lt;p&gt;The dataset described above however, will allow us to solve a task that could be described as a ‘superclass’ of definition extraction, which essentially aims to extract all sentences &lt;strong&gt;relevant&lt;/strong&gt; to an entity as opposed to only sentences that help &lt;strong&gt;define &lt;/strong&gt;it. We call it ‘Entity Discovery’, a term coined by AI2 when they originally proposed this type of task during the early stages of the capstone. Given our ranking scheme, we will still tend towards selecting sentences conducive to definitions, but we’re not quite confident enough that every sentence our heuristics will choose resemble a definition or add to one.&lt;/p&gt;&lt;p&gt;Rather, we now see the potential of our model (which, given this dataset, does not have to change at all!). This new dataset will allow us to train a model to learn latent representations of queries and map them to latent representations of sentences. Note that often times, chemical and medical terms have numerous aliases that take different but systematic forms. Given a reasonably trained model on such data, it should theoretically generalize to novel terms and learn what synonyms would look like and the contexts in which they would appear, ones that have not been added to KBs yet, aiding researchers and medical students to learn about ill-defined terms that have synonyms and reference that have not yet been fully documented.&lt;/p&gt;&lt;p&gt;We call this a ‘superclass’ of definition extraction presumably because, if we were successful at extracting all sentences pertinent to an entity, that definition extraction would simply take a subset of these sentences.&lt;/p&gt;&lt;h3&gt;In Conclusion&lt;/h3&gt;&lt;p&gt;It has taken some time and experimentation to find our footing in creating this dataset, but we’ve found it. The scripts are running, the data looks reasonable, and we are excited to finally see how our model will perform.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=ef93c2149aa2&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/advanced-model-update-from-definition-extraction-to-entity-discovery-ef93c2149aa2&quot;&gt;Advanced Model Update: From Definition Extraction to Entity Discovery&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</description>
	<pubDate>Wed, 23 May 2018 05:34:45 +0000</pubDate>
</item>
<item>
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Week 9 Milestone: Advanced model attempt #2 (continued)</title>
	<guid isPermaLink="false">https://medium.com/p/97cca6e26c14</guid>
	<link>https://medium.com/@hongnin1/week-9-milestone-advanced-model-attempt-2-continued-97cca6e26c14?source=rss-c450eb982161------2</link>
	<description>&lt;p&gt;A little recap from our &lt;a href=&quot;https://medium.com/@hongnin1/advanced-model-attempt-2-continued-49da49607c01&quot;&gt;Advanced model attempt&lt;/a&gt; #2.&lt;/p&gt;&lt;p&gt;During week 7 we implemented BLEU score, which is a crucial indicator of the performance of our prediction. And during this week, we have been trying to improve our performance on primarily BLEU score and training loss. The result is not satisfying yet, but we are getting closer to our goal.&lt;/p&gt;&lt;p&gt;Earlier we discovered that our BLEU score and training batch loss stopped improving after a few thousand batches. And from Ari’s suggestion, we changed our optimizer from Adam to Stochastic Gradient Descent with learning rate decay, hoping to see the improvements. But we now discovered that it might not be our issue after a week of trying and tuning. Here is the performance we got with SGD:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/796/0*vrNjGo0XJnLzRFO9.&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/711/0*UW7xg-U7JSLSfGvh.&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/697/0*tASlStdM-dv0Nzkh.&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Compared to what we had last week, we didn’t see a dramatic improvement.&lt;/p&gt;&lt;p&gt;Therefore, we believe that we have issues with other parts of our code. To find out which part causes our issue, we decided to run others’ code and compare the results with ours. Originally, with the same data set, the “&lt;em&gt;Show, Attend and Tell&lt;/em&gt;” model (&lt;a href=&quot;https://github.com/yunjey/show-attend-and-tell&quot;&gt;https://github.com/yunjey/show-attend-and-tell&lt;/a&gt;) converged to a slightly higher BLEU score but much lower loss than our implementation. In order to find where the issue came from, we substituted different parts of their model with ours and observed the result. After several trials, we discovered that when we substitute only the model itself (keeping their data pre-processing and train solver), we got similar result to their original model. Thus we drew the conclusion that the issue lies in our data pre-processing implementation.&lt;/p&gt;&lt;p&gt;The first thing we will do for this week is to address this issue. We will spend more time on investigating the implementation of our data pre-processing and try to find out the issue with that. Secondly, if that can go well, we will try out even more architectures based on the approach introduced by the paper (&lt;a href=&quot;https://arxiv.org/pdf/1704.07489.pdf&quot;&gt;https://arxiv.org/pdf/1704.07489.pdf&lt;/a&gt;) as it doesn’t publish its hyperparameters. Last but not the least, after fixing the issues above we can continue our attempt to add another language encode (inspired by &lt;a href=&quot;https://arxiv.org/pdf/1704.07489.pdf&quot;&gt;this paper&lt;/a&gt;) to improve the fluency of the annotation, as mentioned last time.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=97cca6e26c14&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 23 May 2018 04:53:36 +0000</pubDate>
</item>
<item>
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Incorporating more Data</title>
	<guid isPermaLink="false">https://medium.com/p/ff44ae10d41e</guid>
	<link>https://medium.com/@ryanp97/incorporating-more-data-ff44ae10d41e?source=rss-6378d85d3a9b------2</link>
	<description>&lt;p&gt;Based on the results of last week’s hyper-parameter tuning, I wanted to incorporate more data to see if the issue was with the dataset or not. So this week I worked on adding the Kyoto corpus as well as Japanese WordNet (parallel corpus) definitions and examples into my dataset.&lt;/p&gt;&lt;p&gt;I began parsing the graphs from the Kyoto Corpus similar to how I did with the Tanaka Corpus earlier this week until Michael Goodman, the linguistics grad student I have also been working with gave me access to his preprocessed version. Michael had split up the data in many tiny chunks for each corpus such that each subdirectory was a subset of the actual data. Also the data was stored in a different format than I was expecting, but it was easily converted to the Penman format using the mrs-to-penman script mentioned in earlier blog posts.&lt;/p&gt;&lt;p&gt;In total, this allowed me to triple the size of my dataset from ~124,000 examples to ~325,000 examples (the result of combining Tanaka, Kyoto, and WordNet corpora from Michael). Although this is not as significant as I was hoping, it adds a lot of variety to the types of sentences and graphs that the model has been trained on up until this point.&lt;/p&gt;&lt;p&gt;The Tanaka Corpus is very casual in nature since it was essentially crowd-sourced by a teacher asking his students to translate sentences for him. As a result, the sentences are usually in casual speech form, some examples are from songs, some of the translations include mistakes, etc. The Kyoto Corpus, on the other hand, was created from manual translation of Wikipedia articles with the purpose of the data being used for travel brochures and similar tasks in mind. As a result, a translation pair from the Kyoto Corpus is likely to contain named entities that are not often found in the other translation pairs in the dataset, which causes errors mentioned later in this post. The Japanese WordNet examples are the definitions of words and examples of the words being used in sentences. This also seems like it may contribute to the issues mentioned later.&lt;/p&gt;&lt;p&gt;Something to note is that the preprocessed data I got from Michael contained a different version of the Tanaka Corpus than the one I am currently using. It seems to have been segmented differently and/or seems that it might be a non-current version since it was shipped with the Jacy grammar. So the Tanaka Corpus that came with Michael’s data accounts for ~1,000/325,000 examples which is significantly fewer parsed graphs than I was able to obtain.&lt;/p&gt;&lt;p&gt;In the coming week, I want to experiment with training a model with the Michael’s dataset combined with my current dataset as well as different subsets of the data (i.e. just the Kyoto Corpus or just the Japanese WordNet corpus).&lt;/p&gt;&lt;h4&gt;Retraining the Baseline&lt;/h4&gt;&lt;p&gt;After I preprocessed the data from Michael, I chose the best model from my previous attempts (though they were all pretty similar in performance), which happened to be my initial baseline, and trained it on Michael’s combined dataset. I was optimistically hoping for improved performance in terms of predicate precision and recall, but the model performed much worse.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*IhuwDSj7OrdW4n_Z9bivNA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Using the same concept as last week, I calculated the most commonly mis-predicted/overly-predicted predicates:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*XGtwfRJBrQcPyptwMBth5Q.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Just like with the previous models and dataset, the model still has issues mis-predicting abstract predicates and grammar concepts like particles. Something new to the list is the named abstract predicate. This was not even in the top 20 mis-predicted predicates in any of the previous models, but it shot up to the top of the charts with this new dataset. Like I mentioned earlier, the cause for this can most likely be attributed to the Kyoto Corpus and Japanese WordNet translation pairs containing many more named entities compared to the Tanaka Corpus.&lt;/p&gt;&lt;p&gt;Something else that I noticed is that this new dataset resulted in many more predictions having large length differences. In the baseline model, ~2,200/12,000 translation pairs differed by 5 or more predicates. With this new dataset, ~4,400/13,000 translation pairs differed by 5 or more predicates. One possible reason for this is the Kyoto Corpus — the Kyoto Corpus has translation pairs for both titles and summaries. The length difference between these is usually quite large since summaries are just longer by nature and carry much more semantic meaning.&lt;/p&gt;&lt;h4&gt;Future Work&lt;/h4&gt;&lt;p&gt;Like I mentioned earlier, I want to continue experimenting with different subsets of the data for training the model. I mentioned last week that I wanted to try implementing parent feeding into the decoder to try and force the model to really learn the semantic meanings of the non-terminals. I didn’t have time to do that for this blog post, but it is something that is a possibility for the next blogpost and/or the final presentation. Something else that I plan to try is ELMo embeddings as suggested by Yejin. OpenNMT currently does not have ELMo embedding support, as far as I am aware, so I may switch back over to AllenNLP to explore this addition to the model.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=ff44ae10d41e&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 23 May 2018 04:48:56 +0000</pubDate>
</item>
<item>
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: Adversarial Data Collection Pilot &amp; Reddit Comment Storage Design (Advanced Attempt II Continue)</title>
	<guid isPermaLink="false">http://cse481n-capstone.azurewebsites.net/?p=114</guid>
	<link>http://cse481n-capstone.azurewebsites.net/2018/05/22/adversarial-data-collection-pilot-reddit-comment-storage-design-advanced-attempt-ii-continue/</link>
	<description>&lt;h3&gt;Adversarial Data Collection&lt;/h3&gt;
&lt;h4&gt;Motivation for adversarial Data collection&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;As we had shown previously, our model is very fixated with certain keywords when deciding that something is hate speech. For example, consider this sentence from a support subreddit:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;“My wife was raped My wife tonight was raped, she doesn’t want to go to the police.”&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our best model considers this to be hate with 89% certainty. In particular it thinks the use of “raped” and “wife” (as opposed to “husband”) demonstrates hate speech. We think this has a lot to do with hateful speech online being commonly sexist against women while there isn’t as much obvious sexism against men. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;However, we still want it to learn more sophisticated patterns. We decided that the best way to do this would be to find examples that we know are not hateful but that use the same keywords as the sentences the model deems hateful. This would add more uncertainty to the dataset, allowing us to train a more complex model. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Since we do not have any labeled data to choose from, we had to pick some sort of unlabeled data that we know in advance is going to almost always be not hateful. For this purpose, we decided to use news article headlines. Thus, these headlines are our adversarial data. They have the keywords that our model thinks are hateful, but they are almost guaranteed to not be hateful at all. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We considered two choices for collecting news article headlines. We could either use Reddit again, or use the Bing News search API through Azure. With Reddit, we would have to filter out everything except for a whitelist of news congregation subreddits and then search through the titles of the posts there for our keywords. In this type of subreddit, it’s enforced by moderators that the title of the post be the same as the headline of the article being linked. Because of our existing data collection experience, we would not need to learn anything new to go this route. With Bing, we would have to get access to their Search API resource in Azure and then pass it our keywords and manipulate the results. We ended up going with Bing because we decided that, as a search engine, Bing would be better at deciding the most relevant headlines for our keywords and thus provide us with better data for less effort.&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;Pilot pipeline&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;First, we want to extract ngrams from reddit posts that are labeled as hate by our current best model. These ngrams would later be ranked using document frequencies, and the most frequent ngrams would be used to search for news titles through Bing Search API.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We used the model with gru seq2vec encoder, 50d glove twitter embeddings, and ELMo, trained on combined twitter dataset to make predictions on 18k collected reddit posts (removing all posts with less than 4 tokens after preprocessing). We then extracted all posts labeled “hate” by this model. A total number of 3410 posts were labeled “hate”. Then we extracted all 1-3 ngrams after removing stop words and punctuations but keeping the special character apostrophe (‘) in place because there are words like can’t, won’t, don’t, etc. The output file contains multiple lines of lists. Each list consists of all 1-3 grams of a posts classified as “hate”, and each line is for each post.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;After we generated a list of ngrams for each post, we need to process it to get ngrams that our model feels really hateful. Therefore, we decided to find the most frequent ngrams that appeared in all posts that are labeled as hate.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;First, we need to do some data preprocessing.&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We turned all words into lowercase and removed all non-alphabetic characters (including numbers) except punctionations within word like “can’t”. &lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Then we split each ngram into words, for each word, we used NLTK toolkit to tag it, and lemmatized back to the stem form. &lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Lastly, we decided to remove all unigrams after previous steps, because we found unigrams less descriptive comparing to bigram or trigrams.&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Then we compute the document frequency, and come up with a list of top 100 ngrams for us to explore on search engine.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For example, our ngrams looks like [“year old”, “what’s difference”, “year ago”, “old girl”, “year old girl”, “walk bar” ….]. We are aware that words like “year old”, “year old girl” looks really similar, and we would like to combine them, however, because this requires a lot more engineering and this is a pilot, we think it would be worth doing as a future work.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Once we have the ngrams we want to use as keywords for finding headlines, we send them through Bing’s news search feature. This required some experimenting to get right because if you just give Bing the keywords it will find articles that have them in the body, or sometimes just on the same website as the article. We had to make sure to use the “intitle:” restriction for each keyword of the ngram in question. As well, we found that Bing will silently filter out keywords that it thinks are offensive. For example, if you just search for articles with “intitle: jew” it removes that keyword and does an empty news search instead, which just returns the most recent articles published. This led to us doing a second round of filtering on top of the Bing results to make sure the sentences we give to the model actually have the keywords we want to test it against. As well, because our Azure tier doesn’t include Bing search, we had to use a 7-day free trial. For future work on this project we would have to change our Azure tier to keep using Bing, or we would have to switch to using Reddit for article headlines.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In the end we took the top 100 ngrams from the hate predicted posts and got up to 20 articles for each one. This resulted in almost 900 article headlines. However, when we ran these through our model we only got 73 of them labeled hate. This was an unexpected result, because we were expecting these headlines to confuse the model. On further investigation, we realized that because of Bing’s built in silent filtering, we were much less likely to get headlines that had these potentially controversial phrases in them. This happens even with safe-search turned off. If we had known about this when we were first brainstorming, we would have probably gone with using Reddit to get headlines instead.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Here are a few of the headlines that made it through Bing’s silent filter that the model did think were hate speech:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Teen scarred for life after masked thug hurled acid over her in racially-motivated attack because she dated a black guy&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;I had to brutally murder the black gay guy because he hit on me&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Starbucks drops the Jewish group Anti-Defamation League from its racial bias training after activists criticized their support of Israel and their failure to endorse Black …&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Neo-Nazi who beat a black man with a 2-by-4 in Charlottesville pleads guilty&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Kansas Cops Detain Black Man Because Of Vegetation On His Windshield&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Can a Black Person Truly Love Black People if They Date Outside the Race?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Community responds after woman calls police on black people barbecuing&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;University survey asking if students want to know whether ‘black people hate America’ draws ire&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Community responds after woman calls police on black people barbecuing&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Oakland Residents Throw “BBQing While Black” Party After White Woman Called Police on Black Men for Grilling&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;3 are arrested in the stabbing of black man that officials call a hate crime&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;WATCH: Racist campers call black man a ‘n*gger’ 30 times after he asks them to leave his street&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Understanding why you don’t call a black man a boy&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Woman can’t get DirecTV to cancel service&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Why you can’t get ‘Chelsea Dagger’ out of your head&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Madonna: I Can’t Get Taylor Swift’s Songs Out of My Head&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Spieth can’t seem to get anything right at Sawgrass&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;‘I hate her, can’t stand the b’: Daniel Heazlewood jailed for 11 years for killing his mother&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;You Can’t Tell Kids to ‘Just Say No’ to Legal Weed&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;It’s clear that these were chosen based on keyword matching. We hope that including them in the training dataset in the future could make the model better, but we might have to iterate on this process using Reddit for headlines instead if we want more and better adversarial data.&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;future work&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;As mentioned above, currently we are using the most common non-trivial ngrams in all posts that are classified as hateful. However, we talked about another interesting way: found ngrams that have greatest ratio of df in predicted hate posts to df in predicted none posts. By doing so, we would be more sure about these ngrams’ contribution to the hatefulness of the post. This is a data processing step we can do in a future iteration to increase the adversarial impact of the data, since right now it is actually mostly clear to the model that these headlines should be labeled none.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Also, from our chosen ngrams, we found a lot of words that looks really similar. For example, we may have “year old” and “year old girl” appear at the same time, we can use other techniques to get rid one of them, like stemming and lemmatization . However, it might be computational challenging because of the large amount of ngrams we have. We think this is definitely a direction to look into.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Currently we are still working with unlabeled data, so we just used everything the model predicted as hate. Once our initial set of reddit data is labeled we could use that to make better decisions about what ngrams to use, such as by leaving out ngrams from true positives and true negatives from the start, and just dealing with the examples the model gets wrong. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Finally, we would like to try out other ways to collect headlines in the future, especially if there is no way around Bing’s silent filter. Using Reddit news subreddits like described up above is a possible alternative.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Context extraction for Reddit Comment Design&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Here we present a high-level idea about how to store Reddit comment objects in DBMS for future context retrieval.  This is a substantial project on its own, therefore the implementation might not happen this quarter. We hope our exploration on this subject would pave the path for future work. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Last week, we presented an interesting paper, Anyone Can Become a Troll:&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;&lt;br /&gt;
&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Causes of Trolling Behavior in Online Discussions by Cheng et. al. To recap, the paper concludes that Negative Mood and Negative Discussion Context are the two causes of trolling behavior online. Moreover, the paper states that when training and evaluating a logistic regression classifier, “features relating to discussion context are most informative” (Cheng et. al.). &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;This gives us an idea that could potentially improve our model performance on reddit comments. Since a reddit post/discussion format is very much like CNN.com’s comment section where the post takes the role of an article, and all subsequent comments either spawn directly from the post or from previous comments, we believe the context of comments would also be a very useful feature for us to use when we use reddit comment data in the future (note: we are collecting reddit posts at this moment, but the data collection pipeline could easily be ported to reddit comments). &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;A reddit comment JSON object looks like this in the comment dumps we get from pushshift.io: &lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;{
    'author': 'LysergicOracle',&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'author_flair_css_class': None,&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'author_flair_text': None,&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'body': '&amp;lt;3',&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'can_gild': True,&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'controversiality': 0,&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'created_utc': 1512086400,&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'distinguished': None,&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'edited': False,&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'gilded': 0,&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'id': 'dql1dzn',&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'is_submitter': False,&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'link_id': 't3_7go27t',&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'parent_id': 't1_dql0d4o',&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'permalink': '/r/freefolk/comments/7go27t/jonerys_first_fight_306_ac_colorized/dql1dzn/',&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'retrieved_on': 1514212661,&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'score': 2,&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'stickied': False,&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'subreddit': 'freefolk',&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'subreddit_id': 't5_37tpy',&lt;/span&gt;

&lt;span style=&quot;font-weight: 400;&quot;&gt;    'subreddit_type': 'public',&lt;/span&gt;
&lt;span style=&quot;font-weight: 400;&quot;&gt;}&lt;/span&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Here we get a comment from subreddit freefolks and the comment’s content is ‘&amp;lt;3’. “link_id” is the id of the post under which this comment is posted, and “parent_id” is the id of the comment or post under which this comment is posted. In the case where the current comment is a top-level comment (directly posted under the post, with no parent comment), “link_id” and “parent_id” would be the same. We can also use “created_utc” to figure out which comment was created first if they are siblings, thus knowing which comment could have had an influence on the other.  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;To recreate the discussion context of a comment, we are thinking of two kinds of queries: 1) queries that find all “ancestor comments” of the current comment; 2) queries that finds all “sibling comments” that were posted at an earlier timestamp than the current comment. Then we can incorporate the level of negativity of these comments as features when learning or making predictions of the current comment.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;To store these comment objects in a DBMS for fast retrieval and context construction, we did some research on which DBMS to use. There are many DBMS systems out there that support JSON datatype, but the two we mainly looked at were MongoDB and PostgreSQL. Both of them are open source projects. MongoDB supports native JSON storage and has its own set of query commands. PostgreSQL was initially designed to be a SQL database, but it has supported JSON datatype for a few years. While both were valid choices, PostgreSQL supports SQL like query language on JSON data. It also supports recursive queries, which is perfect for the purpose of context tree construction. Therefore, we decided PostgreSQL is a better choice for Reddit comment storage.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For future work, we would spin up a PostgreSQL server and write a client-side package for comment context retrieval.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Advanced Model Attempt Update&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Last week, we implemented an attention LSTM model and we reported that there was a bug in pytorch. However, after doing some checking, we realized that there was a dimension error on our side.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Now we have complete statistics for our attention LSTM/GRU model:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;All models are trained and evaluated on twitter Waseem dataset.&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;50d&lt;/td&gt;
&lt;td&gt;LSTM w/o ELMo&lt;/td&gt;
&lt;td&gt;LSTM w/ ELMo&lt;/td&gt;
&lt;td&gt;GRU w/o ELMo&lt;/td&gt;
&lt;td&gt;GRU w/ ELMo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;F1&lt;/td&gt;
&lt;td&gt;0.7907&lt;/td&gt;
&lt;td&gt;0.7879&lt;/td&gt;
&lt;td&gt;0.7823&lt;/td&gt;
&lt;td&gt;0.7770&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Precision&lt;/td&gt;
&lt;td&gt;0.7961&lt;/td&gt;
&lt;td&gt;0.8018&lt;/td&gt;
&lt;td&gt;0.7781&lt;/td&gt;
&lt;td&gt;0.7851&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recall&lt;/td&gt;
&lt;td&gt;0.7862&lt;/td&gt;
&lt;td&gt;0.7784&lt;/td&gt;
&lt;td&gt;0.7875&lt;/td&gt;
&lt;td&gt;0.7708&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Accuracy&lt;/td&gt;
&lt;td&gt;0.8194&lt;/td&gt;
&lt;td&gt;0.8207&lt;/td&gt;
&lt;td&gt;0.8056&lt;/td&gt;
&lt;td&gt;0.8091&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;100d&lt;/td&gt;
&lt;td&gt;LSTM w/o ELMo&lt;/td&gt;
&lt;td&gt;LSTM w/ ELMo&lt;/td&gt;
&lt;td&gt;GRU w/o ELMo&lt;/td&gt;
&lt;td&gt;GRU w/ ELMo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;F1&lt;/td&gt;
&lt;td&gt;0.7907&lt;/td&gt;
&lt;td&gt;0.7864&lt;/td&gt;
&lt;td&gt;0.7913&lt;/td&gt;
&lt;td&gt;0.7931&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Precision&lt;/td&gt;
&lt;td&gt;0.8076&lt;/td&gt;
&lt;td&gt;0.7873&lt;/td&gt;
&lt;td&gt;0.7931&lt;/td&gt;
&lt;td&gt;0.7999&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recall&lt;/td&gt;
&lt;td&gt;0.7798&lt;/td&gt;
&lt;td&gt;0.7854&lt;/td&gt;
&lt;td&gt;0.7895&lt;/td&gt;
&lt;td&gt;0.7876&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Accuracy&lt;/td&gt;
&lt;td&gt;0.8242&lt;/td&gt;
&lt;td&gt;0.8132&lt;/td&gt;
&lt;td&gt;0.8180&lt;/td&gt;
&lt;td&gt;0.8221&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;200d&lt;/td&gt;
&lt;td&gt;LSTM w/o ELMo&lt;/td&gt;
&lt;td&gt;LSTM w/ ELMo&lt;/td&gt;
&lt;td&gt;GRU w/o ELMo&lt;/td&gt;
&lt;td&gt;GRU w/ ELMo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;F1&lt;/td&gt;
&lt;td&gt;0.7944&lt;/td&gt;
&lt;td&gt;0.7745&lt;/td&gt;
&lt;td&gt;0.7948&lt;/td&gt;
&lt;td&gt;0.7725&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Precision&lt;/td&gt;
&lt;td&gt;0.7902&lt;/td&gt;
&lt;td&gt;0.7870&lt;/td&gt;
&lt;td&gt;0.7874&lt;/td&gt;
&lt;td&gt;0.8001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recall&lt;/td&gt;
&lt;td&gt;0.7995&lt;/td&gt;
&lt;td&gt;0.7659&lt;/td&gt;
&lt;td&gt;0.8074&lt;/td&gt;
&lt;td&gt;0.7580&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Accuracy&lt;/td&gt;
&lt;td&gt;0.8166&lt;/td&gt;
&lt;td&gt;0.8091&lt;/td&gt;
&lt;td&gt;0.8132&lt;/td&gt;
&lt;td&gt;0.8132&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;From our results, we see a very consistent performance (~0.79 F1 score) when attention is used. And it seems ELMo, in this case, does not introduce any help. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The reason might be attention with ELMo need much more data in order to have a performance improvement. We will train on the combined dataset in the coming days and update our statistics.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Work Cited&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://files.clr3.com/papers/2017_anyone.pdf&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Cheng, Justin et al. “Anyone Can Become a Troll: Causes of Trolling Behavior in Online Discussions.” CSCW : proceedings of the Conference on Computer-Supported Cooperative Work. Conference on Computer-Supported Cooperative Work 2017 (2017): 1217-1230.&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</description>
	<pubDate>Wed, 23 May 2018 02:11:29 +0000</pubDate>
</item>
<item>
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog 9</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-5600014144802012716.post-7945337673285975235</guid>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/05/blog-9.html</link>
	<description>&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Decided to edit code prototypes. The motivation comes from a high baseline score which uses this &lt;/span&gt;&lt;br /&gt;&lt;span&gt;approach. Namely, when generating a method, if the closest method by maximum comment Jaccard &lt;/span&gt;&lt;br /&gt;&lt;span&gt;distance is picked, a bleu score of .34 is achieved, almost 15 points higher than the actual model! &lt;/span&gt;&lt;br /&gt;&lt;span&gt;Though note em is 0 for this baseline since it's picking a different method.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;I’ve run some experiments with the code prototypes. None seem to be working that well but I have &lt;/span&gt;&lt;br /&gt;&lt;span&gt;some ideas as to why.&lt;/span&gt;&lt;/div&gt;&lt;b id=&quot;docs-internal-guid-4c49591c-89e4-bc06-8efe-ea78d3b9b1e1&quot; style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Since the dataset is slightly altered the baseline is at .336 bleu and .128 em.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;The prototype model which concats encoded prototype and utterance is at .331&lt;/span&gt;&lt;br /&gt;&lt;span&gt; bleu and .119 em, meaning it’s not using the prototype information and is the same &lt;/span&gt;&lt;br /&gt;&lt;span&gt;as the baseline. This is evident after examining the model outputs.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre-wrap;&quot;&gt;The model needs to be designed better in order to figure out what parts of the prototype to copy. This is what I'm currently figuring out.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;</description>
	<pubDate>Tue, 22 May 2018 23:38:00 +0000</pubDate>
	<author>noreply@blogger.com (nlpcapstone)</author>
</item>
<item>
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #8: More Detailed Error Analysis</title>
	<guid isPermaLink="false">https://medium.com/p/922aee66f717</guid>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-8-more-detailed-error-analysis-922aee66f717?source=rss-fad49d942bf3------2</link>
	<description>&lt;p&gt;This week, I spent quite a bit of time doing error analysis and trying to figure out what was wrong with my advanced model, and more specific ways of improving it.&lt;/p&gt;&lt;p&gt;Here is the breakdown of how the model classified various examples on the dev data, as well as what the actual label should’ve been:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/420/1*AnX6uRWHLcySgkKcuAGpRg.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Clearly, it seems that a lot of “no sentiment” examples are being identified as “positive” or “negative.” This is congruous with my previous findings that recall had been much higher than precision for positive and negative examples, as the model was falsely predicting many no sentiment examples to be positive/negative. In fact, the recall seems to be much better than the best recalls from the &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;Choi et al.&lt;/a&gt;:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/514/1*FEr4Kogbpu3vSdpoiYxHdw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;However, poor precision prevents the F1 scores from surpassing the previous paper’s.&lt;/p&gt;&lt;p&gt;To help with this, I tried experimenting a little with adjusting the thresholds. Instead of always picking the “most probable” class as the label for the example, I set the thresholds such that even if the model was 10% sure that the example is “no sentiment,” it would classify it as such. Unfortunately, this did not seem to be extremely effective on the non-training sets, and only seemed to decrease the recall in initial epochs. Eventually, as the model kept iterating through epochs, the recall surpassed precision again and F1 scores on positive and negative were ~0.2, similar to before.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/758/1*g89-UZtyPcaME8IRVLXSdg.png&quot; /&gt;Results on Development Set: With and without thresholds.&lt;/figure&gt;&lt;h3&gt;Adding Features for Number of Mentions&lt;/h3&gt;&lt;p&gt;Looking at the dataset, it became apparent that the entity pairs which hold polarity (are positive/negative) are the most frequently mentioned entities in the document, while nearly all of the rest of the entity pairs expressed “no sentiment.” However, the predictions the model is making so far doesn’t necessarily reflect that:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/856/1*-UEY0RulJNJHbt4fIMAP6g.png&quot; /&gt;Average # of mentions for each label: across all 3 datasets, the apparent trend is that mention pairs labelled “no sentiment” has less mentions than those labelled “positive” or “negative”.&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/593/1*peoDMPxJLBC7DzlDHLegCA.png&quot; /&gt;Average # of mentions based off the predicted labels: especially on the development set, the pairs predicted “no sentiment” actually have a greater number of holder and target mentions, contrary to what should be the case.&lt;/figure&gt;&lt;p&gt;This suggests that adding features for the number of times the holder and target entities are mentioned may be useful.&lt;/p&gt;&lt;h3&gt;Is the Model Just Memorizing Entity Pairs?&lt;/h3&gt;&lt;p&gt;To answer this question, I looked into how the model classified the same pair of entity in multiple document. For example,the pair “China” to “US” occurred in 4 different documents. I wondered if each time the pair appeared, they were being classified the same way (“uniformly”) — i.e., was the model memorizing a label for the pairs and assigning it each time the pair came up? Given the architecture of my model, I hypothesized that this shouldn’t be the case. My error analysis results seemed to corroborate this hypothesis. My model wasn’t predicting the same pairs of entity the same way throughout each document. In fact, the actual labels were uniform more often than my predictions.&lt;/p&gt;&lt;p&gt;On MPQA Test Data,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Total # of entity pairs: 3601&lt;/li&gt;&lt;li&gt;# of pairs appearing multiple times (* conservative estimate): 155&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;(* Note: This represents a conservative estimate as I used exact token matching to determine whether entity pairs appeared more than once. That is to say, entities “US” and “United States” would’ve been considered different entities, and thus “US-&amp;gt;China” and “United States-&amp;gt;China” would not be considered appearing multiple times.)&lt;/p&gt;&lt;p&gt;The table below depicts how “uniform” and non-“uniform” pairs are actually classified, vs. how they are predicted. “Uniform” refers to the pair having the same sentiment label for each document it appears in. “Non-uniform” refers to the pair having a different sentiment label for at least one document.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/485/1*IGyWKDu43Qn084XzyIprNw.png&quot; /&gt;Note that entity pairs are actually uniform more than they are predicted to be uniform, suggesting that the model is not memorizing pairs and assigning labels accordingly.&lt;/figure&gt;&lt;h3&gt;Is the Model Classifying Solely Based Off of Co-occurring Sentences?&lt;/h3&gt;&lt;p&gt;From last time, I had added a feature to my model specifying how many sentences the entity pair co-occurred in. This led to the question of whether the model was paying too much attention to this feature and simply classifying low co-occurrence pairs as having no sentiments. However, analysis seemed to show this was not the case:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/486/1*pnr9nyTBh2171ej75G4CGA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Had the model just been classifying pairs that did not co-occurred frequently as no sentiment, we would’ve seen predicted “0” and “2” be much higher than predicted “1”.&lt;/p&gt;&lt;h3&gt;Entity Pairs Per Document&lt;/h3&gt;&lt;p&gt;Currently, my model is not training and labeling examples by document (i.e. one document at a time). However, doing so may improve results.&lt;/p&gt;&lt;p&gt;For each documents, most entity pairs are labelled as “no sentiments,” with a few (the most major/important entity pairs in the document) being classified as positive or negative. However, my model’s predictions do not follow that trend.&lt;/p&gt;&lt;p&gt;A typical example for a single document:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/343/1*xPQjwbBDXksKuYTSHHzM-Q.png&quot; /&gt;&lt;/figure&gt;&lt;h3&gt;Future Directions&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;Under Eunsol’s suggestion, implementing separate models for co-occurring and non-co-occurring entity pairs in documents, and integrating the two.&lt;/li&gt;&lt;li&gt;Implementing transitivity and other constraints within the loss function.&lt;/li&gt;&lt;li&gt;Adding features for # of times holder/target entities are mentioned in the document.&lt;/li&gt;&lt;li&gt;Training by document.&lt;/li&gt;&lt;/ol&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=922aee66f717&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Thu, 17 May 2018 04:25:15 +0000</pubDate>
</item>
<item>
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: Advanced Model # 2</title>
	<guid isPermaLink="false">https://medium.com/p/ca5a7f69db85</guid>
	<link>https://medium.com/nlp-capstone-blog/advanced-model-2-ca5a7f69db85?source=rss----9ba3897b6688---4</link>
	<description>&lt;p&gt;As explained in our previous blog post, our current challenge is involves constructing the dataset in an efficient and effective manner. This blog post will detail the progress made in the past week in relation to data construction and the challenges we have faced. We will also briefly highlight our plan for the next week.&lt;/p&gt;&lt;h4&gt;Training&lt;/h4&gt;&lt;p&gt;We’ve attempted training on a small subset of the data in the format the model should expect, and ran into an issue with batching.&lt;/p&gt;&lt;p&gt;As of now, we backpropagate once per batch. But at this point, latent representations of sentences and the document have already been computed. Meaning, the RNNs have already encoded every word in the document before parameters are updated. This, in conjunction with the series of affines for each sentence will likely produce a computation graph that we won’t have enough memory to backprop on.&lt;/p&gt;&lt;p&gt;We’ve now switched to backpropagation once per sentence, which will hopefully lead to faster learning.&lt;/p&gt;&lt;h4&gt;New Heuristics&lt;/h4&gt;&lt;p&gt;Last week we described a method of BIO tagging sentences that involved using ROUGE. While this method might produce good tags, we found that ROUGE was incredibly slow to run. Since we run ROUGE once for every sentence in every document, we chose to develop a difference heuristic that worked like ROUGE but was much faster. This led us to experimenting with two new approaches, which are detailed below.&lt;/p&gt;&lt;h4&gt;Skip-bigrams&lt;/h4&gt;&lt;p&gt;When learning about ROUGE, we learned of a variety of ROUGE called ROUGE-SU. Here, SU stands for Skip Bigrams and Unigrams. Skip Bigrams refer to bigrams which are formed as any subsequent pair of words in the sentence. In other words, it’s every bigram possible in a sentence such that the bigram follows sentence order. For our first approach, we decided to use the same greedy algorithm described from previous blog posts, except we try to maximize the skip bigram overlap between the reference skip bigrams and the set of sentences we choose to extract. We implemented this functionality from scratch.&lt;/p&gt;&lt;h4&gt;Cosine Similarity&lt;/h4&gt;&lt;p&gt;Cosine similarity is defined as a measure of similarity between two vectors. Essentially, it is a way of determining the cosine of the angle between two sequences of text in Euclidean space. A value tending toward 1 means that the two pieces of text are more similar, while smaller values mean there is less correlation. In NLP, this metric is used as a bag-of-words comparison, combining the words of both sequences into a master set of words, and computing the cosine similarity between each sequence’s respective frequency vector whose dimensionality is equal to the size of the set. In other words, it is the cosine of the angle between their tf-idf vectors.&lt;/p&gt;&lt;p&gt;Currently, we’re using spaCy’s implementation of cosine similarity.&lt;/p&gt;&lt;h4&gt;Example Data&lt;/h4&gt;&lt;p&gt;When we are sufficiently strict (ex. enforcing a skip-bigram intersection of at least 10 with a cosine similarity of at least 0.94) then we can get promising matches:&lt;/p&gt;&lt;p&gt;PAPER: Mechanisms of NO/cGMP-Dependent Vasorelaxation TERMs: {‘omega-Nitro-L-Arginine, N’, ‘NO2Arg’, ‘N omega Nitro L Arginine’, ‘L-NNA’, ‘Nitroarginine [Chemical/Ingredient]’, ‘NG-Nitro-L-Arginine’, ‘omega-Nitroarginine’, ‘NG-nitro-L-arginine’, ‘N(omega)-Nitroarginine’, ‘NOLA’, ‘N omega-Nitro-L-Arginine’, ‘N OMEGA NITROARGININE L’, ‘omega Nitroarginine’, ‘NOARG’, ‘NG-Nitroarginine’, ‘N(G)-Nitroarginine’, ‘NG NITROARGININE L’, ‘NG Nitro L Arginine’, ‘Nitroarginine’, ‘NG Nitroarginine’}&lt;br /&gt;TERM FOUND: True&lt;br /&gt;100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 145/145 [00:01&amp;lt;00:00, 103.03it/s]&lt;br /&gt;REFERENCE: An amino acid derivative and nitric oxide synthase (NOS) inhibitor with potential antineoplastic and antiangiogenic activities. Upon administration, NG-nitro-L-arginine inhibits the enzyme nitric oxide synthase, thereby preventing the formation of nitric oxide (NO). By preventing NO generation, the vasodilatory effects of NO are abrogated leading to vasoconstriction, reduction in vascular permeability and an inhibition of angiogenesis. As blood flow to tumors is restricted, this may result in an inhibition of tumor cell proliferation. NO plays an important role in tumor blood flow and stimulation of angiogenesis, tumor progression, survival, migration and invasiveness.&lt;br /&gt;CHOSEN: [‘NO coordinates the blood-flow distribution between arterioles and the microvasculature by regulating the diameter of small arteries.7 The importance of NO and cGMP for the regulation of vascular tone and blood pressure has been recently strengthened by the observation that mice deficient in eNOS, ANP, the ANP receptor guanylyl cyclase A, or cGKI develop hypertension.2–6,17’]&lt;br /&gt;MAX SKIPGRAM MATCHES: 38&lt;br /&gt;MAX COSINE SIMILARITY: 0.9430605549205237&lt;/p&gt;&lt;p&gt;The ‘CHOSEN’ array contains a sentence that was extracted with our heuristics. Like the reference, it mentions nitric oxide, blood flow, and regulation of vascularity. The goal is to train model on instances such as these and were able to extract sentences of such relevance.&lt;/p&gt;&lt;p&gt;We should also note that definitions define multiple terms (in other words, terms may have multiple synonyms). The extracted sentence ‘NO coordinates the blood-flow distribution between arterioles …’ itself contains none of the entities explicitly, and yet seems to align well and be indicative of it’s definition. Robustness in recognizing a given term and its synonyms, along with being able to extract sentences about that term without the term actually being in it is extremely desirable for us. A model that is able to recognize sentences about a particular technical term without the term being present would be an especially helpful research tool.&lt;/p&gt;&lt;p&gt;Unfortunately, the heuristic is not perfect and can be lead astray:&lt;br /&gt; “title”: “On the influence of various physicochemical properties of the CNTs based implantable devices on the fibroblasts’ reaction in vitro”&lt;br /&gt; },&lt;br /&gt; “e_gold”: “ A record of something that is being done, has been done, can be done, or is intended or requested to be done. Examples: The kinds of acts that are common in health care are (1) a clinical observation, (2) an assessment of health condition (such as problems and diagnoses), (3) healthcare goals, (4) treatment services (such as medication, surgery, physical and psychological therapy), (5) assisting, monitoring or attending, (6) training and education services to patients and their next of kin, (7) and notary services (such as advanced directives or living will), (8) editing and maintaining documents, and many others. Discussion and Rationale: Acts are the pivot of the RIM; all domain information and processes are represented primarily in Acts. Any profession or business, including healthcare, is primarily constituted of intentional and occasionally non-intentional actions, performed and recorded by responsible actors. An Act-instance is a record of such an action. Acts connect to Entities in their Roles through Participations and connect to other Acts through ActRelationships. Participations are the authors, performers and other responsible parties as well as subjects and beneficiaries (which includes tools and material used in the performance of the act, which are also subjects). The moodCode distinguishes between Acts that are meant as factual records, vs. records of intended or ordered services, and the other modalities in which act can appear. One of the Participations that all acts have (at least implicitly) is a primary author, who is responsible of the Act and who \”owns\” the act. Responsibility for the act means responsibility for what is being stated in the Act and as what it is stated. Ownership of the act is assumed in the sense of who may operationally modify the same act. Ownership and responsibility of the Act is not the same as ownership or responsibility of what the Act-object refers to in the real world. The same real world activity can be described by two people, each being the author of their Act, describing the same real world activity. Yet one can be a witness while the other can be a principal performer. The performer has responsibilities for the physical actions; the witness only has responsibility for making a true statement to the best of his or her ability. The two Act-instances may even disagree, but because each is properly attributed to its author, such disagreements can exist side by side and left to arbitration by a recipient of these Act-instances. In this sense, an Act-instance represents a \”statement\” according to Rector and Nowlan (1991) [Foundations for an electronic medical record. Methods Inf Med. 30.] Rector and Nowlan have emphasized the importance of understanding the medical record not as a collection of facts, but \”a faithful record of what clinicians have heard, seen, thought, and done.\” Rector and Nowlan go on saying that \”the other requirements for a medical record, e.g., that it be attributable and permanent, follow naturally from this view.\” Indeed the Act class is this attributable statement, and the rules of updating acts (discussed in the state-transition model, see Act.statusCode) versus generating new Act-instances are designed according to this principle of permanent attributable statements. Rector and Nolan focus on the electronic medical record as a collection of statements, while attributed statements, these are still mostly factual statements. However, the Act class goes beyond this limitation to attributed factual statements, representing what is known as \”speech-acts\” in linguistics and philosophy. The notion of speech-act includes that there is pragmatic meaning in language utterances, aside from just factual statements; and that these utterances interact with the real world to change the state of affairs, even directly cause physical activities to happen. For example, an order is a speech act that (provided it is issued adequately) will cause the ordered action to be physically performed. The speech act theory has culminated in the seminal work by Austin (1962) [How to do things with words. Oxford University Press]. An activity in the real world may progress from defined, through planned and ordered to executed, which is represented as the mood of the Act. Even though one might think of a single activity as progressing from planned to executed, this progression is reflected by multiple Act-instances, each having one and only one mood that will not change along the Act-instance life cycle. This is because the attribution and content of speech acts along this progression of an activity may be different, and it is often critical that a permanent and faithful record be maintained of this progression. The specification of orders or promises or plans must not be overwritten by the specification of what was actually done, so as to allow comparing actions with their earlier specifications. Act-instances that describe this progression of the same real world activity are linked through the ActRelationships (of the relationship category \”sequel\”). Act as statements or speech-acts are the only representation of real world facts or processes in the HL7 RIM. The truth about the real world is constructed through a combination (and arbitration) of such attributed statements only, and there is no class in the RIM whose objects represent \”objective state of affairs\” or \”real processes\” independent from attributed statements. As such, there is no distinction between an activity and its documentation. Every Act includes both to varying degrees. For example, a factual statement made about recent (but past) activities, authored (and signed) by the performer of such activities, is commonly known as a procedure report or original documentation (e.g., surgical procedure report, clinic note etc.). Conversely, a status update on an activity that is presently in progress, authored by the performer (or a close observer) is considered to capture that activity (and is later superceded by a full procedure report). However, both status update and procedure report are acts of the same kind, only distinguished by mood and state (see statusCode) and completeness of the information. “,&lt;br /&gt; “entity”: “act”,&lt;br /&gt; “extracted”: [&lt;br /&gt; “Since their discovery in 1952, carbon nanotubes (CNTs) have been attracting increasing attention in being applied in various areas of materials science due to their outstanding mechanical properties, high chemical and thermal stability and, in some cases, very good conductivity via an electron transfer.”,&lt;br /&gt; “Thus, at that time point, differences in fibroblasts’ proliferation rate may have been governed by different chemical composition of the samples and an increased amount of COOH species in the CNT_ox [28].”&lt;br /&gt; ],&lt;/p&gt;&lt;p&gt;Note that since the script selected this example, that the cosine similarity score between the extracted sentences and the reference were above 0.93. Not only is the cosine similarity too generous as a heuristic, the fact that the reference is so large means that it will almost always overlap with more than enough skip-bigrams to reach our skip-bigram threshold.&lt;/p&gt;&lt;p&gt;Examples like these and others are concerning, but the hope is that helpful examples like the NO example outnumber the noise that make it past our heuristics.&lt;/p&gt;&lt;h4&gt;Going Forward&lt;/h4&gt;&lt;p&gt;Our goals for the next week include fine tuning our data collection thresholds to maximize the quality of our dataset, training the model, and hopefully producing results. As we mentioned in our previous blog post, the model is ready for training, but the real challenge might be with the way we produce our dataset. In the upcoming week, we expect to experiment with new heuristics for sentence similarity and tweak the existing heuristics in order to produce the best dataset.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=ca5a7f69db85&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/advanced-model-2-ca5a7f69db85&quot;&gt;Advanced Model # 2&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</description>
	<pubDate>Wed, 16 May 2018 05:39:36 +0000</pubDate>
</item>
<item>
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Advanced model attempt #2 (continued)</title>
	<guid isPermaLink="false">https://medium.com/p/49da49607c01</guid>
	<link>https://medium.com/@hongnin1/advanced-model-attempt-2-continued-49da49607c01?source=rss-c450eb982161------2</link>
	<description>&lt;p&gt;Recap: Our previous base line and first advanced model attempts can be found &lt;a href=&quot;https://medium.com/@hongnin1/advanced-model-attempt-1-continued-b3b687dd74ae&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;This week we have made progress on building infrastructure, and we are one big step closer to our finished product. We have accomplished the goals set by our blogpost “advanced model attempt #1” linked above.&lt;/p&gt;&lt;p&gt;First of all, we have implemented BLEU score on our model to test our model’s performance:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*EGT21AAmeM8LLDzAcgun3Q.png&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*fnFU2rf6R0ArMexA13LfXA.png&quot; /&gt;BLEU score on training set&lt;/figure&gt;&lt;p&gt;Above BLEU score was run on ~1000 training images for 4k iterations.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/424/1*064QGSE0Dn-TWYUNJu06aQ.jpeg&quot; /&gt;BLEU score generated in terminal&lt;/figure&gt;&lt;p&gt;Above BLEU score was run on ~10,000 images in test set with beam search, which we implemented during this iteration of model advancement. However, we have found that continuing our model training does not decrease our loss nor increasing our BLEU score. While it makes more sense for the loss to decrease as we continue training, our model is not behaving as expected. We are trying out some small changes to our model in the hope that this will fix the problem.&lt;/p&gt;&lt;p&gt;In addition to this, we have decided to resize the images during data pre-processing instead of cropping the image and then resizing. We have implemented this change because we believe there will be information loss when we crop an image, so in order to preserve all the information we decided to keep the same image by only resize the image. As mentioned by Ari, we have changed our Adam optimizer to Stochastic Gradient Descent with learning rate decay.&lt;/p&gt;&lt;p&gt;There are two other big improvements we would like to add to our model. First, we would like to use each channel(512) of the VGG output as one attention vector instead of using each pixel(192) from every channel. We have made this change because we believe each channel contains all the information from input image, while one pixel’s information is very limited. Secondly, Inspired by &lt;a href=&quot;https://arxiv.org/pdf/1704.07489.pdf&quot;&gt;this paper&lt;/a&gt;, we would like to add another language encoder to our model in conjunction to the decoder we currently have:for every batch of input, we take turn using the image encoder(what we currently have) and the new language encoder to encode the input and then feed it into our decoder. We expect this to improve the model’s performance by increase the fluency of our annotation because our word embedding does not improve much with our current training method, and by adding another language encoder, word embedding matrix and weight matrix in LSTM will be improved with training.&lt;/p&gt;&lt;p&gt;The following annotations are generated by our current improved model with beam search:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/224/1*AwpUY7FyQDHo5ykAwixT4Q.jpeg&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*KDZSwhIKnnybX33r4DjZnQ.jpeg&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/224/1*QJzbzrx1q3PoQu2Ri2zpVw.png&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*1G0C7WtfZc4Ce3XEteQ3bA.jpeg&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/224/1*kNcdqFe3wTBbu6XjaSf6lw.jpeg&quot; /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*_cX3071uK2D7jYUs6_pRNA.jpeg&quot; /&gt;&lt;/figure&gt;&lt;p&gt;The major improvement we see from these annotations are: first of all, previously generated annotations are not complete sentences whereas sentences now have a proper ending, for example, previously our annotations would be “a group of people walking down a snow covered slope covered slope covered slope.” Also, we reduced the repetition in our annotations. Our annotations are more accurate in general and generally captures the essence of the images.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=49da49607c01&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 16 May 2018 04:02:31 +0000</pubDate>
</item>
<item>
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: Advanced Attempt II</title>
	<guid isPermaLink="false">http://cse481n-capstone.azurewebsites.net/?p=105</guid>
	<link>http://cse481n-capstone.azurewebsites.net/2018/05/15/advanced-attempt-ii-2/</link>
	<description>&lt;h3&gt;Data Collection&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In our last post, we described our final collection of Reddit sentences that we would send for labeling. We decided to add subreddit metadata to each sentence before sending it off, but this created an unexpected slight setback because of the way we spliced posts into sentences after throwing away all of the metadata associated with the original dump. We did finish connecting each sentence to its originating subreddit however, and from doing so discovered a slight issue with our data. It turned out that from doing the hate lexicon matching described earlier, we got a few posts that looked like: “[uncensored-r/Bitcoin] &amp;lt;&lt;/span&gt;&lt;i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;some original post&lt;/span&gt;&lt;/i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;&amp;gt; The following post by vichuu is being replicated because the post has been silently removed.” In the end we decided to add the subreddit “noncensored_bitcoin” to our blacklist and removed all its sentences from our dataset, but this decreased our dataset size to ~18k, down 2k from the last blog post. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In our sixth blog post we showed the top 10 subreddits for quantity of posts. Here are the top 10 subreddits just in our dataset. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;As expected, we have a lot of MeanJoke sentences as well as sentences from other joke subreddits that probably have similar language structure. The inclusion of the hate lexicon set of sentences most likely helped bring diversity to this final set of sentences. Otherwise, it looks like a good collection of discussion subreddits. As well, since there are 18k sentences and the top 10 subreddits only sum to ~6000 of them we once again see the sparse nature of our data’s origins. &lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Subreddit&lt;/td&gt;
&lt;td&gt;Number of Sentences&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MeanJokes&lt;/td&gt;
&lt;td&gt;2964&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AskReddit&lt;/td&gt;
&lt;td&gt;706&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Jokes&lt;/td&gt;
&lt;td&gt;565&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;darkjokes&lt;/td&gt;
&lt;td&gt;387&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;depression&lt;/td&gt;
&lt;td&gt;284&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;relationships&lt;/td&gt;
&lt;td&gt;280&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DebateConservatives&lt;/td&gt;
&lt;td&gt;253&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;offmychest&lt;/td&gt;
&lt;td&gt;227&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;raisedbynarcissists&lt;/td&gt;
&lt;td&gt;168&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;relationship_advice&lt;/td&gt;
&lt;td&gt;166&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Advanced Model Attempt&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We implemented a new model which incorporates attention.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The way we did it is by writing an seq2seq (in this case, is LSTM) encoder with attention and then take the last dimension of each sequence as our vector representation of the whole sentence.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For example, if we have (batch_size, sentence_length, dim), our vector representation will become (batch_size, dim).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;After that, we passed it through a 2-layer feedforward neural network and map the output to a 2-dimensional vector which represents each class (hate, none in this case). Then we do a softmax and pick the one with the highest probability as our prediction.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Statistics&lt;/b&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;50d&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Without ELMo&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;0.7907&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;0.7961&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;0.7862&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;0.8194&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;100d&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Without ELMo&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;0.7907&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;0.8076&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;0.7798&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;0.8242&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;200d&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Without ELMo&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;0.7944&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;0.7902&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;0.7995&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;0.8166&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;b&gt;Unexpected Problem encountered:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;There was a bug in pytorch saying “fn” is undefined when we want to train LSTM with ELMo using CUDA, and we have no luck on fixing that. After googling it, it turns out to be a problem related to pytorch 0.3.1 and it’s fixed in a later version like 0.4.0. Since Allennlp requires pytorch 0.3.1, we may have to try something else instead.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We will try to use GRU instead (hopefully it works) and report all metrics in detail in next week’s blog post.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Demo Frontend&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Previously, we used default AllenNLP frontend for demoing purpose. Although it was easy to set up and use by us the developers who knew exactly what numbers we are looking for, it was not intuitive to use at all for other people. The default demo page looked like this:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; class=&quot;alignnone size-full wp-image-107&quot; height=&quot;922&quot; src=&quot;http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/old_demo.png&quot; width=&quot;2712&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In order to make the demo page more straightforward, we decided to only keep the most relevant fields of the model output: predicted label and class probabilities. Since it is also interesting to see how prediction changes every time a new word is entered, we designed the page to make a prediction everytime a blank space or ENTER is entered in the text input box (in addition to the Predict button being hit). Our new demo page looked like this (serving a different model):&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; class=&quot;alignnone size-full wp-image-108&quot; height=&quot;1034&quot; src=&quot;http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/new_demo.png&quot; width=&quot;2758&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;As shown above, while the overall page format looked the same (the left half is input, the right half is output), the output display changed quite a bit. The predicted label is nicely put into a sentence and highlighted under “Prediction”, and the class probabilities are in a table with the “Hate probability” entry highlighted under “Summary Breakdown”. We also added hate probability visualization at the end. A linear gradient from bright green to red represented hate probability from 0% to 100%, a dark vertical line representing the predicted hate probability of the current sentence, and a small label under the dark line showing the hate probability. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Not only is the new demo page easier to read and see the changes when each word is entered, this simple output display could potentially be ported to AMT surveys in the future if we want turkers to come up with hard to detect examples.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Next Steps:&lt;/h3&gt;
&lt;p&gt;On the data collection front, we will push forward in the process of getting our reddit dataset labeled.  For modeling, we would perform more hyperparameter tuning and error analysis as well as incorporating ELMo into the current model, and hopefully, we get to explore potential ways to make neural nets work with relatively small datasets like these twitter datasets that we have.&lt;/p&gt;</description>
	<pubDate>Wed, 16 May 2018 03:27:56 +0000</pubDate>
</item>
<item>
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog 8</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-5600014144802012716.post-6025423166932572092</guid>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/05/blog-8.html</link>
	<description>I'm focusing now on combining coding patterns from the new class at test time with the patterns learned from the training set. This means both the code and javadocs for all the other methods are now being utilized as well, as opposed to the previously where just the method name is used. This is really interesting, both from code and language understanding, but also very challenging. This week I've been doing analysis on this approach, and preparing the dataset for this modified task through filtering heuristics.&lt;br /&gt;&lt;br /&gt;I ran an experiment to empirically test whether using the method implementations will help. For each method whose code your supposed to generate, I picked the class method which maximized the bleu and em score. This provides a soft upper bound. This naive baseline gets a bleu of .42(2 times the state of the art!) and an em of 0.06. When I randomly picked another implementation(soft lower bound) I got bleu of .13 which is half the state of the art.&lt;br /&gt;&lt;br /&gt;Additionally, to learn these coding patterns, the documentation and code needs to be of high quality hence I've been filtering the data to reduce the noise. Here are the criteria, if any are true the class is removed. This so far has cut down the number of classes from around 2 million to a hundred thousand. There's more work still to be done here.&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;A method uses identifiers not present in the class and occurring in the training set under 7 times.&lt;/li&gt;&lt;li&gt;Use of too many integer literals&lt;/li&gt;&lt;li&gt;Too few method javadocs.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;br /&gt;</description>
	<pubDate>Tue, 15 May 2018 18:08:00 +0000</pubDate>
	<author>noreply@blogger.com (nlpcapstone)</author>
</item>
<item>
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Error Analysis and the Transformer Architecture</title>
	<guid isPermaLink="false">https://medium.com/p/5c8a38264cae</guid>
	<link>https://medium.com/@ryanp97/error-analysis-and-the-transformer-architecture-5c8a38264cae?source=rss-6378d85d3a9b------2</link>
	<description>&lt;p&gt;In the past week, I’ve been tuning hyper-parameters for the LSTM based models, exploring the Transformer architecture, and looking more in-depth at the predictions the model is making for error analysis.&lt;/p&gt;&lt;h4&gt;Swapping LSTM with Transformer&lt;/h4&gt;&lt;p&gt;As suggested by Nelson, I trained a model using the Transformer architecture from “&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;Attention Is All You Need&lt;/a&gt;” (Vaswani et al.) to explore and see if different architecture would help improve the performance of the LSTM based models.&lt;/p&gt;&lt;p&gt;I used the same hyper-parameters that was used to achieve the results listed in the paper. Unfortunately, the model showed a significant decrease in performance compared to the LSTM models. The main cause of this was due to ill-formed graphs. Unlike the LSTM models, the transformer was unable to learn the general requirements for a well-formed graph even after training for 50 epochs.&lt;/p&gt;&lt;p&gt;Every graph starts with an opening paren, (, which is the only place this token will occur (other opening parentheses are always attached to edge labels). The LSTM based model was able to correctly identify that this token only occurs at the beginning of the sequence while the transformer was unable to make this connection. Another issue that the transformer ran into was predicting a set of features after an edge label without any predicate, something that the LSTM based models also did not run into.&lt;/p&gt;&lt;p&gt;I believe that one of the causes of these issues is the fact that the Transformer was trained using batches of tokens rather than batches of sentences. However, even after training the transformer on batches of graphs, it still experienced issues of predicting edges without a predicate. Even more unfortunately, neither model was able to increase the SMATCH score or the surface/abstract predicate scores by any significant number.&lt;/p&gt;&lt;pre&gt;SMATCH (F1):&lt;br /&gt;    Transformer (Tokens) - 0.60&lt;br /&gt;    Transformer (Graphs) - 0.63&lt;/pre&gt;&lt;pre&gt;Surface Predicate F1:&lt;br /&gt;    Transformer (Tokens) - 0.52&lt;br /&gt;    Transformer (Graphs) - 0.54&lt;/pre&gt;&lt;pre&gt;Abstract Predicate F1:&lt;br /&gt;    Transformer (Tokens) - 0.71&lt;br /&gt;    Transformer (Graphs) - 0.73&lt;/pre&gt;&lt;h4&gt;Error Analysis&lt;/h4&gt;&lt;p&gt;This week I also trained some different models with larger hidden sizes, but unfortunately still did not see much improvement (all of them had similar performances as all the previously listed and tested models).&lt;/p&gt;&lt;p&gt;With the baseline and other models that did not have a coverage attention mechanism, there were around 2,200~2,300 predictions that differed in length by at least 15 tokens (around 5 predicates less than the gold-label graph). We hoped that adding a coverage mechanism would alleviate this issue, but the model with coverage actually resulted in more predictions having a large length difference. After looking into the parsed graph data more carefully, there doesn’t seem to be a clear correlation between the length of the graph in English and the graph length in Japanese. Furthermore, attending to the English predicates may not be the correct thing to do since there are pieces of grammar that may not be able to be directly related to some predicate in the input.&lt;/p&gt;&lt;p&gt;I also calculated which predicates were commonly mis-predicted as well as ones that were not predicted when they should have been. Surprisingly There was quite a big overlap between these two sets. Below is a small subset of the predicates for the baseline model trained with features:&lt;/p&gt;&lt;pre&gt;Abstract Predicates:&lt;br /&gt;    def_q&lt;br /&gt;    udef_q&lt;br /&gt;    pron&lt;br /&gt;    cop_id&lt;br /&gt;    nominalization&lt;/pre&gt;&lt;pre&gt;Surface Predicates:&lt;br /&gt;    _wa_d&lt;br /&gt;    _ni_p&lt;br /&gt;    _no_p&lt;br /&gt;    _koto_n_nom&lt;br /&gt;    _sono_q&lt;/pre&gt;&lt;p&gt;Something interesting to note are the surface predicates. The first three listed are called particles in the Japanese grammar. The first predicate, _wa_d, serves multiple purposes. は can be used to be a topic marker as well as being used to show contrast between subjects. The second predicate, _ni_p, also has multiple purposes with more varied usage. に can be used to mark time, destination, place, etc. So it seems that the models have trouble predicting predicates which have widely varied usage. Despite having a fairly high SMATCH score, it seems that the model is not quite learning the semantic structure of the Japanese graphs.&lt;/p&gt;&lt;p&gt;There are a couple reasons for this. Just like I mentioned in early posts, the sequence based models is likely not the best method of performing this semantic transfer and something like a TreeLSTM is more likely to be able to accurately capture the tree structure and the semantic meaning in the tree structure.&lt;/p&gt;&lt;p&gt;The second reason may be because of inconsistencies in the data. The Tanaka corpus is known to have a lot of casual speech, and despite taking some precautionary steps to avoid this (by using a modified version of Jacy to account for casual speech and slang), the data may still be too noisy for the number of examples in the training set that we have. To work around this, I can look into incorporating the Tanaka corpus as mentioned in one of my earlier posts. Michael Goodman was nice enough to link me the &lt;a href=&quot;https://github.com/goodmami/xmt/blob/master/scripts/data-preparation/kyoto-wiki.sh&quot;&gt;script&lt;/a&gt; he used to grab the sentence pairs from the Kyoto corpus, so this is a reasonable goal for the up-coming week. Additionally, Michael suggested looking into Japanese WordNet for additional data on top of the Kyoto Corpus.&lt;/p&gt;&lt;h4&gt;Plans for the Next Week&lt;/h4&gt;&lt;p&gt;As mentioned above, I’ll be looking into incorporating more data from both the Kyoto Corpus and WordNet. Another thing I plan to look into is parent feeding which acts as a nice middle ground between the LSTM based models I’m currently using and a TreeLSTM. The idea behind this attention mechanism is to feed the previous decoder time step from the parent node into the current decoder time step. This allows the model to make more direct connections when it comes to long range dependencies.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=5c8a38264cae&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 15 May 2018 06:58:30 +0000</pubDate>
</item>
<item>
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: Advanced Model Attempt #1 (Continued)</title>
	<guid isPermaLink="false">https://medium.com/p/2ac19f7510f9</guid>
	<link>https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-continued-2ac19f7510f9?source=rss----9ba3897b6688---4</link>
	<description>&lt;p&gt;This week we continued work on the advanced version of the model which incorporates the SummaRuNNer architecture from (Nallapati et al.) and adds in a character level RNN for parsing UMLS terms. The models are complete but we ran into several challenges along the way, namely constructing our final training dataset from Semantic Scholar and UMLS terms. Without the dataset, we have not been able to train our model. In this blog post, we dissect our current challenges and our plans looking forward.&lt;/p&gt;&lt;h4&gt;Building the Model&lt;/h4&gt;&lt;p&gt;The original SummaRuNNer model fit our requirements pretty well. However, we ran into several challenges when implementing the model. First, there were no existing implementations of the architecture, so we had to implement the model from scratch. Furthermore, the time we spent building the model detracted from time that we could have spent working on gathering the data. The model used by Nallapati et al. also did not condition on a particular term in the paper. Our approach requires that we somehow incorporate the term so we spent additional time figuring out a character level RNN which encodes the term and includes it in the many affine transformations described in the SummaRuNNer paper.&lt;/p&gt;&lt;p&gt;Moreover, we had to consider how to optimize the model when it came to large amounts of data. One of the approaches we worked on was batching, essentially evaluating many sentences at once. This allowed us to speed up training by a significant amount. Finally, we spent time integrating our model into the existing architecture. Ultimately, the model was difficult, but completed.&lt;/p&gt;&lt;h4&gt;Collecting the Data&lt;/h4&gt;&lt;p&gt;There are no current datasets that we can use to train our model. To quickly recap the requirements for the data, each training example must comprise of one document, one entity or technical term, and a target representing the ideal summary of the document. In order to build this dataset, we had to individually collect each of these aspects and combine them.&lt;/p&gt;&lt;p&gt;In order to gather technical terms, we used the UMLS dataset, which contains over 150,000 medical terms. Obtaining the license to download UMLS and the actual process of downloading the data through the UMLS specialized data downloader took several days. Parsing the data was fairly straightforward however.&lt;/p&gt;&lt;p&gt;In order to gather documents, we are using the AI2 Semantic Scholar dataset, which contains over 7 million research papers. While the downloading process for Semantic Scholar was incredibly slow, we realized that we couldn’t simply download all the documents because some of them were not medical papers. Handling Computer Science papers becomes an issue because our entities are medical terms and we do not expect a computer science paper to have any relation to medical terms.&lt;/p&gt;&lt;p&gt;This brings us to the process of combining Semantic Scholar documents with UMLS terms. We use a distant supervision method which essentially applies a greedy approach to extract a group of sentences from each document with the highest ROUGE score while using the UMLS definitions as reference summaries. This is precisely where we are currently struggling. Computing ROUGE takes a very long time, considering the fact that we compute ROUGE as many times as there are sentences in each document. Though we filter out document-term pairs based on whether the term appears in the document, it seems that the ROUGE metric may not yield the best target sentences for us because our reference summaries tend to be fairly short while our documents tend to be fairly long. Currently, we have all the scripts running for this data collection process but aim to develop a better heuristic to collect data.&lt;/p&gt;&lt;h4&gt;Next Steps&lt;/h4&gt;&lt;p&gt;At this point, we have not been able to test our data because we are still building the dataset. In order to speed up this process, our immediate goal will be to develop a faster and more accurate heuristic to gather target sentences from each document. We will also explore filtering out the Semantic Scholar papers to only retain medical papers, which are more likely to correlate to the terms in UMLS. Once we build our dataset, we can test our completed models, tune hyper-parameters, and potentially utilize attention mechanisms while constructing the document level representation used by SummaRuNNer.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=2ac19f7510f9&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-continued-2ac19f7510f9&quot;&gt;Advanced Model Attempt #1 (Continued)&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</description>
	<pubDate>Thu, 10 May 2018 06:40:34 +0000</pubDate>
</item>
<item>
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #7: Advanced Model I Part 2</title>
	<guid isPermaLink="false">https://medium.com/p/d9145c25606a</guid>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-7-advanced-model-i-part-2-d9145c25606a?source=rss-fad49d942bf3------2</link>
	<description>&lt;p&gt;Note for all of the analysis below, I’m focusing on performance on the positive/negative sentiments. Performance on “no sentiment” seems to naturally better due to “no sentiment” being the majority class in the data.&lt;/p&gt;&lt;h3&gt;Datasets&lt;/h3&gt;&lt;p&gt;This week I created a few more datasets to experiment with in an attempt to improve my models’ performance on the developments set. I used a combination of two approaches to create the datasets:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Adding new weakly labelled “no sentiment” examples to the training data. To do this, we took the existing documents in the training data, found all the un-labelled entities in the document, and labelled pairs of these entities as “no sentiment.” We thought it would be a reasonable assumption to make in assuming all un-labelled entities are “no sentiment,” because usually documents are focused on interactions/sentiments between a particular set of entity pairs, which the annotators would consider important enough to label. Thus, usually the rest of the peripheral entity pairs should not express or be the target of any significant sentiment. Of course, this is not always the case. I took a look into examples generated through this approach, and there are quite a few inaccurate labels. Some of the inaccuracies came from bad co-reference resolution from the initial dataset, which propagated and created inaccuracies on this new dataset. For example, if a mention of the original holder entity wasn’t found the first time around, it might have been labelled as a new entity in the new dataset, and thus an inaccurately labelled “no sentiment” pair is added between that mention of the holder and the target entity. Other times, the assumption was simply incorrect: peripheral entities still expressed positive/negative sentiment towards other entities, or were the recipients of positive/negative sentiment.&lt;/li&gt;&lt;li&gt;Adding samples from “dev_tune.” As I explained in the previous blog post, &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;Choi et al.&lt;/a&gt; had split the development set into two subsets: “eval” and “tune.” I used samples from “tune” to train and used “eval” for all the rest of the functions of a development set. Since “dev_tune” should follow to a similar distribution as my development and test data, this could help performance, by bringing the distribution of the training dataset closer to the distribution of the development and test datasets. For some of datasets I created, I also tried oversampling this set to try and get even closer to the distribution of my development/test set.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;In total I generated 6 datasets:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/587/1*-Oaiiq1AEgT2lu9g9UUiJg.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;The F1 scores of the baseline model on these datasets look as follows:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/690/1*G6FwBUjDx3bI9-ULwvDiCw.png&quot; /&gt;F1 scores of baseline on each dataset.&lt;/figure&gt;&lt;p&gt;Note that when I added “dev_tune” samples, F1 scores seems to decrease slightly, whereas when I added the weakly generated “no sentiment” training data, F1 scores actually seems to increase slightly. I looked into this and I noticed that adding “dev_tune” samples seems to &lt;em&gt;improve recall&lt;/em&gt; and &lt;em&gt;decrease precision&lt;/em&gt; on pos/neg data, whereas adding weakly labelled “no sentiment” samples &lt;em&gt;improves precision&lt;/em&gt; and &lt;em&gt;decreases recall&lt;/em&gt; on pos/neg data. Since the original dataset already had higher recall than precision, adding dev_tune exacerbated the difference between precision/recall whereas adding weakly labelled data made the two metrics more similar. Thus the results make sense as F1 scores tend prefer when precision/recall are closer. See examples below:&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Scores on Dataset 1 (Original)&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Note recall is higher than precision on positive and negative sentiments.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/532/1*ADWr0nmp6dSL2Y6GkIE_eA.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;Scores on Dataset 4 (+ Weakly Generated Training Data)&lt;/h4&gt;&lt;p&gt;Note precision and recall are closer, resulting in higher overall F1 score.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/532/1*9VTXI1ZBfD8yvzd9utVZKw.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;Scores on Dataset 3 (+ Dev_tune Samples)&lt;/h4&gt;&lt;p&gt;Note precision and recall are farther, resulting in lower overall F1 score.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/532/1*iKtmGfzu9WP4YbqX5cF_5Q.png&quot; /&gt;&lt;/figure&gt;&lt;h3&gt;Advanced Model&lt;/h3&gt;&lt;p&gt;I also made a few changes to the advanced model. First, since all my datasets had imbalanced classes, I tried using class weights to mitigate the effect. Specifically, if there were 10x more “no sentiment” examples than “positive sentiment” examples, then “no sentiment” would be weighted by 0.1.&lt;/p&gt;&lt;p&gt;I also simplified the advanced model’s architecture a little to try and reduce the number of parameters the model needs to learn. Previously, after I encoded the tokens with the biLSTM, I extracted spans representing holder/target entities and fed them through two separated FFNNs. This time, I simply concatenated the holder/target span representations and fed them through a single FFNN which predicted their sentiment.&lt;/p&gt;&lt;p&gt;The F1 scores of this model on the datasets 1 (original), 4 and 6 (datasets that baseline performed best on) are reported below:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/697/1*sNaYsByqW5kdX7H_NKMa3w.png&quot; /&gt;F1 scores of baseline vs. advanced model on datasets 1, 4, and 6.&lt;/figure&gt;&lt;p&gt;While there is some slight improvement in F1 scores (especially on dataset 4), unfortunately, none of these results approach the state of the art reported by the previous Choi et al. paper. For next time, I plan to do more focused error analysis to isolate the issue. I may also take Yejin and Ari’s advice in experimenting with encoding transitivity and other relations within the model through various means (which hopefully I’ll have the time to do).&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=d9145c25606a&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 09 May 2018 06:45:30 +0000</pubDate>
</item>
<item>
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: Advanced Attempt I Continues</title>
	<guid isPermaLink="false">http://cse481n-capstone.azurewebsites.net/?p=82</guid>
	<link>http://cse481n-capstone.azurewebsites.net/2018/05/08/advanced-attempt-ii/</link>
	<description>&lt;h3&gt;Data Collection&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Since our last blog post we have settled on using Dice Index with a cutoff of .5 for including post sentences in our dataset. This final dataset was generated by running similarity between 24574 “hateful” sentences (r/MeanJokes posts and sentences from the general set that were matched by the hate lexicon described previously) and 8,662,875 general Reddit sentences that were from subreddits that were not black-listed. With a .5 cutoff only 2308 of the “hateful” sentences were matched with general reddit posts other than themselves. There were 16940 such general posts. This gives us a total dataset of ~19k potentially interesting sentences to use for the pilot program. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;An interesting observation from running this is just how sparse the data truly is. Reddit is very diverse and its various communities use very different language from each other. Only .2% of all posts were within .5 Dice Index of the “hateful” posts. With .7 Dice Index it was .01% with only 1000 matched sentences. However, this is just from one month of Reddit data. If we need to increase the cutoff or just get more examples in the future, there are many more dumps we could use to get us enough data to feed our neural net models. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;What follows are some examples from the dataset and how our current best model performs on them.&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Matched by hate lexicon, presumably from a support subreddit: “My wife was raped My wife tonight was raped, she doesn’t want to go to the police.” labeled hate with 88% certainty&lt;/span&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Similar post: “I don’t want to go. My wife is being guilted into going.” labeled none with 78% certainty&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Matched by hate lexicon: “It really fucks with me and makes me feel like a slave.” labeled none with 62% certainty&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Matched by hate lexicon: “Saying the word ”gentleman” is not sexist is like saying the word ”nigger” is not racist.” labeled hate with 95% certainty&lt;/span&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Similar post: “Saying a word such as Nigger isn’t racist. It’s only racist when aimed at a person.” labeled hate with 94% certainty&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Matched by hate lexicon: “I denied it, would say “I can’t believe that pathetic faggot would make something like that up”.” labeled hate with 99% certainty (model doesn’t understand what quotes are)&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;From r/MeanJokes: “What do you call a 5 year old with no friends? A sandy hook survivor.” labeled none with 83% accuracy (model doesn’t know current events)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Here are some more examples of issues:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We did use December after all:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; class=&quot;alignnone size-full wp-image-85&quot; height=&quot;835&quot; src=&quot;http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/b4-p1.png&quot; width=&quot;1033&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;People like to repost mean jokes with variations:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; class=&quot;alignnone size-full wp-image-86&quot; height=&quot;127&quot; src=&quot;http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/b4-p2.png&quot; width=&quot;974&quot; /&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Advanced Model Attempt 2:&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Last week we explored a new model that uses CNN as encoder, however, we had little luck on making it work better. This week we tried to take contextual information into account using ELMo provided by Allennlp.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In short, ELMo enables us to adjust our word/character embedding based on words/characters nearing it. Comparing to the pure Glove embedding we used before, ELMo enables us to take context into consideration which is meaningful as illustrated in our results.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;During our experiments, we used the &lt;/span&gt;&lt;a href=&quot;http://allennlp.org/elmo&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;pretrained ELMo embeddings&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; (1-layer) provided by Allennlp. The ELMo embeddings and glove embeddings were concatenated before being passed through an NN seq2vec encoder.&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Setup 1: gru + 1 layer feed forward with 50 dimension glove embedding, trained and evaluated on twitter_waseem dataset&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Without ELMo&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;With ELMo&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7940&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7982&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8022&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7979&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7876&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7986&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8235&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8228&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Setup 2: gru + 1 layer feed forward with 50 dimension glove embedding, trained on combined dataset, evaluated on twitter dataset&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Without ELMo&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;With ELMo&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7788&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8047&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7979&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8029&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7672&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8065&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8152&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8276&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Setup 3: gru + 1 layer feed forward with 200 dimension glove embedding, trained and evaluated on twitter_waseem dataset&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Without ELMo&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;With ELMo&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7915&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8027&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7908&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8067&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7923&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7993&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8166&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8290&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Setup 4: gru + 1 layer feed forward with 200 dimension glove embedding, trained on combined dataset and evaluated on twitter dataset&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Without ELMo&lt;/td&gt;
&lt;td&gt;With ELMo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;F1&lt;/td&gt;
&lt;td&gt;0.7953&lt;/td&gt;
&lt;td&gt;0.7813&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Precision&lt;/td&gt;
&lt;td&gt;0.7997&lt;/td&gt;
&lt;td&gt;0.8044&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recall&lt;/td&gt;
&lt;td&gt;0.7914&lt;/td&gt;
&lt;td&gt;0.7681&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Accuracy&lt;/td&gt;
&lt;td&gt;0.8228&lt;/td&gt;
&lt;td&gt;0.8187&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;From the aboving statistics, we can see that ELMo does help us on getting a better result. It seems with ELMo, our model has a noticeable &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;improvement on Recall &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;when training on combined dataset. And precision and accuracy remains about the same with a little bit fluctuation.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We think with context taken into consideration, our model now is better at telling sentences that intend to be hateful but with few or no clearly hateful words or phrases. We will talk more about it in the following part.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Next we will do some error analysis on our new models:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;First, we will check how ELMo helps on correcting mistakes we had in our models without ELMo:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall these examples from our baseline model blog posts:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;False Negatives:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;@RichardDawkins @AbuelitoSerge Really, Muslims understand this. They just want to be able to use the name “racism” to shut us up.&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;@RTUKnews An Islamist human rights group? LOL. Now there is a contradiction in terms.&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt; &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;False Positives:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;3. @Strubbestition Name one thing that is not an opinion but is still sexist. I will wait.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;4. @Bipartisanism @AllooCharas Terrorism involves a political or religious objective to the terror. Most mass murderers have personal objectives&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In our new model with ELMo, we now correctly classifies 1 and 2. However, We still think 3 and 4 to be hateful while they aren’t.&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Hate prob&lt;/td&gt;
&lt;td&gt;None prob&lt;/td&gt;
&lt;td&gt;Correct?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.8673&lt;/td&gt;
&lt;td&gt;0.1327&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.8408&lt;/td&gt;
&lt;td&gt;0.1592&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.6350&lt;/td&gt;
&lt;td&gt;0.3650&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.7003&lt;/td&gt;
&lt;td&gt;0.2997&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;From the aboving results, we can see that with ELMo, our model does a better job on classifying things that means bad while has nothing really bad. From the class probabilities, we can tell our model is pretty confident about its prediction.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Since we still misclassified those two non-hateful sentences, this may introduce a concern that our model may have a tendency to predict everything as hateful. However, we didn’t observe a noticeable drop in precision or accuracy as they remains about the same. We think it is safe to conclude that by using ELMo embeddings, our model makes some improvements.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Next we will did some error analysis on our new model. However, after reading through the error sentence files, we discovered a lot of them looks like to the noise of the original dataset. For example, “islam must be outlawed.” is labeled as none as gold label in original dataset and actually our model did a great job on classifying it as hateful.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Finally, we also tried ELMo on our preliminary CNN model:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;With the same setting as previous, we do observe some improvement on our CNN model, however, comparing to other gru models we have, CNN one shows no improvement but the training time is way much longer.&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Without ELMo&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;With ELMo&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7745&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7987&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7616&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8014&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7879&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7963&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8084&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8248&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Next Step:&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our next steps for Data Collection are going to be to get the posts formatted for Mechanical Turk and begin the process of getting our Reddit data labeled.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For Modeling, we want to try and incorporate attention into our model and see if that improves our model’s performance.&lt;/p&gt;
&lt;h6 style=&quot;text-align: left;&quot;&gt;&lt;em&gt;&lt;strong&gt;&lt;span style=&quot;color: #000000;&quot;&gt;We have a running demo available on our blog in the RUNNING DEMO section of the sidebar.&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/h6&gt;</description>
	<pubDate>Wed, 09 May 2018 06:07:12 +0000</pubDate>
</item>
<item>
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Advanced model attempt #1 (continued)</title>
	<guid isPermaLink="false">https://medium.com/p/b3b687dd74ae</guid>
	<link>https://medium.com/@hongnin1/advanced-model-attempt-1-continued-b3b687dd74ae?source=rss-c450eb982161------2</link>
	<description>&lt;p&gt;To recap, this is our baseline approach:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/@hongnin1/image-annotation-model-baseline-dataset-and-evaluation-framework-d1d1b2d1f34c&quot;&gt;Image Annotation Model Baseline, Dataset and Evaluation Framework&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Our previous model involves InceptionNet as our CNN encoder and we feed the image vector into LSTM to generate caption. This model we used was heavily based on &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/im2txt&quot;&gt;this model&lt;/a&gt;. However we have since then fixed several bugs, improved model’s training speed performance and reduced the training time significantly, at the same time we incorporated more models into our overall model to improve the accuracy.&lt;/p&gt;&lt;p&gt;First of all, we realized what hindered our training speed is the logging we are doing. We had previously logged every training batch onto TensorBot; our training speed increase by 60% once we started to only update logs on TensorBot every 20 batches.&lt;/p&gt;&lt;p&gt;In addition to the training speed, we added a single layer neuronetwork to calculate attention weights: We first project input image feature (output from VGG encoder) to the same dimension as the original dimension, then we projected previous hidden state to the same dimension as input feature. Then we added the feature projection and hidden state projection together. We used Relu function as activation function and lastly we reduced the output from Relu function to a one dimensional vector to generate output. In addition to this, we also take one step further on attention by also putting attention on different data points in a single batch, so that the model can learn more from a more important data point. After getting the context matrix from previous steps in attention, we also apply a Beta scale to the matrix, which means that we multiply the matrix by beta vector to get a new context matrix and then generate our final output.&lt;/p&gt;&lt;p&gt;Coding up our own attention network reduced our training error. In addition to adding attention NN to our model, we also coded a new VGG network for encoding our images, this also significantly improved our model’s accuracy. We have found resources for VGG &lt;a href=&quot;http://www.vlfeat.org/matconvnet/pretrained/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Lastly, we also tried to keep track of the training process by keeping checkpoints during training. Our current rate is one checkpoint every ~50 minutes (800 batches). We are planning on keeping at most 50 checkpoints around to monitor the process in the past 2 days and detect any issue (like overfitting) early.&lt;/p&gt;&lt;p&gt;For the first couple of months, we have been studying the &lt;a href=&quot;https://arxiv.org/pdf/1502.03044.pdf&quot;&gt;paper&lt;/a&gt; and our baseline model is heavily based on this model &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/im2txt&quot;&gt;here&lt;/a&gt;. Now that we have learned all the necessary tool we have coded up most of the model ourselves with some added touch that improved the model’s accuracy. Our git repository: &lt;a href=&quot;https://gitlab.cs.washington.edu/sujiez/NLP-481&quot;&gt;https://gitlab.cs.washington.edu/sujiez/NLP-481&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=b3b687dd74ae&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 09 May 2018 03:10:45 +0000</pubDate>
</item>
<item>
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Evaluating Predicates and Hyper-Parameter Tuning</title>
	<guid isPermaLink="false">https://medium.com/p/82274b3d7f68</guid>
	<link>https://medium.com/@ryanp97/evaluating-predicates-and-hyper-parameter-tuning-82274b3d7f68?source=rss-6378d85d3a9b------2</link>
	<description>&lt;p&gt;The past week, I have been working on stripping the predicates from the graphs to calculate precision and recall over as well as a handful of unsuccessful attempts at hyper-parameter tuning.&lt;/p&gt;&lt;h4&gt;Predicate Evaluation&lt;/h4&gt;&lt;p&gt;As mentioned in the last blog post, one of the ideas was to separate the predicates from the graphs and treat them as a bag of words in order to calculate the precision and recall scores over them separately. We can further separate the predicates into two categories: surface predicates and abstract predicates. Surface predicates usually consist of a lemma, a part-of-speech tag, and sense. For more information on surface and abstract predicates, you can read up more on their characteristics &lt;a href=&quot;http://moin.delph-in.net/PredicateRfc&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;This allows us to get more fine-grained detail on what each model is struggling with. In particular, all the models so far have trouble with predicting surface predicates compared to abstract predicates. For reference, the F1 score differential between Abstract and Surface predicates is nearly 20 points better on the three models I’ve successfully trained so far.&lt;/p&gt;&lt;pre&gt;Abstract:&lt;br /&gt;    Baseline - Featureless:    &lt;br /&gt;        Precision   :  0.77&lt;br /&gt;        Recall      :  0.73&lt;br /&gt;        F1          :  0.75&lt;br /&gt;    Baseline - Features Removed:&lt;br /&gt;        Precision   :  0.79&lt;br /&gt;        Recall      :  0.75&lt;br /&gt;        F1          :  0.74&lt;br /&gt;    3 Layer, 600 Hidden, 500 Embed - Features Removed:&lt;br /&gt;        Precision   :  0.80&lt;br /&gt;        Recall      :  0.69&lt;br /&gt;        F1          :  0.74&lt;/pre&gt;&lt;pre&gt;Surface:&lt;br /&gt;    Baseline - Featureless:    &lt;br /&gt;        Precision   :  0.59&lt;br /&gt;        Recall      :  0.53&lt;br /&gt;        F1          :  0.56&lt;br /&gt;    Baseline - Features Removed:&lt;br /&gt;        Precision   :  0.59&lt;br /&gt;        Recall      :  0.50&lt;br /&gt;        F1          :  0.54&lt;br /&gt;    3 Layer, 600 Hidden, 500 Embed - Features Removed:&lt;br /&gt;        Precision   :  0.59&lt;br /&gt;        Recall      :  0.50&lt;br /&gt;        F1          :  0.54&lt;/pre&gt;&lt;p&gt;After looking into this a little more, there is a large difference in the size of the abstract predicate and surface predicate sets. In the development dataset, there are 7,235 possible surface predicates and only 42 possible abstract predicates to guess from. Furthermore, a small subset of abstract predicates appear much more often than the rest, so it is significantly easier to boost the precision and recall scores for abstract predicates than surface predicates. On the other hand, from eye-balling, surface predicates have a much more even distribution of appearing, so this in combination with the large number of possibilities is likely the cause for the significant difference between the precision and recall scores between abstract and surface predicates.&lt;/p&gt;&lt;h4&gt;Hyper-parameter Tuning&lt;/h4&gt;&lt;p&gt;As evidenced from the previous section, there was not a huge difference between the baseline model on fully-featured data and a beefier model on fully-featured data. One of the reasons for this is that the dropout rate was not changed between the two models. So it’s likely that the larger model was able to leverage it’s parameters without overfitting to the data. On top of that, it’s unclear whether or not adding a 3rd layer to the model is beneficial considering the training dataset is fairly small (~95,000 training examples). In this coming week, I plan on performing more error analysis as I continue to tune hyper-parameters. Ideally, I would like to get more information on what surface and abstract predicates the model is able to predict well and which predicates give it the most trouble.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=82274b3d7f68&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 08 May 2018 23:41:05 +0000</pubDate>
</item>
<item>
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog 7</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-5600014144802012716.post-8581531123022969204</guid>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/05/blog-7.html</link>
	<description>&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;I've read through a lot of the data, to understand the data set and view the different types of error &lt;/span&gt;&lt;br /&gt;&lt;span&gt;cases. in general, most(⅔) of the data is just noise, meaning that a programmer won't be able to &lt;/span&gt;&lt;br /&gt;&lt;span&gt;generate the target code from the utterance. &lt;/span&gt;&lt;/div&gt;&lt;b id=&quot;docs-internal-guid-42510a76-41f6-9607-1678-1da3293c9502&quot; style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Error breakdown(whats consistently wrong). Rule based fixes.&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Not understanding functions versus fields&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;return func_debug; or func_x = …&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Fix &quot;Expression--&amp;gt;Expression___(___)&quot;, &lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Expresion -&amp;gt;identifer&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;identifier-&amp;gt;classfields variables&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;&quot;Expression--&amp;gt;Expression___.___Nt_33&quot;&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;If parent has parenthesis then allow nt_33 to generate functions otherwise fields ok&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Not able to initialize correctly&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;List y = new File&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;sriniclass_event = new StatisticGenerationEvent()&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt; text-indent: 36pt;&quot;&gt;&lt;span&gt;&quot;Expression--&amp;gt;Expression___Nt_68___Expression&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Expression--&amp;gt;Primary&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Primary--&amp;gt;IdentifierNT&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;IdentifierNT--&amp;gt;sriniclass_event&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Nt_68--&amp;gt;=&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Expression--&amp;gt;new___Creator&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Creator--&amp;gt;CreatedName___Nt_37&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;CreatedName--&amp;gt;IdentifierNT&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;IdentifierNT--&amp;gt;StatisticGenerationEvent&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Nt_37--&amp;gt;ClassCreatorRest&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;ClassCreatorRest--&amp;gt;Arguments&quot;,&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-left: 36pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;            &quot;Arguments--&amp;gt;(___ExpressionList___)&quot;&lt;/span&gt;&lt;/div&gt;&lt;b style=&quot;font-weight: normal;&quot;&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li dir=&quot;ltr&quot; style=&quot;background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;&quot;&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Generics incorrect and missing generics&lt;/span&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span&gt;List&amp;lt;List&amp;gt;  Map&amp;lt;Map, Map&amp;gt;&lt;/span&gt;&lt;/div&gt;&lt;span&gt;&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space: pre;&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;span&gt;List x&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;I want pivot now since adding all these rules doesn't seem interesting from a language perspective. I am considering finding a filtering heuristic to run on this dataset to get better quality utterances.&lt;/span&gt;</description>
	<pubDate>Tue, 08 May 2018 22:55:00 +0000</pubDate>
	<author>noreply@blogger.com (nlpcapstone)</author>
</item>
<item>
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #6: Overfitting and Advanced Model I</title>
	<guid isPermaLink="false">https://medium.com/p/d73810e0c390</guid>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-6-overfitting-of-neural-baseline-and-advanced-model-d73810e0c390?source=rss-fad49d942bf3------2</link>
	<description>&lt;p&gt;This week, I continued working on correcting the apparent train/development performance discrepancy in baseline neural model through modifying the training data as well as model itself. I also attempted to construct a new advanced model. Unfortunately, the performance on the dev data, of both models, is still not as good as I’d like it to be.&lt;/p&gt;&lt;h3&gt;Dataset and Work on Baseline&lt;/h3&gt;&lt;p&gt;I’ve ran my baseline model on a total of 4 different datasets.&lt;/p&gt;&lt;h4&gt;Dataset A: Original&lt;/h4&gt;&lt;p&gt;I ran the baseline model using the original dataset provided by &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;Choi et al., 2016&lt;/a&gt;. Results are reported from previous blog posts, reposted below:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*iPBZ5X-TctKEGWXub_8Klw.png&quot; /&gt;f1 scores across 20 epochs&lt;/figure&gt;&lt;h4&gt;Dataset B: Splitting train&lt;/h4&gt;&lt;p&gt;This dataset was created through splitting the original training dataset provided by &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;Choi et al.&lt;/a&gt; into an 80/10/10 train/dev/test ratio. The motivations behind this were stated in my previous blog post, but I basically wanted to see how my model would perform if the train/dev/test distributions were similar. Results are reported from previous blog posts, reposted below:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*JYd_FY4ZYnUJj5Fnn-kLKg.png&quot; /&gt;f1 scores across 20 epochs&lt;/figure&gt;&lt;h4&gt;Dataset C: Training on weakly generated data&lt;/h4&gt;&lt;p&gt;This dataset used the dev/test of dataset A, but supplemented the original training set by generating weakly labeled ‘null’ examples to mimic the distribution of the development dataset. This was done by assuming all unlabeled entity pairs in the training set express no sentiment to each other.&lt;/p&gt;&lt;p&gt;As I had reported in my previous blog posts, the discrepancy in performance between the dev and train data is large on the original dataset (dataset A). I originally hypothesized this discrepancy as due to the difference in distribution between the training and development dataset, and this seemed to be supported by my results in dataset B. So I believed that by modifying my training dataset in this way, I would be able to improve performance. The results are plotted below:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*ZrOouM_61fTPOUZWIE3zTg.png&quot; /&gt;f1 scores across 20 epochs&lt;/figure&gt;&lt;p&gt;Unfortunately, there is still a large discrepancy in performance, especially on positive and negative sentiment. This led me to wonder what would happened if I trained my model on data similar to the original development data. That is, rather than using splitting the development data off the training data as in dataset B, what would happen if I split the training data off the development data…&lt;/p&gt;&lt;h4&gt;Dataset D: 3rd Modification (Training on subset of dev data)&lt;/h4&gt;&lt;p&gt;Originally, &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;Choi et al.&lt;/a&gt; had split the development set into two subsets: “eval” (development data used for error analysis and ablations) and “tune” (development data used for tuning hyper-parameters). For each of the above datasets A-C, I had used “eval” as the development data and thrown away the “tune” set. However, I wondered what would happen if I used the “tune” set as my training set. The distribution of my training data and development data should be similar in this case. The results are as follows:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*3ET0QKkUWLLhInHQuXOKxA.png&quot; /&gt;f1 scores across 100 epochs&lt;/figure&gt;&lt;p&gt;Note that there is still a large discrepancy, and performance on the new development data still barely, if ever, exceeds 0.2 for positive and negative labels. This is strongly indicative of overfitting. I have additionally plotted the losses for the train and development results, respectively, and on this plot overfitting is extremely apparent. Development loss does not even begin to decrease, suggesting that the baseline neural model is not learning what its intended to learn — perhaps it’s just memorizing particular configurations of text in the training set.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/640/1*KBxQ8qLMwxgU4AtPUuPjZQ.png&quot; /&gt;loss across 100 epochs&lt;/figure&gt;&lt;h3&gt;Dealing with the Overfitting&lt;/h3&gt;&lt;p&gt;To deal with the issue, the first thing I tried was applying some regularization. Unfortunately all it seemed to do was decrease/slow down improvement in performance on the train set, and leaving dev set performance unaffected.&lt;/p&gt;&lt;p&gt;(Results from running neural baseline model on dataset D.)&lt;/p&gt;&lt;p&gt;Dropout = 0.2: I added a dropout layer after the LSTM and set its dropout rate to 0.2.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*FT7361ZyaXIGgp4aXkf-nA.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;I’m still not exactly sure as to what is causing the overfitting. Just looking at positive and negative sentiment performance, what’s really strange is the fact that the model’s dev performance was relatively high on dataset B where both train and dev were based on the training distribution, but not on dataset D where both train and dev were based on the development distribution. My guess is that perhaps since there’s relatively little positive and negative examples in the development set, the model had an easier time overfitting them for dataset D. Whereas for the training set, there was a plethora of positive and negative examples (and relatively little no sentiment examples), making it harder for the model to overfit the positives and negatives for dataset B.&lt;/p&gt;&lt;p&gt;Following Eunsol’s recommendation, I plan next time to experiment with strategically increase the size of the training data, and hopefully the model will demonstrate less overfitting.&lt;/p&gt;&lt;h3&gt;Advanced Model&lt;/h3&gt;&lt;p&gt;The final thing I did this week was implement a new model. The model combines elements from the two papers: &lt;a href=&quot;https://arxiv.org/pdf/1707.07045.pdf&quot;&gt;Lee et al.&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/pdf/1802.10569.pdf&quot;&gt;Verga et al.&lt;/a&gt; Basically, like my baseline neural model, this model encodes the inputs through a biLSTM. However, it then extracts span representations of the holder and target mentions using the encoded endpoints of the span, as in Lee et al. Then it runs the representations through separate FFNN as in Verga et al., and aggregates across holder and target mentions through a summation expression, and extracts the final score through a bi-affine operation. This is a very rough architecture at this stage, and there’s definitely parts of it that aren’t very well thought through, so I’ll definitely tinker with it in the future. However, the following are some preliminary results.&lt;/p&gt;&lt;p&gt;Results on Dataset C:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*9tPF6HytReejUKE3NDwVOw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;At this point, it looks like this model has very similar performance, if not worse, to the neural baseline. I will continue tinkering with it, as well as the data, in hopes of improving performance. In particular, in accordance with the &lt;a href=&quot;https://arxiv.org/pdf/1707.07045.pdf&quot;&gt;Lee et al.&lt;/a&gt; paper, perhaps adding features (i.e. the paper used distance and width features, which improved f1 scores by 3.1) or the head-finding attention mechanism will improve performance.&lt;/p&gt;&lt;h3&gt;Future Plans&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;Continue expanding training data by strategically combining the training data of Dataset C and D.&lt;/li&gt;&lt;li&gt;If this doesn’t work, do some error analysis to hopefully gain more insight into why the overfit is occurring.&lt;/li&gt;&lt;li&gt;Tinker with the advanced model by adding features or the head-finding attention mechanism. Also experiment more with the architecture.&lt;/li&gt;&lt;/ol&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=d73810e0c390&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Thu, 03 May 2018 04:25:50 +0000</pubDate>
</item>
<item>
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: Advanced Model Attempt #1: Neural-Based Definition Extraction</title>
	<guid isPermaLink="false">https://medium.com/p/d01e84c5e1da</guid>
	<link>https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-neural-based-definition-extraction-d01e84c5e1da?source=rss----9ba3897b6688---4</link>
	<description>&lt;p&gt;We last left off on the idea of using an FSA with a restricted vocabulary; restricted in the sense that we extract sentences coupled with a neural language model to assure semantic quality while allowing a generative RNN model a reasonable amount of improvisation to produce abstractive definitions.&lt;/p&gt;&lt;p&gt;Here, we discuss our approach for the extractive component of this model, and consider it our first attempt at an advanced model for the task.&lt;/p&gt;&lt;h3&gt;Introducing Extractive Summarization&lt;/h3&gt;&lt;p&gt;Recall that extractive summarization is the idea of reducing text down to a subset of its sentences that still preserves its semantic integrity. In particular, we intend to build on the work of a successful nerual-based extractive summarizer and tailor it to solve our task.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.04230&quot;&gt;SummaRuNNer&lt;/a&gt; is an RNN-based extractive summarization algorithm developed by Nallapati et al. that encodes documents from the word level up to and across the sentence level before making inference. Essentially, the model is a binary classifier on sentences within a document on whether it should be included in a summary. Its decisions are conditioned on&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Averaged-pooled word-level hidden states of the sentence&lt;/li&gt;&lt;li&gt;Average-pooled sentence-level hidden states of the document&lt;/li&gt;&lt;li&gt;An abstract representation of the summary built so far (average-pooling of the word-level pooled hidden states of sentences selected thus far)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;After which, there are several affine transformations conducive to selecting and filtering sentences:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Content: affine on the abstract sentence representation that measures semantic richness&lt;/li&gt;&lt;li&gt;Salience: bilinear affine on the abstract sentence representation and the document representation to measure cohesiveness&lt;/li&gt;&lt;li&gt;Novelty: bilinear affine on the abstract sentence representation and the running summary representation to address redundancy&lt;/li&gt;&lt;li&gt;Absolute and Relative Positioning: two separate affines on the embedded index of the sentence to allow how far we are into the document to influence inference&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As of now, we have built from scratch our own &lt;a href=&quot;https://github.com/NLP-Capstone-Project/machine-dictionary/blob/development-tam/machine_dictionary_rc/models/SummaRuNNer.py&quot;&gt;unofficial implementation of this model&lt;/a&gt; with inspiration from another &lt;a href=&quot;https://github.com/hpzhao/SummaRuNNer&quot;&gt;unofficial implementation&lt;/a&gt; and is capable of summarizing documents the way we’ve formatted them. What’s left is for us to tailor this model to fit the task.&lt;/p&gt;&lt;h3&gt;A Slight Twist on an Established Task&lt;/h3&gt;&lt;p&gt;As of now, the model summarizes documents. We’d like it so that it instead zeroes in on query terms we give it given a research paper, to intelligently extract only sentences from that paper conducive to defining that term.&lt;/p&gt;&lt;p&gt;Our approach for augmenting SummaRuNNer to be a definition extractor involves&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Encoding the query term with a character-level RNN and using its concatenated hidden states as its representation&lt;/li&gt;&lt;li&gt;Introducing this new query-term abstract representation when constructing the document representation through a bilinear affine&lt;/li&gt;&lt;li&gt;Further introducing this query term by converting many of the non-bilinear affines (content, positioning, and possibly new ones for the task) to further condition inference on the query term.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Essentially, the sentences we extract from the document are being conditioned on the term we’re trying to define. Encoding technical terms using a character level RNN allows similar technical terms to have similar hidden representations. For example, if we see the term “Chronic Lymphocytic Leukemia” in the training data and encounter “Chronic Myelogenous Leukemia” in the testing data, we would have more of an idea of how to approach this new term because of its character level similarities to the term we have already seen during training time. This might help us break down more complicated novel technical terms at testing time.&lt;/p&gt;&lt;p&gt;Experiments have yet to be conducted on the effectiveness of this approach but will be discussed later in &lt;strong&gt;Advanced Model Attempt #1 (cont.):&lt;/strong&gt; another post later in this series discussing the results of the groundwork we’ve laid out here.&lt;/p&gt;&lt;h3&gt;Training Methods&lt;/h3&gt;&lt;h4&gt;Collection Training Data with UMLS and ROUGE:&lt;/h4&gt;&lt;p&gt;Recall that SummaRuNNer is a model that aims to extract the sentences in a document that summarize it best. It does so by training on examples that teach the model which sentences to extract from the document.&lt;/p&gt;&lt;p&gt;SummaRuNNeR uses a &lt;em&gt;distant supervision&lt;/em&gt; method that relies on ROUGE in order to produce training examples for the model. This portion of the architecture, which we refer to as the “extractor”, extracts the sentences out of each document which maximize the ROUGE score when compared against the gold standard definition for the term in question. The extractor in a summarization context can use a greedy approach as follows:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Look at each sentence in the document one at a time and consider appending it to the extracted sentences that we have already chosen.&lt;/li&gt;&lt;li&gt;Calculate the ROUGE score of the old extracted sentences + this new sentence in comparison to the gold standard summarization for the document.&lt;/li&gt;&lt;li&gt;If the ROUGE score increases from the previous ROUGE score, keep the new sentence.&lt;/li&gt;&lt;li&gt;Otherwise, we don’t keep the new sentence and move on.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Although this method may not produce the most optimal and compact set of sentences that are relevant, this approach will be faster and is reasonable. The output of the extractor for each document is a tensor whose length is the number of sentences in the document, and is 0 if the sentence is tagged with O or 1 if the sentence is tagged with I.&lt;/p&gt;&lt;p&gt;To tailor this style of data collection to our task however, we optimize on ROUGE with respect to an entity’s gold-standard definition instead of a gold-standard summarization of the document. We collect entity-definition pairs through &lt;a href=&quot;https://www.nlm.nih.gov/research/umls/&quot;&gt;UMLS&lt;/a&gt; and creating training examples of the form&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Entity (the technical term to define)&lt;/li&gt;&lt;li&gt;Gold-standard definition for the entity&lt;/li&gt;&lt;li&gt;The target sentence IO tags found via distant supervision with ROUGE on sentences of a research paper with the gold-standard definition being the reference&lt;/li&gt;&lt;li&gt;A Semantic Scholar research paper in which the sentences came from (provides the sentences in which to perform inference)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;With this data, we can train the definition extraction model discussed earlier; we train using these &amp;lt;entity, IO-tagged sentences, publication&amp;gt; examples to learn a tagger that can extract sentences most relevant to a term given a publication.&lt;/p&gt;&lt;p&gt;While this may result in an unnecessarily large number of training data points, we can also consider pruning this dataset later on if we have irrelevant entities for a particular document. For example, if we were trying to find a training example that used the entity “dental cavity” for a document that was about blood cancers, we might not want to keep this training example because there wouldn’t be much of a correlation between the two. In order to do this, we can introduce a ROUGE threshold, where we only keep the training example if the ROUGE score of the sentences extracted by the tagger are above a particular threshold. This might be an optimization for the future.&lt;/p&gt;&lt;p&gt;Our previous approach was unsupervised and it relied only on the semantic scholar dataset to produce definitions. Our current approach is an extension of SummaRuNNer which requires gold standard definitions for entities that we’d like to define in each paper. We chose to focus on medical terms, and one of the most complete datasets for medical terms and their definitions happens to be the UMLS dataset. This dataset contains a &lt;em&gt;Metathesaurus&lt;/em&gt; which contains, amongst many other pieces of data, medical terms and their definitions. The technical terms in the dataset serve as references for ROUGE in the tagging phase above.&lt;/p&gt;&lt;h4&gt;In summary&lt;/h4&gt;&lt;p&gt;Training is fairly straightforward; loss between predicted and target sentences is computed with log loss (each sentence in a document is IO-tagged where sentences labeled with &lt;em&gt;I &lt;/em&gt;are to be included in the definition). Essentially, the definition extractor, much like SummaRuNNer, is trained as a sentence tagger.&lt;/p&gt;&lt;h4&gt;Attention as a Stretch Goal&lt;/h4&gt;&lt;p&gt;The first part of our basic SummaRuNNer-based model uses a document representation to predict tags for sentences in a document. The current document representation is constructed by averaging the hidden states from words in each sentence and averaging the hidden states from each sentence in the document. However, we believe that simply averaging the sentences may not be the best approach to constructing the latent document representation. One of our stretch goals for us to optimize the model will be to attend to the most important parts of sentences in each document. We can do this using the method proposed in Hierarchical Attention Networks for Document Classification (Yang et. al 2016).&lt;/p&gt;&lt;p&gt;This approach introduces a word level context vector and a sentence level context vector which allow us to calculate attention coefficients on the fly for every word in each sentence and every sentence in the document. In this manner, we can take a weighted sum of the hidden states in the sentences and will hopefully produce better document representations overall. The word level and sentence level context vectors can be initialized randomly and learned throughout training.&lt;/p&gt;&lt;h4&gt;Conclusion&lt;/h4&gt;&lt;p&gt;We are very excited to have found a supervised approach to this task per the advice of AI2 researchers. It’s a straightforward approach with measurable loss and clearer metrics.&lt;/p&gt;&lt;p&gt;We also hope to have enough time before the capstone is over to introduce attention!&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=d01e84c5e1da&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/advanced-model-attempt-1-neural-based-definition-extraction-d01e84c5e1da&quot;&gt;Advanced Model Attempt #1: Neural-Based Definition Extraction&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</description>
	<pubDate>Thu, 03 May 2018 03:32:53 +0000</pubDate>
</item>
<item>
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: Advanced Attempt I</title>
	<guid isPermaLink="false">http://cse481n-capstone.azurewebsites.net/?p=64</guid>
	<link>http://cse481n-capstone.azurewebsites.net/2018/05/01/advanced-attempt-i/</link>
	<description>&lt;h3&gt;&lt;b&gt;Data Collection: &lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We have made a lot of progress increasing our data quality since the last blog post. We have fine-tuned our filtering parameters and experimented with a few different definitions for set similarity. On top of Jaccard Index, we tried Dice Index and Cosine Similarity. We found that depending on the threshold, these different methods gave very similar results, but Dice Index seemed to provide the highest quality sentences while being more tolerant of long sentences (unlike Jaccard, which favored short sentences). Although it’s very picky, we’re certain that due to the huge amount of raw Reddit data we have we can still get a dataset big enough to train our complex neural nets.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;As an example of our improvement, consider this example from the last post:&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;MeanJokes Post: “Don’t be offended but Fuck you”
&lt;/span&gt;Similar Post: “fuck Foligno”
Similar Post: “fuck narek”
Similar Post: “fuck”
Similar Post: “Fuck me?”
Similar Post: “Fuck me”
Similar Post: “fuck me”
Similar Post: “Fuck it”
Similar Post: “Fuck”
Similar Post: “Who the fuck are you?”&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Now our output would look like:&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;MeanJokes Post: Don't be offended but Fuck you
&lt;/span&gt;Similar Post: why the fuck does he have to talk in a screaming voice
Similar Post 171137: &quot;Officer, I have no idea what in the fuck you're talking about.
Similar Post 92163: Or maybe you just fuck me in public for all too see.
Similar Post 18052: &quot;you know, I'm finally happy&quot;. UGH, fuck off.
Similar Post 2567: So reddit, that's my fuck up. Any advice if any of you are in HR?
Similar Post 160778: Now I'm questining what numbers are real and what was put down to fuck with me and what's serious.
Similar Post 210956: And when i ask him about it, he cusses me out (tells me to fuck off) and i just die/break down internally.&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;As well, since Dice is so picky and because the MeanJokes set tends to have a very particular structure to all its posts, we are also adding in some other obviously offensive posts to use for our set similarity step. We’re using a hate speech lexicon developed by Tom Davidson (linked below) to extract hateful posts from the general Reddit set. We will concatenate this with the MeanJokes set before running Set Similarity against all of the posts again, hopefully giving us a wider range of language structure for our dataset. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our final improvement was discovering that a handful of subreddits contribute a majority of the noise in our data. This noise is mostly of two varieties: 1. Personal ads for intimate encounters and 2. Trading requests, for both physical and virtual items. A handful of these subreddits are very activate and are surprisingly a large chunk of Reddit’s posts, although none of them ever get nearly enough upvotes to be noticed by the average user. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;So in order to combat having a lot of posts of this sort in our dataset:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; class=&quot;alignnone wp-image-65&quot; height=&quot;144&quot; src=&quot;http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/Picture1-300x110.png&quot; width=&quot;393&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We have put together a blacklist of subs and filtered them out of the posts we consider for set similarity.&lt;/span&gt;&lt;/p&gt;
&lt;table style=&quot;height: 620px;&quot; width=&quot;414&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;100k posts&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Top 10 Black List&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Top 10 White List&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;RocketLeagueExchange’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;1860&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;AskReddit’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;5978&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;A5XHE’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;1373&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Showerthoughts’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;1709&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dirtykikpals’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;1128&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The_Donald’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;850&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dirtypenpals’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;870&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;teenagers’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;720&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;DirtySnapchat’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;792&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;GlobalOffensiveTrade’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;681&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dirtyr4r’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;438&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Bitcoin’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;651&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;AppNana’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;372&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;relationships’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;586&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Roleplaykik’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;368&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;FIFA’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;558&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;buildapc’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;364&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;explainlikeimfive’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;500&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;AgeplayPenPals’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;329&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Fireteams’&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;469&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;&lt;br /&gt;
Running the filter on 100k posts, we can see that most of the most common subreddits that remained are conversational in nature, while those that were removed would not make very useful sentences.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Advanced Model Attempt: &lt;/b&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Combining Datasets: &lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In our last blog post, we mentioned our concern about the small size of &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem’s twitter dataset&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;. This week, we combined that dataset with another twitter hate speech dataset made by Thomas Davidson. The Davidson dataset contains 24,802 labeled tweets. Each tweet is coded by at least 3 CrowdFlower users. Each row contains 5 columns:&lt;/span&gt;&lt;/p&gt;
&lt;table style=&quot;height: 336px;&quot; width=&quot;584&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;count&lt;/td&gt;
&lt;td&gt;number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hate_speech&lt;/td&gt;
&lt;td&gt;number of CF users who judged the tweet to be hate speech.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;offensive_language&lt;/td&gt;
&lt;td&gt;number of CF users who judged the tweet to be offensive.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;neither&lt;/td&gt;
&lt;td&gt;number of CF users who judged the tweet to be neither offensive nor non-offensive.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;class label for majority of CF users. 0 – hate speech 1 – offensive language 2 – neither&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Davidson et. al. used the following definition for hate speech: language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. According to the paper, &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;“only 5% of tweets were coded by the majority of coders”&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;. If we directly combine Waseem Data with this we might get a even more skewed class distribution (31% ‘hate’, 69% ‘none’). Therefore, we decided to change the class labels of Davidson a little bit: if all CF users unanimously coded a tweet hate_speech or offensive_language, the tweet would be labeled ‘hate’; otherwise, the tweet would be labeled ‘none’. The modified Davidson dataset has a class distribution of 76% ‘hate’ and 24% ‘none’. Then we combined these two datasets (removed duplicate tweets if there are any). The new combined dataset has 40,509 tweets and a class distribution of 59% ‘hate’ and 41% ‘none. The combined dataset is much larger than the altered Waseem dataset (~15k tweets) and the labels are more balanced. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We do have the concern whether this more these more generously labeled ‘hate’ tweets are noisy. However, because the Waseem dataset is also more generous to ‘none’ labels (as long as the tweet is neither racist or sexist), we believe they would have some counter effect on each other. After all, data noise is very unlikely to be completely removed. &lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Preprocessing: &lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Since the model we tried requires each sentence to have at least 4 tokens, we decided to ignore sentences with less than 4 tokens after pre-processing.&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The effectiveness of Combined Dataset:&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;To illustrate the effectiveness of the combined dataset, we chose the best NN model set up from baseline II to train on Waseem dataset and combined dataset separately and evaluated the two trained models on Waseem dev data. We decided not to evaluate on test data yet because we don’t want any leaked info from test.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Set up — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer GRU&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Trained on Waseem Dataset, epoch chosen: 13&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem-Dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8235&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8022&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7876&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.7940&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Trained on Combined Dataset, epoch chosen: 16&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem-Dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8152&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7979&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7672&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.7788&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Although the two models had the same setup, the one trained on the combined dataset got performance close to the one trained on the original Waseem dataset despite the fact that we now have really different class distributions in the two datasets. &lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Retrain Some Baseline Models on Combined Dataset:&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Here we retained some baseline models with different set ups on the combined dataset and evaluated them on both Waseem dev data and combined dev data.&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model1 — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer GRU, epoch chosen: 16&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Combined-dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem-dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8665&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8152&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8614&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7979&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8628&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7672&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8621&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;0.7788&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model2 — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer LSTM, epoch chosen: 19&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Combined-dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem-dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8625&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8091&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8577&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7875&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8578&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7648&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8578&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;0.7739&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; Model3 — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer BiGRU, epoch chosen: 16&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Combined-dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem-dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8618&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8104&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8567&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7916&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8575&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7620&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8571&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;0.7732&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model4 — embedding: 100 dimensional glove twitter embeddings, encoder: 1 layer GRU, epoch chosen: 11&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Combined-dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem-dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8651&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8194&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8605&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7971&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8603&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7834&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8604&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;0.7894&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model:&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our first Advanced model will be a CNN model.&lt;/span&gt;&lt;/p&gt;
&lt;h5&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The intuition of choosing this model:&lt;/span&gt;&lt;/h5&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;CNN provides us a convenient way to extract the most important information within the given fragment of a sentence through filters and max pooling. We found it might be a worth trying model on our task.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our model looks like:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; class=&quot;alignnone wp-image-67&quot; height=&quot;158&quot; src=&quot;http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/Picture2-300x116.png&quot; width=&quot;409&quot; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Image credit:&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Gambäck, B., &amp;amp; Sikdar, U.K. (2017). Using Convolutional Neural Networks to Classify Hate-Speech.&lt;/span&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Preprocess all words and encode them using pretrained glove embeddings.&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Feed result into a convolution neural network, taking 2, 3 and 4-grams into consideration. Output dimension is 28, 26 for English alphabets, 1 for digits and 1 for all other symbols. &lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Feed result into a 2-layer feed-forward neural net, with dimension (28, 2)  and dropout (0.3, 0.3)&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Softmax on the result and pick the major class&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;With not much tuning, here’s what our best model looks like:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;200 dimension embedding, filters=100, trained on Waseem twitter dataset&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.79169&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.78274&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.80086&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.82142&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;200 dimension embedding, filters=100, trained on the combined dataset&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;*revised, due to an imperfection in the combined dataset, there was a mistake in numbers&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;combined dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem dev&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.85983&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;0.77451&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.86029&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.76156&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.85937&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.78791&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.86437&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400; color: #ff0000;&quot;&gt;0.80837&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;del&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;With no doubt, our combined dataset provides a huge boost on performance on original Waseem twitter dataset.&lt;/span&gt;&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;From the above results, it seems CNN hasn’t show an improvement on our job. We think doing more hyper parameter tuning should give us some improvement. Furthermore, we would like to incorporate Elmo to see if that will help us our not.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;del&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;However, we do have some concern about our models: almost all of the best models we have with a large number of filters have their best epoch generally to be the first few epochs. We are a little bit concerned about that since that may be a sign of overfitting.&lt;/span&gt;&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;Since our result hasn’t show any improvement, we think it’s more appropriate to do error analysis once we gain some improvement.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;del&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Therefore, before we start to do any error analysis, we would like to do a little bit more hyperparameters since we haven’t really try different drop out rate or other output dimension values other than the one specified in the paper we referenced.&lt;/span&gt;&lt;/del&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Next Step:&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;First, we will dig deeper on the model we have right now. We will first play with its parameters and then conduct error analysis on it.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;As suggested in previous blog post feedback, we would like to try Elmo and see how much can we improve with it. Furthermore, we would like to try things like character level embedding as well as another very interesting model which combines CNN with GRU to make prediction.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Work Cited:&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06&quot;&gt;Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Automated-Hate-Speech-Detection-and-the-Problem-of-Davidson-Warmsley/6ccfff0d7a10bf7046fbfd109b301323293b67da&quot;&gt;Davidson, Thomas J et al. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM (2017).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Using-Convolutional-Neural-Networks-to-Classify-Gamb%C3%A4ck-Sikdar/0dca29b6a5ea2fe2b6373aba9fe0ab829c06fd78&quot;&gt;Gambäck, Björn and Utpal Kumar Sikdar. “Using Convolutional Neural Networks to Classify Hate-Speech.” (2017).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;</description>
	<pubDate>Wed, 02 May 2018 06:17:57 +0000</pubDate>
</item>
<item>
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Image Captioning Model: Advanced model attempt #1</title>
	<guid isPermaLink="false">https://medium.com/p/e05ee9a7eaa8</guid>
	<link>https://medium.com/@hongnin1/image-captioning-model-advanced-model-attempt-1-e05ee9a7eaa8?source=rss-c450eb982161------2</link>
	<description>&lt;p&gt;To recap, this is our baseline approach:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/@hongnin1/image-annotation-model-baseline-dataset-and-evaluation-framework-d1d1b2d1f34c&quot;&gt;Image Annotation Model Baseline, Dataset and Evaluation Framework&lt;/a&gt;&lt;/p&gt;&lt;p&gt;For the past week, we have been trying to improve our model’s performance from two different aspect: training speed and model accuracy. In order to improve speed, we further investigated in TensorFlow’s API. We realized that the bottleneck of our model’s performance is in extracting and batching our data. After some research, we have decided to try tf.record (API: &lt;a href=&quot;https://www.tensorflow.org/programmers_guide/datasets#consuming_tfrecord_data&quot;&gt;https://www.tensorflow.org/programmers_guide/datasets#consuming_tfrecord_data&lt;/a&gt;). We are working on using tf.record and tf.dataset to prepare and feed our data as proposed in the link, and we are hoping to see a performance increase (fingers crossed).&lt;/p&gt;&lt;p&gt;In order to improve model accuracy, we mentioned in previous post that we are working towards finding the right way to calculate attention weights. The APIs we have tried are: &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention&quot;&gt;https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention&lt;/a&gt; and &lt;a href=&quot;https://www.tensorflow.org/programmers_guide/datasets#consuming_tfrecord_data&quot;&gt;https://www.tensorflow.org/programmers_guide/datasets#consuming_tfrecord_data&lt;/a&gt;, however the way they are calculating attention weights do not align with what the paper did, so we are considering implementing our own small neural network to extract attention weights, but it will take a while. Our teammates are currently working on developing our own neural network for this week and see if it will work. It’s a trail and error process so it might take some time.&lt;/p&gt;&lt;p&gt;In terms of the model itself, we have previously implemented InceptionNet as our CNN encoder. However we encountered the same problem as we had for calculating attention weights, this InceptionNet is a different implementation from the paper we are basing our model on, so we decided to switch to VGG. We are still trying to find a suitable VGG16 (CNN) implementation with checkpoint and this is what we are currently considering: &lt;a href=&quot;https://github.com/machrisaa/tensorflow-vgg&quot;&gt;https://github.com/machrisaa/tensorflow-vgg&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Overall speaking, our baseline model has a lot of problems we have been fixing for the past week in terms of training speed and attention visualization. Also changing our neural network for CNN encoder is taking a large chunk of our time as well. We are hoping to fix these problems but it might take a while for us to figure out the best approach. We are trying our best and hopefully come up with an even more advanced model sometime soon.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=e05ee9a7eaa8&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 02 May 2018 04:26:35 +0000</pubDate>
</item>
<item>
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Advanced model attempt one</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-5600014144802012716.post-7248717662742163548</guid>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/05/advanced-model-attempt-one.html</link>
	<description>I fixed the performance issues, and I have been working on adding enhancements to the model such as restricting the action space, by only allowing the model to generate types from the class, as well as debugging it to improve accuracy. The results are: 7.7 EM and 22.1 Bleu.</description>
	<pubDate>Tue, 01 May 2018 15:57:00 +0000</pubDate>
	<author>noreply@blogger.com (nlpcapstone)</author>
</item>
<item>
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Fully-featured Model Results and More Ideas for Evaluation</title>
	<guid isPermaLink="false">https://medium.com/p/e7e525549f94</guid>
	<link>https://medium.com/@ryanp97/fully-featured-model-results-and-more-ideas-for-evaluation-e7e525549f94?source=rss-6378d85d3a9b------2</link>
	<description>&lt;p&gt;In the past week, I’ve been working on training a fully-featured baseline model as well as a large, fully-featured model. This post will cover the results from these two models as well as a comparison between the featureless model.&lt;/p&gt;&lt;p&gt;Additionally, in the past week Jan, Michael, and I have been discussing different metrics for evaluating graphs. Based on this, the project may pivot away from building more complex models for the semantic transfer task to focus on exploring new and novel techniques for evaluation.&lt;/p&gt;&lt;p&gt;So in the coming week, I’ll be continuing to explore with slightly larger models as well as exploring different evaluation techniques.&lt;/p&gt;&lt;h4&gt;Fully-featured Model Results&lt;/h4&gt;&lt;p&gt;To be expected, evaluating the fully-featured model is not as simple as running SMATCH and taking the F1 score at face value. After expanding the model’s predictions, the baseline model received an F1 score of 0.77, with all features included. As mentioned in the previous post, this score is inflated since SMATCH equally weights predicate and feature accurate despite features generally being easier to predict compared to predicates or arguments.&lt;/p&gt;&lt;p&gt;Considering this, I stripped away all features from the predictions except for tense and mood and evaluated the model again. Note the only thing that has changed is an extra post-processing step has been added to the same model and the same predictions. This change resulted in a SMATCH score of 0.65. From here, I removed all features from the predictions and re-ran SMATCH and got an F1 score of 0.63. It seems that we are able to train a model using data with squashed features with little to no risk of performance loss.&lt;/p&gt;&lt;p&gt;For recap, here is a small table with the results so far.&lt;/p&gt;&lt;pre&gt;Fully-featured (all)          -  0.77&lt;br /&gt;Fully-featured (tense, mood)  -  0.65&lt;br /&gt;Fully-featured (none)         -  0.63&lt;/pre&gt;&lt;pre&gt;Featureless (baseline)        -  0.65&lt;/pre&gt;&lt;p&gt;As a side note: I also attempted to train a larger model with 600 embedding size, 750 hidden size, 3 encoder and decoder layers. This, however, resulted in extreme overfitting. The model predictions (without features) received an abysmal F1 of 0.32. In an effort to not go as crazy with the parameters while making the model larger to account for adding extra information in the input, I’m training a model with 600 hidden size and 3 layers and reverting back to a 500 embedding size.&lt;/p&gt;&lt;h4&gt;Other Ideas for Evaluation Exploration&lt;/h4&gt;&lt;p&gt;As mentioned earlier in this post, an issue with SMATCH is that it weights predicates, arguments, and features all equally when features are significantly easier to predict. To address this, Jan, Michael, and I have discussed some different evaluation ideas that we might want to explore.&lt;/p&gt;&lt;p&gt;The first idea is to separate the predicates from the features and calculate precision and recall on just the predicates. We can also further divide the predicates into surface predicates and abstract predicates (abstract predicates often have the _ prefix). This allows us to get a more detailed insight on the model’s ability to predict predicates that SMATCH has issues revealing.&lt;/p&gt;&lt;p&gt;A second idea is to replace all predicted features with some dummy feature so that we can upper bound the score when features are being predicted. By doing this, we can get a more accurate gauge of how well the model is predicting features.&lt;/p&gt;&lt;p&gt;Finally, the third idea is to eventually do something similar to this &lt;a href=&quot;https://github.com/mdtux89/amr-evaluation&quot;&gt;AMR evaluator&lt;/a&gt;. It computes SMATCH and F-scores on various different versions of the predictions by selectively cleaning up portions that are not useful to a specific metric. This would be an interesting idea and an interesting thing to reimplement/repurpose for the use of DMRS graphs.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=e7e525549f94&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 01 May 2018 07:10:59 +0000</pubDate>
</item>
<item>
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #5: More Baselines and Error Analysis</title>
	<guid isPermaLink="false">https://medium.com/p/27c90aa3c1aa</guid>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-5-more-baselines-and-error-analysis-27c90aa3c1aa?source=rss-fad49d942bf3------2</link>
	<description>&lt;p&gt;I spent this week implementing other baselines, running last week’s neural baseline on the re-split training dataset, and analyzing results and errors from the run on the new dataset.&lt;/p&gt;&lt;h3&gt;Baselines&lt;/h3&gt;&lt;h4&gt;Random Baseline&lt;/h4&gt;&lt;p&gt;This baseline randomly assigned labels to examples in accordance with the training distribution. If 45% of the training examples were negative, it would assign a negative label to any given example with a 45% probability. I implemented this baseline as a comparison tool to ensure that other model I implemented were actually learning something. The f1 scores are as reported:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/503/1*GhPCrHpMs81djY06j3cFTA.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;Sentence-level Neural Baseline&lt;/h4&gt;&lt;p&gt;In the &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;Choi et al., 2016&lt;/a&gt; paper that I’m working on improving, this model is used as a baseline. I decided the report the results here and use them as baseline results for my own project as well.&lt;/p&gt;&lt;p&gt;This baseline is derived from &lt;a href=&quot;https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf&quot;&gt;Socher et al., 2013&lt;/a&gt;, in which the authors implemented a sentence-level RNN model for sentiment classification. To adapt this model for the purpose of entity-entity relation extraction, Choi et al. performed four steps:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Collect all sentences where entities co-occur (appear in the same sentence)&lt;/li&gt;&lt;li&gt;Run each of the collected sentences through the classifier, and amalgamate the positive/negative sentiment results for each&lt;/li&gt;&lt;li&gt;Assign positive label to entity pair if at least one collected sentence was classified positively&lt;/li&gt;&lt;li&gt;Otherwise, assign negative label to entity pair if all sentences classified negatively&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The f1 scores for this model are as reported:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/402/1*H0uAoCZuPsanMJKNYfhebg.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;ILP Model Baseline&lt;/h4&gt;&lt;p&gt;This model is the main model introduced by the &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;Choi et al., 2016&lt;/a&gt; paper. It is an ILP model that is basically combines a base SVM model, which predicts entity-entity pairwise sentiment, with soft constraints from social science theories.&lt;/p&gt;&lt;p&gt;The f1 scores for this model are as reported:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/402/1*tCbKTqxFbNYcuGfbtrlyzA.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;Neural Baseline&lt;/h4&gt;&lt;p&gt;This model and the f1 scores for it are described in my previous post (see &lt;a href=&quot;https://medium.com/@be.li.nda/nlp-capstone-blog-4-baseline-model-i-7de5277b5be&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;In my last blog post, I mentioned that one of the challenges I was facing was the fact that the train set and the dev/test sets were drastically different. This week, I have brainstormed a few ideas to approach this problem:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Weak supervision approach: generate more training examples with the “no sentiment” label by assuming all unlabeled entity pairs in the training set express no sentiment to each other. Since the training set has many examples, but is very sparsely annotated (annotates few entities per document), it should be relatively easy to find many unlabeled entity pairs.&lt;/li&gt;&lt;li&gt;Adjust class weights to have the model pay more attention to minority classes in the training data, such as the “no sentiment” class.&lt;/li&gt;&lt;li&gt;Adjust the decision threshold of each sentiment class for the model. Given the final LSTM output (a probability score for each of the three classes), rather than just labeling the example by the majority class, perhaps tune the threshold for labeling an example as each of the three sentiments.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;In the meantime, I wanted to see how the model would perform if the datasets were more similar in distribution. I created a new dataset (which I will refer to as train’/dev’/test’) by splitting the train data by document using an 80/10/10 ratio. The results of the neural baseline on this dataset are report below:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*JYd_FY4ZYnUJj5Fnn-kLKg.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;The performance on the new dev’ set is much better than on the original dev set, although there is still some discrepancy.&lt;/p&gt;&lt;h3&gt;Error Analysis&lt;/h3&gt;&lt;p&gt;The final thing I did this week is error analysis. I used the results on the new dev’ of the neural baseline model for this analysis. The main thing I looked for was how each entity within a document was being classified, as I had hypothesized that the target/holder-ness of entities weren’t being encoded well enough.&lt;/p&gt;&lt;p&gt;The statistics from my error analysis appear to support my hypothesis.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Of documents that had multiple entity pairs annotated (52% of documents), the model predicted 84.8% of them to have the same sentiment relation for all entity pairs, whereas only 30.3% of them actually classify the same sentiment for all entity pairs.&lt;/li&gt;&lt;li&gt;Up to 77% of the mistakes on the dev set could have been due to the model focusing on the classifying the central sentiment of the document, rather than classifying sentiments between specific (possibly peripheral) entity pairs in the document.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;For the second statistic, I defined the “central sentiment” of the document as the sentiment dynamic that the document was focused on , expressed by the main entities of the document. As a more concrete example, take an article headlined “&lt;em&gt;Protestors Storm BBC over Far-Right Leader’s TV Slot&lt;/em&gt;,” which has a negative central sentiment. I manually went through the misclassified examples and counted how many of them were predicted in line with the central sentiment of the document (i.e. for the above example article, how many of its entity pairs are mistakenly predicted “negative”).&lt;/p&gt;&lt;p&gt;In terms of possible solutions for this problem, I’m planning to augment the architecture of my neural model to encode target/holder-ness in the architecture itself, rather than just within the input embeddings. Two possible architectures I plan to experiment with are either the &lt;a href=&quot;https://homes.cs.washington.edu/~luheng/files/emnlp2017_lhlz.pdf&quot;&gt;Lee et al. paper&lt;/a&gt; or the &lt;a href=&quot;https://arxiv.org/pdf/1802.10569.pdf&quot;&gt;Verga et al. paper&lt;/a&gt;. I describe both of these architecture, as well as how I’m going to adjust them for the entity-entity sentiment analysis task, in more detail in my project proposal blog post (see &lt;a href=&quot;https://medium.com/@be.li.nda/nlp-capstone-blog-3-project-proposal-c8a12d3ae611&quot;&gt;here&lt;/a&gt;). While I haven’t figured out the exact mechanics of how I’m going to incorporate them, I definitely plan to experiment with some ideas from each paper next time.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=27c90aa3c1aa&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 25 Apr 2018 06:30:23 +0000</pubDate>
</item>
<item>
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Image Annotation Model Improved Baseline</title>
	<guid isPermaLink="false">https://medium.com/p/fd61ae20a2f1</guid>
	<link>https://medium.com/@hongnin1/image-annotation-model-improved-baseline-fd61ae20a2f1?source=rss-c450eb982161------2</link>
	<description>&lt;p&gt;Our original baseline can be found here:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/@hongnin1/image-annotation-model-baseline-dataset-and-evaluation-framework-d1d1b2d1f34c&quot;&gt;Image Annotation Model Baseline, Dataset and Evaluation Framework&lt;/a&gt;&lt;/p&gt;&lt;p&gt;There are several major improve we have done to our baseline:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;We added Attention to our deep learning network to improve our decoding scheme: as can be seen in the snapshot below, we decided to change LSTM (previous baseline) to GRU due to performance reasons. We discovered that because our training corpus is extremely large, it takes way to long to train our model using LSTM, hence the switch to GRU.&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/838/0*mAvISRtQGogv4cQ-.&quot; /&gt;code block we added for adding attention/GRU&lt;/figure&gt;&lt;p&gt;2. Now that we are more familiar with TensorFlow (we have been watching tutorials online), we improved our APIs and incorporated better TensorFlow code to make our code base more organized and efficient. More specifically, we added “tf.contrib.seq2seq.GreedyEmbeddingHelper” to help feed data into the network, as well as adding “tf.contrib.seq2seq.BasicDecoder” and “tf.contrib.seq2seq.dynamic_decode” to perform dynamic unroll of RNN when doing decoding.&lt;/p&gt;&lt;p&gt;3. We had some bugs with our data parsing, and we have spent a large chunk of time debugging and eventually fixed the problem.&lt;/p&gt;&lt;p&gt;As for evaluation, we dived deep into the code base we found online for evaluation (for more detail see our previous post for baseline) and have decided to use the same evaluation method as the baseline approach because it is pretty established and work pretty well in determining whether an annotation is valid or not by using BLEU score).&lt;/p&gt;&lt;p&gt;4. We kept trying different attention weights and visualized the attention weight on input graphs.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=fd61ae20a2f1&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 25 Apr 2018 05:48:06 +0000</pubDate>
</item>
<item>
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: Baseline II and Updates</title>
	<guid isPermaLink="false">https://medium.com/p/79ed786e5c74</guid>
	<link>https://medium.com/nlp-capstone-blog/baseline-ii-and-updates-79ed786e5c74?source=rss----9ba3897b6688---4</link>
	<description>&lt;p&gt;This week, our focus was to improve the original baseline model with an approach more tailored to the task of generating definitions. There were a few key challenges that the original baseline approach did not address. This included generating grammatically sound English sentences, and incorporating keywords. Over the past few weeks, we also explored text generation techniques used in poetry (Ghazvininejad et. al). Our new approach is inspired by the techniques used in this paper and aims to address the two major problems with our first baseline.&lt;/p&gt;&lt;h4&gt;Revised Approach&lt;/h4&gt;&lt;p&gt;One of the biggest issues with the first baseline models, which were neural language models, was that the sentence outputs were not coherent or grammatical. Ghazvininejad et. al addressed a similar structural issue by creating a large Finite State Machine of all possible paths one could take while generating a sonnet. Each path is grounded in the filtered vocabulary developed in earlier steps and technically would have produced a structurally sound sonnet. Though all of these paths were not great, the FSM provided a foundation for generating the best sonnet. We use a similar approach, where we create an FSM of all possible paths through the training corpus. For example, if we encountered the following sentence in the corpus, it’s corresponding FSM would look like this:&lt;/p&gt;&lt;blockquote&gt;Osteoporosis is a bone disease.&lt;/blockquote&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*uWBb5Vc4EerUU9hEvzHDuQ.png&quot; /&gt;Example FSM&lt;/figure&gt;&lt;p&gt;In this manner, we construct an FSM for the entire corpus.&lt;/p&gt;&lt;h4&gt;Extraction&lt;/h4&gt;&lt;p&gt;Another challenge of the original baseline was that it did not focus on the topic we provided to start with. Even though we provided the seed word, it tended to stray off topic very quickly. In order to fix this, we chose to use some techniques from extractive summarization and the keyword identification process from the poetry generation paper. To start, we have to identify some common words associated with the term we’re defining. In Generating Topical Poetry, the authors use distance between word vectors as a metric for similarity, which we also do in our model. This allows us to determine the top few keywords associated with the term.&lt;/p&gt;&lt;p&gt;Now, we can proceed to look through the corpus and extract all sentences that contain a keyword or the term itself. This allows us to capture the context in which a term is mentioned. In the future, we can capture more sentences than this, especially if we use a window larger than one sentence for capturing context. Once we have our extracted sentences, we can proceed to modify the FSM.&lt;/p&gt;&lt;p&gt;Our modified FSM will only contain words that are included in the extracted sentences and a set of pre-selected connective words required to generate grammatical sentences. We hope that this will narrow down the search space in our decoding phase.&lt;/p&gt;&lt;h4&gt;Definition Generation via Beam Search&lt;/h4&gt;&lt;p&gt;The FSM mentioned previously needs a scoring mechanism with which to extract likely paths. We plan on using an RNN language model (LSTM, GRU, etc.) to decode the FSM and produce the generated definition one sentence at a time. Although it is asking a bit much to trust the neural language model to produce coherent, semantically rich sentences, we trust that the amount of structure we’re introducing before inference nudges the model to connect the dots in the most sensible way.&lt;/p&gt;&lt;p&gt;An issue we’re left with is the actual beam search itself. Beam search is a useful alternative to exhaustive search for fixed sized sequences in that we only continue paths that are one of the top K likely paths at every step. The difficulty lies in the fact that beam search is best used for deriving most likely paths of a fixed length. When we’re at the stage of generating definitions, we’ll have to figure out how exactly sentence length will be enforced or relaxed. It’ll be especially difficult to enforce grammar, in particular, how to terminate sentences. In the most cases this can be mitigated by generating the sentences backwards and appending punctuation.&lt;/p&gt;&lt;p&gt;Difficulties also lie in what seed to use per generated sentence. It may make sense for semantically relevant terms to appear at the end of each sentence but that isn’t necessarily how all of these words are used in practice. Regardless, an FSA using a restricted vocabulary from extracted sentences coupled with a neural language model, we believe, will be the best of both worlds. We gain assurance in semantic quality uses aspects of extractive summarization and structure we introduce while allowing the generation a reasonable amount of improvisation.&lt;/p&gt;&lt;h4&gt;BIO-Tagging Approach for Sentence Extraction&lt;/h4&gt;&lt;p&gt;For what could be a part of the FSM-style definition generation, or even a standalone definition extractor, we plan on labeling sentences throughout the corpus using BIO tags. This approach proposed by AI2 researchers involves the use of distant supervision to label sentences conducive to definition structure and semantics by picking sentences that meet a ROUGE threshold w.r.t to gold standard definitions. We would then collect triples of terms, their gold standard definitions, and their BIO-tagged sentences. We could then train a sequence tagger to recognize what sentences in a paper are conducive to definitions and which ones aren’t.&lt;/p&gt;&lt;p&gt;Possible sources of gold standards to use for ROUGE are include WordNet, which has a large breadth of glosses but each gloss tends to be very short. We are also exploring the idea of using UMLS which would provide technical medical terms along the lines of what we’d like the model to be able to define, and another data set composed of NELL and Freebase which can be found &lt;a href=&quot;http://rtw.ml.cmu.edu/wk/WebSets/glossFinding_wsdm_2015_online/index.html&quot;&gt;here&lt;/a&gt;. With “Automatic Gloss Finding for a Knowledge Base using Ontological Constraints”, Dalvi et al. set out to simplify KBs the same way we are, and they were kind enough to make this dataset of ~500k glosses available to the research community for continuing this work.&lt;/p&gt;&lt;p&gt;A supervised aspect of this project has been lacking until now, and we believe that incorporating this sequence-tagging or other intelligent forms of extracting rich, definition-like sentences will mean the language model and beam search won’t have to work as hard. The added assurance of a restricted search space to only what is relevant is better both for inference and training.&lt;/p&gt;&lt;h4&gt;Progress&lt;/h4&gt;&lt;p&gt;So far, the most difficult part of our project has been determining a more advanced approach to start with. Though the initial baseline model was easy to come up with, this model took several days to design. Most of the work we accomplished over the past few weeks involved talking with Waleed Ammar from AI2 and reading several research papers in order to define the architecture we have proposed above. As such, we have not made enough progress on this approach to evaluate it thoroughly. In this section, we list a breakdown of all the tasks we have.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The infrastructure and interface for constructing the FSM is complete&lt;/li&gt;&lt;li&gt;The system used to determine keywords given a term is in progress. We are currently debating whether or not to use pre-trained word vectors and are working on finishing the code&lt;/li&gt;&lt;li&gt;The extraction phase of the model (after retrieving the keywords) is not complete as it relies on the keywords. However, this part should be fairly straightforward and should be complete by the end of the week&lt;/li&gt;&lt;li&gt;Beam search decoding is in progress and the functionality to find the next beam is complete but the infrastructure for deciding when to terminate the search is in progress.&lt;/li&gt;&lt;li&gt;Although the RNN for beam search through the FSM is already written (we can use the same RNN from the original baseline), it needs to be trained on a corpus, preferably the Semantic Scholar corpus&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Another challenge we are currently facing involves organizing the data. In our original baseline model, the loss would steadily decrease as we trained on a single document but would then suddenly spike upward when we switched to a new document. The AI2 Semantic Scholar dataset consists of many different types of research papers including Computer Science, Medical, and various other domains. As such, the language in each domain differs drastically, so organizing the papers into linguistically similar groups has remained a challenge. Currently, the API provides no such tools for categorizing the papers.&lt;/p&gt;&lt;h4&gt;Conclusion&lt;/h4&gt;&lt;p&gt;Overall, we hope that this approach is a step closer to defining an architecture specific to the definition generation task. That said, there are several ways to improve the individual pieces of this architecture. We can change the hyper parameters of models in every phase, change the way they are trained, and introduce new concepts such as sequence tagging to improve the quality of the text generated by the model. This current architecture gives us a baseline on which we can continually improve.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=79ed786e5c74&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/baseline-ii-and-updates-79ed786e5c74&quot;&gt;Baseline II and Updates&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</description>
	<pubDate>Wed, 25 Apr 2018 05:43:39 +0000</pubDate>
</item>
<item>
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: More Data Collection and Baseline</title>
	<guid isPermaLink="false">http://cse481n-capstone.azurewebsites.net/?p=51</guid>
	<link>http://cse481n-capstone.azurewebsites.net/2018/04/24/more-data-collection-and-baseline/</link>
	<description>&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Data Collection: &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In the last blog we discussed the challenges of trying to find general Reddit posts that were similar to the collected MeanJokes posts. Even limiting to posts with Jaccard Similarity &amp;gt; .3 a lot of the data looked like the following:&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;MeanJokes Post: “Don’t be offended but Fuck you”
    Similar Post: “fuck Foligno”
    Similar Post: “fuck narek”
    Similar Post: “fuck&quot;
    Similar Post: “Fuck me?”
    Similar Post: “Fuck me”
    Similar Post: “fuck me”
&lt;/span&gt;    Similar Post: “Fuck it”
    Similar Post: “Fuck”
    Similar Post: “Who the fuck are you?”&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;These wouldn’t be very interesting examples to eventually train a model on. We also noticed that because of the nature of Jaccard Similarity and the sparsity of language in our collected Reddit posts, most of the posts that matched our MeanJokes posts would be very short, containing one or two key phrases from the MJ post. Posts made to Reddit are typically either very long or very short, so to make use of those long posts we decided to split them up by sentence and consider every sentence individually. We would also filter out sentences that are below a certain number of tokens, so that we avoid examples like the above.&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;MeanJokes Post: “How is ScizorSci like Hoss McDank? They’re both faggots!”
&lt;/span&gt;    Similar Post: “How long was he like that?”
    Similar Post: “More like CRAPitalism (this but unironically)
    Similar Post: “Volcanoes are like earth pimples”
    Similar Post: “I cried like a bitch”
    Similar Post: “She doesn’t like jewelry”
    Similar Post: “Everyone was like daaaaayum”
    Similar Post: “Don’t speak to me like that”
    Similar Post: “Don’t like the smell of this at all”
    Similar Post: “A few others I like are”
    Similar Post: “It’s like I’m on fire”&lt;/pre&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;These are better than the previous examples but still the similarities are very shallow. Most of the matches are just because there were one or two content phrases that matched between them. This could be expected from having a Jaccard Index cutoff as low as .3, since usually you want one that is somewhere above .7, but the language used in these posts is too sparse to be this picky and still have enough data to train a neural network. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We could possibly try similarity on word embeddings or sentence embeddings, but we liked using Jaccard Index because we actually care about the specific words used and not just the semantic meaning. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our main issue ended up being that we assumed most posts would be conversationally structured with short-ish text, when in reality we found that posts are either really short, some collection of tags for indexing or trading, really long posts with at least a paragraph of text about some abstract subject, or requests for sexual favors. Overall this makes general reddit posts quite different from r/meanjokes, so at the surface level jaccard index won’t really do much, and furthermore general reddit posts won’t be conversational in structure the way r/meanjokes posts are. For these reasons we will have to move on to looking at comments instead, since we believe that they will be more conversational than posts. We originally wanted to use posts instead of comments since posts are contextually self-containing while comments are typically responses to multi-person conversations. However, we ended up splitting posts into independent sentences anyway, so this reasoning for avoiding comments became moot.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Baseline Model:&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;This week, we developed a baseline Neural Network model using allennlp. The model architecture is simple. We used pretrained &lt;/span&gt;&lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;glove twitter word embeddings&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;, encode each tweet with a recurrent neural network (e.g. RNN, LSTM, GRU) sequence to vector encoder, and finally feed the vector into a feed-forward network with softmax at the end. We experimented with glove twitter word embeddings with 50 dimensions. We also tried different flavors of 1 layer recurrent neural network sequence to vector encoders, more specifically, GRU, LSTM, BiLSTM, and RNN. By the time this blog is written, we have yet performed extensive hyperparameter tuning. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Among the 4 different model setups that we tried, one of the models (Model1) got the highest accuracy, recall, and f1 score on test data, while another model (Model2) got the highest precision on test data. Below are their performances on dev and test dataset. &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model1 — embedding: 50-dimensional glove twitter embeddings, encoder: 1 layer GRU, epoch chosen: 20&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Test&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8245&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8181&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7934&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7896&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7995&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7947&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7964&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7921&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model2: — embedding: 50 dimensional glove twitter embeddings, encoder: 1 layer BiLSTM, epoch chosen: 5&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Dev&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Test&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Accuracy &lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8409&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8175&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Precision&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8239&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8004&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Recall&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7909&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7627&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;F1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.8070&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;0.7811&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;These two models’ setups are almost the same except for model1 uses a GRU encoder while model2 uses an BiLSTM encoder. Surprisingly, Model1 ends up having better overall performance on test data than the ones with more complex encoders like BiLSTM. &lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Error Analysis:&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We will use the Model2’s errors in our error analysis:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;First we will look into sentences that are hateful but our model classified as none:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;@RichardDawkins @AbuelitoSerge Really, Muslims understand this. They just want to be able to use the name “racism” to shut us up.&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;@RTUKnews An Islamist human rights group? LOL. Now there is a contradiction in terms.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;From the above examples, we found that our model is not good at understanding the underlying meaning of a sentence. For example, the 2rd one implies Islamist doesn’t care about human rights, which is attacking Islam people. However, since this sentence does not have any words that are very sensitive, our model considered it as OK instead of hateful.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Here are some other sentences that are not hateful but our model classified then as hateful:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;@Strubbestition Name one thing that is not an opinion but is still sexist. I will wait.&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;@Bipartisanism @AllooCharas Terrorism involves a political or religious objective to the terror.Most mass murderers have personal objectives&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;On the other hand, we found a trend that sentences including words like “sexist”, “crime” are classified as hateful disregarding what exactly the post means. For a concrete example, the 3rd sentence from 2nd group is not saying anything hateful but our model considered it as hateful. We suspect that because “murderers” appeared in that sentence and in our training data and most other sentences with such word is hateful, our model picked up such pattern and made a wrong decision.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Why we end up have a pretty bad result? We have two possible reasons:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our dataset used is really small (15k sentences in total) and dataset itself is really noisy. For example, “@dgbattaglia Saw this this morning… http://t.co/9YUwOuZugw” is somehow labeled as hateful as true label in original dataset.&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our model is not expressive enough to recognize more complicated patterns. This also has something to do with the dataset. With such a small dataset, we cannot really train a deep or more complicated model.&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Next Steps: &lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;What we are seeing in training is is this general pattern. We suspect it is because the dataset we have (around 15k tweets) is too small for a neural network model. We would want to try combine another &lt;/span&gt;&lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;twitter hate speech dataset (by Thomas Davidson et. al.)&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; and Waseem’s twitter dataset and train different neural net models on the combined dataset. &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;&lt;br /&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; class=&quot;alignnone wp-image-54&quot; height=&quot;192&quot; src=&quot;http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/04/P3-300x138.png&quot; width=&quot;417&quot; /&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Data Sources: &lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06&quot;&gt;Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Automated-Hate-Speech-Detection-and-the-Problem-of-Davidson-Warmsley/6ccfff0d7a10bf7046fbfd109b301323293b67da&quot;&gt;Davidson, Thomas J et al. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM (2017).&lt;/a&gt;&lt;/p&gt;</description>
	<pubDate>Wed, 25 Apr 2018 05:00:21 +0000</pubDate>
</item>
<item>
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog 5: Strawman II</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-5600014144802012716.post-1597347106413431292</guid>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/04/blog-5-strawman-ii.html</link>
	<description>&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Currently my model is at 20.1 Bleu and .02 EM. The state of the art has 23 Bleu and .08 EM. I'm&lt;/span&gt;&lt;br /&gt;&lt;span&gt; not doing error analysis yet since I'm only able to train my model on 1/3 of the data due to &lt;/span&gt;&lt;br /&gt;&lt;span&gt;performance and memory issues. I've cut training time in half through several optimization &lt;/span&gt;&lt;br /&gt;&lt;span&gt;but there is a memory bug which I haven't found yet which prevents me from training on the &lt;/span&gt;&lt;br /&gt;&lt;span&gt;whole dataset. Thus, I will list what I've done on the performance end, and my action plan.&lt;/span&gt;&lt;/div&gt;&lt;span id=&quot;docs-internal-guid-17abcdb6-f838-25e9-608e-d57755b90e92&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Speed Optimizations:&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Problem - Training ⅓ dataset taking 6 hours per epoch.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Solutions implemented:&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;Get_states_to_consider slowest part of take_step. This was optimized along with the expensive padding operation in get_action_embeddings, and the inefficient looping and sorting in compute_new_states. This cuts the training time in half, but still takes 3 hours on training set for 1 epoch&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre-wrap;&quot;&gt;Solutions to explore:&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;Further areas to improve which cut down by about ½ hour are create_grammar_state, embed_actions, map_entity_productions. Here since there are 10,000 global rules which are processed for every batch in every iteration, just process them once in the constructor.&lt;/li&gt;&lt;/ul&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Memory Optimization:&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Problem - Low identifier threshold or high embedding dim like in paper, on 40,000 or more instances causes gpu out of memory error.&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Debugging:&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;Printing all tensors in memory at the end of forward using python garbage collection package. Total size is around 115mb but gpu uses 12gb! Perhaps this is a memory leak?&lt;/li&gt;&lt;li&gt;Tried different configurations to see where its crashing, high embedding dim causes crash in action index select, while just large dataset causes crash in backward.&lt;/li&gt;&lt;/ul&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span style=&quot;background-color: transparent; color: black; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Solutions that I've tried:&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;ul style=&quot;margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;li&gt;Doing index select on action_embeddings for previous embedding to save space.&lt;/li&gt;&lt;li&gt;All finished states del’d&lt;/li&gt;&lt;li&gt;Del keyword used frequently after tensor no longer used.&lt;/li&gt;&lt;li&gt;Disabling cudnn backend&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre-wrap;&quot;&gt;&lt;span&gt;Solutions to explore:&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Using pytorch DataParallel package.&lt;/li&gt;&lt;li&gt;Split only when a state has finished.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;Let me know if you guys have any ideas on solving this gpu out of memory issue!&lt;/div&gt;&lt;/div&gt;</description>
	<pubDate>Tue, 24 Apr 2018 15:14:00 +0000</pubDate>
	<author>noreply@blogger.com (nlpcapstone)</author>
</item>
<item>
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Baseline Evaluation and Ideas for Evaluating Fully-featured Graphs</title>
	<guid isPermaLink="false">https://medium.com/p/79ba10bbc70c</guid>
	<link>https://medium.com/@ryanp97/baseline-evaluation-and-ideas-for-evaluating-fully-featured-graphs-79ba10bbc70c?source=rss-6378d85d3a9b------2</link>
	<description>&lt;p&gt;Since the last week, I’ve been working on cleaning the model’s predictions in order to be able to evaluate it using SMATCH (note the following sections focus on the model trained on and predicting featureless graphs). There were a couple of cases in which the model was unable to produce a well formed graph, but those were few and far between.&lt;/p&gt;&lt;h4&gt;Dealing with Ill-formed Graphs&lt;/h4&gt;&lt;p&gt;I decided to deal with ill-formed graphs in a harsh way until Jan any I can decide on a better heuristic for dealing with them. Though, for now, whenever I encounter an invalid graph, I immediately replace it with (999999999 / invalid). I’ve verified that this graph does not appear in the development set and works as a dummy graph. Whenever SMATCH encounters this graph, it will output an F1 score of 0.0 for that prediction, label pair since there are no matched triples.&lt;/p&gt;&lt;p&gt;Ideally, we would like to be more generous with partially correct graphs, but dealing with that is non-trivial and is probably something I will explore more after training and evaluating a model that is able to predict features.&lt;/p&gt;&lt;h4&gt;Example of Ill-formed Graphs&lt;/h4&gt;&lt;p&gt;Below is an example of an ill-formed graph that the model predicted. Though the parenthesis structure is well-formed (i.e. none are missing or mismatched), the model predicted that the node _hito_n was a re-entrancy, despite it not appearing anywhere else in the graph&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/482/1*YlYEDUelqNqShF1bnjc5RA.png&quot; /&gt;An example of the model predicting a re-entrancy that has not yet occurred in the graph.&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/439/1*2QvzOMoTdU_BhDB8Zhj-ig.png&quot; /&gt;The target graph for the above example. The model’s prediction is pretty far off, but it’s interesting to see that it is able to have fairly high precision on the nodes that it is able to generate.&lt;/figure&gt;&lt;p&gt;The model that was trained on the featureless data was able to learn the parenthesis structure and there were no cases, that I could find, where the model predicted mismatched or missing parenthesis.’&lt;/p&gt;&lt;h4&gt;Baseline Evaluation Results&lt;/h4&gt;&lt;p&gt;Even with the harsh treatment of ill-formed graphs, the model was able to achieve a F1 score of 0.65 on the development dataset. This was much higher than I was expecting and seems to be promising for further exploration. I suspect that once I begin to train the model on data that includes features, the model will have a harder time making completely accurate predictions (though the F1 score will likely be inflated as explained in the next section)&lt;/p&gt;&lt;h4&gt;Ideas for Evaluating the Feature-full Model&lt;/h4&gt;&lt;p&gt;Though it’s nice to have the model correctly predict the mood, tense, etc. of a sentence, these may not be entirely helpful when evaluating the model. By including these features, we increase the number of triples that SMATCH looks for. There are a couple different ideas that Jan, Michael, and I discussed when it comes to evaluating the predicted graphs:&lt;/p&gt;&lt;p&gt;The first option is to leave the features squashed. This is probably the simplest approach, but is not ideal since it does not allow for partial credit. If the model misses a single feature (i.e. predicts the mood as tensed rather than untensed, the entire triple would be wrong even though other features are correct).&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*iQNyCAiGWw0w5YIXND8THw.png&quot; /&gt;Example of squashed features. Note this is the same example from last week’s post.&lt;/figure&gt;&lt;p&gt;The second option is to uncompress the features. In the case of the previous picture, it would be removing the equals signs and splitting the “super feature” based on the colons. This approach is a bit nicer in terms of giving partial credit, but is still not ideal since in increases the number of triples. We believe that this would inflate the SMATCH score to something that is not entirely useful and most of the score would be based on the feature triples and not the actual semantic meaning.&lt;/p&gt;&lt;p&gt;The third and final option we discussed was to selectively include features during evaluation. During training and prediction, the model would take in the entire squashed feature as input and output squashed features during prediction. However, we would uncompress the features and then choose a couple (or none at all) to keep when we post-process the graphs. With this, we can evaluate using a couple different combinations of features and see how those affect the model’s performance.&lt;/p&gt;&lt;p&gt;Some of the features the model predicts are: tense, mood, pass, etc. We can choose to keep only mood and tense if we believe that these are the most useful features for semantic transfer and see how that affects performance vs. a featureless model or some other combination of features.&lt;/p&gt;&lt;h4&gt;Training a Fully-featured Model&lt;/h4&gt;&lt;p&gt;Currently I’m training a model on data that includes the squashed features, though it will not be evaluated in time for this post or for the presentation. The model is the same exact model as the featureless model, so I expect that it may have a more noticeable issue matching parentheses and generating well-formed graphs in general until I have some time to experiment with the hyperparameters.&lt;/p&gt;&lt;p&gt;For reference, the models (both featureless and feature-full) are trained with the following architecture and hyperparameters:&lt;/p&gt;&lt;pre&gt;Encoder:&lt;br /&gt;    RNN Type       :  LSTM    &lt;br /&gt;    Embedding Dim  :  500&lt;/pre&gt;&lt;pre&gt;Decoder:&lt;br /&gt;    RNN Type       : Stacked LSTM&lt;br /&gt;    Layers         : 2&lt;br /&gt;    Embedding Dim  : 500&lt;br /&gt;    Dropout        : 0.3&lt;/pre&gt;&lt;pre&gt;Attention:&lt;br /&gt;    Type           : Global (&lt;a href=&quot;https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb&quot;&gt;general [Luong]&lt;/a&gt;)&lt;/pre&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=79ba10bbc70c&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 24 Apr 2018 06:17:52 +0000</pubDate>
</item>
<item>
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog 4: Strawman I</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-5600014144802012716.post-2741407940361589303</guid>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/04/blog-4-strawman-i.html</link>
	<description>I'm implementing the parser within the the wikitables semantic parser(Neural Semantic parsing with type constraints by Krishnamurthy et al) since both are solving similar tasks and it'd be a cool result if the same architecture worked for both tasks. The similarity is that both tasks need to generate the logical form which incorporates elements of a context. For wikitables its cell and column names and for this task its variable and method names.&lt;br /&gt;&lt;br /&gt;Results:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Wikitables framework - 4% accuracy.&lt;/li&gt;&lt;li&gt;Wikitables framework + parent states - 12% accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The wikitables baseline performs poorly and integrating the parent production rule used as input to the decoder cell in the java paper but not the wikitables paper results in a eight percent improvement. This is since the java production rule sequences are much longer and cannot just rely on the previous rule.&lt;/div&gt;&lt;div&gt; &lt;/div&gt;&lt;br /&gt;These results demonstrate that while that tasks are similar there are some key differences in the datasets that won't allow the exact same architecture to be used. Nonetheless, I will still be implementing the java parser within the wikitables framework incorporating necessary elements to boost performance.&lt;br /&gt;&lt;br /&gt;My code is here:&lt;br /&gt;&lt;b&gt;https://github.com/rajasagashe/allennlp/tree/enviro-linking&lt;/b&gt;</description>
	<pubDate>Thu, 19 Apr 2018 19:13:00 +0000</pubDate>
	<author>noreply@blogger.com (nlpcapstone)</author>
</item>
<item>
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: First Impressions: Baselines and the Evaluation Framework</title>
	<guid isPermaLink="false">https://medium.com/p/15357a82fe06</guid>
	<link>https://medium.com/nlp-capstone-blog/first-impressions-baselines-and-the-evaluation-framework-15357a82fe06?source=rss----9ba3897b6688---4</link>
	<description>&lt;p&gt;Ultimately, our goal is to go beyond basic language modeling and create a new text generation architecture conducive to producing technical definitions. To get a feel for the data though, we approach it with familiar, simple baselines that give us a foundation in which we can improve from.&lt;/p&gt;&lt;p&gt;The baselines that we’ve experimented with are&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Vanilla RNN:&lt;/strong&gt; Hidden states as a function of the input and the previous hidden state, with Tanh activation (in particular, we’re using the &lt;a href=&quot;https://medium.com/@tamdangnadmat/first-impressions-baselines-and-the-evaluation-framework-15357a82fe06&quot;&gt;Elman Network&lt;/a&gt;).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;GRU:&lt;/strong&gt; An RNN architecture that learns to throttle the influence and usage of particular parameters on inference using a gating mechanism.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;LSTM: &lt;/strong&gt;Another RNN architecture that specializes in intelligently remembering relevant details and forgetting irrelevant details through several gating mechanisms and a “cell state” in addition to the conventional hidden states. &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Here’s more detail&lt;/a&gt; about this particular kind of RNN.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All baselines were trained as language models with cross entropy loss and were used to get a sense of how learnable the language of the Semantic Scholar dataset is. The metrics we are focusing on now are&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Perplexity: &lt;/strong&gt;A measure of how “confused” the model is at any point it’s attempting to predict the next word.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Feature extraction through hidden states:&lt;/strong&gt; Can the hidden states be used as features for a classification task?&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Our first metric is fairly straightforward; we calculate the aggregate sum of log probabilities for every word in the corpus and normalize by the size of the corpus.&lt;/p&gt;&lt;p&gt;Our second metric however, is inspired by Dieng et al.’s method for sentiment analysis in &lt;a href=&quot;https://www.semanticscholar.org/paper/TopicRNN%3A-A-Recurrent-Neural-Network-with-Semantic-Dieng-Wang/412068c7e8e77b73add471789d58df3d2f3e08d8&quot;&gt;TopicRNN&lt;/a&gt;, in which the final hidden states after a forward pass of the model on a movie review were used to predict positive or negative sentiment using a single-layer neural network. We aim to adopt this metric from a multi-class classification perspective in which the passages we use are excerpts of research documents with an omitted, technical term. The question we aim to answer with this metric is “is the model capable of representing semantics in a latent space?”&lt;/p&gt;&lt;p&gt;Our labels will then be a defined set of these omitted, technical terms, and our goal will then be to predict them given the hidden states of the passage via a two-layer neural network. The framework for this evaluation metric can be found in &lt;a href=&quot;https://github.com/NLP-Capstone-Project/machine-dictionary/tree/evaluation&quot;&gt;this branch&lt;/a&gt; of our codebase.&lt;/p&gt;&lt;h4&gt;Why we chose these metrics&lt;/h4&gt;&lt;p&gt;Our task is fairly novel given the way we’re approaching it, so currently no dataset exists that pairs domain-specific words with definitions that are to the caliber of research technicality. Because of this, metrics that depend on gold standards such as ROUGE and BLEU are currently out of reach at this time.&lt;/p&gt;&lt;p&gt;There’s a chance we’ll experiment with these metrics if we can find a labeled dataset to supplement Semantic Scholar’s Open Research Corpus. We are also considering using the publications themselves as the gold standards, which may be helpful since a desirable trait of our model would be its ability to produce language similar to that of the corpus.&lt;/p&gt;&lt;h3&gt;Challenges Encountered while Baselining&lt;/h3&gt;&lt;p&gt;In establishing our baselines and metrics, there were several issues we ran into, both in training.&lt;/p&gt;&lt;h4&gt;Dealing with a larger corpus&lt;/h4&gt;&lt;p&gt;Given that our baselines are language models, and that our later prototype models will most likely contain an LM component, we have to deal with efficient learning given there are several million documents to process.&lt;/p&gt;&lt;p&gt;For efficient backpropagation, we opted to introduce a “backpropagation through time” as a hyperparameter defaulted at 50, which specifies the number of words we allow the model to see before updating our parameters.&lt;/p&gt;&lt;p&gt;Currently, batching is supported by our codebase but was not used in our initial experiments. Given that it takes roughly one minute for the GPUs on the cloud to process a single publication, we plan to concatenate document vectors and reshape into batch-by-length tensors in the future.&lt;/p&gt;&lt;h4&gt;Sorting By Domain&lt;/h4&gt;&lt;p&gt;We’d like our model to be trained on a single domain/field of study for our future case studies comparing dictionaries built on one domain versus others. This is further motivated by some of our experiment results discussed later, how loss tends to spike between documents.&lt;/p&gt;&lt;p&gt;Currently, the Semantic Scholar Open Research Corpus doesn’t include anything in the set of JSON fields that we could find for filtering the data. However, we’ve been assured by AllenNLP researchers that its possible to sort the data by research domain. We may performing another round of baseline experiments once we’ve sorted the data, but for now the results below are on publications of mixed domains.&lt;/p&gt;&lt;h3&gt;Experimental Results&lt;/h3&gt;&lt;p&gt;The Semantic Scholar Open Research Corpus provides a sample subset of its dataset: a JSON file containing 4000 entries. Within each entry, the URL of the publication’s online PDF is provided. We use a GET request to &lt;a href=&quot;https://github.com/allenai/science-parse&quot;&gt;AI2’s Science Parse&lt;/a&gt; service to extract the PDF contents.&lt;/p&gt;&lt;p&gt;From there, we run our experiments on 121 of the 4000 extracted documents, stopping training early at 15 documents and calculated perplexity on a validation set of 30 documents. The total vocabulary used was 11,330 words. Words outside of this vocabulary are replaced with an unknown token at training and test time.&lt;/p&gt;&lt;p&gt;Loss is calculated and normalized on the last 50 words the model is trained on.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Elman RNN:&lt;/strong&gt; Perplexity of 250.25 with an average loss of 7.908 over the last 50 words.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;GRU:&lt;/strong&gt; Perplexity of 265.50 with an average loss of 7.085 over the last 50 words&lt;/li&gt;&lt;li&gt;&lt;strong&gt;LSTM:&lt;/strong&gt; Perplexity of 261.95 with an average loss of 6.588 over the last 50 words&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Perplexities calculated using untrained models (with randomized parameters) were several orders of magnitude larger than the ones listed above, so it’s good to know that our baseline models can learn a significant amount of surface-level patterns with such a small subset of the corpus.&lt;/p&gt;&lt;p&gt;In terms of feature extraction and classification, the framework has been implemented but the data for this has not been created yet. We plan on evaluating our baselines along with our final model on this metric using publications from Semantic Scholar after establishing a vocabulary of semantically significant technical terms and creating the dataset using those terms. This will involve iterating over documents and replacing occurrences of these technical terms with a specialized, unknown token that won’t aid in inference.&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Our methods helped us familiarize ourselves with the data as well as handle significant amounts of overhead in terms of processing the data, toggling between different models, and integration of our evaluation metrics in an organized fashion.&lt;/p&gt;&lt;p&gt;We are excited to see how novel architectures tailored to the task perform on these metrics!&lt;/p&gt;&lt;p&gt;To keep up to date with our progress in baselining, evaluation, and other things, you can watch &lt;a href=&quot;https://github.com/NLP-Capstone-Project/&quot;&gt;this repository&lt;/a&gt;.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=15357a82fe06&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/first-impressions-baselines-and-the-evaluation-framework-15357a82fe06&quot;&gt;First Impressions: Baselines and the Evaluation Framework&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</description>
	<pubDate>Wed, 18 Apr 2018 05:40:07 +0000</pubDate>
</item>
<item>
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: Data Collection and First Baseline</title>
	<guid isPermaLink="false">http://cse481n-capstone.azurewebsites.net/?p=41</guid>
	<link>http://cse481n-capstone.azurewebsites.net/2018/04/17/data-collection-and-first-baseline/</link>
	<description>&lt;h3&gt;Data Collection&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In a previous blog, we mentioned using RAKE to collect content phrases from posts in order to compare their similarity. However, we decided since then that it would be easier to just look at ngrams, since they can capture the same information as the content phrases. So, after removing stopwords we collect all of the unigrams, bigrams, and trigrams on each MeanJokes post and on each general Reddit post. However, this creates a ton of data compared to the posts or even their content phrases. So, we must do some cleaning before we make use of this data.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For data cleaning, we decided to use document frequency to further clean content ngrams gathered from both mean-jokes and other Reddit posts. The reason why we choose df is:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;First, we are using content ngrams, so it does not make sense to compute ngram term back on the original post since bigram, trigram term frequency is likely to be 1.&lt;/span&gt;&lt;/li&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Second, df provides us a good way to remove too frequent and too infrequent ngrams. If an ngram is too frequent or too infrequent, it won’t be very informative about the pattern of the sentence.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;By using document frequency, we achieved the following purpose:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Much shorter, cleaner content ngrams&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Example:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[(‘took’, ‘family’, ‘camping’), (‘camping’, ‘concentration’), (‘took’, ‘family’), (‘family’, ‘camping’), (‘took’,), (‘family’,), (‘concentration’, ‘camping’), (‘family’, ‘camping’, ‘concentration’), (‘camping’,), (‘concentration’,), (‘camping’, ‘concentration’, ‘camping’)]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;After filtering:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[(‘took’,), (‘family’,), (‘camping’,), (‘concentration’,)]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li style=&quot;font-weight: 400;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Filtered out posts written in a different language (like written in Spanish or Germany)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;[(‘adan’,), (‘zapata-con’,), (‘carino’,), (‘para’,), (‘ti’,), (‘letra’,), (‘encanta’,), (‘sii’,), (‘adan’, ‘zapata-con’), (‘zapata-con’, ‘carino’), (‘carino’, ‘para’), (‘para’, ‘ti’), (‘ti’, ‘letra’), (‘letra’, ‘encanta’), (‘encanta’, ‘sii’), (‘adan’, ‘zapata-con’, ‘carino’), (‘zapata-con’, ‘carino’, ‘para’), (‘carino’, ‘para’, ‘ti’), (‘para’, ‘ti’, ‘letra’), (‘ti’, ‘letra’, ‘encanta’), (‘letra’, ‘encanta’, ‘sii’)]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;However, we sometimes encounter a problem that after filtering, the content ngrams becomes empty. We decide to skip these lines when we do set similarity because by looking back to the original posts, posts that end up with empty content ngram are generally &lt;/span&gt;&lt;b&gt;very short&lt;/b&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; and &lt;/span&gt;&lt;b&gt;non-informative&lt;/b&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;. For example, the post “Bolivian coastline MeanJokes” ended up with an empty content ngrams. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;To determine our final dataset, we want to find Reddit posts that use similar language to the MeanJokes posts. To do this, we use &lt;/span&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Jaccard index&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; to determine similarity. Due to the nature of MeanJokes posts being rather short compared to most Reddit posts, the Jaccard Index of most posts compared to these posts would be very low. By filtering as described above, Jaccard index becomes much more useful because the number of n-grams we consider for each post is greatly reduced, down to those we consider most identifying of those posts. However, Reddit posts are very diverse so there is a great sparsity of similar posts. Very few posts have Jaccard Indices greater than .2 even after this filtering. Before filtering, very few posts would even get close to .1. For context, Jaccard similarity could be thought of as a percentage from 0 to 1, where 1 means the posts are identical and 0 means they have no intersection. We have over 3 million Reddit posts just in one month though, so we are not worried about not getting a big enough dataset if we have a high threshold for similarity.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Here’s an example of what this data looks like:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Intersection: &lt;/b&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;{(‘ca’, “n’t”, ‘spell’), (‘ca’, “n’t”), (“n’t”, ‘spell’), (‘spell’,), (‘ca’,)}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;MeanJokes Post:&lt;/b&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; {(‘ca’, “n’t”), (‘therapist’,), (‘without’,), (‘ca’, “n’t”, ‘spell’), (‘rapist’,), (“n’t”, ‘spell’), (‘spell’,), (‘remember’,), (‘ca’,)}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Reddit Post:&lt;/b&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; {(‘ca’, “n’t”), (‘ca’, “n’t”, ‘spell’), (“n’t”, ‘spell’), (‘spell’,), (‘crisis’,), (‘ca’,)}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Jaccard index:&lt;/b&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; 0.5&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;5 / (9 + 6 – 5) -&amp;gt; 5/10 -&amp;gt; .5&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Baseline Model&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Because we are in the process of data collection, we want to build baseline models on similar datasets and later port the model over to our own dataset. The dataset we chose for this purpose is the &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Twitter Hate Speech dataset&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; created by&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; Zeerak Waseem and Dirk Hovy&lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;. In this dataset, each tweet is labeled as “racism”, “sexism”, or “none”. Since our goal is to build tools that can not only detect if the text is offensive or non-offensive but also detect towards which group the text is offensive, the Twitter Hate Speech dataset serves as a good starting point. Here, we built a logistic regression model as a baseline for offensiveness detection. We consider both “racism” and “sexism” as offensive, and “none” non-offensive. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The each example in the dataset is of the format &amp;lt;tweet_id&amp;gt;, &amp;lt;label&amp;gt;. We first used &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;python-twitter &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(a python wrapper around Twitter API) to collect the original tweets by the tweet_ids given. In this process, we noticed that 1133 tweets from this dataset are already removed from Twitter. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;To make the results more reproducible, we made a train, dev, test dataset split (80%, 10%, 10%) using sklearn train_test_split function. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For data preprocessing, we used an existing &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;text preprocessor made by Zhang et. al. to clean each tweet down to plain text, removing extra space, URLs, hashtags, special characters, etc. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For text vectorization, we experimented with two different sklearn vectorizers: tf-idf vectorizer and count vectorizer. Count vectorizer converts a corpus to a document-term matrix. TF-IDF vectorizer converts a corpus to a tf-idf weighted document-term matrix. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;After that, we trained a logistic regression model on training data and tuned epoch on dev data. Both the trained model and trained text vectorizer would be saved. We then loaded in the saved model and text vectorizer to make predictions and evaluate on test data. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The metrics we chose for performance evaluation are accuracy, precision, recall, and f1 score. Here we decided to be more conservative and paid more attention to precision because the expected downstream application (a.k.a. the targeting group detector)  relies on text predicted “offensive” to be actually offensive. We also focused on the f1 score to evaluate the overall performance of this baseline model.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We ran experiments with this baseline model with the different types of vectorizers and different vectorizer setups. We decided to keep follow ngram_range (1, 4) chosen by &lt;/span&gt;Zeerak Waseem and Dirk Hovy.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set-up1:
&lt;ul&gt;
&lt;li&gt;Vectorizer:&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;type&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;strip_accents&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;analyzer&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;ngram_range&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;max_features&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;tf-idf&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;unicode&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;word&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(1, 4)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;10000&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;Model: &lt;span style=&quot;font-weight: 400;&quot;&gt;Logistic regression, epoch_chosen=5&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;dev accuracy: 0.8162&lt;/td&gt;
&lt;td&gt;test accuracy: 0.8131&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dev precision: 0.8170&lt;/td&gt;
&lt;td&gt;test precision: 0.8203&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dev recall: 0.7336&lt;/td&gt;
&lt;td&gt;test recall: 0.7333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dev f1: 0.7560&lt;/td&gt;
&lt;td&gt;test f1: 0.7552&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Set-up2:
&lt;ul&gt;
&lt;li&gt;Vectorizer:&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;type&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;strip_accents&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;analyzer&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;ngram_range&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;max_features&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;tf-idf&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;unicode&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;char&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(1, 4)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;10000&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;Model: &lt;span style=&quot;font-weight: 400;&quot;&gt;Logistic regression, epoch_chosen=18&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev accuracy: 0.8175&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test accuracy: 0.8067&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev precision: 0.8040&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test precision: 0.7951&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev recall: 0.7482&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test recall: 0.7388&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev f1: 0.7664&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test f1: 0.7561&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;Comparison: &lt;span style=&quot;font-weight: 400;&quot;&gt;By taking character level ngrams, test f1 actually improves, but test precision drops drastically. &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Set-up3:
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Vectorizer:&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;type&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;strip_accents&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;analyzer&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;ngram_range&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;max_features&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;tf-idf&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;unicode&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;char&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(1, 4)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;30000&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model: Logistic regression, epoch_chosen=10&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev accuracy: 0.8194&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test accuracy: 0.8105&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev precision: 0.8099&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test precision: 0.8021&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev recall: 0.7473&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test recall: 0.7416&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev f1: 0.7669&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test f1: 0.7598&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Comparison: By increasing the max number of features a document-term matrix can have, both test precision and test f1 improved compared with those of set-up2. &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Set-up4:
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Vectorizer:&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;type&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;strip_accents&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;analyzer&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;ngram_range&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;max_features&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;count&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;unicode&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;word&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(1, 4)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;30000&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model: Logistic regression, epoch_chosen=100&lt;/span&gt;&lt;br /&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev accuracy: 0.8321&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test accuracy: 0.8213&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev precision: 0.8213&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test precision: 0.8097&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev recall: 0.7690&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test recall: 0.7618&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev f1: 0.7871&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test f1: 0.7781&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Comparison: Vectorizer is now changed to count vectorizer. With this set-up, test f1 is the highest among all set-ups, while test precision is higher than that of set-up3 and lower of that of set-up1. &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Set-up5:
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Vectorizer:&lt;/span&gt;&lt;br /&gt;
&lt;table style=&quot;height: 96px;&quot; width=&quot;328&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;type&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;strip_accents&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;analyzer&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;ngram_range&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;max_features&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;count&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;unicode&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;char&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(1, 4)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;30000&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Model: Logistic regression, epoch_chosen=30&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;
&lt;table style=&quot;height: 291px;&quot; width=&quot;388&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev accuracy: 0.8226&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test accuracy: 0.8093&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev precision: 0.8001&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test precision: 0.7860&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev recall: 0.7690&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test recall: 0.7594&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;dev f1: 0.7812&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;test f1: 0.7698&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Comparison: Test precision is the lowest of all the set-ups, and test f1 is higher than the first 3 set-ups, but lower than set-up4.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Although &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Zeerak Waseem and Dirk Hovy’s paper stated that &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;character n-grams have better performance than work n-grams as features, our experiment results suggested otherwise. All of our set-ups have higher test precision and f1 score but lower test recall that the best model of the paper. These differences might be because of the removal of tweets from the original dataset. The train, dev, test data split method might also contribute to the differences. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Next Steps: We would like to experiment with other baseline models, especially neural networks and perform error analysis on these baseline models.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Work Cited:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/publication/323723283_Detecting_hate_speech_on_Twitter_using_a_convolution-GRU_based_deep_neural_network&quot;&gt;Zhang, Ziqi &amp;amp; Robinson, D &amp;amp; Tepper, J. (2018). Detecting hate speech on Twitter using a convolution-GRU based deep neural network.&lt;/a&gt;&lt;/p&gt;</description>
	<pubDate>Wed, 18 Apr 2018 02:36:38 +0000</pubDate>
</item>
<item>
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Image Annotation Model Baseline, Dataset and Evaluation Framework</title>
	<guid isPermaLink="false">https://medium.com/p/d1d1b2d1f34c</guid>
	<link>https://medium.com/@hongnin1/image-annotation-model-baseline-dataset-and-evaluation-framework-d1d1b2d1f34c?source=rss-c450eb982161------2</link>
	<description>&lt;h3&gt;Baseline approach:&lt;/h3&gt;&lt;h4&gt;Overview:&lt;/h4&gt;&lt;p&gt;We are using a deep neural network that learns how to output the description of a general image. Example:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*RWtQGi8MpUzCsQTmWpR0Og.png&quot; /&gt;Image from paper &lt;a href=&quot;https://arxiv.org/pdf/1412.6632.pdf&quot;&gt;https://arxiv.org/pdf/1412.6632.pdf&lt;/a&gt;&lt;/figure&gt;&lt;p&gt;Our baseline model is an encoder-decoder neural network. First it encodes a image into a vector using a deep convolutional neural network (CNN), our baseline choice of network is &lt;a href=&quot;https://arxiv.org/abs/1512.00567&quot;&gt;Inception v3&lt;/a&gt; image recognition model pre-trained on the &lt;a href=&quot;http://www.image-net.org/challenges/LSVRC/2012/&quot;&gt;ILSVRC-2012-CLS&lt;/a&gt; image classification dataset. Then decode the vector into a paragraph of description using a long short-term memory network (LSTM). Words in the output description are represented with an embedding model with each word in the dictionary represented by a fixed-length vector (learned during training).&lt;/p&gt;&lt;h4&gt;Architecture:&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*na7zdPNZQG66QVCJK1Kqmg.png&quot; /&gt;Baseline Model Architecture&lt;/figure&gt;&lt;p&gt;Above architecture outputs the log likelihoods of the correct words at each step (logp1(S1)); later we use beam search to generate the description for given image.&lt;/p&gt;&lt;h4&gt;Running the model/Experiment:&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;we downloaded Cuda, Bazel, TensorFlow, Numpy, Natural Language ToolKit(NLTK).&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;2. Download necessary data from Microsoft COCO:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Ap25lgpLRu8lkr0omWUdqQ.png&quot; /&gt;Bash screenshot of downloading MSCOCO data.&lt;/figure&gt;&lt;p&gt;3. Prepare all the necessary data we just downloaded for training:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*KBg_dBwZX6w3gIw2XzJDeA.png&quot; /&gt;Last few lines of bash logs for preparing data.&lt;/figure&gt;&lt;p&gt;4. Run training scripts and require: We only trained for about ~3 minutes.&lt;/p&gt;&lt;p&gt;5. After acquiring the model, we tried it out by input following images:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*a7WVZZT-k0fYKq82WUFLhA.png&quot; /&gt;input image: Surfer Guy&lt;/figure&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/623/1*H89koub2tfhDoxN4WPCYQA.png&quot; /&gt;input image 2: Rowing&lt;/figure&gt;&lt;p&gt;Our model’s output is not idea, for “Surfer Guy” image, our output is:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/784/1*Wl_AHUOJ8inu91LYEIKVZw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;For “Rowing” image, our output is:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/780/1*mHCnFYULsaI80jZWjfav5Q.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;6. Then by using TensorBoard, we plotted the log loss for our model in training:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*kLU6ytNXYM9ytsKUS9bUrQ.png&quot; /&gt;TensorBoard Log Loss Graph&lt;/figure&gt;&lt;p&gt;As well as our model’s architecture:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Coh87C_W_jfo4Pg4aObPwA.png&quot; /&gt;Image Captioning Baseline Model Architecture on TensorBoard.&lt;/figure&gt;&lt;h3&gt;Evaluation Framework:&lt;/h3&gt;&lt;p&gt;The model we are referencing on &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/im2txt#prepare-the-training-data&quot;&gt;git&lt;/a&gt; hub has a built in evaluation framework, so we were using their evaluation script. It logs evaluation metrics to TensorBoard which allows training progress to be monitored in real-time (refer to the image “TensorBoard Log Loss Graph”).&lt;/p&gt;&lt;h3&gt;Resources:&lt;/h3&gt;&lt;p&gt;paper referenced: &lt;a href=&quot;https://arxiv.org/pdf/1411.4555.pdf&quot;&gt;https://arxiv.org/pdf/1411.4555.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;RMSProp referenced: &lt;a href=&quot;https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop&quot;&gt;https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=d1d1b2d1f34c&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 17 Apr 2018 07:30:16 +0000</pubDate>
</item>
<item>
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #4: Baseline Model I</title>
	<guid isPermaLink="false">https://medium.com/p/7de5277b5be</guid>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-4-baseline-model-i-7de5277b5be?source=rss-fad49d942bf3------2</link>
	<description>&lt;p&gt;This week, I implemented the baseline neural model from my minimal viable plan. Specifically, my model was an attentive biLSTM, which took as input concatenated word, polarity, and holder/target embeddings.&lt;/p&gt;&lt;p&gt;As for evaluation frameworks, I’ve implemented f1 scoring. I also kept track of other metrics like accuracy and loss, in addition to f1 score, to get a better understanding of my model’s performance.&lt;/p&gt;&lt;p&gt;The following is a summary of my preliminary results. I’ve done very little hyper-parameter tuning thus far, but of the tuning that I did do, I found that changing the optimizer had the greatest benefit, specifically, using the Adam optimizer over the SGD optimizer. I trained the following two hyper-parameter settings:&lt;/p&gt;&lt;pre&gt;╔═════════════════════╦═════════╦═════════╗&lt;br /&gt;║   Hyperparameters   ║    1    ║    2    ║&lt;br /&gt;╠═════════════════════╬═════════╬═════════╣&lt;br /&gt;║ Optimizer           ║ Adam    ║ SGD     ║&lt;br /&gt;║ Learning Rate       ║ 0.01    ║ 0.05    ║&lt;br /&gt;║ # Epochs            ║ 20      ║ 20      ║&lt;br /&gt;║ Loss Function       ║ NLLLoss ║ NLLLoss ║&lt;br /&gt;║ Hidden Dimension    ║ 150     ║ 150     ║&lt;br /&gt;║ Dropout Rate        ║ 0.20    ║ 0.20    ║&lt;br /&gt;║ Batch Size          ║ 10      ║ 10      ║&lt;br /&gt;║ Embedding Dimension ║ 50      ║ 50      ║&lt;br /&gt;║    (all 3)          ║         ║         ║&lt;br /&gt;╚═════════════════════╩═════════╩═════════╝&lt;/pre&gt;&lt;p&gt;As you can see, the only differences are in the optimizer and learning rate. Note I did experiment a little with learning rate for each optimizer, however, the amount of tuning I did for this is definitely not sufficiently.&lt;/p&gt;&lt;p&gt;Also note hyper-parameter configuration 1 (Adam optimizer) performed much better than 2 (SGD optimizer).&lt;/p&gt;&lt;h3&gt;Preliminary Results&lt;/h3&gt;&lt;h4&gt;Adam Optimizer&lt;/h4&gt;&lt;p&gt;Graph of the &lt;strong&gt;loss&lt;/strong&gt; over time:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/640/1*sYny1MXCplPx-XQ_ToYN4Q.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Graph of &lt;strong&gt;f1 scores&lt;/strong&gt; over time: (Note there are 3 types of sentiments two pairs of entities can share: positive sentiment, negative sentiment, or no sentiment)&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*iPBZ5X-TctKEGWXub_8Klw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Graph of &lt;strong&gt;accuracies&lt;/strong&gt; over time:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/640/1*5TrbatjcthnZDlaz4eNxXg.png&quot; /&gt;&lt;/figure&gt;&lt;h4&gt;SGD Optimizer&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;For SGD, I experimented with a few values for learning rate (0.01, 0.05, and 0.1), and found 0.05 to fairly optimal.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Graph of the &lt;strong&gt;loss&lt;/strong&gt; over time:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/640/1*nweK1MYnyudak8XGAHMnng.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Graph of &lt;strong&gt;f1 scores&lt;/strong&gt; over time:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*VPN0B5UEkH1tWDShzrcSLQ.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;Graph of &lt;strong&gt;accuracies&lt;/strong&gt; over time:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/640/1*9BmjOwEAcPQbHR-a1ZINLg.png&quot; /&gt;&lt;/figure&gt;&lt;h3&gt;Key Takeaways&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;There’s a huge discrepancy between the performance on the train set and the performance on the dev set, indicating that the development data set and training data set are very different.&lt;/li&gt;&lt;li&gt;Analysis of the datasets, as you can see, the training set is very different in composition from the other sets:&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;╔══════════════╦══════════╦══════════╦══════════╦══════════╦═══════╗&lt;br /&gt;║              ║          ║ average  ║    #     ║    #     ║   #   ║&lt;br /&gt;║              ║    #     ║ entities ║ positive ║ negative ║ none  ║&lt;br /&gt;║              ║ articles ║ /article ║  pairs   ║  pairs   ║ pairs ║&lt;br /&gt;╠══════════════╬══════════╬══════════╬══════════╬══════════╬═══════╣&lt;br /&gt;║ train-ACL    ║ 897      ║ 2.63     ║ 648      ║ 815      ║ 355   ║&lt;br /&gt;║ dev-ACL-tune ║ 38       ║ 8.82     ║ 257      ║ 95       ║ 2520  ║&lt;br /&gt;║ dev-ACL-test ║ 36       ║ 9        ║ 237      ║ 118      ║ 2547  ║&lt;br /&gt;║ test-ACL     ║ 79       ║ 9.25     ║ 379      ║ 198      ║ 6037  ║&lt;br /&gt;║ test-MPQA    ║ 54       ║ 11.72    ║ 435      ║ 362      ║ 6813  ║&lt;br /&gt;╚══════════════╩══════════╩══════════╩══════════╩══════════╩═══════╝&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;For the Adam optimizer, the loss is still decreasing at the 20th epoch, meaning that I could probably keep seeing an improvement in performance if I continued running many more epochs.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Plan for Next Time&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;Deal with the discrepancy between train/dev/test data.&lt;/li&gt;&lt;li&gt;Do some more hyper-parameter tuning to try and improve scores.&lt;/li&gt;&lt;li&gt;Implement the end-to-end model architecture based on the &lt;a href=&quot;https://homes.cs.washington.edu/~luheng/files/emnlp2017_lhlz.pdf&quot;&gt;Lee et al. paper&lt;/a&gt;, i.e. scoring holder/target sentiment using a FFNN in a second step. This is point 1 of my MVP/Stretch Goal from my last blog post. If time suffices, also implement bi-affine scoring (&lt;a href=&quot;https://arxiv.org/pdf/1802.10569.pdf&quot;&gt;Verga et al., 2018&lt;/a&gt;, or point 2 in my MVP/Stretch Goal from my last blog post) and compare the results with the baseline model.&lt;/li&gt;&lt;li&gt;Experiment with attention mechanism, instead of calculating attention weights through a linear mapping from hidden state → weight, perhaps use a FFNN and pass in the hidden state.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;All code I’ve written for this post can be found on Github: &lt;a href=&quot;https://github.com/eunsol/document-e2e-sent&quot;&gt;https://github.com/eunsol/document-e2e-sent&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=7de5277b5be&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 17 Apr 2018 06:51:48 +0000</pubDate>
</item>
<item>
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Further Data Simplification and Baseline Exploration</title>
	<guid isPermaLink="false">https://medium.com/p/613328b9a85e</guid>
	<link>https://medium.com/@ryanp97/further-data-simplification-and-baseline-exploration-613328b9a85e?source=rss-6378d85d3a9b------2</link>
	<description>&lt;p&gt;As a side note, after meeting with Michael (the linguistics grad student mentioned in the first post), it seems that the “final” check of the Kyoto corpus is the most semantically accurate translation despite not being the most fluent. So it’s now feasible, and makes sense to, to get the Kyoto corpus incorporated into training at some point. He also suggested that I could use the top 5 statistically “best” parses from ERG/Jacy rather than just the best to increase the size of my dataset. These are both things that I will look into later down the line.&lt;/p&gt;&lt;h3&gt;Data Simplifications&lt;/h3&gt;&lt;p&gt;Jan and I decided to make a couple extra modifications to the input data in hopes of simplifying the model’s output. Ideally, these changes should allow the model to have a smaller output vocabulary size, and hopefully reduce the number of errors when generating a target tree.&lt;/p&gt;&lt;h4&gt;Simplifying Parentheses&lt;/h4&gt;&lt;p&gt;The first basic change was to separate closing parens from each other such that ) is the only way token that contains a closing paren. Originally, the parser would give spit out the closing parens as a single token, which most likely would confuse the model and result in non-wellformed output graphs.&lt;/p&gt;&lt;p&gt;The second step was to move the opening paren from the node label and attach it to the argument. Since the opening paren should only ever occur with an argument, this simplification removes the responsibility of having to figure out when to output an opening paren. We believe that this should not cause any sparsity issues since there is not very many different types of arguments.&lt;/p&gt;&lt;h4&gt;Simplifying Features&lt;/h4&gt;&lt;p&gt;Next, we concatenate argument features into a single token. The statistical parser is deterministic in its output, so the ordering of the features is always the same. Since there are only a few values each feature take on, the number of possible combinations is reasonable.&lt;/p&gt;&lt;p&gt;This is less than ideal since the model may never output sparse feature combinations. However, this is just a simplification to see the viability of the concept. Once we get a model that performs reasonably well, we can use the normal feature format as input and output.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*cFt-96vfvxtMvURKdfNttw.png&quot; /&gt;Simplified Japanese DMRS graph with squashed features.&lt;/figure&gt;&lt;h4&gt;Removing Features&lt;/h4&gt;&lt;p&gt;As a sanity check dataset, we created a dataset that has no features. This way, the model only has to predict arguments and argument labels. Though the features provide a lot of context, we are just using this dataset for preliminary testing and exploration.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*O5_KxoX1_gH9YURuqOu4nQ.png&quot; /&gt;Simplified, featureless, Japanese DMRS graph. Note that this is the featureless version of the above graph.&lt;/figure&gt;&lt;h4&gt;Removing `cargs`&lt;/h4&gt;&lt;p&gt;When parsing the graph, cargs shows up as one of the features of certain nodes. These represent the proper noun that the argument represents. In our scenario, we remove these carg features from the nodes since this should not affect semantic meaning and simplifies the model’s tasks and reduces sparsity issues (especially when we factor in squashing the features into a single token).&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/576/1*mIhOrdHdlF_R2_QCeXyxFQ.png&quot; /&gt;Example of English DMRS before simplification, with `carg` feature.&lt;/figure&gt;&lt;h3&gt;Baseline Exploration&lt;/h3&gt;&lt;p&gt;I’ve taken a look at both AllenNLP and OpenNMT for starting points of my code, but I haven’t quite made a full decision yet. AllenNLP seems much easier to extend and better documented, while OpenNMT is significantly faster but has a much less intuitive interface and codebase. For now, I’ve tested some models using AllenNLP, but I’m still considering OpenNMT. This week, I focused on experimenting on just the simplified, featureless dataset.&lt;/p&gt;&lt;p&gt;As for the AllenNLP model, it was trained with negative log likelihood as the objective function, 3 layer, BiLSTM with (very) small embedding and hidden sizes for 12 epochs. I wanted to see how much a small model could learn, and it learned how to match parens by epoch 5. It didn’t seem to learn much between the semantic structures, however. I assume this is caused mainly by two things: 1) the model was not trained for long enough or 2) the default attention is not particularly helpful for learning semantic structure. For changing the default attention, the structured attention presented in &lt;a href=&quot;https://arxiv.org/pdf/1705.09207.pdf&quot;&gt;&lt;em&gt;Learning Structured Text Representations&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;by&lt;em&gt; &lt;/em&gt;Liu et al. seems like an interesting and potentially useful change to the seq2seq model.&lt;/p&gt;&lt;p&gt;While I take a look at implementing the above attention, I’m going to train a similar model with 256 embedding and hidden sizes for 20 epochs, and see how that affects the ability of learning semantic representations. Ideally, I would like to get an implementation of the described attention in AllenNLP by next week, but depending on the difficult and how the next model goes, I may switch over to using OpenNMT as they already have it &lt;a href=&quot;https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/StructuredAttention.py&quot;&gt;implemented&lt;/a&gt;.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=613328b9a85e&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 17 Apr 2018 05:49:19 +0000</pubDate>
</item>
<item>
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Image Annotation Papers</title>
	<guid isPermaLink="false">https://medium.com/p/b951950ad9a5</guid>
	<link>https://medium.com/@hongnin1/image-summarization-b951950ad9a5?source=rss-c450eb982161------2</link>
	<description>&lt;p&gt;&lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/im2txt#prepare-the-training-data&quot;&gt;https://github.com/tensorflow/models/tree/master/research/im2txt#prepare-the-training-data&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.6632&quot;&gt;[1412.6632] Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1411.4555&quot;&gt;[1411.4555] Show and Tell: A Neural Image Caption Generator&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;a href=&quot;https://medium.com/media/3d1f87b3acd58d4b818d676b6a67c63a/href&quot;&gt;https://medium.com/media/3d1f87b3acd58d4b818d676b6a67c63a/href&lt;/a&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.03044&quot;&gt;[1502.03044] Show, Attend and Tell: Neural Image Caption Generation with Visual Attention&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Image Annotation for medical use:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1603.08486.pdf&quot;&gt;https://arxiv.org/pdf/1603.08486.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Image Annotation for visually impaired:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://cs224d.stanford.edu/reports/mcelamri.pdf&quot;&gt;https://cs224d.stanford.edu/reports/mcelamri.pdf&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=b951950ad9a5&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Thu, 12 Apr 2018 04:21:59 +0000</pubDate>
</item>
<item>
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #3: Project Proposal</title>
	<guid isPermaLink="false">https://medium.com/p/c8a12d3ae611</guid>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-3-project-proposal-c8a12d3ae611?source=rss-fad49d942bf3------2</link>
	<description>&lt;p&gt;The project I’m working on is document-level entity-entity sentiment analysis.&lt;/p&gt;&lt;h4&gt;Objectives and Definition&lt;/h4&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*mfDKzKS3wAEnJZ3gvLHwyw.png&quot; /&gt;Notice in the above example that the arc from Russia to Belarus is negative, which makes sense if our article has the sentence “Russia criticizes Belarus.”&lt;/figure&gt;&lt;p&gt;The goal of my project can be stated thusly: given a document, be able to figure out whether various entities within the document feel positively or negatively towards each other.&lt;/p&gt;&lt;p&gt;In particular, I’m looking to apply a neural model to &lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf&quot;&gt;an existing paper&lt;/a&gt; in an attempt to improve its F1 scores.&lt;/p&gt;&lt;p&gt;Applications of this work include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Modeling social dynamics between entities&lt;/li&gt;&lt;li&gt;Applying sentiment analysis to problems beyond simply movie/product reviews&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A farther motivation for this project is that entity-entity sentiment analysis is a relatively novel task, with little existing work thus far. Also, even work in related fields is usually focused on the sentence level, rather than the document level.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Literature Survey&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Some work in related fields to this project. For each paper, I will be focusing especially on its model /methodologies, which could be of use to developing my own model —&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Targeted Sentiment Analysis&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12074/12065&quot;&gt;Zhang, Meishan, Zhang, Yue, and Vo, Duy-Tin, 2016. Gated Neural Networks for Targeted Sentiment Analysis.&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Introduced the use of gated neural networks to perform targeted sentiment analysis.&lt;/li&gt;&lt;li&gt;Inputs: Concatenation of 2 types of word embeddings: both embeddings incorporate sentiment information in some way&lt;/li&gt;&lt;li&gt;Model: GRNN + G3 model, where outputs of GRNN are pooled and fed into G3. The G3 was used to better model interaction between left and right context for an entity, as sentiment can be dominated by left or right context&lt;/li&gt;&lt;li&gt;Outputs: Targeted sentiment&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Coreference Resolution&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://homes.cs.washington.edu/~luheng/files/emnlp2017_lhlz.pdf&quot;&gt;Lee, Kenton, He, Luheng, Lewis, Mike, and Zettlemoyer, Luke, 2017. End-to-end Neural Coreference Resolution.&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Model: 2-step, 1st step being a biLSTM, and 2nd step being a FFNN&lt;/li&gt;&lt;li&gt;Inputs to FFNN: Something nice about this model is the fact that they were able to encode multi-word entities efficiently by concatenating LSTM outputs.&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/227/1*7yoU47vuvU5CaBm-r3n3aw.png&quot; /&gt;ϕ used to encode size of span. x* are outputs of LSTMs for boundaries of span. x hat are outputs of attention mechanism over span.&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;FFNN architecture:&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/390/1*9KxdIWkYNSzMwDJeSI3fuw.png&quot; /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Document-level Relation Extraction&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cs.jhu.edu/~npeng/papers/TACL_17_RelationExtraction.pdf&quot;&gt;Peng, Nanyun, Poon, Hoifung, Quirk, Chris, Toutanova, Kristina, Yih, and Wen-tau, 2017. Cross-Sentence N-ary Relation Extraction with Graph LSTMs.&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/600/1*-M0TCCGCpmYtYpVnYfhAHg.png&quot; /&gt;Example of a graph LSTM structure used in the paper. Note the additional links between non-adjacent cells encodes for syntactic dependencies and discourse relations.&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;Uses a graph LSTM with architecture following the structure of the document graph (introduced by &lt;a href=&quot;https://arxiv.org/pdf/1609.04873.pdf&quot;&gt;Quirk and Poon, 2017&lt;/a&gt;), which encodes various dependencies, including word adjacency, syntactic dependencies, and discourse relations&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.10569.pdf&quot;&gt;Verga, Patrick, Strubell, Emma, and McCallum, Andrew, 2018. Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction.&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/345/1*6ap---pVvwWNNdCgwhh16w.png&quot; /&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;Bi-affine neural model: First pass document through Transformer, then evaluate relations between pairs of tokens using a bi-affine operator and the LogSumExp function&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;&lt;strong&gt;Methodologies&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;Baseline Neural Model (Minimal Viable Plan)&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Inputs: for each token, concatenate word embeddings, polarity embeddings, and holder/target/none embeddings&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Word embeddings are pre-trained GloVe embeddings&lt;/li&gt;&lt;li&gt;EX: “the cat disliked milk”, holder = “cat”, target = “milk”&lt;/li&gt;&lt;li&gt;Input becomes: [the, 0, 0] [cat, 0, H] [disliked, NEG, 0] [milk, 0, T]&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Model: Attentive biLSTM&lt;/p&gt;&lt;p&gt;Output: Sentiment score for positivity and negativity&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Scoring Holder/Target Sentiment in a Second Step (Minimal Viable Plan / Stretch Goal, i.e. I think I can get this but I can’t guaruntee it)&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Feedforward Neural Network: Incorporating the FFNN from the Lee et al. paper (see above for details). To adapt the architecture for my needs, I will get rid of the mention score, and thus the white and grey cells will be the same. The grey cell, instead of outputting a coreference score, will output a sentiment (positivity and negativity) score.&lt;/li&gt;&lt;li&gt;Bi-affine Scoring: Incorporating the scoring mechanism from the Verga et al. paper (see above for details). Instead of a “Transformer,” I will use the LSTM from my baseline model, which gets fed in to two MLPs, the biaffine operator, and finally aggregated using the LogSumExp function.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;Experimenting with Different Ways of Aggregating across Different Mentions (Stretch Goal)&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;LogSumExp function from the Verga et al. paper (see above).&lt;/li&gt;&lt;li&gt;Encode inputs to second step in a way that incorporates information from all mentions.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;Experimenting with Graph LSTM Structure (Stretch Goal)&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Based off of ideas in Peng et al. paper (see above)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Incorporate Ideas from Targeted Sentiment Analysis (Stretch Goal)&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Add a G3 gate, based on Zhang et al. paper (see above), or even adapt the architecture of the G3 gate to better suit the task of sentiment analysis between entities&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;&lt;strong&gt;Available Resources&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Dataset from Choi et al. (paper I’m improving). The data has already been pre-processed using StanfordNLP. Some of the preprocessing that’s been done includes tokenization, named entity recognition, and co-reference resolution.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Evaluation Plan&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;As an evaluation metric, I’m going to use the F1 scores for both positive and negative sentiments.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=c8a12d3ae611&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 11 Apr 2018 06:47:08 +0000</pubDate>
</item>
<item>
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: Machine Dictionary</title>
	<guid isPermaLink="false">https://medium.com/p/43368563cf97</guid>
	<link>https://medium.com/nlp-capstone-blog/machine-dictionary-43368563cf97?source=rss----9ba3897b6688---4</link>
	<description>&lt;p&gt;We have decided to call our project, the Machine Dictionary. Formally, the goal of this project is to use a large corpus of data to generate definitions for technical terms that are consistent with how those terms are explored in the corpus.&lt;/p&gt;&lt;h4&gt;Motivation&lt;/h4&gt;&lt;p&gt;Our aim is to explore novel text generation approaches and apply them to the specific task of generating definitions. One of these techniques includes a specific approach we have termed “Connecting the Dots”. This approach will allow us to loosely structure the definitions such that they contain meaningful content but also keep the model general enough to generate appropriate context. We will discuss this approach further down. Another large motivation is the amount of research paper data we have from the AI2 Semantic Scholar corpus. This will allow us to use domain specific corpora to support definitions.&lt;/p&gt;&lt;h4&gt;Prior Work&lt;/h4&gt;&lt;p&gt;Although text generation has been a hot topic in NLP research for a while, we did not discover any prior attempts to generate definitions for technical terms. That said, the text generation task has itself been explored in great detail by many others.&lt;/p&gt;&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/pdf/1707.05501.pdf&quot;&gt;&lt;strong&gt;this paper&lt;/strong&gt; (Jain et al., 2017)&lt;/a&gt;, the authors explore the task of generating short stories given a sequence of independent short descriptions. They approach the problem by using an Encoder-Decoder model to connect the descriptions and the short stories. This method might help us generate short text, but in our case, the input would be a large amount of data in the domain.&lt;/p&gt;&lt;p&gt;In this &lt;a href=&quot;https://pdfs.semanticscholar.org/9dad/f5bb0a2182b1509c5ea60d434bb35d4701c1.pdf?_ga=2.15851958.1083977791.1523309085-1136887644.1523309085&quot;&gt;other paper (Ghazvininejad et. al, 2016)&lt;/a&gt;, the authors explore generating poetry based on topics. This paper is relevant to our project for several reasons. For instance, the paper generates poems based on a given topic, much like our goal which is to generate definitions based on a given term. In addition, the authors generate poetry by taking advantage of the structure of Shakespearean sonnets such as the unique rhyme scheme and the iambic pentameter cadence. We can use the techniques proposed in the paper to selectively choose information from the training corpus, based on the term we are asked to define and how we believe definitions should be structured.&lt;/p&gt;&lt;h4&gt;Minimum Viable Plan&lt;/h4&gt;&lt;p&gt;To reiterate, our model should be able to generate definitions that are based on context received from a large corpus. In this MVP, we can make the simplification that our model should generate definitions of a fixed length (for example, 5 sentences). These definitions should be grammatical and technically correct.&lt;/p&gt;&lt;h4&gt;Baseline Approach and Evaluation&lt;/h4&gt;&lt;p&gt;Andrej Karpathy, a former PhD student at Stanford, explores the incredible effectiveness of RNNs in in his blog. The article, &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;The Unreasonable Effectiveness of RNNs&lt;/a&gt;, essentially claims that RNNs have an uncanny ability to learn the structure of training data, such as Wikipedia Articles, Shakespeare, and even code. In our baseline approach, we take advantage of this ability by training an RNN language model on the training data. At testing time, we can provide the term we seek to define as a seed to the RNN, which can then generate text until we hit a word limit or the RNN generates the &amp;lt;STOP&amp;gt; character. We can evaluate this approach by using perplexity, to ensure that we have a good language model. We can also cross-check definitions with other sources like Wikipedia articles. Finally, it might also be prudent to have humans evaluate the definitions.&lt;/p&gt;&lt;h4&gt;Target Approach I and Evaluation&lt;/h4&gt;&lt;p&gt;We propose two different target approaches to this model. In this first approach, we utilize techniques from work done previously in abstractive summarization. Given a term, we could filter the input data on all sentences associated with that term. We could obtain all the sentences that contain the data and a few sentences in the nearby surroundings, which could capture the context for the data. We would then use these sentences to generate a summary, which we would call the definition of the term. In this approach, we might use an attention mechanism to focus on the most important parts of the input. In terms of evaluation, we would use the cross referencing method from above, where we take the produced definition and the “correct definition” as determined by an external source and compare the number of common words.&lt;/p&gt;&lt;h4&gt;Target Approach II and Evaluation&lt;/h4&gt;&lt;p&gt;In this second approach, we explore a concept we have chosen to call “Connecting the Dots”. In this approach, we structure the definition generation by using key words. To elaborate, each term might be closely connected to a certain number of other words which could influence the definition of the term greatly. Consider the term &lt;strong&gt;osteoporosis&lt;/strong&gt;. This term might be closely associated with the words &lt;strong&gt;bones, degrade, bone degradation, fractures, &lt;/strong&gt;and &lt;strong&gt;women. &lt;/strong&gt;We could use these words to structure a definition for &lt;strong&gt;osteoporosis &lt;/strong&gt;as a fill in the blank task.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Osteoporosis is … bones … degrade … bone degradation … fractures … women.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In the above structure, we would rely on the model to appropriately and grammatically fill in the context between each keyword. As a result, we might generate the following definition:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Osteoporosis &lt;/strong&gt;is a disease that cause &lt;strong&gt;bones &lt;/strong&gt;to &lt;strong&gt;degrade.&lt;/strong&gt; &lt;strong&gt;Bone degradation &lt;/strong&gt;often leads to &lt;strong&gt;fractures.&lt;/strong&gt; &lt;strong&gt;Osteoporosis &lt;/strong&gt;most commonly affects &lt;strong&gt;women.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;It’s important to note that when we fill in the context between keywords, we must condition on the original term that we are defining. For example, between the words &lt;strong&gt;fracture &lt;/strong&gt;and &lt;strong&gt;women&lt;/strong&gt;, there might be several sentences we could generate, but we must keep in mind how the keywords are related given that they are about &lt;strong&gt;osteoporosis.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In this approach, we will build on the neural network model and add task-specific architecture to capture relationships between words. We hope that the model can learn how to define keywords associated with terms and use those terms to structure a definition.&lt;/p&gt;&lt;p&gt;In terms of evaluation, we introduce another technique. In this evaluation method we take a paragraph in which the technical term appears and omit the term. If we rephrase the problem as a classification task and ask the model to predict the omitted term, we would be able to conclude whether the model has a contextual understanding of the technical term. This evaluation technique would be a good supplement to the human evaluation method where we request users to rank how correct and readable the definitions are.&lt;/p&gt;&lt;h4&gt;Stretch Goals&lt;/h4&gt;&lt;p&gt;The ideal goal would be to generate text without any constraints on length, order, or keyword usage. This would be a more “hands-off” approach to text generation and could also allow us to train on different domain based corpora. We might be able to achieve this stretch goal if we perform well on the goals outlined in the Minimum Viable Plan.&lt;/p&gt;&lt;p&gt;Another stretch goal has to do with ontology matching, whereby we compare two definitions to determine whether they describe the same concept. We could extend this example to generate definitions for all technical terms across a body of research papers, determine which terms are defined similarly in different papers, and unify the terminology across all papers. This goal is definitely a stretch goal, but if we can perfect the architecture for generating definitions, we see this as a future application of our project.&lt;/p&gt;&lt;h4&gt;Data&lt;/h4&gt;&lt;p&gt;We plan to use the Semantic Scholar Open Research Corpus for this project. This corpus consists of over 20 million research papers classified into two domains (Computer Science and Medicine). Depending on the approach we take to solve this task, we would filter the data and train accordingly.&lt;/p&gt;&lt;h4&gt;Resources and Literature Survey&lt;/h4&gt;&lt;p&gt;We have mentioned several resources above that were the most useful for formulating the project proposal. Here are those resources and a few other resources that we think might be useful in the future.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Text Generation Techniques&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/797d/7d968b88d5b5dd7c3271d08acd7296950d41.pdf?_ga=2.73597202.1083977791.1523309085-1136887644.1523309085&quot;&gt;Using Lexical Chains for Text Summarization (Barzilay et. al, 1997)&lt;br /&gt;&lt;/a&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/9dad/f5bb0a2182b1509c5ea60d434bb35d4701c1.pdf?_ga=2.15851958.1083977791.1523309085-1136887644.1523309085&quot;&gt;Generating Topical Poetry (Ghazvininejad et. al, 2016)&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;The Unreasonable Effectiveness of RNNs&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1707.05501.pdf&quot;&gt;Story Generation from Sequence of Independent Short Descriptions (Jain et al., 2017)&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=43368563cf97&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/machine-dictionary-43368563cf97&quot;&gt;Machine Dictionary&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</description>
	<pubDate>Wed, 11 Apr 2018 06:03:31 +0000</pubDate>
</item>
<item>
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: Formal Proposal</title>
	<guid isPermaLink="false">http://cse481n-capstone.azurewebsites.net/?p=37</guid>
	<link>http://cse481n-capstone.azurewebsites.net/2018/04/10/formal-proposal/</link>
	<description>&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Motivations:&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We want to create novel models for determining if the text is offensive, and why that text is offensive. To do this we want to create a new dataset that makes this task easier. We hope that our dataset and models pave the way for further innovations by others, as well as better trained conversational agents that have a better understanding of what they should or should not say. We’re going to teach them how to watch their language!&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We would like to correctly classify sentences that keywords matching cannot achieve. For example:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;&lt;i&gt;What do you call an adult that has imaginary friends? Religious&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;And we would like to tell the reason why the sentence above is bad as well.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Minimal Viable Plan:&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;By comparing the similarity of content phrases found in r/MeanJokes posts and posts all over Reddit, we hope to create a large, high-quality dataset for training models to detect offensive text. We want to create this dataset and use crowdsourcing to label it. The labels should say if the text was offensive, and if it was then was it an attack against a particular group, what group that was, as well as the reasoning for why the labeler labeled the text this way. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;While we wait for our data to be labeled, we want to start by creating baseline models on existing datasets, such as Twitter Hate Speech, Wiki Detox, and Stanford Politeness. We think that these datasets are similar enough to begin work on classifiers that don’t make use of deep annotation. After this, we can start work on improving performance on these datasets up until our crowdsourcing completes. We will explore novel models on existing datasets and try to improve their performance. &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Stretch Goals:&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Once our new Reddit dataset is fully labeled, we want to test the existing models that we made on the other datasets and continue improving them. We also want to use the new data to experiment with Q&amp;amp;A or Deep Annotation models for creating a model that knows why a particularly offensive post is offensive. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;In case if we can’t receive labeled dataset on time, we will continue to make improvements to novel models on existing datasets.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Evaluation Plan: &lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Classifier Models: Precision, Recall, F1 score&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Rationale Models: deeper comparison to crowdsourced label explanations&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Dataset: Random Sampling + Human Judgement&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Existing Work: &lt;/span&gt;&lt;/h3&gt;
&lt;h5&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Previous Capstone Project: &lt;/span&gt;&lt;/h5&gt;
&lt;h5&gt;&lt;a href=&quot;https://michael0x2a.github.io/nlp-capstone/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Team Inverted Cat&lt;/span&gt;&lt;/a&gt;&lt;/h5&gt;
&lt;h5&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Datasets: &lt;/span&gt;&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/ZeerakW/hatespeech&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Hate Speech Twitter Annotations&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; (Waseem et al. 2016)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Hate Speech and Offensive language dataset &lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; (Davidson et al. 2017)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://meta.wikimedia.org/wiki/Research:Detox/Data_Release&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Wikipedia Talk Corpus &lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; (Wulczyn et al. 2017)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.cs.cornell.edu/~cristian//Politeness.html&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Stanford Politeness Corpus&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;A list of bad words&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Pre-trained word embeddings: GloVe, Facebook FastText, Google Word2Vec&lt;/span&gt;&lt;/p&gt;
&lt;h5&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Papers: &lt;/span&gt;&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Automated-Hate-Speech-Detection-and-the-Problem-of-Davidson-Warmsley/6ccfff0d7a10bf7046fbfd109b301323293b67da&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Davidson, Thomas J et al. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM (2017).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Hate-Speech-Detection-with-Comment-Embeddings-Djuric-Zhou/c9948f7213167d65db79b60381d01ea71d438f94&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Djuric, Nemanja et al. “Hate Speech Detection with Comment Embeddings.” &lt;/span&gt;&lt;i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;WWW&lt;/span&gt;&lt;/i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(2015).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Using-Convolutional-Neural-Networks-to-Classify-Gamb%C3%A4ck-Sikdar/0dca29b6a5ea2fe2b6373aba9fe0ab829c06fd78&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Gambäck, Björn and Utpal Kumar Sikdar. “Using Convolutional Neural Networks to Classify Hate-Speech.” (2017).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Abusive-Language-Detection-in-Online-User-Content-Nobata-Tetreault/e39b586e561b36a3b71fa3d9ee7cb15c35d84203&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Nobata, Chikashi et al. “Abusive Language Detection in Online User Content.” &lt;/span&gt;&lt;i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;WWW&lt;/span&gt;&lt;/i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;(2016).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Ex-Machina%3A-Personal-Attacks-Seen-at-Scale-Wulczyn-Thain/4a7204431900338877c738c8f56b10a71a52e064&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Wulczyn, Ellery et al. “Ex Machina: Personal Attacks Seen at Scale.” &lt;/span&gt;&lt;i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;WWW&lt;/span&gt;&lt;/i&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; (2017).&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</description>
	<pubDate>Tue, 10 Apr 2018 22:37:31 +0000</pubDate>
</item>
<item>
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Project Proposal — Neural Machine Translation with Semantic Transfer</title>
	<guid isPermaLink="false">https://medium.com/p/a1903faeadb7</guid>
	<link>https://medium.com/@ryanp97/project-proposal-neural-machine-translation-with-semantic-transfer-a1903faeadb7?source=rss-6378d85d3a9b------2</link>
	<description>&lt;p&gt;State-of-the-art neural machine translation does not currently utilize much, if any, semantic information, meaning it misses out on a large amount of potentially useful information indirectly embedded in the sentence. This project aims to explore the benefits that semantic transfer could offer to neural machine translation.&lt;/p&gt;&lt;p&gt;In particular, this project will focus on Dependency Minimal Recursion Semantics and translation between English DMRS and Japanese DMRS. Currently, I’m working on this project alone under the supervision of Jan Buys.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*AlAZF1O2uYt4b12hCu7ajA.png&quot; /&gt;An example of English and Japanese DMRS graphs for the above sentence. Note the sentences have the same meaning.&lt;/figure&gt;&lt;h4&gt;Minimal Viable Action Plan&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;Obtain a parallel corpus.&lt;/li&gt;&lt;li&gt;Parse DMRS graphs from the parallel corpus&lt;/li&gt;&lt;li&gt;Simplify graphs (and be able to recover them in a robust manner to handle the model’s output)&lt;/li&gt;&lt;li&gt;Train a seq2seq model to predict a Japanese DMRS graph given an English DMRS graph as well as the required embeddings&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;I’ve chosen the &lt;a href=&quot;http://www.edrdg.org/wiki/index.php/Tanaka_Corpus&quot;&gt;Tanaka Corpus&lt;/a&gt; as my parallel corpus and cleaned it such that the only remaining sentences are English and Japanese sentences which belong to a translation pair. From there, I’ve parsed the corresponding DMRS graphs. For further details on how I cleaned the corpus and parsed the graphs, refer to my &lt;a href=&quot;https://medium.com/@ryanp97/project-logistics-and-package-exploration-3d3651220219&quot;&gt;last&lt;/a&gt; blog post.&lt;/p&gt;&lt;p&gt;Currently, I’m working on how to simplify and recover the graphs robustly. Simplifying the graph seems fairly easy; however, I made some incorrect assumptions about the graphs, so recovery is currently a work in progress. Furthermore, I have yet to handle the robustness aspect. In general, I need to handle the cases where the model does not output a valid “simplified” graph. This includes, but is not limited to: handling mismatched/missing parentheses.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*LdgExRT9YoJ-UwxYxwJfsA.png&quot; /&gt;On the left is the English DMRS and on the right is the Japanese DMRS. Each example is labeled with a sentence ID and the original sentence (prefixed with `#` which Smatch ignores). The model will not have to predict these comments. However, in the future, we’d like to extend the model such that it would be able to recover the original sentence after generating a DMRS graph in the target language.&lt;/figure&gt;&lt;p&gt;From then, the last step in the minimal viable action plan is to train and evaluate model. Ideally, we would train the embeddings for the tokens in the graphs and the seq2seq model end-to-end with &lt;a href=&quot;https://amr.isi.edu/eval/smatch/tutorial.html&quot;&gt;Smatch&lt;/a&gt; as the objective function, though I am still working with Jan on how I should piece this model together.&lt;/p&gt;&lt;h4&gt;Stretch Goals&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;Explore alternatives to standard seq2seq models (i.e. TreeLSTMs, custom architecture, etc.)&lt;/li&gt;&lt;li&gt;Expand/supplement dataset with &lt;a href=&quot;https://alaginrc.nict.go.jp/WikiCorpus/index_E.html&quot;&gt;Kyoto Corpus&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Since we are expecting the model to learn and generate graphs, it would be ideal if we were able to use a tree-like structure that would more accurately represent the data. If I am able to finish the minimal viable action plan with enough time left over, I would like to experiment and test out different architectures and see how that affects the model’s performance. There are a couple different packages for TreeLSTMs (such as &lt;a href=&quot;https://github.com/dasguptar/treelstm.pytorch&quot;&gt;this&lt;/a&gt; one) that would potentially make this task not too difficult. Though, eventually I would like to customize an architecture for this task.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/496/1*omSJj6AYg7TDPUW0XPY6ew.png&quot; /&gt;A figure of a TreeLSTM grabbed from the &lt;a href=&quot;https://arxiv.org/pdf/1503.00075.pdf&quot;&gt;original&lt;/a&gt; paper on TreeLSTMs.&lt;/figure&gt;&lt;p&gt;Each example in the Tanaka Corpus is a single sentence, making it easy to work with in terms of development and debugging. However, if I have extra time, I would like to supplement the Tanaka Corpus with the Kyoto Corpus which is comprised of translated Wikipedia articles. The corpus has much longer examples and provides a much more complex and realistic setting. An issue is that the documentation is in Japanese, and it is not entirely clear which of the different translations is “correct” as each example has a “primary,” “secondary,” and “check” translation. You can read more about it on the Kyoto Corpus website, hyperlinked above.&lt;/p&gt;&lt;h4&gt;Evaluation Plan&lt;/h4&gt;&lt;p&gt;There is not really a good statistical model, nor are there results that are very comparable due to the scope of the project. Since the scope of the project does not include recovering the sentence from a generated DMRS graph, comparisons between this project and papers that do tree-to-tree machine translations don’t really make much sense.&lt;/p&gt;&lt;p&gt;I expect to have some time leftover to explore different architectures, so I hope to make the seq2seq results the baseline for comparison for other architectures I have time to try.&lt;/p&gt;&lt;h4&gt;Related Work&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.08381.pdf&quot;&gt;Neural AMR: Sequence-to-Sequence Models for Parsing and Generation&lt;/a&gt; is fairly relevant to this project in the sense that it has similar motivations for working with semantics in NLP. The paper covers how they overcame a lack of labeled data, achieved competitive results for both parsing AMR graphs and generating text from AMR graphs, as well as extensive ablation studies and analysis. Their graph preprocessing steps are extremely relevant to this project, and I will likely be referencing their paper and &lt;a href=&quot;https://github.com/sinantie/NeuralAmr&quot;&gt;codebase&lt;/a&gt; often as I work on cleaning and preprocessing the Tanaka Corpus.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1503.00075.pdf&quot;&gt;Improved Semantic Representations From Tree-Structured LSTMs&lt;/a&gt; provides a good basis for my stretch goals. The Child-Sum TreeLSTM units seem like a good option to test for representing the DMRS graph as any given node can take any number of children. These children can be used to represent dependencies as outlined in the paper, but it seems it might be possible to have the properties of a head word as children of the corresponding node as well. There’s a couple other useful ideas that come from the paper, but for the sake of brevity, we’ll leave it at this.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1603.06075.pdf&quot;&gt;Tree-to-Sequence Attention Neural Machine Translation&lt;/a&gt; is a slight glimpse at the future of this project. If this project is successfully able to transfer semantic information between languages, the next step would make the model complete the entire cycle, sequence-to-graph-to-graph-to-sequence, in hopes of achieving better results than statistical machine translation.&lt;/p&gt;&lt;p&gt;Additionally, here is a short list of other relevant papers:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.07092.pdf&quot;&gt;Robust Incremental Neural Semantic Graph Parsing&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P13-2131&quot;&gt;Smatch: an Evaluation Metric for Semantic Feature Structures&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=a1903faeadb7&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 10 Apr 2018 05:31:27 +0000</pubDate>
</item>
<item>
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog 3: Formal Proposal</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-5600014144802012716.post-8898628104121215850</guid>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/04/blog-3-formal-proposal.html</link>
	<description>&lt;b style=&quot;font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;Minimal viable action plan &lt;/b&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;Implement the model from the java paper mentioned in the previous blog posts. This includes the variable and method camel case encoding, the two step attention, the type constrained decoding, and many other tasks such as preprocessing, evaluation metrics etc.&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;Stretch goals&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;This depends on my error analysis on the mvp, but here are a couple ideas.&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;1. &lt;/b&gt;Incorporate implementation specific encoding. The encoder just uses the method names, but it'd be interesting to also include the method implementations.&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;&lt;b&gt;2. &lt;/b&gt;More type constraints on the decoder. Currently if the decoder wants to generate a variable, there's nothing to check that the variable was previously declared.&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;Project objectives&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;Reproduce the strong paper baseline. &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;Proposed methodologies&lt;/b&gt; &lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;Implement the model from the paper. Potentially experiment with other semantic parsing task architectures and see if they also perform competitively with the baseline, for example seq2seq.&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;Available resources&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;The dataset and allennlp.&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;Evaluation plan&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;Test on the test set and measure bleu and exact match metrics.&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;Test if this is useful by coming up with a couple of real classes that I've written and see if it generates the method.&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;Perhaps come up with a new metric, such as a binary executability metric (stretch goal).&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;span style=&quot;background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;&quot;&gt;&lt;b&gt;Literature survey&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Palatino, Arial, helvetica, sans-serif;&quot;&gt;&lt;span style=&quot;background-color: white; font-size: 14.44px;&quot;&gt;The type constrained architecture was used in a number of recent papers such as &quot;&lt;/span&gt;&lt;span style=&quot;font-size: 14.44px;&quot;&gt;A syntactic neural model for parsing natural language to executable code&quot;. The dataset is novel in that previous ones haven't used programmatic contexts and have focused on nl2code pairs.&lt;/span&gt;&lt;/span&gt;</description>
	<pubDate>Tue, 10 Apr 2018 03:21:00 +0000</pubDate>
	<author>noreply@blogger.com (nlpcapstone)</author>
</item>
<item>
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: Movie Sentiment Summarization — Project Proposal</title>
	<guid isPermaLink="false">https://medium.com/p/45c89bec2c2e</guid>
	<link>https://medium.com/@hongnin1/movie-sentiment-summarization-project-proposal-45c89bec2c2e?source=rss-c450eb982161------2</link>
	<description>&lt;p&gt;Team: Ning Hong, Zhuchun Liu, Sujie Zhou&lt;/p&gt;&lt;p&gt;Overview: our model will be able to summarize the reviews for the input movies. The summarization of the movie include how the audience feel about the movie and what is the overall rating for the movie, for example, given a movie title, our model should be able to produce something like this: &amp;lt;movie title&amp;gt; is violet but good, most people think this movie is 6/10.&lt;/p&gt;&lt;p&gt;If time permits, we would like to improve our model such that it can also output a more detailed overall review for the movie instead of simple sentences, for example, given a movie title as input, our model should output: “&amp;lt;movie title&amp;gt; got my full attention from beginning to end. I couldn’t turn away. I didn’t want to turn away. For me, that’s extremely rare, I would give it 8/10.”&lt;/p&gt;&lt;p&gt;Another stretch goal for our model is to be able to detect sentiment not only in the US market, but also in China market by using data from DouBan (one of the largest movie review site for China), and compare the sentiment between US and China for a certain movie, for example, given an input movie title, our model can output something like: &amp;lt;movie title&amp;gt; was generally perceived more positively in the US than in China, the Chinese audience mostly felt uncomfortable about its violent and explicit content whereas more American audience appreciated the bloodiness of the film.&lt;/p&gt;&lt;p&gt;Model: we are going to use basic encoder- decoder RNN that serves as our baseline and then propose several novel models for summarization, each addressing a specific weakness in the base- line such as encoder-decoder RNN with attention and large vocabulary trick, capturing keywords using feature-rich encoder, modeling rare/unseen words using switching generator-pointer, and capturing hierarchical document structure with hierarchical attention.&lt;/p&gt;&lt;p&gt;This project has a lot of potential uses. Not only can we use movie dataset to output movie sentiment summarization, if given restaurant review dataset (Yelp), we can also output summarization about how customers feel about a restaurant.&lt;/p&gt;&lt;p&gt;On the other hand, there might be some difficulties we will be facing: we are planning to use sequence2sequence model to generate summarization, it is hard to determine how good is the output we are generating; more research needs to be done for this problem.&lt;/p&gt;&lt;p&gt;Resources:&lt;/p&gt;&lt;p&gt;Data scraping blogpost:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.dataquest.io/blog/web-scraping-beautifulsoup/&quot;&gt;An intermediate tutorial&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Possible IMDb training data:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset&quot;&gt;SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Model paper:&lt;/p&gt;&lt;a href=&quot;https://medium.com/media/1ff2a893f361e9bda813a32d4e89c5f6/href&quot;&gt;https://medium.com/media/1ff2a893f361e9bda813a32d4e89c5f6/href&lt;/a&gt;&lt;p&gt;Model github:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/thunlp/TensorFlow-Summarization&quot;&gt;thunlp/TensorFlow-Summarization&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Movie sentiment paper:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/8076790/&quot;&gt;Movie review summarization and sentiment analysis using rapidminer - IEEE Conference Publication&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=45c89bec2c2e&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 10 Apr 2018 02:54:02 +0000</pubDate>
</item>
<item>
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: Getting Started for the Capstone: Software Installation &amp; Pipeline Brainstorming</title>
	<guid isPermaLink="false">https://medium.com/p/3a2f40b355a5</guid>
	<link>https://medium.com/nlp-capstone-blog/getting-started-for-the-capstone-software-installation-pipeline-brainstorming-3a2f40b355a5?source=rss----9ba3897b6688---4</link>
	<description>&lt;p&gt;Currently, our top two choices for the capstone is&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Machine Dictionary: &lt;/strong&gt;learning definitions of technical terms whose semantics are averaged over all places it is mentioned in training (in this case, research publications in the given field of study)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Visual Reasoning: &lt;/strong&gt;Given three windows, each of which containing a random arrangement of colored, geometric shapes, and a statement about the image, predict whether the statement is true or false.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Despite being problems with very different needs and challenges, the bulk of the tools and frameworks we’ll be using overlap for both tasks. Here, we discuss those tools and frameworks, followed by things we need specific to &lt;strong&gt;Machine Dictionary &lt;/strong&gt;and &lt;strong&gt;Visual Reasoning &lt;/strong&gt;separately.&lt;/p&gt;&lt;h3&gt;Resources Used for Both Tasks&lt;/h3&gt;&lt;h4&gt;PyTorch&lt;/h4&gt;&lt;p&gt;Given the limited time that we have, a neural-based approach using an established framework is preferred over implementing all of the model architecture from scratch, and to help avoid complications that can accompany other methods such as deriving parameter updates for bayesian models. PyTorch is an excellent framework that abstracts away differentiation and tensor arithmetic while still allowing a healthy amount of flexibility with it’s ability to dynamically produce computation graphs.&lt;/p&gt;&lt;h4&gt;AllenNLP&lt;/h4&gt;&lt;p&gt;After the crash course on the framework provided by AI2 in class, along with our experience from using it in the undergraduate NLP class, we’re convinced that the integration of AllenNLP with PyTorch is the best way to be as productive as possible. We plan to use the libraries it provides to make training more streamline and organized.&lt;/p&gt;&lt;p&gt;Since we’ve taken the undergraduate NLP class, we’ve already installed PyTorch and AllenNLP. We installed PyTorch through conda and AllenNLP through pip. Deep Learning projects also tend to involve complicated models which might require more computing resources, so we will also utilize the Azure credits available through the capstone. This process involved installing PyTorch and AllenNLP on Ubuntu VMs on Azure configured into include NVIDIA GPUs so that we can take advantage of PyTorch’s .&lt;/p&gt;&lt;h3&gt;Resources Specific to Machine Dictionary&lt;/h3&gt;&lt;h4&gt;Semantic Scholar Open Research Corpus&lt;/h4&gt;&lt;p&gt;The best dataset we’ve seen so far for this task is the &lt;a href=&quot;http://labs.semanticscholar.org/corpus/&quot;&gt;Semantic Scholar Open Research Corpus&lt;/a&gt; provided by &lt;a href=&quot;http://allenai.org/&quot;&gt;AI2&lt;/a&gt;. The dataset specifically consists of JSON files with metadata for each publication. The most relevant files will be &lt;em&gt;title, pdfUrls, &lt;/em&gt;and &lt;em&gt;year&lt;/em&gt;. Given all of the content besides the paper abstract (which is included as a field called &lt;em&gt;paperAbstract&lt;/em&gt;), we resort to using the &lt;em&gt;pdfUrls&lt;/em&gt; and extracting the text from each.&lt;/p&gt;&lt;h4&gt;Textract&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/deanmalmgren/textract&quot;&gt;Textract&lt;/a&gt; is a Python package that allows the extraction of text from PDFs. We plan to rely on this package given that it’s robust to both compiled and scanned PDFs.&lt;/p&gt;&lt;p&gt;We downloaded Textract to the Azure Linux VM using the Ubuntu installation directions:&lt;/p&gt;&lt;pre&gt;apt-get install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr \&lt;br /&gt;flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig&lt;br /&gt;pip install textract&lt;/pre&gt;&lt;p&gt;but ran into an issue that the developers haven’t dealt with yet. In the first line, they are missing a dependency to the libpulse-dev package, which causes the build to fail when downloading Textract. Doing a pip install libpulse-dev takes care of it.&lt;/p&gt;&lt;p&gt;Grabbing the text from a PDF file is then fairly convenient. We tested this on a &lt;a href=&quot;https://www.semanticscholar.org/paper/Effects-of-anthocyanins-on-the-prevention-and-of-Lin-Gong/1bcf9ae84d4ec5c0aba7918e6784dbfd0e8514b6&quot;&gt;cancer research paper&lt;/a&gt; taken from the Semantic Scholar dataset:&lt;/p&gt;&lt;pre&gt;&lt;strong&gt;import&lt;/strong&gt; textract&lt;br /&gt;text = textract.process(&quot;path-to-doc.pdf&quot;, encoding=&quot;ascii&quot;)&lt;/pre&gt;&lt;p&gt;which produced an excellent parse of the PDF.&lt;/p&gt;&lt;blockquote&gt;b’BJP\n\nBritish Journal of\nPharmacology\n\nBritish Journal of Pharmacology (2016) \n\n1\n\nREVIEW ARTICLE THEMED ISSUE\nEffects of anthocyanins on the prevention and\ntreatment of cancer\nCorrespondence Ying-Yu Cui, Department of Regenerative Medicine, Tongji University School of Medicine, Shanghai 200092,\nChina. E-mail: yycui@tongji.edu.cn\n\nReceived 13 June 2016; Revised 17 August 2016; Accepted 13 September 2016\n\nBo-Wen Lin1, Cheng-Chen Gong1, Hai-Fei Song1 and Ying-Yu Cui1,2,3\n1\n\nDepartment of Regenerative Medicine, Tongji University School of Medicine, Shanghai, China, 2Key Laboratory of Arrhythmias, Ministry of\n\nEducation (Tongji University), Shanghai, China, and 3Institute of Medical Genetics, Tongji University School of Medicine, Shanghai, China\n\nAnthocyanins are a class of water-soluble avonoids, which show a range of pharmacological effects, such as prevention of\ncardiovascular disease, obesity control and antitumour activity.&lt;/blockquote&gt;&lt;h3&gt;Resources Specific to Visual Reasoning&lt;/h3&gt;&lt;h4&gt;Cornell NLVR Dataset&lt;/h4&gt;&lt;p&gt;The dataset for the Visual Reasoning task is easily available through &lt;a href=&quot;https://github.com/clic-lab/nlvr&quot;&gt;github&lt;/a&gt;. This dataset was also very conveniently organized into three folders: testing data, development data, and training data. Each folder contains the actual images which are provided for us to train on.&lt;/p&gt;&lt;p&gt;In addition to the images, each folder also includes a JSON file which contains basic JSON representations of each image and the sentence that needs to be validated in each data point. This JSON representation is very useful because we do not need to parse the image to retrieve the raw elements (shape, color, location).&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=3a2f40b355a5&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/getting-started-for-the-capstone-software-installation-pipeline-brainstorming-3a2f40b355a5&quot;&gt;Getting Started for the Capstone: Software Installation &amp;amp; Pipeline Brainstorming&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</description>
	<pubDate>Fri, 06 Apr 2018 06:06:13 +0000</pubDate>
</item>
<item>
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Project Logistics and Package Exploration</title>
	<guid isPermaLink="false">https://medium.com/p/3d3651220219</guid>
	<link>https://medium.com/@ryanp97/project-logistics-and-package-exploration-3d3651220219?source=rss-6378d85d3a9b------2</link>
	<description>&lt;p&gt;Jan Buys has agreed to advise me while I pursue Neural Machine Translation with Semantic Transfer, so this post will mainly focus on the packages and resources available for completing and exploring the minimal viable action plan as described &lt;a href=&quot;https://medium.com/@ryanp97/project-ideas-ab3d796c422e&quot;&gt;previously&lt;/a&gt;.&lt;/p&gt;&lt;h4&gt;Dataset&lt;/h4&gt;&lt;p&gt;I’m currently working on cleaning the &lt;a href=&quot;http://www.edrdg.org/wiki/index.php/Tanaka_Corpus&quot;&gt;Tanaka Corpus&lt;/a&gt; so that I can segment the sentences and then parse the graphs. This corpus is small and the sentences are short, so it seems like a good option for development. I’ve cleaned the corpus by removing any sentences that are not in English or Japanese as well as removing any sentences that do not belong to a pair of translations. Each sentence has at least one translation; note that some sentences have more than one translation.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/753/1*vajUkTbO551F3mjGQjyHQA.png&quot; /&gt;The Japanese sentence can be translated into the two English sentences. Note that the first translation is more direct, while the second translation seems to be drawing on the author’s bias or some other context.&lt;/figure&gt;&lt;p&gt;Also, in the Tanaka corpus, I noticed some things of interest. The first thing is that some hard to read Kanji are annotated with pronunciations in Katakana. The next were a couple typos in the translation (i.e. the proper noun “Tatoeba” was spelled “Tatoeb” in the translation). Also, on one line (the only one that I could find), the Japanese sentence had the romanization in parentheses following the translation. For the sake of consistency, I’ve removed this from the dataset for the sake of consistency. Aside from this single change, everything else was left untouched.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/838/1*xhYvt3b5aClaW0-pDdK8Cw.png&quot; /&gt;Example of a translation pair from the Tanaka corpus. Note the translated smiley face!&lt;/figure&gt;&lt;p&gt;There is also the &lt;a href=&quot;https://alaginrc.nict.go.jp/WikiCorpus/index_E.html&quot;&gt;Kyoto Corpus&lt;/a&gt; which is larger, more diverse, and has a more accurate representation of ‘real’ sentences. However, this dataset is formatted in a slightly more complex way, so I’m still figuring out how to tackle cleaning this corpus. This corpus also provides at least one translation.&lt;/p&gt;&lt;h4&gt;Word Segmentation&lt;/h4&gt;&lt;p&gt;There were 3 main programs that I considered and experimented with: &lt;a href=&quot;https://github.com/neubig/kytea&quot;&gt;KyTea&lt;/a&gt; (pronounced “cutie”), &lt;a href=&quot;http://www.atilika.org/&quot;&gt;Kuromoji&lt;/a&gt;, and &lt;a href=&quot;http://taku910.github.io/mecab/&quot;&gt;MeCab&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;After comparing the segmentation outputs on some small samples, it seemed that KyTea’s outputs often had significant differences between the outputs from Kuromoji and MeCab. After asking a lecturer from the UW Japanese department for their opinion, it seemed that the outputs from Kuromoji and MeCab were closer to the gold standard than the KyTea’s. As implied by the previous sentiment: the data that I will be working with is NOT gold standard, but it is the best available.&lt;/p&gt;&lt;p&gt;I decided on using MeCab as it had a very easy to use Python interface while Kuromoji did not have a easily accessible Python interface. Following &lt;a href=&quot;http://www.robfahey.co.uk/blog/japanese-text-analysis-in-python/&quot;&gt;this&lt;/a&gt; blog post, I’ve setup MeCab to be able to handle more robust input such as slang and neologisms, though I’m unsure if this will work well with the Jacy grammar mentioned in the Graph Parsing section.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1410.0291.pdf&quot;&gt;Here&lt;/a&gt; is a paper on the accuracy of MeCab when performing segmentation. The paper also gives a nice, brief overview on the differences between English and Japanese. Note the size of the Tanaka corpus is smaller than the current version as translations are added semi-regularly.&lt;/p&gt;&lt;h4&gt;Graph Parsing&lt;/h4&gt;&lt;p&gt;For parsing the MRS graphs and converting MRS to DMRS, I used the &lt;a href=&quot;https://github.com/goodmami/mrs-to-penman&quot;&gt;mrs-to-penmen&lt;/a&gt;. It uses the &lt;a href=&quot;https://github.com/delph-in/pydelphin&quot;&gt;PyDelphin&lt;/a&gt; interface which provides a wrapper for the &lt;a href=&quot;http://sweaglesw.org/linguistics/ace/&quot;&gt;ACE&lt;/a&gt; parser. The ACE parser parses the MRS graph, and then mrs-to-penmen should convert the parsed MRS graph to penmen format.&lt;/p&gt;&lt;p&gt;As for the grammar that ACE uses to parse, I’m using the &lt;a href=&quot;http://www.delph-in.net/erg/&quot;&gt;English Resource Grammar&lt;/a&gt; and &lt;a href=&quot;http://moin.delph-in.net/JacyTop&quot;&gt;Jacy&lt;/a&gt; for English and Japanese respectively.&lt;/p&gt;&lt;p&gt;After getting the parsed penman format, there is still some cleaning that I have to do and some simplification of the penman format to simplify what the seq2seq model is expected to output.&lt;/p&gt;&lt;h4&gt;Deep Learning Packages&lt;/h4&gt;&lt;p&gt;Additionally, since this project will rely on Deep Learning, I’ve installed AllenNLP and PyTorch in anticipation of the seq2seq model that will be trained after linearizing the DMRS graph as well as the TreeLSTM model (stretch goal).&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=3d3651220219&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Fri, 06 Apr 2018 01:20:18 +0000</pubDate>
</item>
<item>
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #2: Learning PyTorch</title>
	<guid isPermaLink="false">https://medium.com/p/280325a548d1</guid>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-2-learning-pytorch-280325a548d1?source=rss-fad49d942bf3------2</link>
	<description>&lt;p&gt;In order to familiarized myself with PyTorch, I built an attentive LSTM RNN for a benchmark dataset (&lt;a href=&quot;http://www.cs.cornell.edu/people/pabo/movie-review-data/&quot;&gt;Rotten Tomato movie reviews&lt;/a&gt;) frequently used for sentiment analysis. It was able to achieve up to 76–78% accuracy when trained to 10 epochs, with a learning rate of 0.05. With more time, a natural next step would have been adapting the model for GPU, implementing dropout, and performing hyper-parameter tuning, but this can all be done when I implement my actual model for my project.&lt;/p&gt;&lt;h3&gt;Preprocessing the Data&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Libraries used&lt;/strong&gt;: torchtext, and SpaCy.&lt;/p&gt;&lt;p&gt;After parsing the data files, I split the dataset into train/dev/test datasets using an 80/10/10 ratio&lt;/p&gt;&lt;p&gt;For the next steps of preprocessing (tokenization, vocab building, embedding, and batching), I utilized the torchtext library extensively. I first defined a custom dataset by inheriting data.Dataset. I define three data.Fields for my dataset (TEXT for the word embeddings, POLARITY for the polarity embeddings, and LABEL for the label of each example). When I defining the TEXT field, I was able to set the tokenizer to SpaCy’s default english tokenizer.&lt;/p&gt;&lt;p&gt;One of the benefits of using torchtext is the ease of building the vocab, loading the embeddings, and batching. A few simple lines of code were able to encapsulate these functionalities.&lt;/p&gt;&lt;pre&gt;TEXT.build_vocab(Xtrain) # 1. generate vocabulary&lt;br /&gt;TEXT.vocab.load_vectors(‘glove.6B.’ + str(embedding_dim) + ‘d’) # 2. load GloVe embeddings&lt;br /&gt;Xtrain, Xdev, Xtest = data.Iterator.splits(&lt;br /&gt;    (Xtrain, Xdev, Xtest),&lt;br /&gt;    batch_sizes=(batch_size, len(Xdev), len(Xtest)),&lt;br /&gt;    repeat=False,&lt;br /&gt;    device = -1 # run on CPU&lt;br /&gt;) # 3. build batches&lt;/pre&gt;&lt;p&gt;One of the troubles I ran into when preprocessing the data was being unable to load two embeddings for each word in a sequence — I couldn’t figure out how to load both the pre-trained GloVe embeddings and the learn-able (untrained) polarity embeddings to encapsulates the prior polarities of words. I eventually resolved this difficulty by adding a Field to the Dataset just for the polarity of each word.&lt;/p&gt;&lt;pre&gt;POLARITY = data.Field(sequential=True, tokenize=get_polarity)&lt;br /&gt;POLARITY.build_vocab(Xtrain)&lt;/pre&gt;&lt;h3&gt;Building the Model&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Libraries used&lt;/strong&gt;: PyTorch&lt;/p&gt;&lt;p&gt;I defined my network as follows —&lt;/p&gt;&lt;pre&gt;class Model(nn.Module):&lt;br /&gt;    def __init__(self, num_labels, vocab_size, embeddings_size,&lt;br /&gt;                 hidden_dim, word_embeddings, num_features,&lt;br /&gt;                 batch_size):&lt;br /&gt;        # Code omitted, but basically everything should be &lt;br /&gt;        # initialized here, including:&lt;br /&gt;        # 1. The embeddings&lt;br /&gt;        # 2. The LSTM&lt;br /&gt;        # 3. The linear layers, including attention&lt;/pre&gt;&lt;pre&gt;    def init_hidden(self):&lt;br /&gt;        # initialize a single hidden layer to all 0s (code omitted)&lt;/pre&gt;&lt;pre&gt;    def forward(self, word_vec, feature_vec):&lt;br /&gt;        # 1. Apply embeddings &amp;amp; prepare input&lt;br /&gt;        word_embeds_vec = self.word_embeds(word_vec)&lt;br /&gt;        feature_embeds_vec = self.feature_embeds(feature_vec)&lt;br /&gt;        lstm_input = \&lt;br /&gt;            torch.cat((word_embeds_vec,feature_embeds_vec),2)&lt;/pre&gt;&lt;pre&gt;        # 2. Pass through lstm&lt;br /&gt;        lstm_out, self.hidden = self.lstm(lstm_input, self.hidden)&lt;/pre&gt;&lt;pre&gt;        # 3. Compute and apply weights (attention) to each layer&lt;br /&gt;        alphas = F.softmax(self.attention(lstm_out), dim=0)&lt;br /&gt;        weighted_lstm_out = \&lt;br /&gt;            torch.sum(torch.mul(alphas, lstm_out), dim=0)&lt;/pre&gt;&lt;pre&gt;        # 4. Get final results, passing in weighted lstm output:&lt;br /&gt;        tag_space = self.hidden2label(weighted_lstm_out)&lt;br /&gt;        log_probs = F.log_softmax(tag_space, dim=1)&lt;br /&gt;        return log_probs&lt;/pre&gt;&lt;p&gt;As can be seen by the forward algorithm, the model first retrieves the embeddings of each word in the sequence, then passes them through an LSTM, applies attention, and finally maps the outputs to the labels.&lt;/p&gt;&lt;h3&gt;Training the Model&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Library used&lt;/strong&gt;: PyTorch&lt;/p&gt;&lt;p&gt;I used the NLLLoss function for loss, which I optimized using the SGD optimizer with a learning rate of 0.05. I trained the model for a maximum of 10 epochs. For each epoch, I iterated across the batches, performing 4 steps:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Clear the accumulated gradients and detach the hidden state from the last instance.&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;model.zero_grad()&lt;br /&gt;model.hidden = model.init_hidden()&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;Run the forward pass.&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;log_probs = model(words, polarity)&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;Compute the loss and gradients.&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;loss = loss_function(log_probs, label)&lt;br /&gt;loss.backward()&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;Call optimizer.step() to update parameters.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Evaluating the Model&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Library used&lt;/strong&gt;: n/a&lt;/p&gt;&lt;p&gt;To evaluate my model, I simply iterated across the batches of each dataset and counted the number of correctly positive and correctly negative instances in each batch. Then, I put the numbers together for an overall accuracy score. This was a simple process that didn’t depend heavily on utilizing library tools.&lt;/p&gt;&lt;p&gt;All code I’ve written for this post can be found on Github: &lt;a href=&quot;https://github.com/eunsol/document-e2e-sent/tree/master/rt-polarity&quot;&gt;https://github.com/eunsol/document-e2e-sent/tree/master/rt-polarity&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=280325a548d1&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Thu, 05 Apr 2018 22:48:30 +0000</pubDate>
</item>
<item>
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: Warm Up!</title>
	<guid isPermaLink="false">http://cse481n-capstone.azurewebsites.net/?p=31</guid>
	<link>http://cse481n-capstone.azurewebsites.net/2018/04/05/warm-up/</link>
	<description>&lt;h3&gt;&lt;b&gt;Data Collection:&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We have already begun the process of collecting data from Reddit for our project, using the Reddit API. We want to train our first neural-net model, which will be able to tell if some text is offensive or not, on a large amount of data.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Besides using the Reddit API to get posts from r/MeanJokes, we also use the Reddit submission dataset from &lt;/span&gt;&lt;a href=&quot;https://pushshift.io/&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;pushshift.io&lt;/span&gt;&lt;/a&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; to get more examples from a wider context. We pre-processed data by filtering out non-text submission and deleted posts. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Now that we have the r/meanJokes posts, we want to determine the content phrases of these posts so that we can use them to find similar sentences all over Reddit. We know that the r/meanJokes posts are all offensive, and similar sentences elsewhere in Reddit could give us non-offensive examples. We have found an algorithm for pulling content phrases out of sentences, the &lt;/span&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Rapid Automatic Keyword Extraction (RAKE) algorithm. We have extracted content phrases from r/meanJokes posts and also want to extract them from the rest of Reddit and the next step is to decide on a way to compare similarity and output the final set of posts we want to train the model on.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;b&gt;Deep Learning Tools Set Up:&lt;/b&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For our modeling purposes, we are exploring PyTorch and AllenNLP. We learned PyTorch basics and went through Nelson’s tutorial in the undergrad NLP course last quarter. PyTorch would be our weapon of choice if we experiment with novel models.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We also installed AllenNLP because it looks like a nice tool to build and evaluate baseline models. We are currently going through AllenNLP’s official tutorial by running some of their existing models and demos.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;So far, these tools are working as we expected.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;</description>
	<pubDate>Thu, 05 Apr 2018 17:41:07 +0000</pubDate>
</item>
<item>
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: A Discussion of Language Tasks for the NLP Capstone (Blog Post #1)</title>
	<guid isPermaLink="false">https://medium.com/p/93d82eed396c</guid>
	<link>https://medium.com/nlp-capstone-blog/a-discussion-of-language-tasks-for-the-nlp-capstone-blog-post-1-93d82eed396c?source=rss----9ba3897b6688---4</link>
	<description>&lt;p&gt;Welcome to Team Illimitatum! In this first blog post, we’ll discuss the top three/four ideas that emerged over several days of brainstorming. We started the week with seven exciting potential projects and carefully narrowed them down to these three ideas after taking into consideration feasibility, dataset availability, scope, and complexity for each project. Here they are!&lt;/p&gt;&lt;h3&gt;1. Machine Dictionary&lt;/h3&gt;&lt;h4&gt;Motivation &amp;amp; Context&lt;/h4&gt;&lt;p&gt;The breadth and ubiquity of data and online material is both a gift and a curse. Availability of resources ultimately leads to better-informed decisions, but puts an enormous strain on a researcher to&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Search for sources while having to determine which ones are credible and which ones are not&lt;/li&gt;&lt;li&gt;Read passages and excerpts of these sources&lt;/li&gt;&lt;li&gt;Reach an understanding that is consistent with all of the sources they have seen&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;On one hand, a concept or idea embedded in lengthy text (a textbook, article, or research paper, etc.), makes it so the reader has to achieve some basic understanding of the context surrounding the concept of interest as well as find all related details in order to learn the concept. On the other hand, basic web definitions and blogs &lt;em&gt;may&lt;/em&gt; provide a more presentable and explicit definition of a particular concept, but may be lacking in the amount detail a researcher is looking for along with confidence in its credibility.&lt;/p&gt;&lt;p&gt;Summarization is a task in NLP that aims to reduce passages to a more condense form without sacrificing semantic meaning. The trouble then, of needing to make sense of lengthy text can be alleviated by solving this task.&lt;/p&gt;&lt;p&gt;Entity recognition is another task in NLP where the goal is to highlight “entities” or figures of significance in text that conventially refer to nouns people, places, and organizations. In a medical context, entities could include chemical names, technical terms pertaining to the anatomy of the human body, and diseases.&lt;/p&gt;&lt;p&gt;We aim to combine aspects of both summarization and entity recognition to solve the three “maladies” of research we’ve outlined above: to produce a model that produces the most probable definition for a given entity that would be consistent with as much of the data as possible.&lt;/p&gt;&lt;p&gt;With the intention of training such a model only on publications and research papers, we hope to pave the way for “specialized dictionaries”. Training the data on all publications from a particular field of study would simultaneously restrict the search space to only credible and relevant sources for any definition contained. Providing a generated definition would also prevent the need for the researcher to hunt for it in every source and make sense of it, themselves.&lt;/p&gt;&lt;h4&gt;Minimum Viable Product&lt;/h4&gt;&lt;p&gt;Steps toward the realization of this model involve finding a &lt;strong&gt;dataset&lt;/strong&gt;, defining &lt;strong&gt;metrics of accuracy and performance&lt;/strong&gt;, and determining a reasonable &lt;strong&gt;baseline&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;Semantic Scholar’s &lt;a href=&quot;http://labs.semanticscholar.org/corpus/&quot;&gt;Open Research Corpus&lt;/a&gt; provides over 20 million publications in Computer Science, Neuroscience, and Medicine. While also providing the metadata for each publication, the dataset is primed for the task as it provides credible sources while opening up the possibility of testing the model in three different fields of study.&lt;/p&gt;&lt;p&gt;Metrics of accuracy and performance will be difficult to define. At the moment, we plan on using precision and recall of relevant terms found in Google, Oxford, and Webster definitions and whether they appear in a generated definition.&lt;/p&gt;&lt;p&gt;As for determining a baseline, we plan to implement a standard approach to the Question-Answering task, where the question is always “What is X?”. Ideally, our model should be able to provide more complete definitions with more tangential detail than a standard QA model.&lt;/p&gt;&lt;h4&gt;Stretch Goal: Ontology Matching&lt;/h4&gt;&lt;p&gt;If we were to succeed at the task, applications of our methodology could lead to advancement in both summarization and entity recognition. However, a working version of the model could also pave the way for a novel approach to Ontology Matching.&lt;/p&gt;&lt;p&gt;We could extend our model to solve Ontology Matching by first extracting all significant entities from the corpora. We could then assign each of these entities a definition through our model, and cluster terms based on the definitions accordingly. By doing so, we can in a sense eliminate entities that are linearly dependent with another, reducing the set of entities to one in which their definitions are unique and different enough to be stand-alone definitions.&lt;/p&gt;&lt;p&gt;Through this, we could achieve a new means of standardizing vocabulary in order to clean up knowledge bases or assist in the advent of a new field. Deep Learning in particular has experienced a lot of redundancy in notation and definitions and would benefit from this application.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;2. Natural Language Visual Reasoning&lt;/strong&gt;&lt;/h3&gt;&lt;h4&gt;&lt;strong&gt;Motivation &amp;amp; Context&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Natural Language Visual Reasoning is a task which involves answering questions about an image. Extracting information from an image is an incredibly important but difficult task. Pictures provide a plethora of context about the world, especially about the relationships between objects. It’s also important to be able to extract knowledge about these relationships through natural language. If we can successfully reason about image content, we can change the way we train models and how they integrate with the physical world.&lt;/p&gt;&lt;p&gt;This task is also exciting because it combines Computer Vision and Natural Language Processing. While this goal is fairly lofty, researchers at Cornell and Facebook have collaborated to create a new visual reasoning language dataset (&lt;a href=&quot;http://lic.nlp.cornell.edu/nlvr/&quot;&gt;http://lic.nlp.cornell.edu/nlvr/&lt;/a&gt;), which is a simplified set of images (which contain shapes and colors of a certain format) and sentences. One of the major motivations of this project is the fact that this dataset is so vast and complete.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Minimum Viable Product&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The Visual Reasoning task can be described as follows. The below image represents one instance of training data.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/924/1*O1uEL-Ljs3OgfkVeTJZJuQ.png&quot; /&gt;Example training instance&lt;/figure&gt;&lt;p&gt;As you can see, each piece of data consists of a statement and an image of three boxes. If the statement is true of the image, then we output true, otherwise false. There are two viable courses of action for this problem. In the first approach, we can take the current best performing model (&lt;a href=&quot;https://arxiv.org/pdf/1511.02799.pdf&quot;&gt;Neural Module Networks&lt;/a&gt;) and improve the model to produce better results. In the second approach, we can start from scratch and try to develop a model that is uniquely trained for this dataset. It’s important to note that the Neural Module Network can be applied to any image, whereas we can train a model that can only be applied to images of this type.&lt;/p&gt;&lt;p&gt;Putting it all together, the end product should be able to take the sentence and image as input, extract all relevant information from the image into some intermediate form, validate the criteria described in the sentence for each image, and output the correct answer. We can start by coming up with a basic model which represents each image as a list of objects (yellow triangle at (a, b), blue square at (c, d), black circle at (e, f)) and finds a relationship between the sentence and the list of objects. A more effective model might try to represent the image in a different way, or utilize some of the techniques from the Neural Module Network paper.&lt;/p&gt;&lt;p&gt;The evaluation metrics for this project are very simple. The output of the model will always be true or false and we can do a simple accuracy test to determine the strength of our model.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Stretch Goals&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;A stretch goal for this project would be to generalize to different pictures of the same type. For example, what would happen if we changed the shapes and colors in the picture? What would happen if we changed the type of picture? Trying to achieve a high accuracy on different types of pictures would definitely be a stretch for this project.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;3. You Can Read This, Machines Can’t&lt;/strong&gt;&lt;/h3&gt;&lt;h4&gt;&lt;strong&gt;Motivation and Context&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The motivation for this project idea came from one of the datasets that Yonatan introduced to the class. Research has shown that humans are able to read the following sentence, but it turns out that computers are not able to do the same:&lt;/p&gt;&lt;blockquote&gt;“Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn’t mttaer in waht oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and lsat ltteer be at the rghit pclae”&lt;/blockquote&gt;&lt;p&gt;This is an interesting project because it explores how robust Machine Learning models can be to error (like misspellings). It also explores how much computers might be able to infer given surrounding words.&lt;/p&gt;&lt;p&gt;Another interesting aspect of this project is the available data. We can constantly generate new scrambled sentences with a simple algorithm.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Minimum Viable Product&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The goal of this task is to take a scrambled English sentence and produce the unscrambled version. In particular, each word in the English sentence will have the correct first and last letter but the letters in between might all be incorrect. There are several variations of this data. In the easiest version, the letters in the middle of each word are all correct but they are scrambled. A slightly more difficult challenge would be evaluating sentences where the letters in the middle of each word are semi-random and the number of letters may not correspond to the number of letters in the actual word.&lt;/p&gt;&lt;p&gt;In order to approach this problem, we can start with a very basic approach (for the easier version of the dataset) where we create a very efficient dictionary search algorithm. This approach would not use any Machine Learning and might be very slow. A better approach might be to train a very effective sequence to sequence model which maps a scrambled sentence to the correct sentence. Since we can mutate the data in many ways, we can train several different versions of the same sentence.&lt;/p&gt;&lt;p&gt;We could also explore a more effective way of training the model, rather than training on several mutations of the same sentence. One approach would be to represent words as a volume in N-Dimensional space, so a model would be able to recognize any data point within that volume as the original word. Another interesting approach would be to infer what the next word might be based on the most likely translation of the previous word. This would not only involve choosing a likely translation of the word, but also choosing the most likely translation of the sentence. A third approach might be to treat the the scrambled sentence as a language and use Machine Translation techniques to “translate” the sentence to English.&lt;/p&gt;&lt;p&gt;The performance metric for this would be the number of correctly predicted words in each sentence.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Stretch Goals&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The stretch goal for this project would be to recognize any mutated sentence as the original sentence. This includes random letters, swapped letters, dropped letters, etc. We also could generate new datasets in different languages pretty easily. It might be interesting to see how our model performs on languages like Spanish or French.&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Questioning the immediate vs. long-term impact, along with the tractability and scope of each task, helped define our motivations while ensuring to the best of our knowledge, the viability of each potential project. We asked each other questions along the lines of “why is this task important?”,&lt;br /&gt;“if other people have done this before, how can we do it better?”, and “can it be done in 10 weeks?”.&lt;/p&gt;&lt;p&gt;Ultimately, we want our efforts and insights to have potentially long-term and significant consequences. Without consciously keeping track of whether an idea was research or industry/product oriented, we both came up with only research-oriented proposals. These proposals, if executed properly, would have impacts on related works, having fundamental and methodological influence through novel approaches and perspectives rather than aiding in the downstream application of any particular product. For the time being, we consider ourselves a research mode team.&lt;/p&gt;&lt;p&gt;Link to Code: &lt;a href=&quot;https://github.com/NLP-Capstone-Project&quot;&gt;https://github.com/NLP-Capstone-Project&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=93d82eed396c&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/a-discussion-of-language-tasks-for-the-nlp-capstone-blog-post-1-93d82eed396c&quot;&gt;A Discussion of Language Tasks for the NLP Capstone (Blog Post #1)&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</description>
	<pubDate>Wed, 04 Apr 2018 03:24:13 +0000</pubDate>
</item>
<item>
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #1</title>
	<guid isPermaLink="false">https://medium.com/p/f7412889d221</guid>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-1-f7412889d221?source=rss-fad49d942bf3------2</link>
	<description>&lt;p&gt;Brainstorming project ideas for my CSE 481N NLP Capstone class.&lt;/p&gt;&lt;p&gt;The following project ideas are all research-oriented (research mode).&lt;/p&gt;&lt;h3&gt;Document Level Entity-Entity Sentiment Analysis&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Github URL&lt;/strong&gt;: &lt;a href=&quot;https://github.com/eunsol/document-e2e-sent&quot;&gt;https://github.com/eunsol/document-e2e-sent&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given a document and named entities within the documents, we try to model the sentiment between the various entities in a document, categorizing the sentiments as “positive,” “negative,” or “neutral/no sentiment.” We focus especially on world news, which usually involve many named entities (i.e. countries, world leaders, etc.). For example, given such a document and the named entities in the document:&lt;/p&gt;&lt;blockquote&gt;Iran’s President &lt;strong&gt;Mahmoud Ahmadinejad&lt;/strong&gt; said &lt;strong&gt;Iran&lt;/strong&gt; opposes the “aggressive and arrogant policies” of the &lt;strong&gt;United States&lt;/strong&gt;, local satellite &lt;strong&gt;Press TV&lt;/strong&gt; reported Sunday.&lt;/blockquote&gt;&lt;blockquote&gt;“The &lt;strong&gt;Iranian nation&lt;/strong&gt; opposes the aggressive and arrogant policies of the &lt;strong&gt;U.S. government&lt;/strong&gt; and will stand against them forever,” &lt;strong&gt;Ahmadinejad&lt;/strong&gt; told a religious conference Saturday.&lt;/blockquote&gt;&lt;blockquote&gt;&lt;strong&gt;Iranian president&lt;/strong&gt; also urged the &lt;strong&gt;United States&lt;/strong&gt; and other western countries to change their attitude.&lt;/blockquote&gt;&lt;blockquote&gt;“You should change your attitude,” &lt;strong&gt;Ahmadinejad&lt;/strong&gt; said.&lt;/blockquote&gt;&lt;blockquote&gt;&lt;strong&gt;Iran&lt;/strong&gt; has constantly accused the &lt;strong&gt;United States&lt;/strong&gt; and western powers of earmarking funds to stir up protests against the country and its way of dealing with human rights.&lt;/blockquote&gt;&lt;p&gt;We should be able to generate the following set of relations between the &lt;strong&gt;bolded&lt;/strong&gt;, named entities:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/290/1*ThoNzALuI1a78NOYImbGiw.png&quot; /&gt;Graphical representation of the annotations for the document above. Note that the &lt;strong&gt;red edges&lt;/strong&gt; represent &lt;strong&gt;negative&lt;/strong&gt; sentiment, &lt;strong&gt;green edges &lt;/strong&gt;represent &lt;strong&gt;positive&lt;/strong&gt; sentiment, &lt;strong&gt;light green edges&lt;/strong&gt; represent &lt;strong&gt;non-negative&lt;/strong&gt; sentiments, and &lt;strong&gt;light red edges&lt;/strong&gt; represent &lt;strong&gt;non-positive&lt;/strong&gt; sentiments.&lt;/figure&gt;&lt;h4&gt;Minimal Viable Action Plan&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Fortunately, I already have a pre-existing, labelled dataset I can work with (&lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/project_page/acl16/index.html&quot;&gt;some examples here&lt;/a&gt;), so I don’t have worry about procuring data.&lt;/li&gt;&lt;li&gt;Implement and train a basic LSTM. This will serve as the baseline model for which to compare results.&lt;/li&gt;&lt;li&gt;Improve the LSTM with attention, word and sentiment embeddings, dropout, etc. Experiment with various hyper-parameter settings such as learning rate, dropout rate, batch sizes, etc. to maximize performance.&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Stretch Goals&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Implement and train a bi-affine relation attention network as delineated in &lt;a href=&quot;https://arxiv.org/pdf/1802.10569.pdf&quot;&gt;this paper&lt;/a&gt;, and adapt it for sentiment analysis between entities.&lt;/li&gt;&lt;li&gt;Experiment with new network architectures for the bi-affine network to hopefully improve results.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As I am 99.99% sure that I will be doing entity-entity sentiment analysis for this class, the two ideas below are just here to represent research directions that I’m generally interested in, and how I would approach working on them.&lt;/p&gt;&lt;h3&gt;Generating Conversational Responses&lt;/h3&gt;&lt;p&gt;Given an input sentence, generate an appropriate and believable sentence in response. As chat bots are on the rise, these sorts of problems are becoming increasingly pervasive.&lt;/p&gt;&lt;h4&gt;Minimal Viable Action Plan&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Procure data from some pre-existing dataset. &lt;a href=&quot;http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html&quot;&gt;This movie dialogue corpus&lt;/a&gt; could be a good source. Note though that movie dialogue can be written quite differently from everyday dialogue, and may vary depending on movie genre and setting.&lt;/li&gt;&lt;li&gt;Implement beam search as described in the lecture slides and in &lt;a href=&quot;https://geekyisawesome.blogspot.com/2016/10/using-beam-search-to-generate-most.html&quot;&gt;the referenced blog post&lt;/a&gt;. This will be used as the benchmark model to which the performance of other models will be compared to.&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Stretch Goals&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Implement the semantically-conditioned LSTM-based model as described in &lt;a href=&quot;https://arxiv.org/pdf/1508.01745.pdf&quot;&gt;the following paper&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Attempt to improve the model in some way.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Fake News/Misinformation Detection&lt;/h3&gt;&lt;p&gt;Given quotes from news articles, notable politicians, or online communities, categorize it as “true,” “mostly true,” “half true,” “mostly false,” “false,” or “pants on fire.”&lt;/p&gt;&lt;h4&gt;Minimal Viable Action Plan&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Politifact is a great data source, and an inspiration for this project.&lt;/li&gt;&lt;li&gt;Based off of &lt;a href=&quot;https://arxiv.org/pdf/1803.03786.pdf&quot;&gt;this paper&lt;/a&gt;, implement and train a model that combines a bidirectional attention-based LSTM with an SVM. In particular, the model works by feeding the output of the last hidden layer in the LSTM network into an SVM, which then outputs the label.&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Stretch Goals&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Since the source paper applied the model to news articles, the model may or may not adapt well to quotes. It would be good to experiment with the architecture of the LSTM and/or determine whether the LSTM + SVM model actually performs better than just an LSTM or just an SVM.&lt;/li&gt;&lt;li&gt;Experiment with various combinations of LSTM and other machine learning models in an attempt to improve performance.&lt;/li&gt;&lt;/ul&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=f7412889d221&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 04 Apr 2018 00:34:38 +0000</pubDate>
</item>
<item>
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog Post 2: Warmup</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-5600014144802012716.post-4534461914269998368</guid>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/04/blog-post-2-warmup.html</link>
	<description>I'm working on my project in my fork of Allennlp. I got the dataset and have written the DatasetReader code to preprocess and index the dataset. I plan to add tests for this, and I need to add further preprocessing code such as splitting variables on camel casing.</description>
	<pubDate>Tue, 03 Apr 2018 21:53:00 +0000</pubDate>
	<author>noreply@blogger.com (nlpcapstone)</author>
</item>
<item>
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: NLP</title>
	<guid isPermaLink="false">https://medium.com/p/5258ddd9eedd</guid>
	<link>https://medium.com/@hongnin1/nlp-5258ddd9eedd?source=rss-c450eb982161------2</link>
	<description>&lt;p&gt;This is a blog about Natural Language Processing.&lt;/p&gt;&lt;p&gt;Group member: Ning Hong, Zichun Liu, Zhou Sujie&lt;/p&gt;&lt;p&gt;Team name: The Bugless&lt;/p&gt;&lt;p&gt;Three topics our team is considering doing (All start-up mode):&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Movie summarization (imdb)/Food review summarization (yelp)&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;minimal viable action plan: scrap data from twitter/imdb/rotten tomato/yelp and train our model to be able to summarize the reviews for movies (or food if we are scrapping data from yelp). The summarization of the movie is how the audience feel about the movie in general, for example, given a movie title, our model should be able to produce something like this: It is violent but good.&lt;/p&gt;&lt;p&gt;stretch goals: Output a overall review for the movie instead of simple sentences, for example, given a movie title as input, our model should output: “The Terror got my full attention from beginning to end. I couldn’t turn away. I didn’t want to turn away. For me, that’s extremely rare.”&lt;/p&gt;&lt;p&gt;Another stretch goal is to be able to detect sentiment not only in the US market, but also in China market by using data from DouBan (one of the largest movie review site for China), and compare the sentiment between US and China for a certain movie.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2.Chinese Phoneticization mapping:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The way input Chinese to machine is by typing Pinyin, a kind of phoneticization for Chinese sentence and words, which almost every boy in China know about. However, one Chinese character may have many phoneticization can one Chinese phoneticization sequence may map to many Chinese sentences. When typing Chinese to a machine by Pinyin, the machine will rank the potential Chinese sentences by preferences. However, the ranking may not be so consistent to the context. Therefore, we want to generate a language model that map from Pinyin (English character) to Chinese words and sentences depend on context and speaking habit of this person. In addition, this can be potentially turned into an online learn algorithm.&lt;/p&gt;&lt;p&gt;First step: finding data, where input is Chinese phoneticization (pinyin) and output is Chinese sentences and words. Also, we need to find a good algorithm to do that, and onlint learning algorithm will be better.&lt;/p&gt;&lt;p&gt;Ideas come from Neural Input Method Engine of last quarter: &lt;a href=&quot;https://www.dropbox.com/sh/z3idncggfpwm8rs/AAAnHVvHIPvt_CxTXsaDVqvda?dl=0&amp;amp;preview=teamverynatural_3417269_43042970_Very+Natural+Final+Report.pdf&quot;&gt;https://www.dropbox.com/sh/z3idncggfpwm8rs/AAAnHVvHIPvt_CxTXsaDVqvda?dl=0&amp;amp;preview=teamverynatural_3417269_43042970_Very+Natural+Final+Report.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. Flirt tutor:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When you chat with your beloved boy/girl, you must have some hard time picking interesting and flirting word to response. Then let the machine teach you! Specifically, we want to build a model that can generate cute response giving the context of chatting. This model is neural based.&lt;/p&gt;&lt;p&gt;First step: figuring out what is the right dataset and find such. Meanwhile, consider the right model to use and find paper/resources about is.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;APIs:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Weibo API:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cs.cmu.edu/~lingwang/weiboguide/&quot;&gt;Sina Weibo API Guide&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Allen NLP API:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://allenai.github.io/allennlp-docs/&quot;&gt;Home - AllenNLP 0.4.1 documentation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;PyTorch basic functions:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://pytorch.org/docs/0.3.1/torch.html&quot;&gt;torch - PyTorch master documentation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;PyTorch examples:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/examples&quot;&gt;pytorch/examples&lt;/a&gt;&lt;/p&gt;&lt;p&gt;More detailed examples on PyTorch:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/drive/11iLtGFDpnIuHj5B0rQDGG5lqq6BQ8FRh&quot;&gt;https://colab.research.google.com/drive/11iLtGFDpnIuHj5B0rQDGG5lqq6BQ8FRh&lt;/a&gt;&lt;/p&gt;&lt;p&gt;PyTorch Tutorial:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://courses.cs.washington.edu/courses/cse446/18wi/sections/section7/446_pytorch_tutorial.html&quot;&gt;446_pytorch _tutorial&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://pytorch.org/tutorials/&quot;&gt;Welcome to PyTorch Tutorials - PyTorch Tutorials 0.3.1.post2 documentation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=5258ddd9eedd&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 03 Apr 2018 02:48:17 +0000</pubDate>
</item>
<item>
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Capstone Ideas</title>
	<guid isPermaLink="false">https://medium.com/p/ab3d796c422e</guid>
	<link>https://medium.com/@ryanp97/project-ideas-ab3d796c422e?source=rss-6378d85d3a9b------2</link>
	<description>&lt;p&gt;I plan on following a research track and hope to pursue one of the following ideas:&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Neural Machine Translation with Semantic Transfer&lt;/strong&gt; (as outlined by Jan Buys)&lt;/h4&gt;&lt;p&gt;&lt;em&gt;Minimal Viable Action Plan:&lt;/em&gt; &lt;br /&gt;1) Use statistical parser (&lt;a href=&quot;http://sweaglesw.org/linguistics/ace/&quot;&gt;ACE&lt;/a&gt;) to get MRS graphs of English/Japanese sentences and convert to DMRS graph (using &lt;a href=&quot;https://github.com/delph-in/pydelphin&quot;&gt;PyDelphin&lt;/a&gt; interface to do parsing and conversion)&lt;br /&gt;2) Linearize DMRS graph&lt;br /&gt;3) Train a seq2seq that takes linearized English DMRS graph and outputs linearized Japanese DMRS graph&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*J3l_TjWr3A-jOAcrCznqvA.png&quot; /&gt;Example of penmen format that DMRS can be represented with (non-linearized). Note that the representation shown is an AMR graph, not a DMRS graph. Figure taken from this &lt;a href=&quot;https://arxiv.org/pdf/1704.08381.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Stretch Goals:&lt;br /&gt;&lt;/em&gt;1) Explore different architectures for semantic transfer (e.g. TreeLSTM as opposed to seq2seq)&lt;br /&gt;2) Explore ways to learn correspondences between semantic concepts in the two languages&lt;/p&gt;&lt;h4&gt;DMRS to Text Generation (as outlined by Jan Buys)&lt;/h4&gt;&lt;p&gt;&lt;em&gt;Minimal Viable Action Plan:&lt;br /&gt;&lt;/em&gt;1) Generate DMRS graph serialization similar to what was outlined above&lt;br /&gt;2) Train seq2seq model to generate text directly from graph serialization&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Bndfdhs3ixwG6-JUQPC72A.png&quot; /&gt;Example of different graph representations. Figure taken from this &lt;a href=&quot;http://www.lrec-conf.org/proceedings/lrec2016/pdf/634_Paper.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Stretch Goals:&lt;/em&gt;&lt;br /&gt;1) Experiment with different seq2seq architectures&lt;br /&gt;2) Attempt semi-supervised training using a high-precision grammar-based parser&lt;/p&gt;&lt;h4&gt;Reproduce results / expand on a Paper&lt;/h4&gt;&lt;p&gt;In this option, I would be attempting to expand on this &lt;a href=&quot;https://arxiv.org/pdf/1704.04859.pdf&quot;&gt;paper&lt;/a&gt; regarding hybrid models. The paper attempts to alleviate and/or solve the issue of out-of-vocabulary words and characters, specifically in Chinese and Japanese. It describes using a CNN in conjunction with an RNN to do so; the CNN learns the radicals of characters and attempts to choose characters with similar radicals since the meaning of radicals remains constant between characters.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/822/1*5GyInYVxc8ifP_YRCHiHHA.png&quot; /&gt;Image taken from the linked paper.&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Minimal Viable Action Plan:&lt;br /&gt;&lt;/em&gt;1) Obtain/create a visual dataset of the characters in order to train the CNN&lt;br /&gt;2) Design and explore different methods for joining the two models such as described in the paper&lt;br /&gt;3) Train the model end-to-end&lt;/p&gt;&lt;p&gt;&lt;em&gt;Stretch Goals:&lt;br /&gt;&lt;/em&gt;1) Error analysis on different methods for joining and potential points of error&lt;br /&gt;2) Exploration of model variations and architecture (incremental changes similar to this &lt;a href=&quot;https://arxiv.org/pdf/1708.04755.pdf&quot;&gt;paper&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;For current progress, visit the &lt;a href=&quot;https://github.com/ryanp97/NeuralEmpty&quot;&gt;repo&lt;/a&gt;.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=ab3d796c422e&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 03 Apr 2018 00:55:49 +0000</pubDate>
</item>
<item>
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: First Blog Post!</title>
	<guid isPermaLink="false">http://cse481n-capstone.azurewebsites.net/?p=15</guid>
	<link>http://cse481n-capstone.azurewebsites.net/2018/04/02/first-blog-post/</link>
	<description>&lt;h3&gt;&lt;strong&gt;Team Name: &lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Team Watch Your Language!&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Three Project Ideas:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Offensive Text Recognition&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our minimal viable plan is to create two models, one which determines if text is offensive or not, and another which determines if any particular group is targeted by the text, such as a racial, religious, or political grouping. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our stretch goal is to use these models to make a third model which can use the first two outputs as assumptions to then provide human-readable explanations as to why that particular text was labeled the way it was. This way we can determine if text is offensive or not and provide reasons for that labeling. This can be used to assist in teaching conversational agents common sense about what to say.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Domain-Specific Conversational Agent&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our minimal viable plan is to create a conversational agent which can provide information and hold a conversation with a well-intentioned user about a particular domain. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The stretch goal here would be to just continually make it better at conversing, at least in the particular domain it is trained to be good at talking about.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Image Description&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our minimal viable plan is to create a model that can describe a simple image correctly, like generating a sentence describing the spatial relation between a box and a cylinder.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For the stretch goal, we would like to improve our model that can describe a unique pattern of an image among 3 (or multiple) other images. For example, if we have 3 images A, B, and C, we would like to come up with a model to generate a sentence that describes image A but not image B or C.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;GitLab Repo:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; &lt;/span&gt;&lt;a href=&quot;https://gitlab.cs.washington.edu/danielby/nlp-capstone&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;https://gitlab.cs.washington.edu/danielby/nlp-capstone&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Mode:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We will be tackling the Offensive Text Recognition task in&lt;/span&gt; &lt;em&gt;research mode&lt;/em&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;!&lt;/span&gt;&lt;/p&gt;</description>
	<pubDate>Mon, 02 Apr 2018 06:58:42 +0000</pubDate>
</item>
<item>
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog Post 1: Project Ideas</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-5600014144802012716.post-6768023392170538237</guid>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/03/blog-post-1.html</link>
	<description>&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt;I am interested in working in the code generation/semantic parsing space on the research track. My code &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt;will be in various branches of my fork of allennlp (https://github.com/rajasagashe/allennlp). I will keep you&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt; updated on which branch/commits I worked on during each blog post. Also note that project idea 1 has &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt;the most detail since I have picked it as my project!&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;h2 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;&quot;&gt;&lt;span&gt;Project Idea 1&lt;/span&gt;&lt;/h2&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Minimum Viable Plan:&lt;/span&gt;&lt;span&gt; Implement the model in the recent UW paper which introduces the task of &lt;/span&gt;&lt;br /&gt;&lt;span&gt;generating the code for a java function from a natural language description. To further aid code &lt;/span&gt;&lt;br /&gt;&lt;span&gt;generation, the class in which the generated function is to reside is provided, i.e. the class variables &lt;/span&gt;&lt;br /&gt;&lt;span&gt;and methods. Thus the encoder encodes the class as well as the utterance and the decoder uses a &lt;/span&gt;&lt;br /&gt;&lt;span&gt;two step attention mechanism and decodes through the java grammar production rules. &lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Stretch Goals:&lt;/span&gt;&lt;span&gt; Reproduce the state of the art results in the paper. I’m putting this in the stretch goals &lt;/span&gt;&lt;br /&gt;&lt;span&gt;since successfully implementing a neural semantic parser with type constraints is pretty challenging. &lt;/span&gt;&lt;br /&gt;&lt;span&gt;In addition, I hope to experiment with other improvements like encoding the entire class method body&lt;/span&gt;&lt;br /&gt;&lt;span&gt; which wasn’t done.&lt;/span&gt;&lt;/div&gt;&lt;h2 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;&quot;&gt;&lt;span&gt;Project Idea 2&lt;/span&gt;&lt;/h2&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Minimum Viable Plan: &lt;/span&gt;&lt;span&gt;Implement this paper: &lt;/span&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.01696.pdf&quot; style=&quot;text-decoration: none;&quot;&gt;&lt;span&gt;https://arxiv.org/pdf/1704.01696.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;. The model is &lt;/span&gt;&lt;br /&gt;&lt;span&gt;similar to that of the previous idea, but the datasets are for python and ifttt instead.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Stretch Goals: &lt;/span&gt;&lt;span&gt;Improve the paper’s result.&lt;/span&gt;&lt;/div&gt;&lt;h2 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;&quot;&gt;&lt;span&gt;Project Idea 3&lt;/span&gt;&lt;/h2&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Minimum Viable Plan: &lt;/span&gt;&lt;span&gt;Perform transfer learning across several code generation tasks by using the &lt;/span&gt;&lt;br /&gt;&lt;span&gt;same encoder for them all. This technique would be similar to what was used in the Cove paper &lt;/span&gt;&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.00107.pdf&quot; style=&quot;text-decoration: none;&quot;&gt;&lt;span&gt;https://arxiv.org/pdf/1708.00107.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;. &lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Stretch Goals: &lt;/span&gt;&lt;span&gt;Improve the individual paper results with this technique.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;</description>
	<pubDate>Fri, 30 Mar 2018 22:41:00 +0000</pubDate>
	<author>noreply@blogger.com (nlpcapstone)</author>
</item>

</channel>
</rss>
