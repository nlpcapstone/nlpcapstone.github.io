<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">

<channel>
	<title>NLP Capstone Spring 2018</title>
	<link>https://nlpcapstone.github.io/</link>
	<language>en</language>
	<description>NLP Capstone Spring 2018 - https://nlpcapstone.github.io/</description>
	<atom:link rel="self" href="https://nlpcapstone.github.io/rss20.xml" type="application/rss+xml"/>

<item>
	<title>Kuikui Liu &lt;br/&gt; Team INLP: 3-Coloring the Plane</title>
	<guid isPermaLink="false">http://mathstoc.wordpress.com/?p=272</guid>
	<link>https://mathstoc.wordpress.com/2018/04/04/3-coloring-the-plane/</link>
	<description>&lt;p&gt;Today, I would like to present a very neat problem my friend, Alex Tsun, told me recently while he was playing Pokemon Go. I’ll give the problem a name for convenience below.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;3-Coloring the Plane:&lt;/b&gt;&lt;em&gt; Suppose you assign a color to every point in the plane &lt;img alt=&quot;\mathbb{R}^{2}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mathbb{R}^{2}&quot; /&gt;. Your only choices are red, green, and blue. Prove that there must exist a rectangle (in any orientation, located anywhere), such that all four corners of the rectangle have the same color.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For convenience, let’s call such a desired rectangle a monochromatic rectangle.&lt;/p&gt;
&lt;p&gt;Seems daunting, and the key difficulty I had with this problem was its lack of “structure”. The rectangle could be anywhere! Yet, the infinitude of &lt;img alt=&quot;\mathbb{R}^{2}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mathbb{R}^{2}&quot; /&gt;, and hence, the sheer abundance (uncountably many) of rectangles, leads one to believe that there should be at least one (and perhaps, also infinitely many). Such an observation hinted at a Pigeonhole Principle type argument.&lt;/p&gt;
&lt;p&gt;Here is how one way of setting it up. The key to think about it (at least for me) was to introduce additional structure into the rectangles. Here, we will consider a specific way of generating rectangles. Fix a pair of distinct parallel lines &lt;img alt=&quot;L_{1},L_{2}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%2CL_%7B2%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1},L_{2}&quot; /&gt; and then fix another pair of distinct parallel lines &lt;img alt=&quot;L_{1}^{\perp},L_{2}^{\perp}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%5E%7B%5Cperp%7D%2CL_%7B2%7D%5E%7B%5Cperp%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1}^{\perp},L_{2}^{\perp}&quot; /&gt;, which are perpendicular to &lt;img alt=&quot;L_{1},L_{2}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%2CL_%7B2%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1},L_{2}&quot; /&gt;. The intersections of the lines &lt;img alt=&quot;L_{1},L_{2}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%2CL_%7B2%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1},L_{2}&quot; /&gt; with &lt;img alt=&quot;L_{1}^{\perp},L_{2}^{\perp}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%5E%7B%5Cperp%7D%2CL_%7B2%7D%5E%7B%5Cperp%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1}^{\perp},L_{2}^{\perp}&quot; /&gt; will form the corners of a rectangle. Now, imagine sliding &lt;img alt=&quot;L_{1}^{\perp},L_{2}^{\perp}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%5E%7B%5Cperp%7D%2CL_%7B2%7D%5E%7B%5Cperp%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1}^{\perp},L_{2}^{\perp}&quot; /&gt; along &lt;img alt=&quot;L_{1},L_{2}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%2CL_%7B2%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1},L_{2}&quot; /&gt;. This, again, produces infinitely many rectangles. And the set of all possible rectangles in &lt;img alt=&quot;\mathbb{R}^{2}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mathbb{R}^{2}&quot; /&gt; certainly contains the set of all possible rectangles that can be produced via this method. Thus, it suffices to find a monochromatic rectangle produced in this manner.&lt;/p&gt;
&lt;p&gt;Unfortunately, this doesn’t work, and there is an easy way to see it. Suppose &lt;img alt=&quot;L_{1}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1}&quot; /&gt; is completely colored, say, blue, while &lt;img alt=&quot;L_{2}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B2%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{2}&quot; /&gt; is completely colored, say, red. Then, no matter how one positions &lt;img alt=&quot;L_{1}^{\perp},L_{2}^{\perp}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%5E%7B%5Cperp%7D%2CL_%7B2%7D%5E%7B%5Cperp%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1}^{\perp},L_{2}^{\perp}&quot; /&gt;, there will always be two corners blue and two corners red.&lt;/p&gt;
&lt;p&gt;However, this leads one to the following idea: instead of having two distinct parallel lines &lt;img alt=&quot;L_{1},L_{2}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%2CL_%7B2%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1},L_{2}&quot; /&gt;, let’s have four distinct parallel lines &lt;img alt=&quot;L_{1},L_{2},L_{3},L_{4}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%2CL_%7B2%7D%2CL_%7B3%7D%2CL_%7B4%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1},L_{2},L_{3},L_{4}&quot; /&gt; (with &lt;img alt=&quot;L_{1}^{\perp},L_{2}^{\perp}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%5E%7B%5Cperp%7D%2CL_%7B2%7D%5E%7B%5Cperp%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1}^{\perp},L_{2}^{\perp}&quot; /&gt; still lying perpendicular to these and sliding along them to produce rectangles). Now, this completely avoids the previous problem, as the intersection points of &lt;img alt=&quot;L_{1}^{\perp}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%5E%7B%5Cperp%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1}^{\perp}&quot; /&gt; with the lines &lt;img alt=&quot;L_{1},L_{2},L_{3},L_{4}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%2CL_%7B2%7D%2CL_%7B3%7D%2CL_%7B4%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1},L_{2},L_{3},L_{4}&quot; /&gt; give a configuration of four colors which necessarily contain two points with the same color (by the Pigeonhole Principle, with 4 intersection points for the pigeons and 3 colors for the holes).&lt;/p&gt;
&lt;p&gt;This is good because now, we just have to find another position for &lt;img alt=&quot;L_{2}^{\perp}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B2%7D%5E%7B%5Cperp%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{2}^{\perp}&quot; /&gt; along &lt;img alt=&quot;L_{1},L_{2},L_{3},L_{4}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%2CL_%7B2%7D%2CL_%7B3%7D%2CL_%7B4%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1},L_{2},L_{3},L_{4}&quot; /&gt; that yields the same configuration when intersected with &lt;img alt=&quot;L_{1},L_{2},L_{3},L_{4}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B1%7D%2CL_%7B2%7D%2CL_%7B3%7D%2CL_%7B4%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{1},L_{2},L_{3},L_{4}&quot; /&gt;. This is obviously true because there are only finitely many (exactly &lt;img alt=&quot;3^{4} = 81&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=3%5E%7B4%7D+%3D+81&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;3^{4} = 81&quot; /&gt;) possible configurations (holes) but infinitely many possible places to place &lt;img alt=&quot;L_{2}^{\perp}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7B2%7D%5E%7B%5Cperp%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{2}^{\perp}&quot; /&gt; (pigeons)!&lt;/p&gt;
&lt;p&gt;Thus we have the claim.&lt;/p&gt;
&lt;p&gt;Upon examining this pigeonhole argument, we see that one doesn’t actually need infinite lines. All we need is a &lt;img alt=&quot;4 \times 82&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=4+%5Ctimes+82&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;4 \times 82&quot; /&gt; rectangular grid, and the conclusion that there is a monochromatic rectangle (with corners on the grid) still holds. This observation immediately shows us that in fact, there are uncountably infinitely many monochromatic rectangles in &lt;img alt=&quot;\mathbb{R}^{2}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7B2%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mathbb{R}^{2}&quot; /&gt;. Furthermore, there is one for any desired orientation.&lt;/p&gt;</description>
	<pubDate>Wed, 04 Apr 2018 07:34:40 +0000</pubDate>
</item>
<item>
	<title>Nicholas Ruhland &lt;br/&gt; Team nlp-wikipedia: Week 2 Milestone</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-354195895616735293.post-6386758378464642504</guid>
	<link>http://nlp-wikipedia.blogspot.com/2018/04/milestone.html</link>
	<description>&lt;h2&gt;Project ideas&lt;/h2&gt;&lt;hr /&gt;&lt;h3&gt;Wikipedia vandalism detection&lt;/h3&gt;One of the primary challenges facing Wikipedia's content is vandals, or users who intentionally add incorrect or irrelevant information to articles. To combat this ongoing issue, bots automatically check each incoming edit and make an attempt to label the edit as contributive or vandalism.&lt;br /&gt;&lt;br /&gt;Existing research tends to focus on labelling users as either editing in good faith or as vandals, encoded by embedding each user's edits into a dense vector (Yuan). Information about series of edits allows much more accurate classification than considering each edit independently, but is also causes a more complex model.&lt;br /&gt;&lt;br /&gt;At least 2 datasets have been publicly provided in the cited papers: UMDWikipedia dataset (Kumar) and SpamDataset (Green). The UMDWikipedia dataset has 33K users and 770 edits, and the SpamDataset has 4.2K users and 75.6K edits.&lt;br /&gt;&lt;br /&gt;A minimal action plan could include the following steps:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;- Implement a baseline model that considers edits independently, as opposed to generating a user embedding.&lt;/li&gt;&lt;li&gt;- Extract features as described in the Kumar paper, and attempt to replicate results with an SVM model. Stretch: try other models to try to improve accuracy. Stretch: find more features that could potentially improve the results.&lt;/li&gt;&lt;li&gt;- Implement the model proposed in the Yuan paper (user embedding by LSTM) and attempt to replicate results.&lt;/li&gt;&lt;li&gt;- Implement the model proposed in the Green paper and attempt to replicate results. Compare these features with those in the earlier Kumar paper.&lt;/li&gt;&lt;li&gt;- Use the larger (UMDWikipedia) dataset in the Kumar model. Attempt to improve the accuracy by using the larger dataset.&lt;/li&gt;&lt;li&gt;- Novel models. Implement a GAN that will use a generative model trained to fool the discriminator. &lt;/li&gt;&lt;li&gt;- Stretch: Context specific vandalism. Train a model to recognize common vandalism patterns specific to one article topic. &lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Papers&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;[1] Kumar - VEWS: A Wikipedia Vandal Early Warning System (2015) https://arxiv.org/abs/1507.01272&lt;/li&gt;&lt;li&gt;[2] Yuan - Wikipedia Vandal Early Detection: from User Behavior to User Embedding (2017) https://arxiv.org/abs/1706.00887&lt;/li&gt;&lt;li&gt;[3] Green - Spam Users Identification in Wikipedia via Editing Behavior (2017) https://sites.google.com/site/francescaspezzano/publications/spammers&lt;/li&gt;&lt;/ol&gt;&lt;hr /&gt;&lt;h3&gt;Suggesting Missing Wikipedia Articles&lt;/h3&gt;Wikipedia as a resource has the potential to present cutting edge research to a large audience in an accessible manner. Many recent academic research topics could be qualified to have articles, but people knowledgable enough in a relevant field do not know which topics need to be added or expanded on Wikipedia.&lt;br /&gt;&lt;br /&gt;Current research includes: providing potentially missed citations given a paper (&lt;a href=&quot;http://allenai.org/semantic-scholar/citeomatic/&quot;&gt;http://allenai.org/semantic-scholar/citeomatic/&lt;/a&gt;)&lt;br /&gt;&lt;br /&gt;This project combines aspects of keyword extraction or article summary with a classification task of determining which topics are notable enough to have articles. This sort of data could be extracted from Wikipedia category listings for various topics.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Tasks include:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Parsing out topics (article names) from a collection of academic papers&lt;/li&gt;&lt;li&gt;Determining which topics are eligible to have articles, but either lack articles or are missing important information from the articles&lt;/li&gt;&lt;li&gt;Possible demo: input the content of a paper and output which wikipedia articles would be relevant to include the information in this paper. &lt;/li&gt;&lt;li&gt;Possible demo: given a topic, find articles that do not yet exist and provide relevant papers for those articles.&lt;/li&gt;&lt;/ul&gt;&lt;hr /&gt;&lt;h3&gt;Analyzing Citations in Wikipedia&lt;/h3&gt;&lt;div&gt;Though not as rigorous as academic, Wikipedia generally has strict guidelines about when to include citations. The quality of citations is important for the integrity of an article, so it makes sense to check that the sources provided are being used appropriately.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Tasks of forming datasets have helped to structure the task of analyzing Wikipedia's citations (2). &lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;From this dataset, we hope to train a classifier to predict good and bad citations. Labels can be computed as those that remain in the current state of an article versus those that get removed after being added.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Tasks include:&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;Adding labels to the dataset as described.&lt;/li&gt;&lt;li&gt;Training models based on the features provided in the dataset. These models include the context of the citation (its position in an article) and the text that is being referenced.&lt;/li&gt;&lt;li&gt;More advanced processing could include checking for relevance of the source, if possible. Features could be extracted from some of the structured data, such as the URL or other content contained in the citation.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;References&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;[1] Nielsen - Scientific citations in Wikipedia (2007) https://arxiv.org/abs/0705.2106&lt;/li&gt;&lt;li&gt;[2] https://meta.wikimedia.org/wiki/Research:Understanding_the_context_of_citations_in_Wikipedia&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;br /&gt;</description>
	<pubDate>Wed, 04 Apr 2018 07:29:53 +0000</pubDate>
	<author>noreply@blogger.com (Nicholas Ruhland)</author>
</item>
<item>
	<title>Aaron Johnston, Lynsey Liu &lt;br/&gt; Team Viterbi Or Not To Be: Preliminary Ideas</title>
	<guid isPermaLink="false">https://medium.com/p/9a0f5382cff5</guid>
	<link>https://medium.com/@viterbi.or.not/preliminary-ideas-9a0f5382cff5?source=rss-c522ef075bb3------2</link>
	<description>&lt;p&gt;While considering various ideas to explore for our project, our team ultimately decided to approach the project in “research mode”, with a greater focus on novel models and analysis of their performance. Although it would be exciting to build a complete platform incorporating a Natural Language Processing techniques, we decided it would be more educational overall to examine the workings of the models themselves. With that overarching direction in mind, we began to think about the issues in the field that would be most interesting to work with, and came up with a few ideas:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Automatic Conversation Summarization&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;One idea is to explore various approaches for the automatic summarization of written conversations. As a topic, the usefulness of written conversation summaries is undeniable — as so many methods of communication are being powered through online e-mail or chat interfaces, there is value in being able to extract relevant topics or action items from conversations just as there is for documents or news articles. As a task, however, there are plenty of challenges to overcome to determine what properties of a topic in a discussion influence its importance to the overall summary and how a summary can be generated based off that data.&lt;/p&gt;&lt;p&gt;In our preliminary research, we found that relatively few efforts to automatically summarize natural language text have focused on conversational input specifically, but those that have show promise. One paper in particular examines the &lt;a href=&quot;https://pdfs.semanticscholar.org/efe0/fffe080ac4b1a943f62cc56f2baa27c6e195.pdf&quot;&gt;summarization of both spoken and written conversations&lt;/a&gt;, and notes that there are substantial differences in the two types of data. Although the results of the summarization efforts presented in this paper are well below the baseline established by human summarizers, we think it would be interesting to implement a comparable system and explore modifications that could be made or alternative models that could be used to get a more complete picture of the possibilities. By reading the research papers of related projects, we have identified a number of datasets that would allow us to train our model, including &lt;a href=&quot;http://groups.inf.ed.ac.uk/ami/corpus/&quot;&gt;meeting summaries&lt;/a&gt;, &lt;a href=&quot;https://www.cs.cmu.edu/~./enron/&quot;&gt;summaries of email threads&lt;/a&gt;, and &lt;a href=&quot;https://flossmole.org/content/software-archaeology-gnue-irc-data-summaries&quot;&gt;summaries of chat logs&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;If we were to pursue this option, we would initially focus on replicating the approaches used by previous research projects to automatically summarize conversations as a baseline. Once that is working, we would move toward creating a minimum viable product by examining the features and models used in past approaches and performing an analysis of possible alternatives. Although it may not be within the scope of this quarter-long project to apply an entirely novel model to the problem, at a minimum we would seek to determine the relationship between different features and the various types of conversation available through experimentation.&lt;/p&gt;&lt;p&gt;Beyond the minimum viable product, there are several stretch goals we would like to tackle. One of the more seemingly impactful would be using model parameters and features extracted from one type of data to improve the model’s performance on another, such as using the result of training a model on spoken meeting data to improve the automatic summarization of emails. In the previously linked research paper, the authors mention that a future goal for their research is to implement such a system, and they assert that preliminary results are promising. Another stretch goal would be attempting to beat previous approaches by focusing on one specific domain and using the unique properties of that data to produce better summaries. An example could be to focus on chat logs from the GNU dataset that deal specifically with bugfixes — by restricting the domain to a set of code-related topics that likely share a much smaller vocabulary and a consistent notion of “importance”, such as action items during the lifecycle of a bugfix, it may be possible to produce better summaries than in the case of general summarization. Finally, we are interested in comparing the results of extractive and abstractive summarization — while the former works by identifying the most important sentences in a text and combining them, the latter attempts to make a more “human” summary by identifying topics in a text and generating new sentences that paraphrase the intent. One extension could therefore be to try extending extractive models proposed previously to be abstractive.&lt;/p&gt;&lt;p&gt;While our primary interest would be in conversation summarization, we would also consider doing a similar summarization project based on research papers if the conversation data proved to be insufficient. To do so, we would identify corpuses of research papers, and use the author-written abstracts as the summaries for our training data.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Multiple Premise Entailment&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Another idea is to pursue a project that tackles the problem of Multiple Premise Entailment, which involves being able to make inferences based on multiple premises. This would contribute to the making of more “knowledgeable” models that aim to use and understand contexts across multiple ideas, a more challenging problem than making inferences from a single sentence as is done in standard entailment tasks.&lt;/p&gt;&lt;p&gt;If we were to take on this challenge, we would start by looking at &lt;a href=&quot;http://aclweb.org/anthology/I17-1011&quot;&gt;existing research&lt;/a&gt; and begin our approach in a similar way, first using baseline neural models for standard entailment. Once this is running on the MPE dataset, we would do some study and error analysis of these runs, then aim to improve upon these models in a way that takes our findings on the baseline models into account and includes adaptations to tackle multiple premises to create a minimum viable product.&lt;/p&gt;&lt;p&gt;Beyond the minimum viable product, a stretch goal for this problem would be to present a unique model that possibly uses a novel approach from the baseline models to tackle multiple premises in a way that best suits this new challenge, relying less on models for standard entailment.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Visual Reasoning&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The last idea we are interested in involves the &lt;a href=&quot;http://lic.nlp.cornell.edu/nlvr/&quot;&gt;Cornell Natural Language Visual Reasoning dataset&lt;/a&gt;, which contains 92.244 pairs of natural language statements grounded in synthetic images. The challenge that involves language is to determine whether a statement about the image is true or false. Doing so would typically involve reasoning based on spatial relations, quantities, and other qualities about sets of objects that might appear in the NLVR dataset images.&lt;/p&gt;&lt;p&gt;If we were to work on this problem, we would focus first on reimplementation of the &lt;a href=&quot;https://arxiv.org/pdf/1511.02799.pdf&quot;&gt;state of the art model&lt;/a&gt; from UC Berkeley. As noted in the “Future Work” section of the UC Berkeley paper, their model maintains a strict separation between predicting network structures and learning network parameters. As a stretch goal, we could work on integrating the current approach with existing tools for learning semantic parsers to achieve an integration between the two components that would possibly improve performance or make way for a novel approach to the problem.&lt;/p&gt;&lt;p&gt;As a start for our to-be-determined project, we’ve created a &lt;a href=&quot;https://github.com/viterbi-or-not-to-be/viterbi-or-not-to-be&quot;&gt;git repository&lt;/a&gt; where you can access our code and see our progress!&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=9a0f5382cff5&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 04 Apr 2018 07:03:32 +0000</pubDate>
</item>
<item>
	<title>Kuikui Liu &lt;br/&gt; Team INLP: NLP Capstone Post #1: Ideation</title>
	<guid isPermaLink="false">http://mathstoc.wordpress.com/?p=277</guid>
	<link>https://mathstoc.wordpress.com/2018/04/04/nlp-capstone-post-1-ideation/</link>
	<description>&lt;p&gt;In this post, I’d like to briefly discuss three different ideas I have for my capstone project.&lt;/p&gt;
&lt;h1&gt;A Theoretical Analysis of RNNs (Research Mode):&lt;/h1&gt;
&lt;p&gt; A recent &lt;a href=&quot;https://arxiv.org/abs/1703.00810&quot;&gt;paper of Professor Naftali Tishby&lt;/a&gt; provided some useful observations on the behavior of feedforward neural networks, and proposed a promising approach to understanding their performance. Earlier empirical work done in the vision community showed that when a convolutional neural network is trained, layers closer to the input learn lower level features (such as edges and corners) and layers closer to the output learn higher level features (“this part of the image resembles a nose, and this other part resembles an eye”). One might expect similar behavior to occur with general feedforward neural networks: that earlier layers learn lower level features of the input and later levels learn higher level features of the input. The key insight here was to think of each layer of a neural network as a Markov chain, where each layer &lt;img alt=&quot;L_{i}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7Bi%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{i}&quot; /&gt; is a (vector-valued) random variable that is conditionally independent of &lt;img alt=&quot;L_{j}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7Bj%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{j}&quot; /&gt; for all &lt;img alt=&quot;j &amp;lt; i - 1&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=j+%3C+i+-+1&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;j &amp;lt; i - 1&quot; /&gt; given &lt;img alt=&quot;L_{i-1}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=L_%7Bi-1%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;L_{i-1}&quot; /&gt;. In this way, information flowing forward in the network can be quantified via notions of entropy from traditional information theory.&lt;/p&gt;
&lt;p&gt;The paper contains some empirical work, observing that there are generally two phases to learning artificial neural networks via stochastic gradient descent: the fitting phase, and the compression phase. The fitting phase is the shorter phase, where the model is quickly tuning itself to minimize the empirical loss function. At the end of this phase, we don't necessarily have a model that will generalize to new data. The compression phase is where the model begins to learn the relevant features in the input, with the intuition that there are many irrelevant parts of the input (I don't need to know every atom in an object to identify it). &lt;/p&gt;
&lt;p&gt;The goal of this project would be to perform a similar theoretical analysis and empirical work for RNN architectures (whose &quot;natural&quot; Markov chain isn't as simple, as there are cycles) on some traditional NLP task, such as Machine Translation, with the goal of studying the flow of information in an RNN architecture, rather than performing comparably to state-of-the-art Machine Translation models (although this can be a stretch goal).&lt;/p&gt;
&lt;p&gt;The relevant steps in this project will likely look like the following:&lt;br /&gt;
1. Reading up on the relevant work by Tishby et. al. (and any other theoretical papers on deep learning).&lt;br /&gt;
2. Understand basic and traditional RNN architectures.&lt;br /&gt;
3. Learning PyTorch.&lt;br /&gt;
4. Implementing several of these architectures and testing (for example, to see if learning also comes in two distinct phases: fitting and compression)&lt;br /&gt;
5. Using these empirical observations, and information theory to analyze these architectures.&lt;br /&gt;
6. Time permitted, play around with new RNN architectures.&lt;/p&gt;
&lt;h1&gt;Musical Style Learning from Musical Scores (Research/Start-Up Mode):&lt;/h1&gt;
&lt;p&gt; This idea lies somewhat outside traditional NLP in that it tackles the language of music. While the alphabet of a musical score consist chiefly of the 12 musical notes, there is added challenge in that several notes may be played simultaneously, especially if there are several instruments involved or simply the two hands of a pianist. Furthermore, the exact timing of each note played matters, note merely the ordering of the notes.&lt;/p&gt;
&lt;p&gt;The idea here is simply to, given the score of a musical piece, represented as a sequence of notes at each time, predict the era (Baroque, Classical, Romantic, etc.) or even, the composer of the piece (Bach, Beethoven, Brahms, etc.) There are several problems to be solved step by step for this project.&lt;/p&gt;
&lt;p&gt;1. Data collection from a large library of musical scores (ex: &lt;a href=&quot;http://imslp.org/&quot;&gt;IMSLP&lt;/a&gt;)&lt;br /&gt;
2. Data formatting so as to be usable.&lt;br /&gt;
3. Model selection.&lt;br /&gt;
4. Model implementation (PyTorch).&lt;br /&gt;
5. Model testing.&lt;/p&gt;
&lt;p&gt;There are also several extensions that can be viewed as stretch goals. For these, the first two can be reused.&lt;/p&gt;
&lt;h3&gt;Musical Score Generation:&lt;/h3&gt;
&lt;p&gt; Now, we learn how to compose a piece that “sounds” similar to a given composer. This will involve learning from the pieces written by a given input composer, and outputting a new piece. One core challenge here is ensuring that the output is syntactically correct.&lt;/p&gt;
&lt;h1&gt;Story Illustration (Start-Up Mode):&lt;/h1&gt;
&lt;p&gt; Given a short story and a specific scene (or place in the text), produce an image that is representative of the scene. This project combines aspects of NLP and vision. This project may also explore generative adversarial methods. One well-known challenge here is convergence.&lt;/p&gt;
&lt;p&gt;Here are the general steps for this project:&lt;br /&gt;
1. Data collection (image captioning dataset can be helpful)&lt;br /&gt;
2. Model selection.&lt;br /&gt;
3. Model implementation (PyTorch).&lt;br /&gt;
4. Model testing.&lt;/p&gt;
&lt;p&gt;As an extension, one can also generate several frames to form a short “movie”. Another can be comic book pane generation.&lt;/p&gt;</description>
	<pubDate>Wed, 04 Apr 2018 06:53:35 +0000</pubDate>
</item>
<item>
	<title>Ananth Gottumukkala &lt;br/&gt; Team Turing Test: Top 3 Project Ideas I’m Excited For</title>
	<guid isPermaLink="false">http://deeplearningturingtest.wordpress.com/?p=3</guid>
	<link>https://deeplearningturingtest.wordpress.com/2018/04/04/the-journey-begins/</link>
	<description>&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Visualizing a text-based description:&lt;/strong&gt; Train a model to learn a language to image mapping with simple descriptions. The minimum plan would be to feed the model short text descriptions like “white hat” and “black cat” with their corresponding visual outputs. Then, if the text “white cat” is input at test time, the model should output the cat with the same shade of white as the hat. Stretch goals include visualizing multiple objects in the same picture and/or visualizing them in different spatial orientations with respect to each other (e.g. on top of, inside, underneath, next to, etc.).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generating Multimodal Word Embeddings:&lt;/strong&gt; The goal is to create word embeddings that describe a word more holistically from multiple modalities like audio and visual inputs. One possible approach is to concatenate pre-trained word embeddings (e.g. GloVe vector) with additional features based on what context the word is in the present sentence. Then, concatenate this with features generated from a deep fully connected layer of a ConvNet where the input is an image of the actual word (e.g. car). Stretch goals include using these augmented embeddings to enhance performance in applications like sentiment analysis or further augmenting these embeddings with audio features of the word being pronounced (which can help distinguish different meanings of the word).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dialogue and Information State Tracking:&lt;/strong&gt; The goal is to create a model that can either receive contextual information as text or probe the environment with questions and receive an answer as text. This text can either be input into a linear or tree LSTM for entity extraction, coreference resolution, parsing, and/or other algorithms which can extract valuable contextual information. Then, this text can be used to update the current dialogue and/or information state using deep reinforcement learning. The policy then uses the updated state to choose the next action and hopefully keep repeating this until the text is satisfactorily understood. Stretch goals include using this model to answer test questions about the reading material using a machine comprehension model like the ReasoNet architecture.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Git Repo URL: &lt;a href=&quot;https://gitlab.cs.washington.edu/ananthgo/cse481n-capstone&quot; rel=&quot;nofollow&quot;&gt;https://gitlab.cs.washington.edu/ananthgo/cse481n-capstone&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This project is in research mode with a heavy focus on making improvements in keeping track of the meaning behind lines of input text.&lt;/p&gt;</description>
	<pubDate>Wed, 04 Apr 2018 05:42:14 +0000</pubDate>
</item>
<item>
	<title>Tam Dang, Karishma Mandyam &lt;br/&gt; Team Illimitatum: A Discussion of Language Tasks for the NLP Capstone (Blog Post #1)</title>
	<guid isPermaLink="false">https://medium.com/p/93d82eed396c</guid>
	<link>https://medium.com/nlp-capstone-blog/a-discussion-of-language-tasks-for-the-nlp-capstone-blog-post-1-93d82eed396c?source=rss----9ba3897b6688---4</link>
	<description>&lt;p&gt;Welcome to Team Illimitatum! In this first blog post, we’ll discuss the top three/four ideas that emerged over several days of brainstorming. We started the week with seven exciting potential projects and carefully narrowed them down to these three ideas after taking into consideration feasibility, dataset availability, scope, and complexity for each project. Here they are!&lt;/p&gt;&lt;h3&gt;1. Machine Dictionary&lt;/h3&gt;&lt;h4&gt;Motivation &amp;amp; Context&lt;/h4&gt;&lt;p&gt;The breadth and ubiquity of data and online material is both a gift and a curse. Availability of resources ultimately leads to better-informed decisions, but puts an enormous strain on a researcher to&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Search for sources while having to determine which ones are credible and which ones are not&lt;/li&gt;&lt;li&gt;Read passages and excerpts of these sources&lt;/li&gt;&lt;li&gt;Reach an understanding that is consistent with all of the sources they have seen&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;On one hand, a concept or idea embedded in lengthy text (a textbook, article, or research paper, etc.), makes it so the reader has to achieve some basic understanding of the context surrounding the concept of interest as well as find all related details in order to learn the concept. On the other hand, basic web definitions and blogs &lt;em&gt;may&lt;/em&gt; provide a more presentable and explicit definition of a particular concept, but may be lacking in the amount detail a researcher is looking for along with confidence in its credibility.&lt;/p&gt;&lt;p&gt;Summarization is a task in NLP that aims to reduce passages to a more condense form without sacrificing semantic meaning. The trouble then, of needing to make sense of lengthy text can be alleviated by solving this task.&lt;/p&gt;&lt;p&gt;Entity recognition is another task in NLP where the goal is to highlight “entities” or figures of significance in text that conventially refer to nouns people, places, and organizations. In a medical context, entities could include chemical names, technical terms pertaining to the anatomy of the human body, and diseases.&lt;/p&gt;&lt;p&gt;We aim to combine aspects of both summarization and entity recognition to solve the three “maladies” of research we’ve outlined above: to produce a model that produces the most probable definition for a given entity that would be consistent with as much of the data as possible.&lt;/p&gt;&lt;p&gt;With the intention of training such a model only on publications and research papers, we hope to pave the way for “specialized dictionaries”. Training the data on all publications from a particular field of study would simultaneously restrict the search space to only credible and relevant sources for any definition contained. Providing a generated definition would also prevent the need for the researcher to hunt for it in every source and make sense of it, themselves.&lt;/p&gt;&lt;h4&gt;Minimum Viable Product&lt;/h4&gt;&lt;p&gt;Steps toward the realization of this model involve finding a &lt;strong&gt;dataset&lt;/strong&gt;, defining &lt;strong&gt;metrics of accuracy and performance&lt;/strong&gt;, and determining a reasonable &lt;strong&gt;baseline&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;Semantic Scholar’s &lt;a href=&quot;http://labs.semanticscholar.org/corpus/&quot;&gt;Open Research Corpus&lt;/a&gt; provides over 20 million publications in Computer Science, Neuroscience, and Medicine. While also providing the metadata for each publication, the dataset is primed for the task as it provides credible sources while opening up the possibility of testing the model in three different fields of study.&lt;/p&gt;&lt;p&gt;Metrics of accuracy and performance will be difficult to define. At the moment, we plan on using precision and recall of relevant terms found in Google, Oxford, and Webster definitions and whether they appear in a generated definition.&lt;/p&gt;&lt;p&gt;As for determining a baseline, we plan to implement a standard approach to the Question-Answering task, where the question is always “What is X?”. Ideally, our model should be able to provide more complete definitions with more tangential detail than a standard QA model.&lt;/p&gt;&lt;h4&gt;Stretch Goal: Ontology Matching&lt;/h4&gt;&lt;p&gt;If we were to succeed at the task, applications of our methodology could lead to advancement in both summarization and entity recognition. However, a working version of the model could also pave the way for a novel approach to Ontology Matching.&lt;/p&gt;&lt;p&gt;We could extend our model to solve Ontology Matching by first extracting all significant entities from the corpora. We could then assign each of these entities a definition through our model, and cluster terms based on the definitions accordingly. By doing so, we can in a sense eliminate entities that are linearly dependent with another, reducing the set of entities to one in which their definitions are unique and different enough to be stand-alone definitions.&lt;/p&gt;&lt;p&gt;Through this, we could achieve a new means of standardizing vocabulary in order to clean up knowledge bases or assist in the advent of a new field. Deep Learning in particular has experienced a lot of redundancy in notation and definitions and would benefit from this application.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;2. Natural Language Visual Reasoning&lt;/strong&gt;&lt;/h3&gt;&lt;h4&gt;&lt;strong&gt;Motivation &amp;amp; Context&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;Natural Language Visual Reasoning is a task which involves answering questions about an image. Extracting information from an image is an incredibly important but difficult task. Pictures provide a plethora of context about the world, especially about the relationships between objects. It’s also important to be able to extract knowledge about these relationships through natural language. If we can successfully reason about image content, we can change the way we train models and how they integrate with the physical world.&lt;/p&gt;&lt;p&gt;This task is also exciting because it combines Computer Vision and Natural Language Processing. While this goal is fairly lofty, researchers at Cornell and Facebook have collaborated to create a new visual reasoning language dataset (&lt;a href=&quot;http://lic.nlp.cornell.edu/nlvr/&quot;&gt;http://lic.nlp.cornell.edu/nlvr/&lt;/a&gt;), which is a simplified set of images (which contain shapes and colors of a certain format) and sentences. One of the major motivations of this project is the fact that this dataset is so vast and complete.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Minimum Viable Product&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The Visual Reasoning task can be described as follows. The below image represents one instance of training data.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/924/1*O1uEL-Ljs3OgfkVeTJZJuQ.png&quot; /&gt;Example training instance&lt;/figure&gt;&lt;p&gt;As you can see, each piece of data consists of a statement and an image of three boxes. If the statement is true of the image, then we output true, otherwise false. There are two viable courses of action for this problem. In the first approach, we can take the current best performing model (&lt;a href=&quot;https://arxiv.org/pdf/1511.02799.pdf&quot;&gt;Neural Module Networks&lt;/a&gt;) and improve the model to produce better results. In the second approach, we can start from scratch and try to develop a model that is uniquely trained for this dataset. It’s important to note that the Neural Module Network can be applied to any image, whereas we can train a model that can only be applied to images of this type.&lt;/p&gt;&lt;p&gt;Putting it all together, the end product should be able to take the sentence and image as input, extract all relevant information from the image into some intermediate form, validate the criteria described in the sentence for each image, and output the correct answer. We can start by coming up with a basic model which represents each image as a list of objects (yellow triangle at (a, b), blue square at (c, d), black circle at (e, f)) and finds a relationship between the sentence and the list of objects. A more effective model might try to represent the image in a different way, or utilize some of the techniques from the Neural Module Network paper.&lt;/p&gt;&lt;p&gt;The evaluation metrics for this project are very simple. The output of the model will always be true or false and we can do a simple accuracy test to determine the strength of our model.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Stretch Goals&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;A stretch goal for this project would be to generalize to different pictures of the same type. For example, what would happen if we changed the shapes and colors in the picture? What would happen if we changed the type of picture? Trying to achieve a high accuracy on different types of pictures would definitely be a stretch for this project.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;3. You Can Read This, Machines Can’t&lt;/strong&gt;&lt;/h3&gt;&lt;h4&gt;&lt;strong&gt;Motivation and Context&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The motivation for this project idea came from one of the datasets that Yonatan introduced to the class. Research has shown that humans are able to read the following sentence, but it turns out that computers are not able to do the same:&lt;/p&gt;&lt;blockquote&gt;“Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn’t mttaer in waht oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and lsat ltteer be at the rghit pclae”&lt;/blockquote&gt;&lt;p&gt;This is an interesting project because it explores how robust Machine Learning models can be to error (like misspellings). It also explores how much computers might be able to infer given surrounding words.&lt;/p&gt;&lt;p&gt;Another interesting aspect of this project is the available data. We can constantly generate new scrambled sentences with a simple algorithm.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Minimum Viable Product&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The goal of this task is to take a scrambled English sentence and produce the unscrambled version. In particular, each word in the English sentence will have the correct first and last letter but the letters in between might all be incorrect. There are several variations of this data. In the easiest version, the letters in the middle of each word are all correct but they are scrambled. A slightly more difficult challenge would be evaluating sentences where the letters in the middle of each word are semi-random and the number of letters may not correspond to the number of letters in the actual word.&lt;/p&gt;&lt;p&gt;In order to approach this problem, we can start with a very basic approach (for the easier version of the dataset) where we create a very efficient dictionary search algorithm. This approach would not use any Machine Learning and might be very slow. A better approach might be to train a very effective sequence to sequence model which maps a scrambled sentence to the correct sentence. Since we can mutate the data in many ways, we can train several different versions of the same sentence.&lt;/p&gt;&lt;p&gt;We could also explore a more effective way of training the model, rather than training on several mutations of the same sentence. One approach would be to represent words as a volume in N-Dimensional space, so a model would be able to recognize any data point within that volume as the original word. Another interesting approach would be to infer what the next word might be based on the most likely translation of the previous word. This would not only involve choosing a likely translation of the word, but also choosing the most likely translation of the sentence. A third approach might be to treat the the scrambled sentence as a language and use Machine Translation techniques to “translate” the sentence to English.&lt;/p&gt;&lt;p&gt;The performance metric for this would be the number of correctly predicted words in each sentence.&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Stretch Goals&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The stretch goal for this project would be to recognize any mutated sentence as the original sentence. This includes random letters, swapped letters, dropped letters, etc. We also could generate new datasets in different languages pretty easily. It might be interesting to see how our model performs on languages like Spanish or French.&lt;/p&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Questioning the immediate vs. long-term impact, along with the tractability and scope of each task, helped define our motivations while ensuring to the best of our knowledge, the viability of each potential project. We asked each other questions along the lines of “why is this task important?”,&lt;br /&gt;“if other people have done this before, how can we do it better?”, and “can it be done in 10 weeks?”.&lt;/p&gt;&lt;p&gt;Ultimately, we want our efforts and insights to have potentially long-term and significant consequences. Without consciously keeping track of whether an idea was research or industry/product oriented, we both came up with only research-oriented proposals. These proposals, if executed properly, would have impacts on related works, having fundamental and methodological influence through novel approaches and perspectives rather than aiding in the downstream application of any particular product. For the time being, we consider ourselves a research mode team.&lt;/p&gt;&lt;p&gt;Link to Code: &lt;a href=&quot;https://github.com/NLP-Capstone-Project&quot;&gt;https://github.com/NLP-Capstone-Project&lt;/a&gt;&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=93d82eed396c&quot; width=&quot;1&quot; /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nlp-capstone-blog/a-discussion-of-language-tasks-for-the-nlp-capstone-blog-post-1-93d82eed396c&quot;&gt;A Discussion of Language Tasks for the NLP Capstone (Blog Post #1)&lt;/a&gt; was originally published in &lt;a href=&quot;https://medium.com/nlp-capstone-blog&quot;&gt;NLP Capstone Blog&lt;/a&gt; on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</description>
	<pubDate>Wed, 04 Apr 2018 03:24:13 +0000</pubDate>
</item>
<item>
	<title>Belinda Li &lt;br/&gt; Team Sentimentity: NLP Capstone Blog #1</title>
	<guid isPermaLink="false">https://medium.com/p/f7412889d221</guid>
	<link>https://medium.com/@be.li.nda/nlp-capstone-blog-1-f7412889d221?source=rss-fad49d942bf3------2</link>
	<description>&lt;p&gt;Brainstorming project ideas for my CSE 481N NLP Capstone class.&lt;/p&gt;&lt;p&gt;The following project ideas are all research-oriented (research mode).&lt;/p&gt;&lt;h3&gt;Document Level Entity-Entity Sentiment Analysis&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Github URL&lt;/strong&gt;: &lt;a href=&quot;https://github.com/eunsol/document-e2e-sent&quot;&gt;https://github.com/eunsol/document-e2e-sent&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Given a document and named entities within the documents, we try to model the sentiment between the various entities in a document, categorizing the sentiments as “positive,” “negative,” or “neutral/no sentiment.” We focus especially on world news, which usually involve many named entities (i.e. countries, world leaders, etc.). For example, given such a document and the named entities in the document:&lt;/p&gt;&lt;blockquote&gt;Iran’s President &lt;strong&gt;Mahmoud Ahmadinejad&lt;/strong&gt; said &lt;strong&gt;Iran&lt;/strong&gt; opposes the “aggressive and arrogant policies” of the &lt;strong&gt;United States&lt;/strong&gt;, local satellite &lt;strong&gt;Press TV&lt;/strong&gt; reported Sunday.&lt;/blockquote&gt;&lt;blockquote&gt;“The &lt;strong&gt;Iranian nation&lt;/strong&gt; opposes the aggressive and arrogant policies of the &lt;strong&gt;U.S. government&lt;/strong&gt; and will stand against them forever,” &lt;strong&gt;Ahmadinejad&lt;/strong&gt; told a religious conference Saturday.&lt;/blockquote&gt;&lt;blockquote&gt;&lt;strong&gt;Iranian president&lt;/strong&gt; also urged the &lt;strong&gt;United States&lt;/strong&gt; and other western countries to change their attitude.&lt;/blockquote&gt;&lt;blockquote&gt;“You should change your attitude,” &lt;strong&gt;Ahmadinejad&lt;/strong&gt; said.&lt;/blockquote&gt;&lt;blockquote&gt;&lt;strong&gt;Iran&lt;/strong&gt; has constantly accused the &lt;strong&gt;United States&lt;/strong&gt; and western powers of earmarking funds to stir up protests against the country and its way of dealing with human rights.&lt;/blockquote&gt;&lt;p&gt;We should be able to generate the following set of relations between the &lt;strong&gt;bolded&lt;/strong&gt;, named entities:&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/290/1*ThoNzALuI1a78NOYImbGiw.png&quot; /&gt;Graphical representation of the annotations for the document above. Note that the &lt;strong&gt;red edges&lt;/strong&gt; represent &lt;strong&gt;negative&lt;/strong&gt; sentiment, &lt;strong&gt;green edges &lt;/strong&gt;represent &lt;strong&gt;positive&lt;/strong&gt; sentiment, &lt;strong&gt;light green edges&lt;/strong&gt; represent &lt;strong&gt;non-negative&lt;/strong&gt; sentiments, and &lt;strong&gt;light red edges&lt;/strong&gt; represent &lt;strong&gt;non-positive&lt;/strong&gt; sentiments.&lt;/figure&gt;&lt;h4&gt;Minimal Viable Action Plan&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Fortunately, I already have a pre-existing, labelled dataset I can work with (&lt;a href=&quot;https://homes.cs.washington.edu/~eunsol/project_page/acl16/index.html&quot;&gt;some examples here&lt;/a&gt;), so I don’t have worry about procuring data.&lt;/li&gt;&lt;li&gt;Implement and train a basic LSTM. This will serve as the baseline model for which to compare results.&lt;/li&gt;&lt;li&gt;Improve the LSTM with attention, word and sentiment embeddings, dropout, etc. Experiment with various hyper-parameter settings such as learning rate, dropout rate, batch sizes, etc. to maximize performance.&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Stretch Goals&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Implement and train a bi-affine relation attention network as delineated in &lt;a href=&quot;https://arxiv.org/pdf/1802.10569.pdf&quot;&gt;this paper&lt;/a&gt;, and adapt it for sentiment analysis between entities.&lt;/li&gt;&lt;li&gt;Experiment with new network architectures for the bi-affine network to hopefully improve results.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As I am 99.99% sure that I will be doing entity-entity sentiment analysis for this class, the two ideas below are just here to represent research directions that I’m generally interested in, and how I would approach working on them.&lt;/p&gt;&lt;h3&gt;Generating Conversational Responses&lt;/h3&gt;&lt;p&gt;Given an input sentence, generate an appropriate and believable sentence in response. As chat bots are on the rise, these sorts of problems are becoming increasingly pervasive.&lt;/p&gt;&lt;h4&gt;Minimal Viable Action Plan&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Procure data from some pre-existing dataset. &lt;a href=&quot;http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html&quot;&gt;This movie dialogue corpus&lt;/a&gt; could be a good source. Note though that movie dialogue can be written quite differently from everyday dialogue, and may vary depending on movie genre and setting.&lt;/li&gt;&lt;li&gt;Implement beam search as described in the lecture slides and in &lt;a href=&quot;https://geekyisawesome.blogspot.com/2016/10/using-beam-search-to-generate-most.html&quot;&gt;the referenced blog post&lt;/a&gt;. This will be used as the benchmark model to which the performance of other models will be compared to.&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Stretch Goals&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Implement the semantically-conditioned LSTM-based model as described in &lt;a href=&quot;https://arxiv.org/pdf/1508.01745.pdf&quot;&gt;the following paper&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Attempt to improve the model in some way.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Fake News/Misinformation Detection&lt;/h3&gt;&lt;p&gt;Given quotes from news articles, notable politicians, or online communities, categorize it as “true,” “mostly true,” “half true,” “mostly false,” “false,” or “pants on fire.”&lt;/p&gt;&lt;h4&gt;Minimal Viable Action Plan&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Politifact is a great data source, and an inspiration for this project.&lt;/li&gt;&lt;li&gt;Based off of &lt;a href=&quot;https://arxiv.org/pdf/1803.03786.pdf&quot;&gt;this paper&lt;/a&gt;, implement and train a model that combines a bidirectional attention-based LSTM with an SVM. In particular, the model works by feeding the output of the last hidden layer in the LSTM network into an SVM, which then outputs the label.&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;Stretch Goals&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Since the source paper applied the model to news articles, the model may or may not adapt well to quotes. It would be good to experiment with the architecture of the LSTM and/or determine whether the LSTM + SVM model actually performs better than just an LSTM or just an SVM.&lt;/li&gt;&lt;li&gt;Experiment with various combinations of LSTM and other machine learning models in an attempt to improve performance.&lt;/li&gt;&lt;/ul&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=f7412889d221&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Wed, 04 Apr 2018 00:34:38 +0000</pubDate>
</item>
<item>
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog Post 2: Warmup</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-5600014144802012716.post-4534461914269998368</guid>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/04/blog-post-2-warmup.html</link>
	<description>I'm working on my project in my fork of Allennlp. I got the dataset and have written the DatasetReader code to preprocess and index the dataset. I plan to add tests for this, and I need to add further preprocessing code such as splitting variables on camel casing.</description>
	<pubDate>Tue, 03 Apr 2018 21:53:00 +0000</pubDate>
	<author>noreply@blogger.com (nlpcapstone)</author>
</item>
<item>
	<title>Sarah Yu &lt;br/&gt; Team Jekyll-Hyde: Inaugural Blog Post</title>
	<guid isPermaLink="true">http://sarahyu.weebly.com/cse-481n/inaugural-blog-post</guid>
	<link>http://sarahyu.weebly.com/cse-481n/inaugural-blog-post</link>
	<description>&lt;div class=&quot;paragraph&quot;&gt;Welcome to the first blog post of &lt;u&gt;&lt;em&gt;Jekyll-Hyde&lt;/em&gt;&lt;/u&gt;&lt;em&gt; &lt;/em&gt;&lt;u&gt;(&lt;/u&gt;my very cool and somewhat related group-of-1 name).&lt;br /&gt;As the name might reveal, I’m interested in using NLP to uncover the duality of language, the ability to simultaneously present both sides of a coin, whether in everyday conversation or more curated prose. Because language matters; so much so that we mend and mold our language to navigate the different social environments and spaces we inhabit, whether for power, survival, or acceptance, and often in the most primitive and subconscious ways. And because in today’s (supposedly) civilized world, the language we employ with another can be a sort of proxy for the relationship we share. I'd like to get at some of these ideas through the NLP capstone and think the next three topics are a potential start. &lt;br /&gt;&lt;br /&gt;1) Language Accommodation (or my unlikely paper title: &lt;em&gt;Nice Guy by Day, A**hole by Night: Language Accommodation for Self-Presentation in Subreddit Communities)&lt;/em&gt;&lt;ul&gt;&lt;li&gt;​MVP: Scrape Reddit user data, identify language baselines for a given subreddit or capture linguistic differences of a single user across subreddits&lt;/li&gt;&lt;li&gt;Stretch Goals: Do both (subreddit baselines and user difference) and not only identify a user's language accommodation, but how they fall in line with the communities' baseline (a kind of hive mentality)&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;2) Identifying Condescension (another working title: &quot;&lt;em&gt;Well, Actually&quot;: Identifying Ambiguities in emails from 'helpful' colleagues&lt;/em&gt;)&lt;ul&gt;&lt;li&gt;​MVP: Identify an appropriate data source (ideally emails or more personal interactions), manually identify possible ambiguities, train model (maybe one that doesn't require a large dataset) to identify ambiguous spans of a sentence. &lt;/li&gt;&lt;li&gt;Stretch Goals: Begin identifying entity-entity-relationships with cues from ambiguous interactions or maybe something cooler about context and the different meanings if ambiguous...&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;3) Identifying Disrespect (might as well for consistency: &lt;em&gt;Linguistic (dis)R.E.S.P.E.C.T. - Addressing 90% of Comment Sections)&lt;/em&gt;&lt;ul&gt;&lt;li&gt;​MVP: Scrape Youtube comment data, classify comments as hateful/not hateful, train model on classified comments, test and tune&lt;/li&gt;&lt;li&gt;Stretch Goals: Train a portable model that can work with content from other mediums such as Twitter and Reddit&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;I'll be pursuing one of these ideas in &lt;strong&gt;research mode&lt;/strong&gt; and you can follow along at:&lt;br /&gt;                                      https://github.com/sarahyu17/481n&lt;/div&gt;  &lt;div class=&quot;wsite-spacer&quot; style=&quot;height: 50px;&quot;&gt;&lt;/div&gt;  &lt;div&gt; 				&lt;form action=&quot;http://www.weebly.com/weebly/apps/formSubmit.php&quot; enctype=&quot;multipart/form-data&quot; id=&quot;form-147197638403517045&quot; method=&quot;POST&quot;&gt; 					&lt;div class=&quot;wsite-form-container&quot; id=&quot;147197638403517045-form-parent&quot; style=&quot;margin-top: 10px;&quot;&gt; 						&lt;ul class=&quot;formlist&quot; id=&quot;147197638403517045-form-list&quot;&gt; 							&lt;h2 class=&quot;wsite-content-title&quot;&gt;Any Favorite Paper Titles?&lt;/h2&gt;  &lt;label class=&quot;wsite-form-label wsite-form-fields-required-label&quot;&gt;&lt;span class=&quot;form-required&quot;&gt;*&lt;/span&gt; Indicates required field&lt;/label&gt;&lt;div&gt;&lt;div class=&quot;wsite-form-field&quot; style=&quot;margin: 5px 0px 0px 0px;&quot;&gt;   &lt;label class=&quot;wsite-form-label&quot; for=&quot;input-789590629342031487&quot;&gt;Paper Title &lt;span class=&quot;form-required&quot;&gt;*&lt;/span&gt;&lt;/label&gt;   &lt;div class=&quot;wsite-form-radio-container&quot;&gt;     &lt;span class=&quot;form-radio-container&quot;&gt;&lt;input id=&quot;radio-0-_u789590629342031487&quot; name=&quot;_u789590629342031487&quot; type=&quot;radio&quot; value=&quot;#1&quot; /&gt;&lt;label for=&quot;radio-0-_u789590629342031487&quot;&gt;#1&lt;/label&gt;&lt;/span&gt;&lt;span class=&quot;form-radio-container&quot;&gt;&lt;input id=&quot;radio-1-_u789590629342031487&quot; name=&quot;_u789590629342031487&quot; type=&quot;radio&quot; value=&quot;#2&quot; /&gt;&lt;label for=&quot;radio-1-_u789590629342031487&quot;&gt;#2&lt;/label&gt;&lt;/span&gt;&lt;span class=&quot;form-radio-container&quot;&gt;&lt;input id=&quot;radio-2-_u789590629342031487&quot; name=&quot;_u789590629342031487&quot; type=&quot;radio&quot; value=&quot;#3&quot; /&gt;&lt;label for=&quot;radio-2-_u789590629342031487&quot;&gt;#3&lt;/label&gt;&lt;/span&gt;   &lt;/div&gt;   &lt;div class=&quot;wsite-form-instructions&quot; id=&quot;instructions-Paper Title&quot; style=&quot;display: none;&quot;&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 						&lt;/ul&gt; 					&lt;/div&gt; 					&lt;div style=&quot;display: none;&quot;&gt; 						&lt;input name=&quot;weebly_subject&quot; type=&quot;text&quot; /&gt; 					&lt;/div&gt; 					&lt;div style=&quot;text-align: left; margin-top: 10px; margin-bottom: 10px;&quot;&gt; 						&lt;input name=&quot;form_version&quot; type=&quot;hidden&quot; value=&quot;2&quot; /&gt; 						&lt;input id=&quot;weebly-approved&quot; name=&quot;weebly_approved&quot; type=&quot;hidden&quot; value=&quot;approved&quot; /&gt; 						&lt;input name=&quot;ucfid&quot; type=&quot;hidden&quot; value=&quot;147197638403517045&quot; /&gt; 						&lt;input name=&quot;recaptcha_token&quot; type=&quot;hidden&quot; /&gt; 						&lt;input name=&quot;opted_in&quot; type=&quot;hidden&quot; value=&quot;0&quot; /&gt; 						&lt;input type=&quot;submit&quot; /&gt; 						&lt;a class=&quot;wsite-button&quot;&gt; 							&lt;span class=&quot;wsite-button-inner&quot;&gt;vote&lt;/span&gt; 						&lt;/a&gt; 					&lt;/div&gt; 				&lt;/form&gt; 				&lt;div class=&quot;recaptcha&quot; id=&quot;g-recaptcha-147197638403517045&quot;&gt;&lt;/div&gt; 			  			&lt;/div&gt;</description>
	<pubDate>Tue, 03 Apr 2018 07:00:00 +0000</pubDate>
</item>
<item>
	<title>Zichun Liu, Ning Hong, Sujie Zhou &lt;br/&gt; Team The Bugless: NLP</title>
	<guid isPermaLink="false">https://medium.com/p/5258ddd9eedd</guid>
	<link>https://medium.com/@hongnin1/nlp-5258ddd9eedd?source=rss-c450eb982161------2</link>
	<description>&lt;p&gt;This is a blog about Natural Language Processing.&lt;/p&gt;&lt;p&gt;Group member: Ning Hong, Zichun Liu, Zhou Sujie&lt;/p&gt;&lt;p&gt;Team name: The Bugless&lt;/p&gt;&lt;p&gt;Three topics our team is considering doing (All start-up mode):&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Movie summarization (imdb)/Food review summarization (yelp)&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;minimal viable action plan: scrap data from twitter/imdb/rotten tomato/yelp and train our model to be able to summarize the reviews for movies (or food if we are scrapping data from yelp). The summarization of the movie is how the audience feel about the movie in general, for example, given a movie title, our model should be able to produce something like this: It is violent but good.&lt;/p&gt;&lt;p&gt;stretch goals: Output a overall review for the movie instead of simple sentences, for example, given a movie title as input, our model should output: “The Terror got my full attention from beginning to end. I couldn’t turn away. I didn’t want to turn away. For me, that’s extremely rare.”&lt;/p&gt;&lt;p&gt;Another stretch goal is to be able to detect sentiment not only in the US market, but also in China market by using data from DouBan (one of the largest movie review site for China), and compare the sentiment between US and China for a certain movie.&lt;/p&gt;&lt;p&gt;2.Chinese Phoneticization mapping:&lt;/p&gt;&lt;p&gt;The way input Chinese to machine is by typing Pinyin, a kind of phoneticization for Chinese sentence and words, which almost every boy in China know about. However, one Chinese character may have many phoneticization can one Chinese phoneticization sequence may map to many Chinese sentences. When typing Chinese to a machine by Pinyin, the machine will rank the potential Chinese sentences by preferences. However, the ranking may not be so consistent to the context. Therefore, we want to generate a language model that map from Pinyin (English character) to Chinese words and sentences depend on context and speaking habit of this person. In addition, this can be potentially turned into an online learn algorithm.&lt;/p&gt;&lt;p&gt;First step: finding data, where input is Chinese phoneticization (pinyin) and output is Chinese sentences and words. Also, we need to find a good algorithm to do that, and onlint learning algorithm will be better.&lt;/p&gt;&lt;p&gt;Ideas come from Neural Input Method Engine of last quarter: &lt;a href=&quot;https://www.dropbox.com/sh/z3idncggfpwm8rs/AAAnHVvHIPvt_CxTXsaDVqvda?dl=0&amp;amp;preview=teamverynatural_3417269_43042970_Very+Natural+Final+Report.pdf&quot;&gt;https://www.dropbox.com/sh/z3idncggfpwm8rs/AAAnHVvHIPvt_CxTXsaDVqvda?dl=0&amp;amp;preview=teamverynatural_3417269_43042970_Very+Natural+Final+Report.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;3. Flirt tutor:&lt;/p&gt;&lt;p&gt;When you chat with your beloved boy/girl, you must have some hard time picking interesting and flirting word to response. Then let the machine teach you! Specifically, we want to build a model that can generate cute response giving the context of chatting. This model is neural based.&lt;/p&gt;&lt;p&gt;First step: figuring out what is the right dataset and find such. Meanwhile, consider the right model to use and find paper/resources about is.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=5258ddd9eedd&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 03 Apr 2018 02:48:17 +0000</pubDate>
</item>
<item>
	<title>Ryan Pham &lt;br/&gt; Team NeuralEmpty: Capstone Ideas</title>
	<guid isPermaLink="false">https://medium.com/p/ab3d796c422e</guid>
	<link>https://medium.com/@ryanp97/project-ideas-ab3d796c422e?source=rss-6378d85d3a9b------2</link>
	<description>&lt;p&gt;I plan on following a research track and hope to pursue one of the following ideas:&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;Neural Machine Translation with Semantic Transfer&lt;/strong&gt; (as outlined by Jan Buys)&lt;/h4&gt;&lt;p&gt;&lt;em&gt;Minimal Viable Action Plan:&lt;/em&gt; &lt;br /&gt;1) Use statistical parser (&lt;a href=&quot;http://sweaglesw.org/linguistics/ace/&quot;&gt;ACE&lt;/a&gt;) to get MRS graphs of English/Japanese sentences and convert to DMRS graph (using &lt;a href=&quot;https://github.com/delph-in/pydelphin&quot;&gt;PyDelphin&lt;/a&gt; interface to do parsing and conversion)&lt;br /&gt;2) Linearize DMRS graph&lt;br /&gt;3) Train a seq2seq that takes linearized English DMRS graph and outputs linearized Japanese DMRS graph&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*J3l_TjWr3A-jOAcrCznqvA.png&quot; /&gt;Example of penmen format that DMRS can be represented with (non-linearized). Note that the representation shown is an AMR graph, not a DMRS graph. Figure taken from this &lt;a href=&quot;https://arxiv.org/pdf/1704.08381.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Stretch Goals:&lt;br /&gt;&lt;/em&gt;1) Explore different architectures for semantic transfer (e.g. TreeLSTM as opposed to seq2seq)&lt;br /&gt;2) Explore ways to learn correspondences between semantic concepts in the two languages&lt;/p&gt;&lt;h4&gt;DMRS to Text Generation (as outlined by Jan Buys)&lt;/h4&gt;&lt;p&gt;&lt;em&gt;Minimal Viable Action Plan:&lt;br /&gt;&lt;/em&gt;1) Generate DMRS graph serialization similar to what was outlined above&lt;br /&gt;2) Train seq2seq model to generate text directly from graph serialization&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/1024/1*Bndfdhs3ixwG6-JUQPC72A.png&quot; /&gt;Example of different graph representations. Figure taken from this &lt;a href=&quot;http://www.lrec-conf.org/proceedings/lrec2016/pdf/634_Paper.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Stretch Goals:&lt;/em&gt;&lt;br /&gt;1) Experiment with different seq2seq architectures&lt;br /&gt;2) Attempt semi-supervised training using a high-precision grammar-based parser&lt;/p&gt;&lt;h4&gt;Reproduce results / expand on a Paper&lt;/h4&gt;&lt;p&gt;In this option, I would be attempting to expand on this &lt;a href=&quot;https://arxiv.org/pdf/1704.04859.pdf&quot;&gt;paper&lt;/a&gt; regarding hybrid models. The paper attempts to alleviate and/or solve the issue of out-of-vocabulary words and characters, specifically in Chinese and Japanese. It describes using a CNN in conjunction with an RNN to do so; the CNN learns the radicals of characters and attempts to choose characters with similar radicals since the meaning of radicals remains constant between characters.&lt;/p&gt;&lt;figure&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn-images-1.medium.com/max/822/1*5GyInYVxc8ifP_YRCHiHHA.png&quot; /&gt;Image taken from the linked paper.&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Minimal Viable Action Plan:&lt;br /&gt;&lt;/em&gt;1) Obtain/create a visual dataset of the characters in order to train the CNN&lt;br /&gt;2) Design and explore different methods for joining the two models such as described in the paper&lt;br /&gt;3) Train the model end-to-end&lt;/p&gt;&lt;p&gt;&lt;em&gt;Stretch Goals:&lt;br /&gt;&lt;/em&gt;1) Error analysis on different methods for joining and potential points of error&lt;br /&gt;2) Exploration of model variations and architecture (incremental changes similar to this &lt;a href=&quot;https://arxiv.org/pdf/1708.04755.pdf&quot;&gt;paper&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;For current progress, visit the &lt;a href=&quot;https://github.com/ryanp97/NeuralEmpty&quot;&gt;repo&lt;/a&gt;.&lt;/p&gt;&lt;img height=&quot;1&quot; src=&quot;https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=ab3d796c422e&quot; width=&quot;1&quot; /&gt;</description>
	<pubDate>Tue, 03 Apr 2018 00:55:49 +0000</pubDate>
</item>
<item>
	<title>Boyan Li, Dennis Orzikh, Lanhao Wu &lt;br/&gt; Team Watch Your Language!: First Blog Post!</title>
	<guid isPermaLink="false">http://cse481n-capstone.azurewebsites.net/?p=15</guid>
	<link>http://cse481n-capstone.azurewebsites.net/2018/04/02/first-blog-post/</link>
	<description>&lt;h3&gt;&lt;strong&gt;Team Name: &lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Team Watch Your Language!&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Three Project Ideas:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Offensive Text Recognition&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our minimal viable plan is to create two models, one which determines if text is offensive or not, and another which determines if any particular group is targeted by the text, such as a racial, religious, or political grouping. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our stretch goal is to use these models to make a third model which can use the first two outputs as assumptions to then provide human-readable explanations as to why that particular text was labeled the way it was. This way we can determine if text is offensive or not and provide reasons for that labeling. This can be used to assist in teaching conversational agents common sense about what to say.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Domain-Specific Conversational Agent&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our minimal viable plan is to create a conversational agent which can provide information and hold a conversation with a well-intentioned user about a particular domain. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;The stretch goal here would be to just continually make it better at conversing, at least in the particular domain it is trained to be good at talking about.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Image Description&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;Our minimal viable plan is to create a model that can describe a simple image correctly, like generating a sentence describing the spatial relation between a box and a cylinder.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;For the stretch goal, we would like to improve our model that can describe a unique pattern of an image among 3 (or multiple) other images. For example, if we have 3 images A, B, and C, we would like to come up with a model to generate a sentence that describes image A but not image B or C.&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;GitLab Repo:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt; &lt;/span&gt;&lt;a href=&quot;https://gitlab.cs.washington.edu/danielby/nlp-capstone&quot;&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;https://gitlab.cs.washington.edu/danielby/nlp-capstone&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Mode:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;We will be tackling the Offensive Text Recognition task in&lt;/span&gt; &lt;em&gt;research mode&lt;/em&gt;&lt;span style=&quot;font-weight: 400;&quot;&gt;!&lt;/span&gt;&lt;/p&gt;</description>
	<pubDate>Mon, 02 Apr 2018 06:58:42 +0000</pubDate>
</item>
<item>
	<title>Ron Fan, Aditya Saraf &lt;br/&gt; Team PrimeapeNLP: Blog Post #1</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-3753031463594823927.post-6307882466820480344</guid>
	<link>https://cse481n.blogspot.com/2018/04/blog-post-1.html</link>
	<description>&lt;div style=&quot;font-family: Verdana; text-align: left;&quot;&gt;    &lt;div&gt;Temporary GitHub URL: &lt;a href=&quot;https://github.com/rococode/primeapeNLP&quot; target=&quot;_blank&quot;&gt;https://github.com/rococode/primeapeNLP&lt;/a&gt;&lt;/div&gt;    &lt;div&gt;We plan to be in “research mode” for this capstone.&lt;/div&gt;    &lt;div style=&quot;font-size: 1.1rem;&quot;&gt;Three possible ideas:&lt;/div&gt;     &lt;div style=&quot;font-size: 1.1rem;&quot;&gt;1. New evaluation methods for text generation models&lt;/div&gt;    &lt;div&gt;There don’t seem to be many good ways to evaluate the output of machine generated text (such as in the problem of creating hotel reviews). We want our models to generate text that’s indistinguishable from human text, but it’s hard to quantify how similar generated text is to human text, so it’s hard to see when progress is being made. However, it feels like the problem of actually generating human-like text should be harder than the problem of just checking if that text seems human. As a baseline approach, we would like to explore building a regression model for the “humanness” of a piece of unfamiliar text.&lt;/div&gt;    &lt;div&gt;&lt;strong&gt;        M&lt;span style=&quot;font-size: 0.7rem; font-weight: normal;&quot;&gt;(inimal)&lt;/span&gt;        V&lt;span style=&quot;font-size: 0.7rem; font-weight: normal;&quot;&gt;(iable)&lt;/span&gt;        P&lt;span style=&quot;font-size: 0.7rem; font-weight: normal;&quot;&gt;(lan)&lt;/span&gt;    :&lt;/strong&gt;&lt;/div&gt;    &lt;ol&gt;        &lt;li&gt;Build a reasonably large dataset using a combination of web scraping, existing datasets, and existing generative models&lt;/li&gt;        &lt;li&gt;Build a regression model with decent performance on training set&lt;/li&gt;        &lt;li&gt;Build a regression model with decent performance on development and test sets&lt;/li&gt;    &lt;/ol&gt;    &lt;div&gt;&lt;strong&gt;Stretch goals:&lt;/strong&gt;&lt;/div&gt;    &lt;ol&gt;        &lt;li&gt;Use our model to score various generative models and compare our model’s rankings for these generative models to commonly agreed-upon rankings by researchers&lt;/li&gt;        &lt;li&gt;Integrate model scores into a “generate a lot of possibilities, then search for the best one” approach for text generation&lt;/li&gt;    &lt;/ol&gt;    &lt;div&gt;We would also explore non-neural methods for evaluating Natural Language Generation (NLG). One idea would be to compare the probability distribution of generated sentences to real sentences. A common problem is that generated text repeats the most probable sentences over and over. Thus, the distribution of sentences is front-loaded. We hypothesize that generated text with a sentence distribution that closely mirrors real text would be more difficult to distinguish from real text. However, we first need to investigate what real world sentence distributions look like. It may be likely that individual sentences are not likely to repeat - in that case, the distribution would be more-or-less uniform across all sentences. We may need to come up with a novel method to categorize similar sentences. We would have to decide whether to use semantic similarity or syntactic similarity.&lt;/div&gt;    &lt;div&gt;&lt;strong&gt;MVP:&lt;/strong&gt;&lt;/div&gt;    &lt;ol&gt;        &lt;li&gt;Gather a data set - perhaps the data set Ari showed us for hotel reviews.&lt;/li&gt;        &lt;li&gt;Examine the existing probability distribution at the sentence-level in the corpus. If the distribution is too uniform, design a method to place similar sentences in the same “bucket” and re-compute the probability distribution.&lt;/li&gt;        &lt;li&gt;Design metrics to compute the similarity of two distributions; if a model is “good”, the distribution of the generated sentences will match the distribution of the training corpus.&lt;/li&gt;        &lt;li&gt;Use our metric to score generated text models found in the literature.&lt;/li&gt;    &lt;/ol&gt;    &lt;div&gt;&lt;strong&gt;Stretch goals:&lt;/strong&gt;&lt;/div&gt;    &lt;ul&gt;        &lt;li&gt;Use insights from our metrics to improve on current approaches to NLG.&lt;/li&gt;    &lt;/ul&gt;    &lt;div style=&quot;font-size: 1.1rem;&quot;&gt;2. Single-document summarization&lt;/div&gt;    &lt;div&gt;Single document summarization (SDS) models typically label each sentence in the document as in the summary or not in the summary. The problem then becomes a binary classification problem, and many people train a NN with supervised learning. However, there are other non-neural approaches to this problem that have been tried successfully. One paper shows how SDS can be thought of as a tree based Knapsack problem, which is then solved by a Integer Linear Programming (ILP) solver (see: &lt;a href=&quot;https://www.semanticscholar.org/paper/Single-Document-Summarization-as-a-Tree-Knapsack-Hirao-Yoshida/ed0c8a7ab911cdb30b7e95edada3a55c01eb22c5&quot;&gt;https://www.semanticscholar.org/paper/Single-Document-Summarization-as-a-Tree-Knapsack-Hirao-Yoshida/ed0c8a7ab911cdb30b7e95edada3a55c01eb22c5&lt;/a&gt;). We would like to explore both neural models and non-neural approaches to SDS. &lt;/div&gt;    &lt;div&gt;&lt;strong&gt;MVP:&lt;/strong&gt;&lt;/div&gt;    &lt;ol&gt;        &lt;li&gt;As a baseline approach, build neural models from current papers that solve SDS. Evaluate the performance using F1 as well as ROUGE metrics (see: &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;&gt;https://en.wikipedia.org/wiki/ROUGE_(metric)&lt;/a&gt;).&lt;/li&gt;        &lt;li&gt;Build neural models from current papers that focus on non-neural approaches, such as the combinatorial approach described above.&lt;/li&gt;        &lt;li&gt;Evaluate neural and non-neural approaches: what do these approaches have in common? What key insights are they leveraging? Are there any generalizations of the problem that can be extracted? Hopefully, this study will allow us to either improve the SOTA neural models or fine tune some non-neural approach.&lt;/li&gt;    &lt;/ol&gt;    &lt;div&gt;&lt;strong&gt;Stretch goal:&lt;/strong&gt;&lt;/div&gt;    &lt;ul&gt;        &lt;li&gt;Use insights from our studies to formulate and solve SDS in a unique way. Evaluate our approach using F1 and ROUGE metrics.&lt;/li&gt;    &lt;/ul&gt;    &lt;div style=&quot;font-size: 1.1rem;&quot;&gt;3. Multi-span comprehension&lt;/div&gt;    &lt;div&gt;Reading comprehension models generally operate by extracting an answer to a question by outputting a start and end index on the original passage. This is not a very human-like way of answering questions, and makes it impossible to generate good answers to some simple factual questions using common sentence structures. We would like to explore ways of answering questions from passages without being limited to a single span from the passage text.&lt;/div&gt;    &lt;div&gt;&lt;strong&gt;MVP:&lt;/strong&gt;&lt;/div&gt;    &lt;ol&gt;        &lt;li&gt;Implement a baseline model that performs near state of the art levels on the SQuAD dataset.&lt;/li&gt;        &lt;li&gt;Build a dataset that requires information from multiple spans to answer the questions well. We will likely create this dataset by hand.&lt;/li&gt;        &lt;li&gt;Build a model that answers questions about a passage by generating multiple spans. We envision designing our model to output a sequence of indices such that every pair of indices corresponds to one part of the answer. This model should perform almost as well as the baseline model on SQuAD, since that’s just a specific case (one span) of a multi-span answer.&lt;/li&gt;        &lt;li&gt;Build upon the previous model by using the output spans to generate a formal answer through a generative model, using the output spans and the question sentence as inputs.&lt;/li&gt;    &lt;/ol&gt;    &lt;div&gt;&lt;strong&gt;Stretch goal:&lt;/strong&gt;&lt;/div&gt;    &lt;ul&gt;        &lt;li&gt;Optimize model to actually be effective at answering multispan questions. We expect this to be quite difficult, so while building a functioning model is part of the MVP, actually having it perform comparatively well is a stretch goal.&lt;/li&gt;    &lt;/ul&gt;&lt;/div&gt;</description>
	<pubDate>Mon, 02 Apr 2018 04:32:00 +0000</pubDate>
	<author>noreply@blogger.com (Ron &amp; Aditya)</author>
</item>
<item>
	<title>Rajas Agashe &lt;br/&gt; Team Han Flying Solo: Blog Post 1: Project Ideas</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-5600014144802012716.post-6768023392170538237</guid>
	<link>https://nlpcapstonesemparse.blogspot.com/2018/03/blog-post-1.html</link>
	<description>&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt;I am interested in working in the code generation/semantic parsing space on the research track. My code &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt;will be in various branches of my fork of allennlp (https://github.com/rajasagashe/allennlp). I will keep you&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt; updated on which branch/commits I worked on during each blog post. Also note that project idea 1 has &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;span style=&quot;font-size: 14.6667px; white-space: pre;&quot;&gt;the most detail since I have picked it as my project!&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;h2 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;&quot;&gt;&lt;span&gt;Project Idea 1&lt;/span&gt;&lt;/h2&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Minimum Viable Plan:&lt;/span&gt;&lt;span&gt; Implement the model in the recent UW paper which introduces the task of &lt;/span&gt;&lt;br /&gt;&lt;span&gt;generating the code for a java function from a natural language description. To further aid code &lt;/span&gt;&lt;br /&gt;&lt;span&gt;generation, the class in which the generated function is to reside is provided, i.e. the class variables &lt;/span&gt;&lt;br /&gt;&lt;span&gt;and methods. Thus the encoder encodes the class as well as the utterance and the decoder uses a &lt;/span&gt;&lt;br /&gt;&lt;span&gt;two step attention mechanism and decodes through the java grammar production rules. &lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Stretch Goals:&lt;/span&gt;&lt;span&gt; Reproduce the state of the art results in the paper. I’m putting this in the stretch goals &lt;/span&gt;&lt;br /&gt;&lt;span&gt;since successfully implementing a neural semantic parser with type constraints is pretty challenging. &lt;/span&gt;&lt;br /&gt;&lt;span&gt;In addition, I hope to experiment with other improvements like encoding the entire class method body&lt;/span&gt;&lt;br /&gt;&lt;span&gt; which wasn’t done.&lt;/span&gt;&lt;/div&gt;&lt;h2 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;&quot;&gt;&lt;span&gt;Project Idea 2&lt;/span&gt;&lt;/h2&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Minimum Viable Plan: &lt;/span&gt;&lt;span&gt;Implement this paper: &lt;/span&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.01696.pdf&quot; style=&quot;text-decoration: none;&quot;&gt;&lt;span&gt;https://arxiv.org/pdf/1704.01696.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;. The model is &lt;/span&gt;&lt;br /&gt;&lt;span&gt;similar to that of the previous idea, but the datasets are for python and ifttt instead.&lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Stretch Goals: &lt;/span&gt;&lt;span&gt;Improve the paper’s result.&lt;/span&gt;&lt;/div&gt;&lt;h2 dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;&quot;&gt;&lt;span&gt;Project Idea 3&lt;/span&gt;&lt;/h2&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Minimum Viable Plan: &lt;/span&gt;&lt;span&gt;Perform transfer learning across several code generation tasks by using the &lt;/span&gt;&lt;br /&gt;&lt;span&gt;same encoder for them all. This technique would be similar to what was used in the Cove paper &lt;/span&gt;&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.00107.pdf&quot; style=&quot;text-decoration: none;&quot;&gt;&lt;span&gt;https://arxiv.org/pdf/1708.00107.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;. &lt;/span&gt;&lt;/div&gt;&lt;div dir=&quot;ltr&quot; style=&quot;line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;&quot;&gt;&lt;span&gt;Stretch Goals: &lt;/span&gt;&lt;span&gt;Improve the individual paper results with this technique.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;</description>
	<pubDate>Fri, 30 Mar 2018 22:41:00 +0000</pubDate>
	<author>noreply@blogger.com (nlpcapstone)</author>
</item>
<item>
	<title>Kuikui Liu &lt;br/&gt; Team INLP: Brunn-Minkowski Implies Uniform Measures on Convex Sets are Logconcave</title>
	<guid isPermaLink="false">http://mathstoc.wordpress.com/?p=265</guid>
	<link>https://mathstoc.wordpress.com/2018/01/12/brunn-minkowski-implies-uniform-measures-on-convex-sets-are-logconcave/</link>
	<description>&lt;p&gt;Recently, I’ve been reading about logconcave measures, which is a set of measures contain the uniform measures over convex sets. We like analyzing them as a “generalization of convex sets” because their densities are additionally closed under convolution, unlike the indicators of convex sets. Thus, they are quite useful in convex geometry. TCS researchers study them in pursuit of fast algorithms for sampling points from convex bodies, estimating integrals, and optimization.&lt;/p&gt;
&lt;p&gt;Let’s start with a definition.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;Definition (Logconcave Measure):&lt;/b&gt;&lt;em&gt; A measure &lt;img alt=&quot;\mu&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmu&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;\mu&quot; /&gt; on &lt;img alt=&quot;\mathbb{R}^{n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7Bn%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mathbb{R}^{n}&quot; /&gt; is &lt;strong&gt;logconcave&lt;/strong&gt; if for every measurable &lt;img alt=&quot;A,B \subset \mathbb{R}^{n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=A%2CB+%5Csubset+%5Cmathbb%7BR%7D%5E%7Bn%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;A,B \subset \mathbb{R}^{n}&quot; /&gt; and &lt;img alt=&quot;0 &amp;lt; \lambda &amp;lt; 1&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=0+%3C+%5Clambda+%3C+1&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;0 &amp;lt; \lambda &amp;lt; 1&quot; /&gt; with &lt;img alt=&quot;\lambda A + (1 - \lambda)B&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Clambda+A+%2B+%281+-+%5Clambda%29B&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\lambda A + (1 - \lambda)B&quot; /&gt; measurable, we have &lt;img alt=&quot;\mu(\lambda A + (1 - \lambda)B) \geq \mu(A)^{\lambda}\mu(B)^{1-\lambda}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmu%28%5Clambda+A+%2B+%281+-+%5Clambda%29B%29+%5Cgeq+%5Cmu%28A%29%5E%7B%5Clambda%7D%5Cmu%28B%29%5E%7B1-%5Clambda%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mu(\lambda A + (1 - \lambda)B) \geq \mu(A)^{\lambda}\mu(B)^{1-\lambda}&quot; /&gt;, where &lt;img alt=&quot;\lambda A + (1 - \lambda)B = \{\lambda a + (1 - \lambda)b : a \in A, b \in B\}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Clambda+A+%2B+%281+-+%5Clambda%29B+%3D+%5C%7B%5Clambda+a+%2B+%281+-+%5Clambda%29b+%3A+a+%5Cin+A%2C+b+%5Cin+B%5C%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\lambda A + (1 - \lambda)B = \{\lambda a + (1 - \lambda)b : a \in A, b \in B\}&quot; /&gt; is the &lt;strong&gt;Minkowski sum&lt;/strong&gt; of &lt;img alt=&quot;\lambda A = \{\lambda a : a \in A\}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Clambda+A+%3D+%5C%7B%5Clambda+a+%3A+a+%5Cin+A%5C%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\lambda A = \{\lambda a : a \in A\}&quot; /&gt; and &lt;img alt=&quot;(1 - \lambda)B = \{(1 - \lambda)b : b \in B\}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%281+-+%5Clambda%29B+%3D+%5C%7B%281+-+%5Clambda%29b+%3A+b+%5Cin+B%5C%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;(1 - \lambda)B = \{(1 - \lambda)b : b \in B\}&quot; /&gt;.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For the purposes of this post, we’ll ignore issues of compactness, etc. just to make the presentation clean.&lt;/p&gt;
&lt;p&gt;A typical introduction into the theory here begins with the &lt;strong&gt;Brunn-Minkowski Inequality&lt;/strong&gt;, which simply states that &lt;img alt=&quot;m(A + B)^{1/n} \geq m(A)^{1/n} + m(B)^{1/n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=m%28A+%2B+B%29%5E%7B1%2Fn%7D+%5Cgeq+m%28A%29%5E%7B1%2Fn%7D+%2B+m%28B%29%5E%7B1%2Fn%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;m(A + B)^{1/n} \geq m(A)^{1/n} + m(B)^{1/n}&quot; /&gt; where the volume is with respect to Lebesgue measure &lt;img alt=&quot;m&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=m&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;m&quot; /&gt; on &lt;img alt=&quot;\mathbb{R}^{n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7Bn%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mathbb{R}^{n}&quot; /&gt;. This initially confused me, as it wasn’t clear how this implies that the Lebesgue measure (and more generally, the uniform measure on a convex set) is logconcave, until I discovered a quick proof of the implication using the (generalized) AMGM inequality. Let’s recall what it states.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;Generalized Arithmetic-Mean Geometric-Mean Inequality (AMGM):&lt;/b&gt;&lt;em&gt; Let &lt;img alt=&quot;\alpha_{1},\dots,\alpha_{n} \geq 0&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Calpha_%7B1%7D%2C%5Cdots%2C%5Calpha_%7Bn%7D+%5Cgeq+0&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\alpha_{1},\dots,\alpha_{n} \geq 0&quot; /&gt; be nonnegative numbers and &lt;img alt=&quot;\lambda_{1},\dots,\lambda_{n} \geq 0&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Clambda_%7B1%7D%2C%5Cdots%2C%5Clambda_%7Bn%7D+%5Cgeq+0&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\lambda_{1},\dots,\lambda_{n} \geq 0&quot; /&gt; be weights such that &lt;img alt=&quot;\sum_{i=1}^{n} \lambda_{i} = 1&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Clambda_%7Bi%7D+%3D+1&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\sum_{i=1}^{n} \lambda_{i} = 1&quot; /&gt;. Then &lt;img alt=&quot;\sum_{i=1}^{n} \lambda_{i}\alpha_{i} \geq \prod_{i=1}^{n} \alpha_{i}^{\lambda_{i}}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Clambda_%7Bi%7D%5Calpha_%7Bi%7D+%5Cgeq+%5Cprod_%7Bi%3D1%7D%5E%7Bn%7D+%5Calpha_%7Bi%7D%5E%7B%5Clambda_%7Bi%7D%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\sum_{i=1}^{n} \lambda_{i}\alpha_{i} \geq \prod_{i=1}^{n} \alpha_{i}^{\lambda_{i}}&quot; /&gt;, with the convention that &lt;img alt=&quot;\alpha_{i}^{\lambda_{i}} = 1&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Calpha_%7Bi%7D%5E%7B%5Clambda_%7Bi%7D%7D+%3D+1&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\alpha_{i}^{\lambda_{i}} = 1&quot; /&gt; for any &lt;img alt=&quot;i&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=i&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;i&quot; /&gt; with &lt;img alt=&quot;\alpha_{i} = \lambda_{i} = 0&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Calpha_%7Bi%7D+%3D+%5Clambda_%7Bi%7D+%3D+0&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\alpha_{i} = \lambda_{i} = 0&quot; /&gt;.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Proof of this inequality is a simple application of Jensen’s Inequality and convexity of &lt;img alt=&quot;-\log(x)&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=-%5Clog%28x%29&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;-\log(x)&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;We will also need to know about how the Minkowski sum interacts with intersection with a convex set.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;Lemma:&lt;/b&gt;&lt;em&gt; Let &lt;img alt=&quot;A,B \subset \mathbb{R}^{n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=A%2CB+%5Csubset+%5Cmathbb%7BR%7D%5E%7Bn%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;A,B \subset \mathbb{R}^{n}&quot; /&gt; be measurable and &lt;img alt=&quot;0 &amp;lt; \lambda &amp;lt; 1&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=0+%3C+%5Clambda+%3C+1&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;0 &amp;lt; \lambda &amp;lt; 1&quot; /&gt; such that &lt;img alt=&quot;\lambda A + (1 - \lambda)B&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Clambda+A+%2B+%281+-+%5Clambda%29B&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\lambda A + (1 - \lambda)B&quot; /&gt; is measurable. Let &lt;img alt=&quot;K&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=K&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;K&quot; /&gt; be a convex set (which is automatically convex). Then &lt;img alt=&quot;(\lambda A + (1 - \lambda)B) \cap K \supset \lambda (A \cap K) + (1 - \lambda)(B \cap K)&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%28%5Clambda+A+%2B+%281+-+%5Clambda%29B%29+%5Ccap+K+%5Csupset+%5Clambda+%28A+%5Ccap+K%29+%2B+%281+-+%5Clambda%29%28B+%5Ccap+K%29&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;(\lambda A + (1 - \lambda)B) \cap K \supset \lambda (A \cap K) + (1 - \lambda)(B \cap K)&quot; /&gt;.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Let &lt;img alt=&quot;z \in \lambda(A \cap K) + (1 - \lambda)(B \cap K)&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=z+%5Cin+%5Clambda%28A+%5Ccap+K%29+%2B+%281+-+%5Clambda%29%28B+%5Ccap+K%29&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;z \in \lambda(A \cap K) + (1 - \lambda)(B \cap K)&quot; /&gt;. By definition, we may write &lt;img alt=&quot;z = \lambda x + (1 - \lambda)y&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=z+%3D+%5Clambda+x+%2B+%281+-+%5Clambda%29y&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;z = \lambda x + (1 - \lambda)y&quot; /&gt; for some &lt;img alt=&quot;x \in A \cap K, y \in B \cap K&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=x+%5Cin+A+%5Ccap+K%2C+y+%5Cin+B+%5Ccap+K&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;x \in A \cap K, y \in B \cap K&quot; /&gt;. Since &lt;img alt=&quot;x \in A, y \in B&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=x+%5Cin+A%2C+y+%5Cin+B&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;x \in A, y \in B&quot; /&gt;, &lt;img alt=&quot;z \in \lambda A + (1 -\lambda)B&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=z+%5Cin+%5Clambda+A+%2B+%281+-%5Clambda%29B&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;z \in \lambda A + (1 -\lambda)B&quot; /&gt;. Finally, since &lt;img alt=&quot;x \in K, y \in K&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=x+%5Cin+K%2C+y+%5Cin+K&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;x \in K, y \in K&quot; /&gt; and &lt;img alt=&quot;K&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=K&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;K&quot; /&gt; is convex, &lt;img alt=&quot;z \in K&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=z+%5Cin+K&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;z \in K&quot; /&gt;. Thus, &lt;img alt=&quot;z \in (\lambda A + (1 - \lambda)B) \cap K&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=z+%5Cin+%28%5Clambda+A+%2B+%281+-+%5Clambda%29B%29+%5Ccap+K&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;z \in (\lambda A + (1 - \lambda)B) \cap K&quot; /&gt; as desired. &lt;img alt=&quot;\Box&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5CBox&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\Box&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Note that this containment is strict in general, even in one dimension. For example, one can take &lt;img alt=&quot;A = \{0\}, B = \{2\}, \lambda = 1/2&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=A+%3D+%5C%7B0%5C%7D%2C+B+%3D+%5C%7B2%5C%7D%2C+%5Clambda+%3D+1%2F2&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;A = \{0\}, B = \{2\}, \lambda = 1/2&quot; /&gt; so that &lt;img alt=&quot;\lambda A = \{0\}, (1 - \lambda)B = \{1\}, \lambda A + (1 - \lambda)B = \{1\}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Clambda+A+%3D+%5C%7B0%5C%7D%2C+%281+-+%5Clambda%29B+%3D+%5C%7B1%5C%7D%2C+%5Clambda+A+%2B+%281+-+%5Clambda%29B+%3D+%5C%7B1%5C%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\lambda A = \{0\}, (1 - \lambda)B = \{1\}, \lambda A + (1 - \lambda)B = \{1\}&quot; /&gt;. If &lt;img alt=&quot;K = [1/2,3/2]&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=K+%3D+%5B1%2F2%2C3%2F2%5D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;K = [1/2,3/2]&quot; /&gt;, then &lt;img alt=&quot;(\lambda A + (1 - \lambda)B) \cap K = \{1\}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%28%5Clambda+A+%2B+%281+-+%5Clambda%29B%29+%5Ccap+K+%3D+%5C%7B1%5C%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;(\lambda A + (1 - \lambda)B) \cap K = \{1\}&quot; /&gt; but &lt;img alt=&quot;A \cap K = B \cap K = \emptyset&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=A+%5Ccap+K+%3D+B+%5Ccap+K+%3D+%5Cemptyset&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;A \cap K = B \cap K = \emptyset&quot; /&gt; so that &lt;img alt=&quot;\lambda (A \cap K) + (1 - \lambda)(B \cap K) = \emptyset&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Clambda+%28A+%5Ccap+K%29+%2B+%281+-+%5Clambda%29%28B+%5Ccap+K%29+%3D+%5Cemptyset&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\lambda (A \cap K) + (1 - \lambda)(B \cap K) = \emptyset&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Now, on to the implication.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;Corollary (of the Brunn-Minkowski Inequality):&lt;/b&gt;&lt;em&gt; The uniform measure w.r.t. a convex set &lt;img alt=&quot;K&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=K&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;K&quot; /&gt;, defined as &lt;img alt=&quot;\mu_{K}(S) = m(K \cap S) = \int_{S} \chi_{K} \,dm&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmu_%7BK%7D%28S%29+%3D+m%28K+%5Ccap+S%29+%3D+%5Cint_%7BS%7D+%5Cchi_%7BK%7D+%5C%2Cdm&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mu_{K}(S) = m(K \cap S) = \int_{S} \chi_{K} \,dm&quot; /&gt;, is logconcave.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Let &lt;img alt=&quot;A,B \subset \mathbb{R}^{n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=A%2CB+%5Csubset+%5Cmathbb%7BR%7D%5E%7Bn%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;A,B \subset \mathbb{R}^{n}&quot; /&gt; be measurable and &lt;img alt=&quot;0 &amp;lt; \lambda &amp;lt; 1&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=0+%3C+%5Clambda+%3C+1&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;0 &amp;lt; \lambda &amp;lt; 1&quot; /&gt; with &lt;img alt=&quot;\lambda A + (1 - \lambda)B&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Clambda+A+%2B+%281+-+%5Clambda%29B&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\lambda A + (1 - \lambda)B&quot; /&gt; measurable. The preceding lemma shows that&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img alt=&quot;\begin{aligned}\mu_{K}(\lambda A + (1 -  \lambda)B)^{1/n} = m(K \cap (\lambda A + (1 - \lambda) B))^{1/n} \geq m(\lambda(A \cap K) + (1 - \lambda)(B \cap K))^{1/n}\end{aligned}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cmu_%7BK%7D%28%5Clambda+A+%2B+%281+-++%5Clambda%29B%29%5E%7B1%2Fn%7D+%3D+m%28K+%5Ccap+%28%5Clambda+A+%2B+%281+-+%5Clambda%29+B%29%29%5E%7B1%2Fn%7D+%5Cgeq+m%28%5Clambda%28A+%5Ccap+K%29+%2B+%281+-+%5Clambda%29%28B+%5Ccap+K%29%29%5E%7B1%2Fn%7D%5Cend%7Baligned%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\begin{aligned}\mu_{K}(\lambda A + (1 -  \lambda)B)^{1/n} = m(K \cap (\lambda A + (1 - \lambda) B))^{1/n} \geq m(\lambda(A \cap K) + (1 - \lambda)(B \cap K))^{1/n}\end{aligned}&quot; /&gt;.&lt;/p&gt;
&lt;p&gt; Now, applying the Brunn-Minkowski Inequality, we have a lower bound of&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img alt=&quot;\begin{aligned}m(\lambda(A \cap K))^{1/n} + m((1 -  \lambda)(B \cap K))^{1/n} = \lambda m(A \cap K)^{1/n} + (1 -  \lambda)m(B \cap K)^{1/n} = \lambda \mu_{K}(A)^{1/n} + (1 -  \lambda)\mu_{K}(B)^{1/n}\end{aligned}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7Dm%28%5Clambda%28A+%5Ccap+K%29%29%5E%7B1%2Fn%7D+%2B+m%28%281+-++%5Clambda%29%28B+%5Ccap+K%29%29%5E%7B1%2Fn%7D+%3D+%5Clambda+m%28A+%5Ccap+K%29%5E%7B1%2Fn%7D+%2B+%281+-++%5Clambda%29m%28B+%5Ccap+K%29%5E%7B1%2Fn%7D+%3D+%5Clambda+%5Cmu_%7BK%7D%28A%29%5E%7B1%2Fn%7D+%2B+%281+-++%5Clambda%29%5Cmu_%7BK%7D%28B%29%5E%7B1%2Fn%7D%5Cend%7Baligned%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\begin{aligned}m(\lambda(A \cap K))^{1/n} + m((1 -  \lambda)(B \cap K))^{1/n} = \lambda m(A \cap K)^{1/n} + (1 -  \lambda)m(B \cap K)^{1/n} = \lambda \mu_{K}(A)^{1/n} + (1 -  \lambda)\mu_{K}(B)^{1/n}\end{aligned}&quot; /&gt;.&lt;/p&gt;
&lt;p&gt; Applying AMGM with &lt;img alt=&quot;\alpha_{1} = \mu_{K}(A)^{1/n}, \alpha_{2} = \mu_{K}(B)^{1/n}, \lambda_{1}=\lambda, \lambda_{2}=1 - \lambda&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Calpha_%7B1%7D+%3D+%5Cmu_%7BK%7D%28A%29%5E%7B1%2Fn%7D%2C+%5Calpha_%7B2%7D+%3D+%5Cmu_%7BK%7D%28B%29%5E%7B1%2Fn%7D%2C+%5Clambda_%7B1%7D%3D%5Clambda%2C+%5Clambda_%7B2%7D%3D1+-+%5Clambda&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\alpha_{1} = \mu_{K}(A)^{1/n}, \alpha_{2} = \mu_{K}(B)^{1/n}, \lambda_{1}=\lambda, \lambda_{2}=1 - \lambda&quot; /&gt;, we obtain a final lower bound of &lt;img alt=&quot;\mu_{K}(A)^{\lambda/n}\mu_{K}(B)^{(1-\lambda)/n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmu_%7BK%7D%28A%29%5E%7B%5Clambda%2Fn%7D%5Cmu_%7BK%7D%28B%29%5E%7B%281-%5Clambda%29%2Fn%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mu_{K}(A)^{\lambda/n}\mu_{K}(B)^{(1-\lambda)/n}&quot; /&gt;. Raising both sides of the inequality to the &lt;img alt=&quot;n&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=n&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;n&quot; /&gt;th power gives the result. &lt;img alt=&quot;\Box&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5CBox&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\Box&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Note that in this, all we really needed about the Lebesgue measure is that it is &lt;strong&gt;positive homogeneous of degree &lt;img alt=&quot;n&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=n&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;n&quot; /&gt;&lt;/strong&gt;, i.e. that &lt;img alt=&quot;m(\lambda A) = \lambda^{n}m(A)&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=m%28%5Clambda+A%29+%3D+%5Clambda%5E%7Bn%7Dm%28A%29&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;m(\lambda A) = \lambda^{n}m(A)&quot; /&gt;. Perhaps I am missing some other technical aspects of the Lebesgue measure that are necessary for the proof to go through.&lt;/p&gt;
&lt;p&gt;UPDATE (01/17/2018): It turns out the converse is true as well, i.e. logconcavity of the Lebesgue measure implies the Brunn-Minkowski Inequality. We refer the reader to Corollary 5.3 from &lt;a href=&quot;http://faculty.wwu.edu/gardner/gorizia12.pdf&quot;&gt;Gardner’s survey titled “The Brunn-Minkowski Inequality”&lt;/a&gt; for the proof, which just rescales &lt;img alt=&quot;A,B&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=A%2CB&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;A,B&quot; /&gt; to have unit volume and then makes a clever choice for the parameter &lt;img alt=&quot;\lambda&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Clambda&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\lambda&quot; /&gt;.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;Lemma:&lt;/b&gt;&lt;em&gt; Suppose for all measurable &lt;img alt=&quot;A,B&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=A%2CB&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;A,B&quot; /&gt;, and &lt;img alt=&quot;0 &amp;lt; \lambda &amp;lt; 1&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=0+%3C+%5Clambda+%3C+1&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;0 &amp;lt; \lambda &amp;lt; 1&quot; /&gt; such that &lt;img alt=&quot;\lambda A + (1 - \lambda)B&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Clambda+A+%2B+%281+-+%5Clambda%29B&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\lambda A + (1 - \lambda)B&quot; /&gt; is measurable,&lt;br /&gt;
 that &lt;img alt=&quot;m(\lambda A + (1 - \lambda)B) \geq m(A)^{\lambda}m(B)^{1-\lambda}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=m%28%5Clambda+A+%2B+%281+-+%5Clambda%29B%29+%5Cgeq+m%28A%29%5E%7B%5Clambda%7Dm%28B%29%5E%7B1-%5Clambda%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;m(\lambda A + (1 - \lambda)B) \geq m(A)^{\lambda}m(B)^{1-\lambda}&quot; /&gt;.&lt;br /&gt;
 Then the Brunn-Minkowski Inequality holds.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;</description>
	<pubDate>Fri, 12 Jan 2018 06:25:34 +0000</pubDate>
</item>
<item>
	<title>Kuikui Liu &lt;br/&gt; Team INLP: Gershgorin’s Circle Theorem</title>
	<guid isPermaLink="false">http://mathstoc.wordpress.com/?p=182</guid>
	<link>https://mathstoc.wordpress.com/2017/08/26/gershgorins-circle-theorem/</link>
	<description>&lt;p&gt;Here we present briefly a quick way to obtain crude bounds on eigenvalues of complex square matrices.&lt;/p&gt;
&lt;p&gt;Fix a matrix &lt;img alt=&quot;A \in \mathbb{C}^{n \times n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=A+%5Cin+%5Cmathbb%7BC%7D%5E%7Bn+%5Ctimes+n%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;A \in \mathbb{C}^{n \times n}&quot; /&gt; with eigenvalue-eigenvector pairs &lt;img alt=&quot;(\lambda_{1},v_{1}),\dots,(\lambda_{n},v_{n})&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%28%5Clambda_%7B1%7D%2Cv_%7B1%7D%29%2C%5Cdots%2C%28%5Clambda_%7Bn%7D%2Cv_%7Bn%7D%29&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;(\lambda_{1},v_{1}),\dots,(\lambda_{n},v_{n})&quot; /&gt;. For each &lt;img alt=&quot;i = 1,\dots,n &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=i+%3D+1%2C%5Cdots%2Cn+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;i = 1,\dots,n &quot; /&gt;, define &lt;img alt=&quot;R_{i} \neq \sum_{j \neq i} |A_{ij}|&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=R_%7Bi%7D+%5Cneq+%5Csum_%7Bj+%5Cneq+i%7D+%7CA_%7Bij%7D%7C&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;R_{i} \neq \sum_{j \neq i} |A_{ij}|&quot; /&gt;. Finally, define the &lt;b&gt;Gershgorin discs&lt;/b&gt; to be the closed discs &lt;img alt=&quot;\mathbb{D}(a_{ii}, R_{i})  = \{z \in \mathbb{C} : |z - a_{ii}| \leq R_{i}\} \subset \mathbb{C}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbb%7BD%7D%28a_%7Bii%7D%2C+R_%7Bi%7D%29+%C2%A0%3D+%5C%7Bz+%5Cin+%5Cmathbb%7BC%7D+%3A+%7Cz+-+a_%7Bii%7D%7C+%5Cleq+R_%7Bi%7D%5C%7D+%5Csubset+%5Cmathbb%7BC%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mathbb{D}(a_{ii}, R_{i})  = \{z \in \mathbb{C} : |z - a_{ii}| \leq R_{i}\} \subset \mathbb{C}&quot; /&gt;.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;Gershgorin’s Circle Theorem:&lt;/b&gt; &lt;em&gt; For every &lt;img alt=&quot;i = 1,\dots,n&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=i+%3D+1%2C%5Cdots%2Cn&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;i = 1,\dots,n&quot; /&gt;, &lt;img alt=&quot;\lambda_{i} \in \mathbb{D}(a_{jj}, R_{j})&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Clambda_%7Bi%7D+%5Cin+%5Cmathbb%7BD%7D%28a_%7Bjj%7D%2C+R_%7Bj%7D%29&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\lambda_{i} \in \mathbb{D}(a_{jj}, R_{j})&quot; /&gt; for some &lt;img alt=&quot;j=1,\dots,n&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=j%3D1%2C%5Cdots%2Cn&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;j=1,\dots,n&quot; /&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Let &lt;img alt=&quot;\lambda &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Clambda+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\lambda &quot; /&gt; be an eigenvalue of &lt;img alt=&quot;A&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=A&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;A&quot; /&gt; with corresponding eigenvector &lt;img alt=&quot;v &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=v+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;v &quot; /&gt;. Let us renormalize &lt;img alt=&quot;v&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=v&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;v&quot; /&gt; by &lt;img alt=&quot;1/v_{k} &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=1%2Fv_%7Bk%7D+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;1/v_{k} &quot; /&gt; where &lt;img alt=&quot;k&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=k&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;k&quot; /&gt; is the index &lt;img alt=&quot;i &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=i+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;i &quot; /&gt; maximizing &lt;img alt=&quot;|v_{i}| &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Cv_%7Bi%7D%7C+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;|v_{i}| &quot; /&gt;. Thus &lt;img alt=&quot;\max_{i} |v_{i}| = 1 &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmax_%7Bi%7D+%7Cv_%7Bi%7D%7C+%3D+1+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\max_{i} |v_{i}| = 1 &quot; /&gt; and &lt;img alt=&quot;v_{k} = 1 &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=v_%7Bk%7D+%3D+1+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;v_{k} = 1 &quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Now, by definition, &lt;img alt=&quot;Ax = \lambda x &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=Ax+%3D+%5Clambda+x+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;Ax = \lambda x &quot; /&gt;. In particular, for the &lt;img alt=&quot;k &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=k+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;k &quot; /&gt;th coordinate, &lt;img alt=&quot;a_{kk} + \sum_{j \neq k} A_{kj} v_{j} = a_{kk}v_{k} + \sum_{j \neq k} A_{kj}v_{j} = \sum_{j} A_{kj}v_{j} = \lambda v_{k} = \lambda &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=a_%7Bkk%7D+%2B+%5Csum_%7Bj+%5Cneq+k%7D+A_%7Bkj%7D+v_%7Bj%7D+%3D+a_%7Bkk%7Dv_%7Bk%7D+%2B+%5Csum_%7Bj+%5Cneq+k%7D+A_%7Bkj%7Dv_%7Bj%7D+%3D+%5Csum_%7Bj%7D+A_%7Bkj%7Dv_%7Bj%7D+%3D+%5Clambda+v_%7Bk%7D+%3D+%5Clambda+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;a_{kk} + \sum_{j \neq k} A_{kj} v_{j} = a_{kk}v_{k} + \sum_{j \neq k} A_{kj}v_{j} = \sum_{j} A_{kj}v_{j} = \lambda v_{k} = \lambda &quot; /&gt;. Finally, by the Triangle Inequality, &lt;img alt=&quot;|\lambda - a_{kk}| = \left|\sum_{j \neq k} A_{kj} v_{j} \right|\leq \sum_{j \neq k} |A_{kj}| \cdot |v_{j}| \leq \sum_{j \neq k} |A_{kj}| = R_{k} &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7C%5Clambda+-+a_%7Bkk%7D%7C+%3D+%5Cleft%7C%5Csum_%7Bj+%5Cneq+k%7D+A_%7Bkj%7D+v_%7Bj%7D+%5Cright%7C%5Cleq+%5Csum_%7Bj+%5Cneq+k%7D+%7CA_%7Bkj%7D%7C+%5Ccdot+%7Cv_%7Bj%7D%7C+%5Cleq+%5Csum_%7Bj+%5Cneq+k%7D+%7CA_%7Bkj%7D%7C+%3D+R_%7Bk%7D+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;|\lambda - a_{kk}| = \left|\sum_{j \neq k} A_{kj} v_{j} \right|\leq \sum_{j \neq k} |A_{kj}| \cdot |v_{j}| \leq \sum_{j \neq k} |A_{kj}| = R_{k} &quot; /&gt;. &lt;img alt=&quot;\Box&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5CBox&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\Box&quot; /&gt;&lt;/p&gt;</description>
	<pubDate>Sat, 26 Aug 2017 17:51:26 +0000</pubDate>
</item>
<item>
	<title>Kuikui Liu &lt;br/&gt; Team INLP: GAN Idea Introduction</title>
	<guid isPermaLink="false">http://mathstoc.wordpress.com/?p=148</guid>
	<link>https://mathstoc.wordpress.com/2017/07/22/gan-training/</link>
	<description>&lt;p&gt;Disclaimer: I am no expert on deep learning or even machine learning.&lt;/p&gt;
&lt;p&gt;The hottest new technique in deep learning is one that combines neural networks with ideas from algorithmic game theory: the &lt;em&gt;generative adversarial network (GAN).&lt;/em&gt; It is a technique &lt;a href=&quot;https://arxiv.org/abs/1406.2661&quot;&gt;proposed by Goodfellow et. al.&lt;/a&gt; in 2014 and one that Yann LeCun, the Director of Facebook AI, heralds as&lt;/p&gt;
&lt;blockquote&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;span style=&quot;color: #000000;&quot;&gt;“…the most interesting idea in the last 10 years in machine learning…”&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(This is the &lt;img alt=&quot;n&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=n&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;n&quot; /&gt;th utterance of any variation of this quote, for some large and unknown &lt;img alt=&quot;n&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=n&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;n&quot; /&gt;.)&lt;/p&gt;
&lt;p&gt;Before we proceed, here is a good high-level &lt;a href=&quot;https://blog.openai.com/generative-models/&quot;&gt;blog post by OpenAI&lt;/a&gt; on GANs. What I like most about it is how it refers to other recent techniques in generative models such as variational autoencoders and what they’re doing to improve these models. I also highly recommend the original &lt;a href=&quot;https://arxiv.org/abs/1406.2661&quot;&gt;Goodfellow et. al.&lt;/a&gt; paper; it is very readable.&lt;/p&gt;
&lt;p&gt;Let’s begin with a concrete goal: We want a machine to learn how to generate realistic images of, say, dogs (general photorealistic image generation has essentially been the greatest success for GANs thus far).&lt;/p&gt;
&lt;p&gt;One can model this problem as drawing samples from some fixed, true, unknown probability distribution &lt;img alt=&quot;p_{r}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=p_%7Br%7D&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;p_{r}&quot; /&gt; (in our example, the probability distribution over all possible images, with a higher density value for an image indicating a higher likelihood of it being an image of a dog).&lt;/p&gt;
&lt;p&gt;The set up of a GAN is as follows: we train not one but two neural networks &lt;img alt=&quot;G&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=G&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;G&quot; /&gt; and &lt;img alt=&quot;D&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=D&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;D&quot; /&gt;, canonically referred to as the &lt;em&gt;generator&lt;/em&gt; and the &lt;em&gt;discriminator&lt;/em&gt; respectively. The goal of the generator is to generate samples that “look” like they come from &lt;img alt=&quot;p_{r}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=p_%7Br%7D&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;p_{r}&quot; /&gt;, i.e. generate samples according to some probability distribution &lt;img alt=&quot;p_{g}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=p_%7Bg%7D&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;p_{g}&quot; /&gt; that is “close” to &lt;img alt=&quot;p_{r}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=p_%7Br%7D&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;p_{r}&quot; /&gt; in some sense (generate realistic images of dogs). The goal of the discriminator is to determine correctly whether a given sample is drawn from &lt;img alt=&quot;p_{r}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=p_%7Br%7D&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;p_{r}&quot; /&gt; or from &lt;img alt=&quot;p_{g}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=p_%7Bg%7D&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;p_{g}&quot; /&gt; (distinguish between real and generated images of dogs).&lt;/p&gt;</description>
	<pubDate>Sat, 22 Jul 2017 17:45:57 +0000</pubDate>
</item>
<item>
	<title>Kuikui Liu &lt;br/&gt; Team INLP: Irrational Power of an Irrational</title>
	<guid isPermaLink="false">http://mathstoc.wordpress.com/?p=125</guid>
	<link>https://mathstoc.wordpress.com/2017/07/16/irrational-power-of-an-irrational/</link>
	<description>&lt;p&gt;This was just a neat proof I came across somewhere. I’m definitely not clever enough to come up with this myself.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;Lemma:&lt;/b&gt; &lt;em&gt; There exist positive irrational numbers &lt;img alt=&quot;x,y&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=x%2Cy&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;x,y&quot; /&gt; such that &lt;img alt=&quot;x^{y}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=x%5E%7By%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;x^{y}&quot; /&gt; is rational.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Assume &lt;img alt=&quot;\sqrt{2}^{\sqrt{2}}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Csqrt%7B2%7D%5E%7B%5Csqrt%7B2%7D%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\sqrt{2}^{\sqrt{2}}&quot; /&gt; is rational. Then we are done by taking &lt;img alt=&quot;x = y = \sqrt{2}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=x+%3D+y+%3D+%5Csqrt%7B2%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;x = y = \sqrt{2}&quot; /&gt;. Otherwise, assume &lt;img alt=&quot;\sqrt{2}^{\sqrt{2}}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Csqrt%7B2%7D%5E%7B%5Csqrt%7B2%7D%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\sqrt{2}^{\sqrt{2}}&quot; /&gt; is irrational. Then, taking &lt;img alt=&quot;x = \sqrt{2}^{\sqrt{2}}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=x+%3D+%5Csqrt%7B2%7D%5E%7B%5Csqrt%7B2%7D%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;x = \sqrt{2}^{\sqrt{2}}&quot; /&gt; and &lt;img alt=&quot;y = \sqrt{2}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=y+%3D+%5Csqrt%7B2%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;y = \sqrt{2}&quot; /&gt;, we have &lt;img alt=&quot;x^{y} = (\sqrt{2}^{\sqrt{2}})^{\sqrt{2}} = \sqrt{2}^{\sqrt{2} \cdot \sqrt{2}} = \sqrt{2}^{2} = 2&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=x%5E%7By%7D+%3D+%28%5Csqrt%7B2%7D%5E%7B%5Csqrt%7B2%7D%7D%29%5E%7B%5Csqrt%7B2%7D%7D+%3D+%5Csqrt%7B2%7D%5E%7B%5Csqrt%7B2%7D+%5Ccdot+%5Csqrt%7B2%7D%7D+%3D+%5Csqrt%7B2%7D%5E%7B2%7D+%3D+2&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;x^{y} = (\sqrt{2}^{\sqrt{2}})^{\sqrt{2}} = \sqrt{2}^{\sqrt{2} \cdot \sqrt{2}} = \sqrt{2}^{2} = 2&quot; /&gt;. &lt;img alt=&quot;\Box&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5CBox&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\Box&quot; /&gt;&lt;br /&gt;
Still at the end of this proof, I have no clue whether &lt;img alt=&quot;\sqrt{2}^{\sqrt{2}}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Csqrt%7B2%7D%5E%7B%5Csqrt%7B2%7D%7D&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;\sqrt{2}^{\sqrt{2}}&quot; /&gt; is rational or irrational! A bit more Googling revealed &lt;a href=&quot;https://math.stackexchange.com/questions/446647/irrationality-of-sqrt2-sqrt2&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;</description>
	<pubDate>Sun, 16 Jul 2017 23:53:32 +0000</pubDate>
</item>
<item>
	<title>Kuikui Liu &lt;br/&gt; Team INLP: A Neat Problem: Number Balancing</title>
	<guid isPermaLink="false">http://mathstoc.wordpress.com/?p=14</guid>
	<link>https://mathstoc.wordpress.com/2017/07/14/a-cute-problem-that-is-np-hard/</link>
	<description>&lt;p&gt;Let &lt;img alt=&quot;{a_{1},\dots,a_{n} \in [0,1]}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Ba_%7B1%7D%2C%5Cdots%2Ca_%7Bn%7D+%5Cin+%5B0%2C1%5D%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;{a_{1},\dots,a_{n} \in [0,1]}&quot; /&gt; be numbers.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;The Number Balancing Problem:&lt;/b&gt; &lt;em&gt; Select disjoint subsets &lt;img alt=&quot;I_{1},I_{2} \subset \{1,\dots,n\}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=I_%7B1%7D%2CI_%7B2%7D+%5Csubset+%5C%7B1%2C%5Cdots%2Cn%5C%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;I_{1},I_{2} \subset \{1,\dots,n\}&quot; /&gt;, not both empty, such that &lt;img alt=&quot;\sum_{k \in I_{1}} a_{k}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Csum_%7Bk+%5Cin+I_%7B1%7D%7D+a_%7Bk%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\sum_{k \in I_{1}} a_{k}&quot; /&gt; is as close to &lt;img alt=&quot;\sum_{k \in I_{2}} a_{k}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Csum_%7Bk+%5Cin+I_%7B2%7D%7D+a_%7Bk%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\sum_{k \in I_{2}} a_{k}&quot; /&gt; as possible. Alternatively, we can phrase the problem as finding a nonzero &lt;img alt=&quot;\mathbf{x} \in \{-1,0,1\}^{n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D+%5Cin+%5C%7B-1%2C0%2C1%5C%7D%5E%7Bn%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mathbf{x} \in \{-1,0,1\}^{n}&quot; /&gt; minimizing &lt;img alt=&quot;\left|\sum_{k=1}^{n} a_{k}x_{k}\right|&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Csum_%7Bk%3D1%7D%5E%7Bn%7D+a_%7Bk%7Dx_%7Bk%7D%5Cright%7C&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\left|\sum_{k=1}^{n} a_{k}x_{k}\right|&quot; /&gt;.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;A very simple but cool application of the Pigeonhole Principle shows us there always exists a nonzero &lt;img alt=&quot;\mathbf{x} \in \{-1,0,1\}^{n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D+%5Cin+%5C%7B-1%2C0%2C1%5C%7D%5E%7Bn%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mathbf{x} \in \{-1,0,1\}^{n}&quot; /&gt; such that &lt;img alt=&quot;\left|\sum_{k=1}^{n} a_{k}x_{k}\right|&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Csum_%7Bk%3D1%7D%5E%7Bn%7D+a_%7Bk%7Dx_%7Bk%7D%5Cright%7C&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\left|\sum_{k=1}^{n} a_{k}x_{k}\right|&quot; /&gt; is exponentially small in &lt;img alt=&quot;n&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=n&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;n&quot; /&gt; in &lt;img alt=&quot;n&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=n&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;n&quot; /&gt;. Moreover, the bound is completely independent of the choice of &lt;img alt=&quot;a_{1},\dots,a_{n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=a_%7B1%7D%2C%5Cdots%2Ca_%7Bn%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;a_{1},\dots,a_{n}&quot; /&gt;!&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;Lemma 1:&lt;/b&gt; &lt;em&gt; There exists a nonzero &lt;img alt=&quot;\mathbf{x} \in \{-1,0,1\}^{n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D+%5Cin+%5C%7B-1%2C0%2C1%5C%7D%5E%7Bn%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mathbf{x} \in \{-1,0,1\}^{n}&quot; /&gt; such that &lt;img alt=&quot;\left|\sum_{k=1}^{n} a_{k}x_{k}\right|\leq \frac{n}{2^{n} - 1}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Csum_%7Bk%3D1%7D%5E%7Bn%7D+a_%7Bk%7Dx_%7Bk%7D%5Cright%7C%5Cleq+%5Cfrac%7Bn%7D%7B2%5E%7Bn%7D+-+1%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\left|\sum_{k=1}^{n} a_{k}x_{k}\right|\leq \frac{n}{2^{n} - 1}&quot; /&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Observe that &lt;img alt=&quot;\sum_{k=1}^{n} a_{k}x_{k} \in [0,n]&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Csum_%7Bk%3D1%7D%5E%7Bn%7D+a_%7Bk%7Dx_%7Bk%7D+%5Cin+%5B0%2Cn%5D&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;\sum_{k=1}^{n} a_{k}x_{k} \in [0,n]&quot; /&gt; for all choices of &lt;img alt=&quot;\mathbf{x} \in \{0,1\}^{n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D+%5Cin+%5C%7B0%2C1%5C%7D%5E%7Bn%7D&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;\mathbf{x} \in \{0,1\}^{n}&quot; /&gt;. Partition &lt;img alt=&quot;{[0,n]}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5B0%2Cn%5D%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;{[0,n]}&quot; /&gt; into &lt;img alt=&quot;{2^{n} - 1}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%7D+-+1%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;{2^{n} - 1}&quot; /&gt; intervals, each of length &lt;img alt=&quot;{n / (2^{n} - 1)}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7Bn+%2F+%282%5E%7Bn%7D+-+1%29%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;{n / (2^{n} - 1)}&quot; /&gt;. Now, there are &lt;img alt=&quot;{2^{n}}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%7D%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;{2^{n}}&quot; /&gt; distinct points in &lt;img alt=&quot;{\{0,1\}^{n}}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5E%7Bn%7D%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;{\{0,1\}^{n}}&quot; /&gt; so by the Pigeonhole Principle, there exists distinct &lt;img alt=&quot;{\mathbf{x}, \mathbf{x'} \in \{0,1\}^{n}}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7Bx%7D%2C+%5Cmathbf%7Bx%27%7D+%5Cin+%5C%7B0%2C1%5C%7D%5E%7Bn%7D%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;{\mathbf{x}, \mathbf{x'} \in \{0,1\}^{n}}&quot; /&gt; such that &lt;img alt=&quot;{\sum_{k=1}^{n} a_{k}x_{k}}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D+a_%7Bk%7Dx_%7Bk%7D%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;{\sum_{k=1}^{n} a_{k}x_{k}}&quot; /&gt; and &lt;img alt=&quot;{\sum_{k=1}^{n} a_{k}x_{k}'}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D+a_%7Bk%7Dx_%7Bk%7D%27%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;{\sum_{k=1}^{n} a_{k}x_{k}'}&quot; /&gt; lie within the same interval of the partition. In particular,&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img alt=&quot;\begin{aligned}\left|\sum_{k=1}^{n} a_{k} \cdot (x_{k} - x_{k}')\right| \leq \frac{n}{2^{n} - 1}\end{aligned}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cleft%7C%5Csum_%7Bk%3D1%7D%5E%7Bn%7D+a_%7Bk%7D+%5Ccdot+%28x_%7Bk%7D+-+x_%7Bk%7D%27%29%5Cright%7C+%5Cleq+%5Cfrac%7Bn%7D%7B2%5E%7Bn%7D+-+1%7D%5Cend%7Baligned%7D&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\begin{aligned}\left|\sum_{k=1}^{n} a_{k} \cdot (x_{k} - x_{k}')\right| \leq \frac{n}{2^{n} - 1}\end{aligned}&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Furthermore, &lt;img alt=&quot;\mathbf{x} - \mathbf{x'} \in \{-1,0,1\}^{n} &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D+-+%5Cmathbf%7Bx%27%7D+%5Cin+%5C%7B-1%2C0%2C1%5C%7D%5E%7Bn%7D+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mathbf{x} - \mathbf{x'} \in \{-1,0,1\}^{n} &quot; /&gt; so &lt;img alt=&quot;\mathbf{x} - \mathbf{x'} &quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D+-+%5Cmathbf%7Bx%27%7D+&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\mathbf{x} - \mathbf{x'} &quot; /&gt; is our desired point. &lt;img alt=&quot;\Box&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5CBox&amp;amp;bg=ffffff&amp;amp;fg=000000&amp;amp;s=0&quot; title=&quot;\Box&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Recently, researchers at the University of Washington proved reductions between the &lt;a href=&quot;https://en.wikipedia.org/wiki/Lattice_problem#Shortest_vector_problem_(SVP)&quot;&gt;Shortest Vector Problem&lt;/a&gt; and algorithmically finding a good &lt;img alt=&quot;\mathbf{x} \in \{-1,0,1\}^{n}&quot; class=&quot;latex&quot; src=&quot;https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D+%5Cin+%5C%7B-1%2C0%2C1%5C%7D%5E%7Bn%7D&amp;amp;bg=ffffff&amp;amp;fg=444444&amp;amp;s=0&quot; title=&quot;\mathbf{x} \in \{-1,0,1\}^{n}&quot; /&gt; for this Number Balancing Problem. The paper can be found &lt;a href=&quot;https://arxiv.org/pdf/1611.08757.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
	<pubDate>Fri, 14 Jul 2017 09:02:50 +0000</pubDate>
</item>

</channel>
</rss>
