<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>NLP Capstone Spring 2018</title>
  <updated>2018-04-03T16:00:04Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Nelson Liu</name>
    <email>nfliu[at]cs.washington.edu</email>
  </author>
  <id>https://nlpcapstone.github.io/atom.xml</id>
  <link href="https://nlpcapstone.github.io/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="https://nlpcapstone.github.io/" rel="alternate"/>

  <entry>
    <id>https://medium.com/p/ab3d796c422e</id>
    <link href="https://medium.com/@ryanp97/project-ideas-ab3d796c422e?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
    <title>Project Ideas</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I plan on following a research track and hope to pursue one of the following ideas:</p><h4><strong>Neural Machine Translation with Semantic Transfer</strong> (as outlined by Jan Buys)</h4><p><em>Minimal Viable Action Plan:</em> <br/>1) Use statistical parser (<a href="http://sweaglesw.org/linguistics/ace/">ACE</a>) to get MRS graphs of English/Japanese sentences and convert to DMRS graph (using <a href="https://github.com/delph-in/pydelphin">PyDelphin</a> interface to do parsing and conversion)<br/>2) Linearize DMRS graph<br/>3) Train a seq2seq that takes linearized English DMRS graph and outputs linearized Japanese DMRS graph</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*J3l_TjWr3A-jOAcrCznqvA.png"/>Example of penmen format that DMRS can be represented with (non-linearized). Note that the representation shown is an AMR graph, not a DMRS graph. Figure taken from this <a href="https://arxiv.org/pdf/1704.08381.pdf">paper</a>.</figure><p><em>Stretch Goals:<br/></em>1) Explore different architectures for semantic transfer (e.g. TreeLSTM as opposed to seq2seq)<br/>2) Explore ways to learn correspondences between semantic concepts in the two languages</p><h4>DMRS to Text Generation (as outlined by Jan Buys)</h4><p><em>Minimal Viable Action Plan:<br/></em>1) Generate DMRS graph serialization similar to what was outlined above<br/>2) Train seq2seq model to generate text directly from graph serialization</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Bndfdhs3ixwG6-JUQPC72A.png"/>Example of different graph representations. Figure taken from this <a href="http://www.lrec-conf.org/proceedings/lrec2016/pdf/634_Paper.pdf">paper</a>.</figure><p><em>Stretch Goals:</em><br/>1) Experiment with different seq2seq architectures<br/>2) Attempt semi-supervised training using a high-precision grammar-based parser</p><h4>Reproduce results / expand on a Paper</h4><p>In this option, I would be attempting to expand on this <a href="https://arxiv.org/pdf/1704.04859.pdf">paper</a> regarding hybrid models. The paper attempts to alleviate and/or solve the issue of out-of-vocabulary words and characters, specifically in Chinese and Japanese. It describes using a CNN in conjunction with an RNN to do so; the CNN learns the radicals of characters and attempts to choose characters with similar radicals since the meaning of radicals remains constant between characters.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/822/1*5GyInYVxc8ifP_YRCHiHHA.png"/>Image taken from the linked paper.</figure><p><em>Minimal Viable Action Plan:<br/></em>1) Obtain/create a visual dataset of the characters in order to train the CNN<br/>2) Design and explore different methods for joining the two models such as described in the paper<br/>3) Train the model end-to-end</p><p><em>Stretch Goals:<br/></em>1) Error analysis on different methods for joining and potential points of error<br/>2) Exploration of model variations and architecture (incremental changes similar to this <a href="https://arxiv.org/pdf/1708.04755.pdf">paper</a>)</p><p>For current progress, visit the <a href="https://github.com/ryanp97/NeuralEmpty">repo</a>.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ab3d796c422e" width="1"/></div>
    </content>
    <updated>2018-04-03T00:55:49Z</updated>
    <category term="machine-learning"/>
    <author>
      <name>Ryan Pham</name>
    </author>
    <source>
      <id>https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/1*83KfTWByl5pPq7A8_E8ApA.gif</logo>
      <link href="https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@ryanp97" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Ryan Pham on Medium</subtitle>
      <title>Stories by Ryan Pham on Medium</title>
      <updated>2018-04-03T16:00:04Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-5600014144802012716.post-6768023392170538237</id>
    <link href="https://nlpcapstonesemparse.blogspot.com/2018/03/blog-post-1.html" rel="alternate" type="text/html"/>
    <title>Blog Post 1</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="font-family: arial;"><span style="font-size: 14.6667px; white-space: pre;">I am interested in working in the code generation/semantic parsing space on the research track. My code </span></span><br/><span style="font-family: arial;"><span style="font-size: 14.6667px; white-space: pre;">will be in various branches of my fork of allennlp (https://github.com/rajasagashe/allennlp). I will keep you</span></span><br/><span style="font-family: arial;"><span style="font-size: 14.6667px; white-space: pre;"> updated on which branch/commits I worked on during each blog post. Also note that project idea 1 has </span></span><br/><span style="font-family: arial;"><span style="font-size: 14.6667px; white-space: pre;">the most detail since I have picked it as my project!</span></span></div><h2 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;"><span>Project Idea 1</span></h2><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Minimum Viable Plan:</span><span> Implement the model in the recent UW paper which introduces the task of </span><br/><span>generating the code for a java function from a natural language description. To further aid code </span><br/><span>generation, the class in which the generated function is to reside is provided, i.e. the class variables </span><br/><span>and methods. Thus the encoder encodes the class as well as the utterance and the decoder uses a </span><br/><span>two step attention mechanism and decodes through the java grammar production rules. </span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Stretch Goals:</span><span> Reproduce the state of the art results in the paper. I’m putting this in the stretch goals </span><br/><span>since successfully implementing a neural semantic parser with type constraints is pretty challenging. </span><br/><span>In addition, I hope to experiment with other improvements like encoding the entire class method body</span><br/><span> which wasn’t done.</span></div><h2 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;"><span>Project Idea 2</span></h2><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Minimum Viable Plan: </span><span>Implement this paper: </span><a href="https://arxiv.org/pdf/1704.01696.pdf" style="text-decoration: none;"><span>https://arxiv.org/pdf/1704.01696.pdf</span></a><span>. The model is </span><br/><span>similar to that of the previous idea, but the datasets are for python and ifttt instead.</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Stretch Goals: </span><span>Improve the paper’s result.</span></div><h2 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;"><span>Project Idea 3</span></h2><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Minimum Viable Plan: </span><span>Perform transfer learning across several code generation tasks by using the </span><br/><span>same encoder for them all. This technique would be similar to what was used in the Cove paper </span><br/><a href="https://arxiv.org/pdf/1708.00107.pdf" style="text-decoration: none;"><span>https://arxiv.org/pdf/1708.00107.pdf</span></a><span>. </span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Stretch Goals: </span><span>Improve the individual paper results with this technique.</span></div><div><span><br/></span></div></div>
    </summary>
    <updated>2018-03-30T22:41:00Z</updated>
    <author>
      <name>nlpcapstone</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-5600014144802012716</id>
      <author>
        <name>nlpcapstone</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="https://nlpcapstonesemparse.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="https://nlpcapstonesemparse.blogspot.com/feeds/posts/default?alt=rss" rel="self" type="application/rss+xml"/>
      <title>NlpCapstone</title>
      <updated>2018-04-03T00:28:16Z</updated>
    </source>
  </entry>
</feed>
