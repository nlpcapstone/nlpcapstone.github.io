<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>NLP Capstone Spring 2018</title>
  <updated>2018-04-11T09:00:15Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Nelson Liu</name>
    <email>nfliu[at]cs.washington.edu</email>
  </author>
  <id>https://nlpcapstone.github.io/atom.xml</id>
  <link href="https://nlpcapstone.github.io/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="https://nlpcapstone.github.io/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://deeplearningturingtest.wordpress.com/?p=14</id>
    <link href="https://deeplearningturingtest.wordpress.com/2018/04/11/project-proposal-question-based-knowledge-representation/" rel="alternate" type="text/html"/>
    <title>Project Proposal: Question-Based Knowledge Representation</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Project Objective: The objective of this project is to create a model that can build a representation of the knowledge it’s gathered by reading the input text line by line and asking appropriate clarifying questions to a human for further insight. Literature Survey: Williams, Jason D. et al. “Hybrid Code Networks: practical and efficient end-to-end … <a class="more-link" href="https://deeplearningturingtest.wordpress.com/2018/04/11/project-proposal-question-based-knowledge-representation/">Continue reading <span class="screen-reader-text">Project Proposal: Question-Based Knowledge Representation</span> <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Project Objective:</strong> The objective of this project is to create a model that can build a representation of the knowledge it’s gathered by reading the input text line by line and asking appropriate clarifying questions to a human for further insight.</p>
<p><strong>Literature Survey:</strong></p>
<ul>
<li><cite class="formatted-citation formatted-citation--style-mla">Williams, Jason D. et al. “Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning.” <em>ACL</em> (2017).</cite></li>
<li>
<div class="padded"><cite class="formatted-citation formatted-citation--style-mla">Zhang, Qianqian et al. “A Review on Entity Relation Extraction.” <em>2017 Second International Conference on Mechanical, Control and Computer Engineering (ICMCCE)</em> (2017): 178-183.</cite></div>
</li>
<li>Arvind Neelakantan’s Doctoral Disseration: <em>Knowledge Representation and Reasoning with Deep Neural Networks (2017)</em></li>
<li>
<div class="padded"><cite class="formatted-citation formatted-citation--style-mla">Zhao, Tiancheng and Maxine Eskénazi. “Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning.” <em>SIGDIAL Conference</em> (2016).</cite></div>
</li>
</ul>
<p><strong>Minimal Viable Action Plan:</strong> My minimum action plan would be to implement and train an RL model to ask certain question template(s) as clarification as it processes input text line by line. Then a human (myself) would assign a reward to indicate the quality of the question and give a brief answer to the question. This brief answer would then be used to expand the model’s stored knowledge representation and the assigned reward would be used to train the RL model. This series of exchanges is treated like a one-on-one conversation even if it’s more like a one-sided lecture.</p>
<p><strong>Stretch Goals:</strong> Stretch goals include extracting features from the existing knowledge base and feeding those into the model to further improve the relevancy of questions (for example not asking questions that the model should already know). Another stretch goal would be to translate the knowledge representation back into everyday English text for tasks such as answering reading comprehension questions. This can be done either by training a Machine Translation model, using a parser like ANTLR, or some other method.</p>
<p><strong>Proposed Methodologies:</strong> My proposed methodologies follows a similar outline as given by Williams et al. in their paper on Hybrid Code Networks (HCNs).</p>
<ul>
<li>The input text is read in line-by-line and goes through various preprocessing steps like entity extraction, word embeddings layer, sentiment analysis, bag of words, etc. These features are concatenated and fed into the model.</li>
<li>In the HCN paper, the model was an RNN followed by a softmax layer (probability distribution over the various actions to take).
<ul>
<li>In this problem, the “actions” to take are the different types of question templates to ask so copying this model and substituting their action templates with my question templates would work.</li>
</ul>
</li>
<li>An alternative approach would be to build a Deep Q Network model with an LSTM (or GRU) layer at the end, so the network would update the Q values for each question type that can be asked (and the one with the highest Q value can be greedily selected). For clarification, each input sentence would represent a time step.</li>
<li>After a question type is selected with either of the 2 approaches above, nouns need to be substituted in to form an actual question (as in the HCN paper). For example, a question template might be “Is there a relationship between ______ and ______?” and the two blanks need to be filled in with nouns. If the substitution results in a good question, a good reward is assigned and otherwise, a very negative reward is assigned. If it’s a good question, then an answer is given to update the knowledge representation.
<ul>
<li>Some possible ways to create this knowledge representation is to use lists, trees, semantic networks, production rules, logical propositions and/or other existing NLP knowledge representation models (I haven’t decided on one yet).</li>
</ul>
</li>
</ul>
<p><strong>Available Resources/Databases:</strong> To train the model, any reading comprehension dataset like MS Marco (Microsoft), SQuAD (Stanford), RACE, etc. can be used. If my project is successful enough, I can even use these datasets for evaluation.</p>
<p><strong>Evaluation Plan:</strong> If I only finish my minimum viable action plan, I’m not sure how I can qualitatively assess my knowledge representation except by comparing the representation against the information I wanted to be recorded (or have other people judge what knowledge should be stored). On the other hand, if I finish my stretch goals (translating knowledge representation to English) I can try to use the knowledge representation to train on and respond to queries in a reading comprehension dataset (which would be a much more qualitative evaluation).</p></div>
    </content>
    <updated>2018-04-11T06:59:08Z</updated>
    <category term="Uncategorized"/>
    <author>
      <name>ananthgo</name>
    </author>
    <source>
      <id>https://deeplearningturingtest.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://deeplearningturingtest.wordpress.com/feed/" rel="self" type="application/rss+xml"/>
      <link href="https://deeplearningturingtest.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://deeplearningturingtest.wordpress.com/osd.xml" rel="search" title="NLP Capstone Project Updates - Ananth" type="application/opensearchdescription+xml"/>
      <link href="https://deeplearningturingtest.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>NLP Capstone Project Updates – Ananth</title>
      <updated>2018-04-11T09:00:12Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/7b6d1a9ec67c</id>
    <link href="https://medium.com/@viterbi.or.not/formal-project-proposal-7b6d1a9ec67c?source=rss-c522ef075bb3------2" rel="alternate" type="text/html"/>
    <title>Formal Project Proposal</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cr8jfKVLjgd9Y7YmxXWSYA.png"/></figure><p>Introducing our project topic, <strong>automatic conversation summarization</strong>! Our proposal will outline specific objectives, motivations, and plan we have for this project. We also cover here the resources we have gathered — related work, datasets (as promised), and evaluation frameworks, to demonstrate viability and give background on the topic.</p><h4><strong>Objective</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DmM6aXAhZi61Ec7Yhp6YBw.png"/>Example summarization of an email chain from the <a href="https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/bc3.html">W3C email threads corpus</a></figure><p>In the field of automatic text summarization, there are numerous techniques that can be used to produce summaries of general text data. However, for our project, we seek to work on a more specific type of data by exploring and evaluating techniques for summarization of conversation logs. Therefore, the models we will attempt to implement and measure will take input in the form of natural text from conversations, such as email threads, chat logs, or transcribed spoken conversations, and output more concise summaries that capture the most important parts of the input.</p><p>While a document or paper typically sticks to a single topic at once and represents communication between an author and the reader, conversational data is characterized by a mixing of sub-topics and, in many cases, contributions from multiple different authors before one topic is finished. As a result, the objective of conversational summarization includes identifying topics and threads among a potentially chaotic conversation in order to make a sensible summarization even without the benefit of a single, linear topic progression.</p><h4><strong>Extractive vs. Abstractive</strong></h4><p>When considering text summarization, a distinction has to be made between two different approaches to summarization that offer different levels of implementation difficulty and usefulness. An extractive summary is produced by identifying the most important sentences from the input text and combining them to form a summary that is the concatenation of those sentences.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iZujvW2rP_HSVU2KXLtvew.png"/>Example of a short chatlog and its corresponding extractive summary</figure><p>Alternatively, an abstractive summary consists of new text generated from the topics and important aspects of the input text, but requires the model to create new summary sentences rather than simply re-using existing ones. As a result of this generation process, abstractive summaries are typically considered to be more useful, because they consist of natural-sounding text while still paraphrasing the concepts that are important. However, current techniques perform much better at generating extractive summaries, which are considered much easier to implement because they only require assigning scores to the sentences of the input.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1oiGv-MrfdubjbKv-Vp9fA.png"/>Example of a short chatlog and its corresponding abstractive summary</figure><h4><strong>Motivation</strong></h4><p>The <strong>academic</strong> motivations behind our project are to pursue a topic in summarization that has been relatively less explored. This means tackling the challenges that come with the conversation summarization domain — annotated conversation datasets are typically smaller and more unpredictable, and whereas tasks like document summary usually involve one topic written by one author/voice, conversations and involve many participants and have less well-defined topic segmentation.</p><p>The project is also motivated by the possibly impactful <strong>applications </strong>of conversation summarization. Being able to summarize long chains of emails or group IMs is an increasingly important task to tackle in today’s world and can be a useful augmentation to digital group conversations.</p><h4><strong>Related Work</strong></h4><p>In our literature survey, we will first discuss the two papers that have been most impactful in helping establish our project plan.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*U0i-moKx4s_YB3My."/>Graph of contributions of general feature categories to the performance of logistic regression classifiers</figure><p><a href="http://www.aclweb.org/anthology/D08-1081"><strong>Summarizing Spoken and Written Conversations</strong></a><strong>¹ </strong>uses meeting and email datasets. The authors of this paper approach extractive summarization with logistic regression classifiers and a mix of general summarization features as well as some basic conversation summarization features. The overall feature categories can be seen in the corresponding figure, which graphs the contributions of each feature category to the classifier’s performance. “Participant” category features were found to help achieve competitive results.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1010/0*wLl3u1tR-GKYuIql."/>Table of performances of previously published models and the models used in the experiment (C4.5, NB, MLP, SVM)</figure><p><a href="http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf"><strong>Summarizing Online Conversations: A Machine Learning Approach</strong></a><strong>²</strong> uses chatlog and meeting datasets. The authors of this paper conduct experiments with Decision Tree, Naive Bayes classifier, Multilayer Perceptron (MLP) and Support Vector Machine (SVM) summarizers to create extractive summaries. The input feature vectors to these trainable summarizers use both general summarization features (sentence length, sentence position, similarity to title, etc.) as well as conversation specific summarization features that are more specialized than those mentioned in the previously discussed paper (is question, sentiment score, discourse markers). The paper overall found that the Naive Bayes classifier and MLP performed the best.</p><h4><strong>Datasets</strong></h4><p>The datasets we found cover a variety of conversation domains and are all human-annotated with summaries.</p><p><strong>Email: </strong><a href="https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/bc3.html"><strong>W3C Email Threads</strong></a></p><ul><li>40 email threads of ~80 lines each</li><li>Extractive and Abstractive Summaries</li></ul><p><strong>Chat: </strong><a href="https://flossmole.org/content/software-archaeology-gnue-irc-data-summaries"><strong>GNU Enterprise Chatlogs</strong></a></p><ul><li>~120 chats of ~1200 lines each</li><li>Abstractive Summaries</li></ul><p><strong>Spoken Conversation: </strong><a href="http://groups.inf.ed.ac.uk/ami/corpus/"><strong>AMI Meeting Transcripts</strong></a></p><ul><li>140 meeting transcriptions of ~45 minutes each</li><li>Extractive and Abstractive Summaries</li></ul><h4><strong>Evaluation</strong></h4><p>In terms of <strong>automated</strong> methods of evaluation for the summaries that we will generate, there are a few frameworks we can use:</p><p><strong>BLEU</strong> (BIlingual Evaluation Understudy)</p><ul><li>Precision measure with some enhancements</li></ul><p><strong>METEOR</strong> (Metric for Evaluation of Translation with Explicit ORdering)</p><ul><li>Improves on BLEU by adding recall, synonyms</li><li>Better at sentence-level evaluation</li></ul><p><strong>ROUGE</strong> (Recall-Oriented Understudy for Gisting Evaluation)</p><ul><li><strong>ROUGE-L</strong>: Longest common subsequence</li><li><strong>ROUGE-N</strong>: Overlap of N-Grams between passages</li></ul><p>However, for summarization tasks, <strong>human evaluation</strong> is possibly most ideal and it is unclear if any automatic metric can be as effective.</p><h4><strong>Minimum Viable Product</strong></h4><p>Our plan is to start with two <strong>baseline models</strong> that replicate the best performing models (Naive Bayes and MLP approaches) of “Summarizing Online Conversations: A Machine Learning Approach” using the non-conversation specific features.</p><p>We would then experiment with tweaks to the baseline models that use conversation-specific features, both based off of the ones described in the paper and also based on some of our ideas about what kinds of domain-specific features might benefit conversation summarization. We can also continue to explore alternative models and compare model approaches.</p><h4><strong>Stretch Goals</strong></h4><p>There are a number of ideas we have for stretch goals that can push our project further:</p><p><strong>Neural Network implementation: </strong>Applying general summarization techniques to conversation</p><p><strong>Model entity relationships: </strong>Identifying the role of a contributor or named entity, such as a supervisor in an email thread</p><p><strong>Abstractive summarization: </strong>Convert a previously extractive summary to be natural-sounding and abstractive</p><p><strong>Optimize performance for a specific domain: </strong>For example, in chats about bugfixes, use that specific context to try outperforming the general case</p><p><strong>Relate parameters from different types of data: </strong>For example, use data from spoken corpus to improve results on email</p><p>We hope you enjoyed reading our project proposal and we are excited to get working!</p><p>[1] Murray, G. &amp; Carenini, G. (2008). Summarizing Spoken and Written Conversations. <em>Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, </em>773–782.</p><p>[2] Sood, A. &amp; Varma, V. (2012). Summarizing Online Conversations: A Machine Learning Approach. <em>Centre for Search and Information Extraction Lab International Institute of Information Technology Hyderabad,</em> 500.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7b6d1a9ec67c" width="1"/></div>
    </content>
    <updated>2018-04-11T06:56:31Z</updated>
    <category term="nlp"/>
    <author>
      <name>Viterbi Or Not To Be</name>
    </author>
    <source>
      <id>https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/1*wiisg40Bu4z11RTWJ66mnA.png</logo>
      <link href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@viterbi.or.not" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Viterbi Or Not To Be on Medium</subtitle>
      <title>Stories by Viterbi Or Not To Be on Medium</title>
      <updated>2018-04-11T09:00:12Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/c8a12d3ae611</id>
    <link href="https://medium.com/@be.li.nda/nlp-capstone-blog-3-project-proposal-c8a12d3ae611?source=rss-fad49d942bf3------2" rel="alternate" type="text/html"/>
    <title>NLP Capstone Blog #3: Project Proposal</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The project I’m working on is document-level entity-entity sentiment analysis.</p><h4>Objectives and Definition</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mfDKzKS3wAEnJZ3gvLHwyw.png"/>Notice in the above example that the arc from Russia to Belarus is negative, which makes sense if our article has the sentence “Russia criticizes Belarus.”</figure><p>The goal of my project can be stated thusly: given a document, be able to figure out whether various entities within the document feel positively or negatively towards each other.</p><p>In particular, I’m looking to apply a neural model to <a href="https://homes.cs.washington.edu/~eunsol/papers/acl2016.pdf">an existing paper</a> in an attempt to improve its F1 scores.</p><p>Applications of this work include:</p><ul><li>Modeling social dynamics between entities</li><li>Applying sentiment analysis to problems beyond simply movie/product reviews</li></ul><p>A farther motivation for this project is that entity-entity sentiment analysis is a relatively novel task, with little existing work thus far. Also, even work in related fields is usually focused on the sentence level, rather than the document level.</p><h4><strong>Literature Survey</strong></h4><p>Some work in related fields to this project. For each paper, I will be focusing especially on its model /methodologies, which could be of use to developing my own model —</p><p><strong>Targeted Sentiment Analysis</strong></p><p><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12074/12065">Zhang, Meishan, Zhang, Yue, and Vo, Duy-Tin, 2016. Gated Neural Networks for Targeted Sentiment Analysis.</a></p><ul><li>Introduced the use of gated neural networks to perform targeted sentiment analysis.</li><li>Inputs: Concatenation of 2 types of word embeddings: both embeddings incorporate sentiment information in some way</li><li>Model: GRNN + G3 model, where outputs of GRNN are pooled and fed into G3. The G3 was used to better model interaction between left and right context for an entity, as sentiment can be dominated by left or right context</li><li>Outputs: Targeted sentiment</li></ul><p><strong>Coreference Resolution</strong></p><p><a href="https://homes.cs.washington.edu/~luheng/files/emnlp2017_lhlz.pdf">Lee, Kenton, He, Luheng, Lewis, Mike, and Zettlemoyer, Luke, 2017. End-to-end Neural Coreference Resolution.</a></p><ul><li>Model: 2-step, 1st step being a biLSTM, and 2nd step being a FFNN</li><li>Inputs to FFNN: Something nice about this model is the fact that they were able to encode multi-word entities efficiently by concatenating LSTM outputs.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/227/1*7yoU47vuvU5CaBm-r3n3aw.png"/>ϕ used to encode size of span. x* are outputs of LSTMs for boundaries of span. x hat are outputs of attention mechanism over span.</figure><ul><li>FFNN architecture:</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/390/1*9KxdIWkYNSzMwDJeSI3fuw.png"/></figure><p><strong>Document-level Relation Extraction</strong></p><p><a href="https://www.cs.jhu.edu/~npeng/papers/TACL_17_RelationExtraction.pdf">Peng, Nanyun, Poon, Hoifung, Quirk, Chris, Toutanova, Kristina, Yih, and Wen-tau, 2017. Cross-Sentence N-ary Relation Extraction with Graph LSTMs.</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*-M0TCCGCpmYtYpVnYfhAHg.png"/>Example of a graph LSTM structure used in the paper. Note the additional links between non-adjacent cells encodes for syntactic dependencies and discourse relations.</figure><ul><li>Uses a graph LSTM with architecture following the structure of the document graph (introduced by <a href="https://arxiv.org/pdf/1609.04873.pdf">Quirk and Poon, 2017</a>), which encodes various dependencies, including word adjacency, syntactic dependencies, and discourse relations</li></ul><p><a href="https://arxiv.org/pdf/1802.10569.pdf">Verga, Patrick, Strubell, Emma, and McCallum, </a><a href="https://arxiv.org/pdf/1802.10569.pdf">Andrew</a><a href="https://arxiv.org/pdf/1802.10569.pdf">, 2018. Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/345/1*6ap---pVvwWNNdCgwhh16w.png"/></figure><ul><li>Bi-affine neural model: First pass document through Transformer, then evaluate relations between pairs of tokens using a bi-affine operator and the LogSumExp function</li></ul><h4><strong>Methodologies</strong></h4><p><strong>Baseline Neural Model (Minimal Viable Plan)</strong></p><p>Inputs: for each token, concatenate word embeddings, polarity embeddings, and holder/target/none embeddings</p><ul><li>Word embeddings are pre-trained GloVe embeddings</li><li>EX: “the cat disliked milk”, holder = “cat”, target = “milk”</li><li>Input becomes: [the, 0, 0] [cat, 0, H] [disliked, NEG, 0] [milk, 0, T]</li></ul><p>Model: Attentive biLSTM</p><p>Output: Sentiment score for positivity and negativity</p><p><strong>Scoring Holder/Target Sentiment in a Second Step (Minimal Viable Plan / Stretch Goal, i.e. I think I can get this but I can’t guaruntee it)</strong></p><ol><li>Feedforward Neural Network: Incorporating the FFNN from the Lee et al. paper (see above for details). To adapt the architecture for my needs, I will get rid of the mention score, and thus the white and grey cells will be the same. The grey cell, instead of outputting a coreference score, will output a sentiment (positivity and negativity) score.</li><li>Bi-affine Scoring: Incorporating the scoring mechanism from the Verga et al. paper (see above for details). Instead of a “Transformer,” I will use the LSTM from my baseline model, which gets fed in to two MLPs, the biaffine operator, and finally aggregated using the LogSumExp function.</li></ol><p><strong>Experimenting with Different Ways of Aggregating across Different Mentions (Stretch Goal)</strong></p><ol><li>LogSumExp function from the Verga et al. paper (see above).</li><li>Encode inputs to second step in a way that incorporates information from all mentions.</li></ol><p><strong>Experimenting with Graph LSTM Structure (Stretch Goal)</strong></p><ul><li>Based off of ideas in Peng et al. paper (see above)</li></ul><p><strong>Incorporate Ideas from Targeted Sentiment Analysis (Stretch Goal)</strong></p><ul><li>Add a G3 gate, based on Zhang et al. paper (see above), or even adapt the architecture of the G3 gate to better suit the task of sentiment analysis between entities</li></ul><h4><strong>Available Resources</strong></h4><p>Dataset from Choi et al. (paper I’m improving). The data has already been pre-processed using StanfordNLP. Some of the preprocessing that’s been done includes tokenization, named entity recognition, and co-reference resolution.</p><h4><strong>Evaluation Plan</strong></h4><p>As an evaluation metric, I’m going to use the F1 scores for both positive and negative sentiments.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c8a12d3ae611" width="1"/></div>
    </content>
    <updated>2018-04-11T06:47:08Z</updated>
    <category term="deep-learning"/>
    <category term="naturallanguageprocessing"/>
    <category term="machine-learning"/>
    <author>
      <name>Belinda Zou Li</name>
    </author>
    <source>
      <id>https://medium.com/@be.li.nda?source=rss-fad49d942bf3------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/0*A16vZtRWBzxYPlmn.</logo>
      <link href="https://medium.com/@be.li.nda?source=rss-fad49d942bf3------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@be.li.nda" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Belinda Zou Li on Medium</subtitle>
      <title>Stories by Belinda Zou Li on Medium</title>
      <updated>2018-04-11T09:00:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://mathstoc.wordpress.com/?p=309</id>
    <link href="https://mathstoc.wordpress.com/2018/04/11/nlp-capstone-post-3-proposal/" rel="alternate" type="text/html"/>
    <title>NLP Capstone Post #3: Proposal</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Here, we finally present our project proposal in full. Project Objectives: Our goal for this project is to engineer a model that, given the instrumental (“karaoke”) music for a song in English represented as MIDI data, output a coherent sequence of words corresponding to lyrics for the music. The model will produce timings along with … <a class="more-link" href="https://mathstoc.wordpress.com/2018/04/11/nlp-capstone-post-3-proposal/">Continue reading <span class="screen-reader-text">NLP Capstone Post #3: Proposal</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Here, we finally present our project proposal in full.</p>
<h3>Project Objectives:</h3>
<p>Our goal for this project is to engineer a model that, given the instrumental (“karaoke”) music for a song in English represented as MIDI data, output a coherent sequence of words corresponding to lyrics for the music. The model will produce timings along with the words to align it with the background instrumentals. Additionally, given the output of the model and the input music, we will automate their combination into a song complete with lyrics and supporting instrumentals. This combined output will be playable and we intend to do live demonstration.</p>
<h3>Proposed Methodology:</h3>
<p>Here, we outline the steps we will need to take in detail.</p>
<li>Data collection (datasets of songs, preferably with instrumentals and lyrics already separated)</li>
<li>Decide on vocabulary and how to handle uncommon words</li>
<li>Decide and implement any required preprocessing of the raw MIDI data. Strip lyrics from MIDI data if not already provided in dataset.</li>
<li>Decide and implement model (see Model Design)</li>
<li>Implement model sanity checks</li>
<li>Model tweaking (we expect this will take the majority of the time; see Model Design)</li>
<li>Implement automated combination of model output (lyrics) and model input (instrumentals)</li>
<li>Further testing</li>
<li>Assuming preceding steps are completed satisfactorily, proceed to stretch goals</li>
<li>Presentation and write-up</li>
<h3>Model Design:</h3>
<p>We will pursue a seq2seq RNN approach, taking in input MIDI data represented as a sequence, and outputting a sequence of words from a specified vocabulary. This model will be referred to as the generator. We will employ adversarial training, simultaneously training a many-to-one RNN discriminator that, given the input instrumentals and corresponding lyrics, output if the lyrics were produced by the generator or not. We will follow approaches taken in previous works such as SeqGAN [2] (and [3, 4]), namely using policy gradient ideas from reinforcement learning to obtain gradients that can be backpropagated from the discriminator network through the generator network. We note that syntactic correctness can be enforced in this manner, as malformed lyrical output can be assigned arbitrarily small reward.</p>
<h3>Stretch Goals:</h3>
<p>There are several stretch goals we will consider, time permitted. They are as follows, in no particular order.</p>
<li>Handling multiple languages, particularly those with less available data</li>
<li>Given a specific songwriter/band, produce the instrumentals along with lyrics for a new song that is in the style of that songwriter/band</li>
<li>Lyrics generation for duets, or multi-singer songs</li>
<li>Playing with phoneme-level generation</li>
<h3>Core Challenges:</h3>
<p>The core challenges we will need to overcome include alignment of lyrics with the music, and production of sensible lyrics. On the more technical side, it is well-known that ensuring convergence in adversarial training is difficult.</p>
<h3>Available Resources:</h3>
<p>Existing music datasets for machine learning tasks are made up of audio samples (such as .wav or .mp3), or MIDI data that specifies timing and notes. For karaoke, lyrics are also provided either as a separate text file (.LRC) specifying the timing of each word, or can be embedded into the MIDI file directly (.KAR). It may also be useful to train a lyric model on a larger corpus of song lyrics, since lyrics are easier to collect than fully time-annotated karaoke files.</p>
<p>The MusicNet dataset [9] provides 330 classical instrumental audio files, each of which has associated timing provided for every note. Since we are primarily interested in lyrical generation and alignment, this dataset is not going to be useful for creating a language model.</p>
<p>An existing karaoke dataset called Kara1k [1] provides many features computed from 1000 lyric-annotated songs. This provides lots of metadata about each song, including annotated chords for each timestep of the song. According to the KaraMIR website, these features are extracted from audio samples using Vamp Plugins, which estimates chords with accuracy up to 70%. </p>
<p>We propose a new dataset (name not yet determined) of MIDI karaoke data with embedded lyrics (.KAR). This dataset contains over 700 files, scraped from a karaoke content aggregator [11]. Timed lyrical data has been extracted from these files, and the precise timing of each note is already available by nature of the MIDI format.</p>
<p>Additional datasets for training a lyric model may be useful, and many are available. One such dataset is the 55000+ Song Lyrics on Kaggle [10]. This could help our model generalize its lyrical output beyond the limited set of vocabulary available within the 1000 or fewer annotated karaoke songs.</p>
<h3>Evaluation Plan:</h3>
<p>Evaluation of our model can be done several ways. The first is simply to listen to the music ourselves. This is the most direct method of evaluation but is not efficient, as likely we will need many iterations of tuning; furthermore, will likely need to listen to several songs to be confident of the model’s quality. Hence, we will also design basic “sanity check” tests for our models.</p>
<p>Recall that in our proposed methodology, we intend to use adversarial training. The discriminator network itself gives a direct evaluation of the generator. As long as the discriminator is of vetted quality, and the discriminator is run on sufficiently many examples (with roughly even number of generated and true examples mixed in), the generator will be deemed also of sufficient quality (as a “sanity check”).</p>
<p>Of course, this leaves the question of ensuring the discriminator is good. We can run the discriminator on instrumentals combined with randomly generated words (according to some distribution), or on instrumentals combined with the original lyrics, which are perturbed in some fashion. As an example, one can perturb the original lyrics temporally (making an utterance off-beat when it should be precisely on the down-beat of a bar) or replacing a few words with randomly selected ones (according to some distribution over the vocabulary). These “test inputs” to the discriminator can be generated before-hand.</p>
<h3>Literature Survey:</h3>
<p>Here are some relevant papers (most were already included in preceding posts).</p>
<p>[1] Y. Bayle, L. Marsik, M. Rusek, M. Robine, P. Hanna, K. Slaninova, J. Martinovic, J. Pokorny. “Kara1k: A Karaoke Dataset for Cover Song Identification and Singing Voice Analysis”. IEEE International Symposium on Multimedia (ISM), 2017. <a href="https://ieeexplore.ieee.org/document/8241597/" rel="nofollow">https://ieeexplore.ieee.org/document/8241597/</a></p>
<p>[2] L. Yu, W. Zhang, J. Wang, Y. Yu. “SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient”. Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, 2017. <a href="https://arxiv.org/abs/1609.05473" rel="nofollow">https://arxiv.org/abs/1609.05473</a></p>
<p>[3] S. Lee, U. Hwang, S. Min, S. Yoon. “A SeqGAN for Polyphonic Music Generation”. 2017. <a href="https://arxiv.org/abs/1710.11418" rel="nofollow">https://arxiv.org/abs/1710.11418</a></p>
<p>[4] H. W. Dong, W. Y. Hsiao, L. C. Yang, Y. H. Yang. “MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment”. 2017. <a href="https://arxiv.org/abs/1709.06298" rel="nofollow">https://arxiv.org/abs/1709.06298</a></p>
<p>[5] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio. “Generative Adversarial Nets”. NIPS, 2014. <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets" rel="nofollow">https://papers.nips.cc/paper/5423-generative-adversarial-nets</a></p>
<p>[6] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, X. Chen. “Improved Techniques for Training GANs”. NIPS, 2016. <a href="https://arxiv.org/abs/1606.03498" rel="nofollow">https://arxiv.org/abs/1606.03498</a></p>
<p>[7] M. Arjovsky,  S. Chintala, L. Bottou. “Wasserstein GAN”. 2017. <a href="https://arxiv.org/abs/1701.07875" rel="nofollow">https://arxiv.org/abs/1701.07875</a></p>
<p>[8] J. Faille, Y. Wang. “Using Deep Learning to Annotate Karaoke Songs”. 2016. <a href="https://www.semanticscholar.org/paper/Using-Deep-Learning-to-Annotate-Karaoke-Songs-Faille-Wang/521361762a7327f8fcc77bd9d76eaa2b503f845a" rel="nofollow">https://www.semanticscholar.org/paper/Using-Deep-Learning-to-Annotate-Karaoke-Songs-Faille-Wang/521361762a7327f8fcc77bd9d76eaa2b503f845a</a></p>
<p>[9] J. Thickstun, Z. Harchaoui, S. Kakade. “Learning Features of Music from Scratch”. 2017. <a href="https://arxiv.org/abs/1611.09827" rel="nofollow">https://arxiv.org/abs/1611.09827</a></p>
<p>[10] Additional data <a href="https://www.kaggle.com/mousehead/songlyrics">here</a></p>
<p>[11] Even more additional data <a href="http://vooch.narod.ru/midi/midi.htm">here</a></p></div>
    </content>
    <updated>2018-04-11T06:45:55Z</updated>
    <category term="NLP Capstone"/>
    <author>
      <name>Kuikui Liu</name>
    </author>
    <source>
      <id>https://mathstoc.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://mathstoc.wordpress.com/category/nlp-capstone/feed/" rel="self" type="application/rss+xml"/>
      <link href="https://mathstoc.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://mathstoc.wordpress.com/osd.xml" rel="search" title="Mathematical Distractions" type="application/opensearchdescription+xml"/>
      <link href="https://mathstoc.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A recreational (occasionally research) blog on topics in maths or computer science - by Kuikui</subtitle>
      <title>NLP Capstone – Mathematical Distractions</title>
      <updated>2018-04-11T09:00:15Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3753031463594823927.post-4531878816260312232</id>
    <link href="https://cse481n.blogspot.com/2018/04/blog-post-3.html" rel="alternate" type="text/html"/>
    <title>Blog Post #3</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h1 dir="ltr" id="docs-internal-guid-ea0c9d97-b369-9237-6f13-3675807d7a60" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Project Objectives</span></h1><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Single document summarization (SDS) is one of the remaining challenging problems in natural language processing. Novel methods are presented frequently in new papers, but they often do not include specific code allowing for reproducibility and are evaluated on specific datasets that make comparisons between models meaningless and difficult.</span></div><br/><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">There are many approaches to SDS, but they can be broadly divided into combinatorial approaches and neural approaches. Neural approaches build a neural architecture, such as a seq2seq/encoder-decoder model or single sequence RNNs. Combinatorial approaches will either try to frame the problem as an optimization problem, and then use an ILP solver, or frame the problem as a classic NP-hard problem, like Knapsack or Maximum Coverage. We want to explore both approaches, and compare their performance on the same dataset.</span></div><br/><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">SDS is comprised of two tasks: extractive summarization and abstractive summarization. Extractive summarization compiles a summary by selecting sentences from the document’s text while abstractive summarization generates text for the summary (sentences that may not have been present in the document’s text). While abstractive summarization might have more intuitive appeal, our project will focus on extractive summarization to enable meaningful comparisons between neural and combinatorial approaches (combinatorial approaches often must be extractive).</span></div><br/><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">In this project, we plan to implement at least one neural and one combinatorial model for extractive single document summarization. We hope to establish some meaningful ways to compare the differences between selections made by the different types of models. Our primary goal is to better understand the strengths and weaknesses of neural and combinatorial models for single document summarization - a particular important aspect of SDS given the general roughness of existing evaluation metrics. We will gauge our progress based on reaching acceptable performance on commonly-used evaluation metrics when we implement models.</span></div><h1 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Methodology</span></h1><h3 dir="ltr" style="line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Minimal Viable Action Plan</span></h3><ol style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Build our data set using existing data. Specifically, convert data better suited for training abstractive summarization models into data that can be used for extractive summarization..</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Implement a simple combinatorial model (for example, we can do a simple maximum coverage problem, where we set up the “universe” to be the vocabulary of the document, and treat the sentences as sets of words).</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Implement a simple neural model (just treat the problem as a generic binary classification problem).</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Train models on identical data sets and do a baseline comparison -- how well does a simple neural model do vs. a simple combinatorial model? This doesn’t tell us much about the relative strengths of the two approaches (we can’t quantify “simple”), but with some error analysis, we might be able to see what sentences neural models are misidentifying vs. what sentences combinatorial models are misidentifying.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Build at least one state-of-the-art combinatorial model (adapting from a recent paper). We have two candidate papers: Hirao et al.’s Tree Knapsack approach and Durrett et al.’s Compression/Anaphoricity</span></div></li></ol><br/><h3 dir="ltr" style="line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Stretch Goals</span></h3><ol style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Design our own model that is a combination of the strong points of the combinatorial and neural models. Ideally, our model would be as good as or better than the existing models we implemented on the quantitative and qualitative metrics we use.</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Alternatively, we can use ideas from one domain to improve an aspect of a SOTA model in the other domain. For example, we might learn that neural models are great at dealing with named entities, and so incorporate a neural layer in a combinatorial model (perhaps by allowing the output of the neural layer to determine the weights of named entities).</span></div></li><li dir="ltr" style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Design a common system for comparing performance of extractive summarization models. Rather than a differentiable evaluation metric, we think it may be useful to choose a set of “tough” documents to summarize and bundle them together with specific reasons for their difficulty, so that researchers may more easily identify weaknesses in models they are working on.</span></div></li></ol><h1 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Available Resources</span></h1><h3 dir="ltr" style="line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Dataset/Evaluation</span></h3><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">For this problem, we will be using the DailyMail/CNN dataset. From our initial research, this seems to be the standard dataset for both document summarization as well as basic reading comprehension. The dataset has 400,000 articles, and includes both the full text of the article as well as bullet point “highlights”. For reading comprehension, an important word is omitted from the highlights and the machine is asked to fill in the blank. For text summarization, the bullet points are considered the “gold standard” summaries -- machine generated summaries are evaluated against the bullet points, typically</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"> </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">using ROUGE metrics</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"> </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">(Lin, 2004). While this works fine for abstractive summarization, this training corpus is not annotated enough for extractive summarization. More specifically, extractive summarization requires sentence level binary annotations, to indicate whether each sentence does or doesn’t belong in the summary. So we need to first convert the bullet points into more fine grained annotations.</span></div><br/><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">We’ve looked at two papers which briefly touched on this. Nallapati et. al. used a greedy approach, where they added one sentence at a time to the extractive summary while seeking to maximize the Rouge score with respect to the abstractive summary (the bullet points). They also tried to use an RNN decoder in combination with the abstractive summaries to train the extractive model without using sentence-level annotations. However, this approach was slightly less successful than estimating sentence-level annotations. Cheng and Lapata used a different approach - they created a “rule-based system that determines whether a given sentence matches a highlight...The rules take into account the position of the sentence in the document, the unigram and bigram overlap between document sentences and highlights, [and] the number of entities appearing in the highlight and in the document sentence”. It’s not 100% clear what rules the authors used, but according to Nallapati et. al., the rule-based approach found a better “ground-truth” than the greedy approach.</span></div><h3 dir="ltr" style="line-height: 1.38; margin-bottom: 4pt; margin-top: 16pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">GitHub Repositories</span></h3><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Some researchers publish the code they used in their paper on GitHub. We can use repos for quick comparisons or to see how they design their code.</span></div><br/><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><a href="https://github.com/abisee/pointer-generator" style="text-decoration: none;"><span>https://github.com/abisee/pointer-generator</span></a><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">: This repo is for See et al.’s Pointer-Generator neural model.</span></div><br/><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><a href="https://github.com/cheng6076/NeuralSum" style="text-decoration: none;"><span>https://github.com/cheng6076/NeuralSum</span></a><span style="background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">: This repo is for Cheng and Lapata’s neural model, that combines a sentence level RNN with a word level CNN.</span></div><h1 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 20pt;"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 20pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Related Work and References</span></h1><br/><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Cheng, J., &amp; Lapata, M. (2016). Neural Summarization by Extracting Sentences and Words. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">arXiv:1603.07252 [Cs]</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">. Retrieved from http://arxiv.org/abs/1603.07252</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Durrett, G., Berg-Kirkpatrick, T., &amp; Klein, D. (2016). Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">arXiv:1603.08887 [Cs]</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">. Retrieved from http://arxiv.org/abs/1603.08887</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Hirao, T., Yoshida, Y., Nishino, M., Yasuda, N., &amp; Nagata, M. (2013). Single-Document Summarization as a Tree Knapsack Problem. In </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"> (pp. 1515–1520). Seattle, Washington, USA: Association for Computational Linguistics. Retrieved from http://www.aclweb.org/anthology/D13-1158</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. In S. S. Marie-Francine Moens (Ed.), </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;"> (pp. 74–81). Barcelona, Spain: Association for Computational Linguistics.</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">Nallapati, R., Zhai, F., &amp; Zhou, B. (2016). SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">arXiv:1611.04230 [Cs]</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">. Retrieved from http://arxiv.org/abs/1611.04230</span></div><div dir="ltr"><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">See, A., Liu, P. J., &amp; Manning, C. D. (2017). Get To The Point: Summarization with Pointer-Generator Networks. </span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: italic; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">arXiv:1704.04368 [Cs]</span><span style="background-color: transparent; color: black; font-family: Arial; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; white-space: pre;">. Retrieved from http://arxiv.org/abs/1704.04368</span></div></div>
    </summary>
    <updated>2018-04-11T06:33:00Z</updated>
    <author>
      <name>Ron &amp;amp; Aditya</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3753031463594823927</id>
      <author>
        <name>Ron &amp;amp; Aditya</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="https://cse481n.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss" rel="self" type="application/rss+xml"/>
      <title>PrimeapeNLP</title>
      <updated>2018-04-11T09:00:13Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-9203775015655831448.post-5878905571398539101</id>
    <link href="https://teamoverfit.blogspot.com/2018/04/3-project-proposal.html" rel="alternate" type="text/html"/>
    <title>#3 Project Proposal</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h2 style="height: 0px;"><span style="font-family: Arial, Helvetica, sans-serif;">Team Overfit</span></h2><h3><span style="color: #999999; font-family: Arial, Helvetica, sans-serif;"><br/></span></h3><h3><span style="color: #999999; font-family: Arial, Helvetica, sans-serif;">Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><span style="font-family: Arial, Helvetica, sans-serif;"><br/></span><div/><span style="font-family: Arial, Helvetica, sans-serif;"><br/></span><div style="text-align: start; text-indent: 0px;"><div style="margin: 0px;"><div><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><b>Blog Post: #2: 04/05/2018</b></span></div><div><span style="font-family: Arial, Helvetica, sans-serif;"><span><b><br/></b></span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial, Helvetica, sans-serif; font-size: 13.999999999999998pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;">Social Media Predicts Stock Price (StartUp Mode)</span></div><div><span style="font-family: Arial, Helvetica, sans-serif;"><span><b id="docs-internal-guid-213a19db-b353-3e4c-1df6-5dd289daeb8b" style="font-weight: normal;"><br/></b></span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: transparent; color: black; font-family: Arial, Helvetica, sans-serif; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;">There are vast amount of new information related to companies listed on the stock market appears instantly, with immediate impact on stock prices. Our project is for monitoring those text on the social media platform and extract the key information that </span></div><div><span style="font-family: Arial, Helvetica, sans-serif;"><span><b style="font-weight: normal;"><br/></b></span></span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span style="background-color: white; color: black; font-family: Arial, Helvetica, sans-serif; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre;">Background and Project objectives </span></div><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li dir="ltr" style="background-color: white; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;"><span style="font-family: Arial, Helvetica, sans-serif;"><a href="https://www.investopedia.com/terms/s/stockmarket.asp" style="background-color: transparent;"><span style="color: #1155cc; vertical-align: baseline;">Stock Market</span></a><span style="color: black; vertical-align: baseline;"> refers to the collection of markets and exchanges where the issuing and trading of equities, bonds and other sorts of securities takes place</span></span></li><li><span style="font-family: Arial, Helvetica, sans-serif;">Social media, such as Twitter, often reflects how people think about a company and therefore can be used as an indicator of the changes of stock price in the near future.</span></li><li><span style="font-family: Arial, Helvetica, sans-serif;">Traditionally, analytics use statistical model built on past stock prices and recent news to forecast stock prices. We would like apply Machine Learning and Natural Language Processing models on social media to see if it has enough information for us to make good prediction of future stock price.</span></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"/><div><span style="font-family: Arial, Helvetica, sans-serif;"><span><b style="font-weight: normal;"><br/></b></span></span></div><span style="font-family: Arial, Helvetica, sans-serif;"><b>Proposed methodologies</b><span style="white-space: pre;"><b><br/></b></span></span><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><b><span style="font-family: Arial, Helvetica, sans-serif;">Dataset:</span></b></li><ul><li><span style="font-family: Arial, Helvetica, sans-serif;"><span style="background-color: white; color: black; vertical-align: baseline; white-space: pre;">Twitter data: </span><span style="background-color: white; color: #1155cc; vertical-align: baseline; white-space: pre;"><a href="https://developer.twitter.com/en/docs">https://developer.twitter.com/en/docs</a></span></span></li></ul></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul><li><span style="font-family: Arial, Helvetica, sans-serif;">Preprocess twitter data:</span></li></ul></ul><ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul><li><span style="font-family: Arial, Helvetica, sans-serif;">Tokenization</span></li></ul></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul><li><span style="font-family: Arial, Helvetica, sans-serif;">Stemming</span></li></ul></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul><li><span style="font-family: Arial, Helvetica, sans-serif;">Lemmatization</span></li></ul></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul style="margin-bottom: 0pt; margin-top: 0pt;"/></ul><li><span style="font-family: Arial, Helvetica, sans-serif;"><span style="background-color: white; color: black; vertical-align: baseline; white-space: pre;">Bloomberg financial news dataset: </span><span style="background-color: white; color: #1155cc; vertical-align: baseline; white-space: pre;"><a href="https://github.com/philipperemy/financial-news-dataset">https://github.com/philipperemy/financial-news-dataset</a></span></span></li></ul></ul><span style="font-family: Arial, Helvetica, sans-serif;"><b>Minimal viable action plan</b></span><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span style="font-family: Arial, Helvetica, sans-serif;">Forecast companies’ stock price changes (UP, DOWN, STAY) </span></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><b><span style="font-family: Arial, Helvetica, sans-serif;">Model</span></b></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span style="font-family: Arial, Helvetica, sans-serif;">N-gram with appropriate smoothing as baseline</span></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span style="font-family: Arial, Helvetica, sans-serif;">Use RNN/LSTM/GRU as model</span></li></ul><li><b><span style="font-family: Arial, Helvetica, sans-serif;">User Interface</span></b></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span style="font-family: Arial, Helvetica, sans-serif;">Command line REPL</span></li></ul></ul><span style="font-family: Arial, Helvetica, sans-serif;"><b>Stretch goals</b></span><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span style="font-family: Arial, Helvetica, sans-serif;">We could implement LSTM/GRU model to extract important information from the text in the preprocess</span></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span style="font-family: Arial, Helvetica, sans-serif;">Forecast the approximate future stock price for a company given a future date</span></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span style="font-family: Arial, Helvetica, sans-serif;">Auto trader bot that can take streaming tweets from twitter api and update the model prediction </span></li></ul><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span style="font-family: Arial, Helvetica, sans-serif;">Fusion with 8-K reports to elevate the accuracy</span></li></ul><span style="font-family: Arial, Helvetica, sans-serif;"><b>Evaluation plan</b></span><br/><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span style="font-family: Arial, Helvetica, sans-serif;">F-1 score for (UP/DOWN)</span></li><li><span style="font-family: Arial, Helvetica, sans-serif;">Loss functions for comparing predictions and expectations.</span></li><li><span style="font-family: Arial, Helvetica, sans-serif;">Evaluate on time required to do a prediction.</span></li></ul></div><div style="margin: 0px;"><span style="font-family: Arial, Helvetica, sans-serif;"><b style="white-space: pre;">Reference</b></span><ul style="margin-bottom: 0pt; margin-top: 0pt;"><li><span style="font-family: Arial, Helvetica, sans-serif;"><span style="background-color: white; color: black; vertical-align: baseline; white-space: pre;">On the Importance of Text Analysis for Stock Price Prediction: </span><a href="https://nlp.stanford.edu/pubs/lrec2014-stock.pdf"><span style="background-color: white; color: #1155cc; vertical-align: baseline; white-space: pre;">https://nlp.stanford.edu/pubs/lrec2014-stock.pdf</span></a></span></li><li><span style="font-family: Arial, Helvetica, sans-serif;"><span style="color: black; vertical-align: baseline; white-space: pre-wrap;">Stock Trend Prediction Using News Sentiment Analysis: </span><a href="https://arxiv.org/pdf/1607.01958.pdf"><span style="color: #1155cc; vertical-align: baseline; white-space: pre-wrap;">https://arxiv.org/pdf/1607.01958.pdf</span></a></span></li></ul><div style="font-family: times;"/></div></div></div>
    </summary>
    <updated>2018-04-11T06:10:00Z</updated>
    <author>
      <name>Team Overfit</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-9203775015655831448</id>
      <author>
        <name>Team Overfit</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="https://teamoverfit.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="https://teamoverfit.blogspot.com/feeds/posts/default?alt=rss" rel="self" type="application/rss+xml"/>
      <title>NLP Capstone</title>
      <updated>2018-04-11T09:00:14Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/43368563cf97</id>
    <link href="https://medium.com/nlp-capstone-blog/machine-dictionary-43368563cf97?source=rss----9ba3897b6688---4" rel="alternate" type="text/html"/>
    <title>Machine Dictionary</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We have decided to call our project, the Machine Dictionary. Formally, the goal of this project is to use a large corpus of data to generate definitions for technical terms that are consistent with how those terms are explored in the corpus.</p><h4>Motivation</h4><p>Our aim is to explore novel text generation approaches and apply them to the specific task of generating definitions. One of these techniques includes a specific approach we have termed “Connecting the Dots”. This approach will allow us to loosely structure the definitions such that they contain meaningful content but also keep the model general enough to generate appropriate context. We will discuss this approach further down. Another large motivation is the amount of research paper data we have from the AI2 Semantic Scholar corpus. This will allow us to use domain specific corpora to support definitions.</p><h4>Prior Work</h4><p>Although text generation has been a hot topic in NLP research for a while, we did not discover any prior attempts to generate definitions for technical terms. That said, the text generation task has itself been explored in great detail by many others.</p><p>In <a href="https://arxiv.org/pdf/1707.05501.pdf"><strong>this paper</strong> (Jain et al., 2017)</a>, the authors explore the task of generating short stories given a sequence of independent short descriptions. They approach the problem by using an Encoder-Decoder model to connect the descriptions and the short stories. This method might help us generate short text, but in our case, the input would be a large amount of data in the domain.</p><p>In this <a href="https://pdfs.semanticscholar.org/9dad/f5bb0a2182b1509c5ea60d434bb35d4701c1.pdf?_ga=2.15851958.1083977791.1523309085-1136887644.1523309085">other paper (Ghazvininejad et. al, 2016)</a>, the authors explore generating poetry based on topics. This paper is relevant to our project for several reasons. For instance, the paper generates poems based on a given topic, much like our goal which is to generate definitions based on a given term. In addition, the authors generate poetry by taking advantage of the structure of Shakespearean sonnets such as the unique rhyme scheme and the iambic pentameter cadence. We can use the techniques proposed in the paper to selectively choose information from the training corpus, based on the term we are asked to define and how we believe definitions should be structured.</p><h4>Minimum Viable Plan</h4><p>To reiterate, our model should be able to generate definitions that are based on context received from a large corpus. In this MVP, we can make the simplification that our model should generate definitions of a fixed length (for example, 5 sentences). These definitions should be grammatical and technically correct.</p><h4>Baseline Approach and Evaluation</h4><p>Andrej Karpathy, a former PhD student at Stanford, explores the incredible effectiveness of RNNs in in his blog. The article, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of RNNs</a>, essentially claims that RNNs have an uncanny ability to learn the structure of training data, such as Wikipedia Articles, Shakespeare, and even code. In our baseline approach, we take advantage of this ability by training an RNN language model on the training data. At testing time, we can provide the term we seek to define as a seed to the RNN, which can then generate text until we hit a word limit or the RNN generates the &lt;STOP&gt; character. We can evaluate this approach by using perplexity, to ensure that we have a good language model. We can also cross-check definitions with other sources like Wikipedia articles. Finally, it might also be prudent to have humans evaluate the definitions.</p><h4>Target Approach I and Evaluation</h4><p>We propose two different target approaches to this model. In this first approach, we utilize techniques from work done previously in abstractive summarization. Given a term, we could filter the input data on all sentences associated with that term. We could obtain all the sentences that contain the data and a few sentences in the nearby surroundings, which could capture the context for the data. We would then use these sentences to generate a summary, which we would call the definition of the term. In this approach, we might use an attention mechanism to focus on the most important parts of the input. In terms of evaluation, we would use the cross referencing method from above, where we take the produced definition and the “correct definition” as determined by an external source and compare the number of common words.</p><h4>Target Approach II and Evaluation</h4><p>In this second approach, we explore a concept we have chosen to call “Connecting the Dots”. In this approach, we structure the definition generation by using key words. To elaborate, each term might be closely connected to a certain number of other words which could influence the definition of the term greatly. Consider the term <strong>osteoporosis</strong>. This term might be closely associated with the words <strong>bones, degrade, bone degradation, fractures, </strong>and <strong>women. </strong>We could use these words to structure a definition for <strong>osteoporosis </strong>as a fill in the blank task.</p><p><strong>Osteoporosis is … bones … degrade … bone degradation … fractures … women.</strong></p><p>In the above structure, we would rely on the model to appropriately and grammatically fill in the context between each keyword. As a result, we might generate the following definition:</p><p><strong>Osteoporosis </strong>is a disease that cause <strong>bones </strong>to <strong>degrade.</strong> <strong>Bone degradation </strong>often leads to <strong>fractures.</strong> <strong>Osteoporosis </strong>most commonly affects <strong>women.</strong></p><p>It’s important to note that when we fill in the context between keywords, we must condition on the original term that we are defining. For example, between the words <strong>fracture </strong>and <strong>women</strong>, there might be several sentences we could generate, but we must keep in mind how the keywords are related given that they are about <strong>osteoporosis.</strong></p><p>In this approach, we will build on the neural network model and add task-specific architecture to capture relationships between words. We hope that the model can learn how to define keywords associated with terms and use those terms to structure a definition.</p><p>In terms of evaluation, we introduce another technique. In this evaluation method we take a paragraph in which the technical term appears and omit the term. If we rephrase the problem as a classification task and ask the model to predict the omitted term, we would be able to conclude whether the model has a contextual understanding of the technical term. This evaluation technique would be a good supplement to the human evaluation method where we request users to rank how correct and readable the definitions are.</p><h4>Stretch Goals</h4><p>The ideal goal would be to generate text without any constraints on length, order, or keyword usage. This would be a more “hands-off” approach to text generation and could also allow us to train on different domain based corpora. We might be able to achieve this stretch goal if we perform well on the goals outlined in the Minimum Viable Plan.</p><p>Another stretch goal has to do with ontology matching, whereby we compare two definitions to determine whether they describe the same concept. We could extend this example to generate definitions for all technical terms across a body of research papers, determine which terms are defined similarly in different papers, and unify the terminology across all papers. This goal is definitely a stretch goal, but if we can perfect the architecture for generating definitions, we see this as a future application of our project.</p><h4>Data</h4><p>We plan to use the Semantic Scholar Open Research Corpus for this project. This corpus consists of over 20 million research papers classified into two domains (Computer Science and Medicine). Depending on the approach we take to solve this task, we would filter the data and train accordingly.</p><h4>Resources and Literature Survey</h4><p>We have mentioned several resources above that were the most useful for formulating the project proposal. Here are those resources and a few other resources that we think might be useful in the future.</p><p><strong>Text Generation Techniques</strong></p><p><a href="https://pdfs.semanticscholar.org/797d/7d968b88d5b5dd7c3271d08acd7296950d41.pdf?_ga=2.73597202.1083977791.1523309085-1136887644.1523309085">Using Lexical Chains for Text Summarization (Barzilay et. al, 1997)<br/></a><a href="https://pdfs.semanticscholar.org/9dad/f5bb0a2182b1509c5ea60d434bb35d4701c1.pdf?_ga=2.15851958.1083977791.1523309085-1136887644.1523309085">Generating Topical Poetry (Ghazvininejad et. al, 2016)</a></p><p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of RNNs</a></p><p><a href="https://arxiv.org/pdf/1707.05501.pdf">Story Generation from Sequence of Independent Short Descriptions (Jain et al., 2017)</a></p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=43368563cf97" width="1"/><hr/><p><a href="https://medium.com/nlp-capstone-blog/machine-dictionary-43368563cf97">Machine Dictionary</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>
    </content>
    <updated>2018-04-11T06:03:31Z</updated>
    <category term="projects"/>
    <category term="machine-learning"/>
    <author>
      <name>Karishma Mandyam</name>
    </author>
    <source>
      <id>https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4</id>
      <logo>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</logo>
      <link href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/nlp-capstone-blog" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>A Journey Through CSE 481N, the Natural Language Processing Capstone Course at the University of Washington - Medium</subtitle>
      <title>NLP Capstone Blog - Medium</title>
      <updated>2018-04-11T09:00:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://cse481n-capstone.azurewebsites.net/?p=37</id>
    <link href="http://cse481n-capstone.azurewebsites.net/2018/04/10/formal-proposal/" rel="alternate" type="text/html"/>
    <title>Formal Proposal</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Motivations: We want to create novel models for determining if the text is offensive, and why that text is offensive. To do this we want to create a new dataset that makes this task easier. We hope that our dataset and models pave the way for further innovations by others, as well as better trained … <a class="more-link" href="http://cse481n-capstone.azurewebsites.net/2018/04/10/formal-proposal/">Continue reading<span class="screen-reader-text"> "Formal Proposal"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h3><span style="font-weight: 400;">Motivations:</span></h3>
<p><span style="font-weight: 400;">We want to create novel models for determining if the text is offensive, and why that text is offensive. To do this we want to create a new dataset that makes this task easier. We hope that our dataset and models pave the way for further innovations by others, as well as better trained conversational agents that have a better understanding of what they should or should not say. We’re going to teach them how to watch their language!</span></p>
<p><span style="font-weight: 400;">We would like to correctly classify sentences that keywords matching cannot achieve. For example:</span></p>
<p><b><i>What do you call an adult that has imaginary friends? Religious</i></b></p>
<p><span style="font-weight: 400;">And we would like to tell the reason why the sentence above is bad as well.</span></p>
<h3><span style="font-weight: 400;">Minimal Viable Plan:</span></h3>
<p><span style="font-weight: 400;">By comparing the similarity of content phrases found in r/MeanJokes posts and posts all over Reddit, we hope to create a large, high-quality dataset for training models to detect offensive text. We want to create this dataset and use crowdsourcing to label it. The labels should say if the text was offensive, and if it was then was it an attack against a particular group, what group that was, as well as the reasoning for why the labeler labeled the text this way. </span></p>
<p><span style="font-weight: 400;">While we wait for our data to be labeled, we want to start by creating baseline models on existing datasets, such as Twitter Hate Speech, Wiki Detox, and Stanford Politeness. We think that these datasets are similar enough to begin work on classifiers that don’t make use of deep annotation. After this, we can start work on improving performance on these datasets up until our crowdsourcing completes. We will explore novel models on existing datasets and try to improve their performance. </span></p>
<h3><span style="font-weight: 400;">Stretch Goals:</span></h3>
<p><span style="font-weight: 400;">Once our new Reddit dataset is fully labeled, we want to test the existing models that we made on the other datasets and continue improving them. We also want to use the new data to experiment with Q&amp;A or Deep Annotation models for creating a model that knows why a particularly offensive post is offensive. </span></p>
<p><span style="font-weight: 400;">In case if we can’t receive labeled dataset on time, we will continue to make improvements to novel models on existing datasets.</span></p>
<h3><span style="font-weight: 400;">Evaluation Plan: </span></h3>
<p><span style="font-weight: 400;">Classifier Models: Precision, Recall, F1 score</span></p>
<p><span style="font-weight: 400;">Rationale Models: deeper comparison to crowdsourced label explanations</span></p>
<p><span style="font-weight: 400;">Dataset: Random Sampling + Human Judgement</span></p>
<h3><span style="font-weight: 400;">Existing Work: </span></h3>
<h5><span style="font-weight: 400;">Previous Capstone Project: </span></h5>
<h5><a href="https://michael0x2a.github.io/nlp-capstone/"><span style="font-weight: 400;">Team Inverted Cat</span></a></h5>
<h5><span style="font-weight: 400;">Datasets: </span></h5>
<p><a href="https://github.com/ZeerakW/hatespeech"><span style="font-weight: 400;">Hate Speech Twitter Annotations</span></a><span style="font-weight: 400;"> (Waseem et al. 2016)</span></p>
<p><a href="https://github.com/t-davidson/hate-speech-and-offensive-language"><span style="font-weight: 400;">Hate Speech and Offensive language dataset </span></a><span style="font-weight: 400;"> (Davidson et al. 2017)</span></p>
<p><a href="https://meta.wikimedia.org/wiki/Research:Detox/Data_Release"><span style="font-weight: 400;">Wikipedia Talk Corpus </span></a><span style="font-weight: 400;"> (Wulczyn et al. 2017)</span></p>
<p><a href="http://www.cs.cornell.edu/~cristian//Politeness.html"><span style="font-weight: 400;">Stanford Politeness Corpus</span></a></p>
<p><a href="https://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/"><span style="font-weight: 400;">A list of bad words</span></a></p>
<p><span style="font-weight: 400;">Pre-trained word embeddings: GloVe, Facebook FastText, Google Word2Vec</span></p>
<h5><span style="font-weight: 400;">Papers: </span></h5>
<p><a href="https://www.semanticscholar.org/paper/Hateful-Symbols-or-Hateful-People%3F-Predictive-for-Waseem-Hovy/df704cca917666dace4e42b4d3a50f65597b8f06"><span style="font-weight: 400;">Waseem, Zeerak and Dirk Hovy. “Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.” SRW@HLT-NAACL (2016).</span></a></p>
<p><a href="https://www.semanticscholar.org/paper/Automated-Hate-Speech-Detection-and-the-Problem-of-Davidson-Warmsley/6ccfff0d7a10bf7046fbfd109b301323293b67da"><span style="font-weight: 400;">Davidson, Thomas J et al. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM (2017).</span></a></p>
<p><a href="https://www.semanticscholar.org/paper/Hate-Speech-Detection-with-Comment-Embeddings-Djuric-Zhou/c9948f7213167d65db79b60381d01ea71d438f94"><span style="font-weight: 400;">Djuric, Nemanja et al. “Hate Speech Detection with Comment Embeddings.” </span><i><span style="font-weight: 400;">WWW</span></i><span style="font-weight: 400;">(2015).</span></a></p>
<p><a href="https://www.semanticscholar.org/paper/Using-Convolutional-Neural-Networks-to-Classify-Gamb%C3%A4ck-Sikdar/0dca29b6a5ea2fe2b6373aba9fe0ab829c06fd78"><span style="font-weight: 400;">Gambäck, Björn and Utpal Kumar Sikdar. “Using Convolutional Neural Networks to Classify Hate-Speech.” (2017).</span></a></p>
<p><a href="https://www.semanticscholar.org/paper/Abusive-Language-Detection-in-Online-User-Content-Nobata-Tetreault/e39b586e561b36a3b71fa3d9ee7cb15c35d84203"><span style="font-weight: 400;">Nobata, Chikashi et al. “Abusive Language Detection in Online User Content.” </span><i><span style="font-weight: 400;">WWW</span></i><span style="font-weight: 400;">(2016).</span></a></p>
<p><a href="https://www.semanticscholar.org/paper/Ex-Machina%3A-Personal-Attacks-Seen-at-Scale-Wulczyn-Thain/4a7204431900338877c738c8f56b10a71a52e064"><span style="font-weight: 400;">Wulczyn, Ellery et al. “Ex Machina: Personal Attacks Seen at Scale.” </span><i><span style="font-weight: 400;">WWW</span></i><span style="font-weight: 400;"> (2017).</span></a></p></div>
    </content>
    <updated>2018-04-10T22:37:31Z</updated>
    <category term="Weekly blog"/>
    <author>
      <name>Team Watch Your Language!</name>
    </author>
    <source>
      <id>http://cse481n-capstone.azurewebsites.net</id>
      <link href="http://cse481n-capstone.azurewebsites.net/feed/" rel="self" type="application/rss+xml"/>
      <link href="http://cse481n-capstone.azurewebsites.net" rel="alternate" type="text/html"/>
      <subtitle>Spring2018 CSE481N Capstone</subtitle>
      <title>Team Watch Your Language!</title>
      <updated>2018-04-11T09:00:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/a1903faeadb7</id>
    <link href="https://medium.com/@ryanp97/project-proposal-neural-machine-translation-with-semantic-transfer-a1903faeadb7?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
    <title>Project Proposal — Neural Machine Translation with Semantic Transfer</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>State-of-the-art neural machine translation does not currently utilize much, if any, semantic information, meaning it misses out on a large amount of potentially useful information indirectly embedded in the sentence. This project aims to explore the benefits that semantic transfer could offer to neural machine translation.</p><p>In particular, this project will focus on Dependency Minimal Recursion Semantics and translation between English DMRS and Japanese DMRS. Currently, I’m working on this project alone under the supervision of Jan Buys.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AlAZF1O2uYt4b12hCu7ajA.png"/>An example of English and Japanese DMRS graphs for the above sentence. Note the sentences have the same meaning.</figure><h4>Minimal Viable Action Plan</h4><ol><li>Obtain a parallel corpus.</li><li>Parse DMRS graphs from the parallel corpus</li><li>Simplify graphs (and be able to recover them in a robust manner to handle the model’s output)</li><li>Train a seq2seq model to predict a Japanese DMRS graph given an English DMRS graph as well as the required embeddings</li></ol><p>I’ve chosen the <a href="http://www.edrdg.org/wiki/index.php/Tanaka_Corpus">Tanaka Corpus</a> as my parallel corpus and cleaned it such that the only remaining sentences are English and Japanese sentences which belong to a translation pair. From there, I’ve parsed the corresponding DMRS graphs. For further details on how I cleaned the corpus and parsed the graphs, refer to my <a href="https://medium.com/@ryanp97/project-logistics-and-package-exploration-3d3651220219">last</a> blog post.</p><p>Currently, I’m working on how to simplify and recover the graphs robustly. Simplifying the graph seems fairly easy; however, I made some incorrect assumptions about the graphs, so recovery is currently a work in progress. Furthermore, I have yet to handle the robustness aspect. In general, I need to handle the cases where the model does not output a valid “simplified” graph. This includes, but is not limited to: handling mismatched/missing parentheses.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LdgExRT9YoJ-UwxYxwJfsA.png"/>On the left is the English DMRS and on the right is the Japanese DMRS. Each example is labeled with a sentence ID and the original sentence (prefixed with `#` which Smatch ignores). The model will not have to predict these comments. However, in the future, we’d like to extend the model such that it would be able to recover the original sentence after generating a DMRS graph in the target language.</figure><p>From then, the last step in the minimal viable action plan is to train and evaluate model. Ideally, we would train the embeddings for the tokens in the graphs and the seq2seq model end-to-end with <a href="https://amr.isi.edu/eval/smatch/tutorial.html">Smatch</a> as the objective function, though I am still working with Jan on how I should piece this model together.</p><h4>Stretch Goals</h4><ol><li>Explore alternatives to standard seq2seq models (i.e. TreeLSTMs, custom architecture, etc.)</li><li>Expand/supplement dataset with <a href="https://alaginrc.nict.go.jp/WikiCorpus/index_E.html">Kyoto Corpus</a></li></ol><p>Since we are expecting the model to learn and generate graphs, it would be ideal if we were able to use a tree-like structure that would more accurately represent the data. If I am able to finish the minimal viable action plan with enough time left over, I would like to experiment and test out different architectures and see how that affects the model’s performance. There are a couple different packages for TreeLSTMs (such as <a href="https://github.com/dasguptar/treelstm.pytorch">this</a> one) that would potentially make this task not too difficult. Though, eventually I would like to customize an architecture for this task.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/496/1*omSJj6AYg7TDPUW0XPY6ew.png"/>A figure of a TreeLSTM grabbed from the <a href="https://arxiv.org/pdf/1503.00075.pdf">original</a> paper on TreeLSTMs.</figure><p>Each example in the Tanaka Corpus is a single sentence, making it easy to work with in terms of development and debugging. However, if I have extra time, I would like to supplement the Tanaka Corpus with the Kyoto Corpus which is comprised of translated Wikipedia articles. The corpus has much longer examples and provides a much more complex and realistic setting. An issue is that the documentation is in Japanese, and it is not entirely clear which of the different translations is “correct” as each example has a “primary,” “secondary,” and “check” translation. You can read more about it on the Kyoto Corpus website, hyperlinked above.</p><h4>Evaluation Plan</h4><p>There is not really a good statistical model, nor are there results that are very comparable due to the scope of the project. Since the scope of the project does not include recovering the sentence from a generated DMRS graph, comparisons between this project and papers that do tree-to-tree machine translations don’t really make much sense.</p><p>I expect to have some time leftover to explore different architectures, so I hope to make the seq2seq results the baseline for comparison for other architectures I have time to try.</p><h4>Related Work</h4><p><a href="https://arxiv.org/pdf/1704.08381.pdf">Neural AMR: Sequence-to-Sequence Models for Parsing and Generation</a> is fairly relevant to this project in the sense that it has similar motivations for working with semantics in NLP. The paper covers how they overcame a lack of labeled data, achieved competitive results for both parsing AMR graphs and generating text from AMR graphs, as well as extensive ablation studies and analysis. Their graph preprocessing steps are extremely relevant to this project, and I will likely be referencing their paper and <a href="https://github.com/sinantie/NeuralAmr">codebase</a> often as I work on cleaning and preprocessing the Tanaka Corpus.</p><p><a href="https://arxiv.org/pdf/1503.00075.pdf">Improved Semantic Representations From Tree-Structured LSTMs</a> provides a good basis for my stretch goals. The Child-Sum TreeLSTM units seem like a good option to test for representing the DMRS graph as any given node can take any number of children. These children can be used to represent dependencies as outlined in the paper, but it seems it might be possible to have the properties of a head word as children of the corresponding node as well. There’s a couple other useful ideas that come from the paper, but for the sake of brevity, we’ll leave it at this.</p><p><a href="https://arxiv.org/pdf/1603.06075.pdf">Tree-to-Sequence Attention Neural Machine Translation</a> is a slight glimpse at the future of this project. If this project is successfully able to transfer semantic information between languages, the next step would make the model complete the entire cycle, sequence-to-graph-to-graph-to-sequence, in hopes of achieving better results than statistical machine translation.</p><p>Additionally, here is a short list of other relevant papers:</p><ul><li><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a></li><li><a href="https://arxiv.org/pdf/1704.07092.pdf">Robust Incremental Neural Semantic Graph Parsing</a></li><li><a href="http://www.aclweb.org/anthology/P13-2131">Smatch: an Evaluation Metric for Semantic Feature Structures</a></li></ul><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a1903faeadb7" width="1"/></div>
    </content>
    <updated>2018-04-10T05:31:27Z</updated>
    <category term="machine-learning"/>
    <author>
      <name>Ryan Pham</name>
    </author>
    <source>
      <id>https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/1*83KfTWByl5pPq7A8_E8ApA.gif</logo>
      <link href="https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@ryanp97" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Ryan Pham on Medium</subtitle>
      <title>Stories by Ryan Pham on Medium</title>
      <updated>2018-04-11T09:00:05Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-5600014144802012716.post-8898628104121215850</id>
    <link href="https://nlpcapstonesemparse.blogspot.com/2018/04/blog-3-formal-proposal.html" rel="alternate" type="text/html"/>
    <title>Blog 3: Formal Proposal</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><b style="font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;">Minimal viable action plan </b><br/><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">Implement the model from the java paper mentioned in the previous blog posts. This includes the variable and method camel case encoding, the two step attention, the type constrained decoding, and many other tasks such as preprocessing, evaluation metrics etc.</span></span><br/><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;"><br/></span></span><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>Stretch goals</b></span><br/><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;">This depends on my error analysis on the mvp, but here are a couple ideas.</span><br/><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>1. </b>Incorporate implementation specific encoding. The encoder just uses the method names, but it'd be interesting to also include the method implementations.</span><br/><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;"><b>2. </b>More type constraints on the decoder. Currently if the decoder wants to generate a variable, there's nothing to check that the variable was previously declared.</span></span><br/><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><br/></span><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>Project objectives</b></span><br/><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">Reproduce the strong paper baseline. </span></span><br/><br/><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>Proposed methodologies</b> </span><br/><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">Implement the model from the paper. Potentially experiment with other semantic parsing task architectures and see if they also perform competitively with the baseline, for example seq2seq.</span></span><br/><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;"><br/></span></span><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>Available resources</b></span><br/><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">The dataset and allennlp.</span></span><br/><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b><br/></b></span><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>Evaluation plan</b></span><br/><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;">Test on the test set and measure bleu and exact match metrics.</span><br/><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">Test if this is useful by coming up with a couple of real classes that I've written and see if it generates the method.</span></span><br/><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">Perhaps come up with a new metric, such as a binary executability metric (stretch goal).</span></span><br/><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b><br/></b></span><span style="background-color: white; font-family: Palatino, Arial, helvetica, sans-serif; font-size: 14.44px;"><b>Literature survey</b></span><br/><span style="font-family: Palatino, Arial, helvetica, sans-serif;"><span style="background-color: white; font-size: 14.44px;">The type constrained architecture was used in a number of recent papers such as "</span><span style="font-size: 14.44px;">A syntactic neural model for parsing natural language to executable code". The dataset is novel in that previous ones haven't used programmatic contexts and have focused on nl2code pairs.</span></span></div>
    </summary>
    <updated>2018-04-10T03:21:00Z</updated>
    <author>
      <name>nlpcapstone</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-5600014144802012716</id>
      <author>
        <name>nlpcapstone</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="https://nlpcapstonesemparse.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="https://nlpcapstonesemparse.blogspot.com/feeds/posts/default?alt=rss" rel="self" type="application/rss+xml"/>
      <title>NlpCapstone</title>
      <updated>2018-04-11T09:00:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/45c89bec2c2e</id>
    <link href="https://medium.com/@hongnin1/movie-sentiment-summarization-project-proposal-45c89bec2c2e?source=rss-c450eb982161------2" rel="alternate" type="text/html"/>
    <title>Movie Sentiment Summarization — Project Proposal</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Team: Ning Hong, Zhuchun Liu, Sujie Zhou</p><p>Overview: our model will be able to summarize the reviews for the input movies. The summarization of the movie include how the audience feel about the movie and what is the overall rating for the movie, for example, given a movie title, our model should be able to produce something like this: &lt;movie title&gt; is violet but good, most people think this movie is 6/10.</p><p>If time permits, we would like to improve our model such that it can also output a more detailed overall review for the movie instead of simple sentences, for example, given a movie title as input, our model should output: “&lt;movie title&gt; got my full attention from beginning to end. I couldn’t turn away. I didn’t want to turn away. For me, that’s extremely rare, I would give it 8/10.”</p><p>Another stretch goal for our model is to be able to detect sentiment not only in the US market, but also in China market by using data from DouBan (one of the largest movie review site for China), and compare the sentiment between US and China for a certain movie, for example, given an input movie title, our model can output something like: &lt;movie title&gt; was generally perceived more positively in the US than in China, the Chinese audience mostly felt uncomfortable about its violent and explicit content whereas more American audience appreciated the bloodiness of the film.</p><p>Model: we are going to use basic encoder- decoder RNN that serves as our baseline and then propose several novel models for summarization, each addressing a specific weakness in the base- line such as encoder-decoder RNN with attention and large vocabulary trick, capturing keywords using feature-rich encoder, modeling rare/unseen words using switching generator-pointer, and capturing hierarchical document structure with hierarchical attention.</p><p>This project has a lot of potential uses. Not only can we use movie dataset to output movie sentiment summarization, if given restaurant review dataset (Yelp), we can also output summarization about how customers feel about a restaurant.</p><p>On the other hand, there might be some difficulties we will be facing: we are planning to use sequence2sequence model to generate summarization, it is hard to determine how good is the output we are generating; more research needs to be done for this problem.</p><p>Resources:</p><p>Data scraping blogpost:</p><p><a href="https://www.dataquest.io/blog/web-scraping-beautifulsoup/">An intermediate tutorial</a></p><p>Possible IMDb training data:</p><p><a href="https://github.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset">SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset</a></p><p>Model paper:</p><a href="https://medium.com/media/1ff2a893f361e9bda813a32d4e89c5f6/href">https://medium.com/media/1ff2a893f361e9bda813a32d4e89c5f6/href</a><p>Model github:</p><p><a href="https://github.com/thunlp/TensorFlow-Summarization">thunlp/TensorFlow-Summarization</a></p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=45c89bec2c2e" width="1"/></div>
    </content>
    <updated>2018-04-10T02:54:02Z</updated>
    <category term="data-science"/>
    <author>
      <name>Ning Hong</name>
    </author>
    <source>
      <id>https://medium.com/@hongnin1?source=rss-c450eb982161------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/0*d6QC_ngideag3rTN.</logo>
      <link href="https://medium.com/@hongnin1?source=rss-c450eb982161------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@hongnin1" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Ning Hong on Medium</subtitle>
      <title>Stories by Ning Hong on Medium</title>
      <updated>2018-04-11T09:00:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://deeplearningturingtest.wordpress.com/?p=12</id>
    <link href="https://deeplearningturingtest.wordpress.com/2018/04/06/warm-up-testing-a-codebase/" rel="alternate" type="text/html"/>
    <title>Warm Up: Testing a Codebase</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I installed both the Tensorflow and Pytorch API’s since I’m not sure which framework I will use yet. Then, I downloaded the following repository which implemented Hybrid Code Networks for Dialog State tracking in its respective research paper. Code base URL: https://github.com/voicy-ai/DialogStateTracking Research Paper URL: https://www.semanticscholar.org/paper/Hybrid-Code-Networks%3A-practical-and-efficient-with-Williams-Asadi/0645905d70caf180433145be09c9af266a85c863 Their implementation uses Keras (built on Tensorflow) to build … <a class="more-link" href="https://deeplearningturingtest.wordpress.com/2018/04/06/warm-up-testing-a-codebase/">Continue reading <span class="screen-reader-text">Warm Up: Testing a Codebase</span> <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I installed both the Tensorflow and Pytorch API’s since I’m not sure which framework I will use yet. Then, I downloaded the following repository which implemented Hybrid Code Networks for Dialog State tracking in its respective research paper.</p>
<p>Code base URL: <a href="https://github.com/voicy-ai/DialogStateTracking" rel="nofollow">https://github.com/voicy-ai/DialogStateTracking</a></p>
<p>Research Paper URL: <a href="https://www.semanticscholar.org/paper/Hybrid-Code-Networks%3A-practical-and-efficient-with-Williams-Asadi/0645905d70caf180433145be09c9af266a85c863" rel="nofollow">https://www.semanticscholar.org/paper/Hybrid-Code-Networks%3A-practical-and-efficient-with-Williams-Asadi/0645905d70caf180433145be09c9af266a85c863</a></p>
<p>Their implementation uses Keras (built on Tensorflow) to build the network. The model stores a predetermined set of action templates to execute based on what the user requests. By feeding in features like the previous action taken, a bag of words vector, an entity tracking feature vector, etc. their RNN outputs a softmax distribution over the possible action templates. The action taken is the one with the highest probability. Because the conversation is restricted to a particular domain such as searching for a restaurant, the model performed well when I ran and tested it. The model generally recognized the type of request I was making, but its responses were extremely robotic and towards the end, gave me yes or no questions to answer to narrow down what action it should take. My goal is to generalize the type of information the model can store between time steps to be able to provide responses for requests outside of a restricted domain (like searching for a restaurant in this case).</p></div>
    </content>
    <updated>2018-04-06T06:58:40Z</updated>
    <category term="Uncategorized"/>
    <author>
      <name>ananthgo</name>
    </author>
    <source>
      <id>https://deeplearningturingtest.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://deeplearningturingtest.wordpress.com/feed/" rel="self" type="application/rss+xml"/>
      <link href="https://deeplearningturingtest.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://deeplearningturingtest.wordpress.com/osd.xml" rel="search" title="NLP Capstone Project Updates - Ananth" type="application/opensearchdescription+xml"/>
      <link href="https://deeplearningturingtest.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>NLP Capstone Project Updates – Ananth</title>
      <updated>2018-04-11T09:00:12Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/96fb908765f5</id>
    <link href="https://medium.com/@halden.lin/nlp-capstone-02-getting-started-96fb908765f5?source=rss-2759d54493c0------2" rel="alternate" type="text/html"/>
    <title>NLP Capstone | 02: Getting Started</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5">previous post</a></p><p>Alright, it’s been only 2 days since my last entry, so this will be a relatively short post. The direction I proposed in <strong>Option 1 </strong>of that post was towards a more robust, interpretable, and informative visualization of attention, particularly in the context of text summarization. A quick recap:</p><blockquote>Perhaps interaction can be used to create a more insightful and interpretable visualization framework for understanding attention. For example, text heat-maps are already used widely to visualize sentiment analysis.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*lsgeuBXGGBog4YkuQNgJVw.png"/>Lin et al. (2017) [6]. Visualization of sentiment analysis on a token-by-token basis.</figure><blockquote>In a static context, using this method for attention would require repeat of the same input sequence for each word in the output sequence. Using interaction, however, a model creator could brush over single or sequences of words in the output sequence to view corresponding soft-alignment in the input sequence. Aggregate visualizations could be shown to supplement this view (either aggregates over a particular input / output sequence, or aggregates over all input / output sequences).</blockquote><p>I’m currently working on laying out the groundwork for such a project. Task 1: implement a model. Without one, there’s no data to visualize!</p><p>With that in mind, here’s what I’ve been up to:</p><h4>Finding a Text Summarization Dataset</h4><p>A quick survey of recent research papers [1–5] on text summarization points, as well as online forums, points to three commonly used datasets.</p><ol><li><a href="https://cs.nyu.edu/~kcho/DMQA/">CNN/Daily Mail Corpus</a>. A collection of articles and their bullet point summaries, with each bullet split for Q/A purposes. <a href="https://github.com/abisee/cnn-dailymail">A script</a> [1] can be ran over the original dataset to restore the original bullet point summaries, to be used as a summarization corpus.</li><li><a href="https://www-nlpir.nist.gov/projects/duc/data.html">DUC Corpus</a>. In particular, DUC 2003 and DUC 2004. These contain a collection of documents, each accompanied by a short (~10 word) summary. There is also a longer summary for each cluster of documents.</li><li><a href="https://catalog.ldc.upenn.edu/ldc2003t05">Gigaword Corpus</a>. An annotated collection of millions of documents. The summarization task here would be to predict the headline of each [5]</li></ol><p>The accessibility of the <strong>CNN/Daily Mail Corpus</strong> (a process is required for the other two), in addition to the prevalence of projects that used it as a primary dataset [1, 2, 4], made it the most attractive option. The relatively longer summaries (~4 bullet points as opposed a short blurb in the other two datasets) also lends itself conveniently to the case of an interactive visualization with multi-token selection (e.g. select a whole bullet point and see where it attended). For a baseline, this will be my dataset!</p><h4>Identifying a Baseline Model</h4><p>See et al. (2017) [1] lay out a seq2seq attentional model as their baseline (a bidirectional LSTM). I’ll be using this as a baseline model with which to obtain data.</p><h4>Getting Some Code Up</h4><p>I’ll be using <a href="http://pytorch.org/">PyTorch</a> and the <a href="http://allennlp.org/">AllenNLP</a> toolkit [7] to implement my NN models. These are both ready to go on both my machine and Azure. I’m currently in the process of writing a DatasetReader for the dataset described above.</p><h3>Next Steps</h3><ul><li>Finish writing the DatasetReader for the CNN/Daily Mail Corpus.</li><li>Begin work on a baseline seq2seq attentional model, as described in <strong>Identifying a Baseline Model</strong></li></ul><h4>Works Cited</h4><p>[1] <a href="https://arxiv.org/pdf/1704.04368.pdf">See, Abigail et al. “Get To The Point: Summarization with Pointer-Generator Networks.” <em>ACL</em> (2017).</a></p><p>[2] <a href="https://arxiv.org/pdf/1712.06100.pdf">Hasselqvist, Johan et al. “Query-Based Abstractive Summarization Using Neural Networks.” <em>CoRR</em> abs/1712.06100 (2017): n. pag.</a></p><p>[3] <a href="https://arxiv.org/pdf/1602.06023.pdf">Nallapati, Ramesh et al. “Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.” <em>CoNLL</em> (2016).</a></p><p>[4] <a href="https://arxiv.org/pdf/1705.04304.pdf">Paulus, Romain et al. “A Deep Reinforced Model for Abstractive Summarization.” <em>CoRR</em> abs/1705.04304 (2017): n. pag.</a></p><p>[5] <a href="https://arxiv.org/pdf/1509.00685.pdf">Rush, Alexander M. et al. “A Neural Attention Model for Abstractive Sentence Summarization.” <em>EMNLP</em> (2015).</a></p><p>[6] <a href="https://arxiv.org/pdf/1703.03130.pdf">Lin, Zhouhan, <em>et al.</em>, “A structured self-attentive sentence embedding.”<em>arXiv preprint arXiv:1703.03130</em> (2017).</a></p><p>[7] <a href="https://pdfs.semanticscholar.org/a550/2187140cdd98d76ae711973dbcdaf1fef46d.pdf?_ga=2.150901366.1370831839.1522970228-1363309632.1522194596">Gardner, Matt et al. “AllenNLP: A Deep Semantic Natural Language Processing Platform.” (2017).</a></p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=96fb908765f5" width="1"/></div>
    </content>
    <updated>2018-04-06T06:53:32Z</updated>
    <category term="computer-science"/>
    <category term="visualization"/>
    <category term="machine-learning"/>
    <category term="data-visualization"/>
    <category term="nlp"/>
    <author>
      <name>Halden Lin</name>
    </author>
    <source>
      <id>https://medium.com/@halden.lin?source=rss-2759d54493c0------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/1*Hzu2ZqgloT0I1F6Kwg8OPA.jpeg</logo>
      <link href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@halden.lin" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Halden Lin on Medium</subtitle>
      <title>Stories by Halden Lin on Medium</title>
      <updated>2018-04-11T09:00:14Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/278789e4d04a</id>
    <link href="https://medium.com/@viterbi.or.not/warming-up-278789e4d04a?source=rss-c522ef075bb3------2" rel="alternate" type="text/html"/>
    <title>Warming Up</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qc9b3NkzkWe1kNnqmlHukA.png"/></figure><p>In order to begin implementing our baseline model, for which we intend to duplicate the results presented in another research paper covering discussion summarization, we began by identifying software that was referenced by other papers as being useful for their implementations.</p><p>Using these leads, we decided to “warm up” by installing the software and gaining some familiarity with it. The main libraries that we identified are listed here:</p><h4><a href="https://www.nltk.org/"><strong>Natural Language Toolkit (nltk)</strong></a></h4><p>Perhaps it is no surprise that this resource ended up first on our list, but it was a clear choice to familiarize ourselves with because of the sheer variety of tools it provides. While performing an initial survey of conversation summarization papers, we discovered a reference to the TextTiling algorithm, described in <a href="http://www.aclweb.org/anthology/J97-1003">this paper</a> and referenced as a technique used in <a href="http://www.aclweb.org/anthology/D08-1081">another paper</a> about summarization. Broadly, the algorithm detects boundaries between topics in text, so it was used by this summarization paper as part of a pipeline before assigning scores to those topics representing their importance. For our baseline model, one possibility is to implement a similar pipeline, so having access to an implementation of the TextTiling algorithm would allow us to quickly implement that component and spend more time on other design decisions and implementation details. The nltk library provides a <a href="https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.texttiling">TextTiling module</a> with this functionality.</p><h4><a href="http://scikit-learn.org/stable/"><strong>SciKit-Learn</strong></a></h4><p><a href="http://www.aclweb.org/anthology/P05-1037">Another research paper</a> we found concerning the topic of conversation summarization had several sections dedicated to the task of identifying portions of a chatlog with direct relevance to one another — for example, a question asked by one contributor and answered by another contributor several messages down would be considered a pair of directly relevant sections. As part of their technique for identifying these pairs, the researchers used <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">Support Vector Machines</a> and <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">Maximum Entropy</a> models in order to determine the sections that most likely directly respond to previous portions of the conversation. SciKit-Learn provides these functionalities, and in order to familiarize ourselves with additional existing tools that might be useful in building a baseline model we have installed this tool and begun experimenting with it.</p><h4><a href="https://www.cs.waikato.ac.nz/ml/weka/"><strong>Weka Toolkit</strong></a></h4><p>Another less commonly seen method for chat summarization can be found in a more recent (in comparison to the others we have referenced) <a href="http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.8b32440f2dc771c4.323031325f414e445f43616d6572612e706466.pdf">research paper</a> which explores the usage of Multilayer Perceptrons (MLP) for the task, among several other approaches. The MLP approach in the paper is broadly composed of a feedforward neural network with more layers between the input and output layers using backpropagation to train the network and built with the Weka toolkit, a collection of machine learning algorithms that can applied to a dataset and which contains tools for developing a variety of schemes for processing data. Although the paper finds an approach using Naive Bayes to be the most effective on the GNUe archives, their MLP implementation comes in relatively close second and we think the idea is worth pursuing further. For a baseline model, we could start by working on a similar MLP system to the one in the paper using the same Weka toolkit implementation and strive to improve from there.</p><h4><a href="http://pytorch.org/"><strong>PyTorch</strong></a></h4><p>Though not referenced by any of the papers we have encountered so far, PyTorch would be a really helpful tool for us to further the explore the usage of neural network models in chat summarization, which we can use for the aforementioned MLP approach as well as apply to our stretch goal of working on a less commonly used deep learning based model.</p><p>In addition to researching libraries and tools that we might use in our project, we have begun the process of finding and enumerating datasets that might be useful for our project ideas — more to come on the datasets and other resources in our next blog post!</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=278789e4d04a" width="1"/></div>
    </content>
    <updated>2018-04-06T06:41:19Z</updated>
    <category term="nlp"/>
    <author>
      <name>Viterbi Or Not To Be</name>
    </author>
    <source>
      <id>https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/1*wiisg40Bu4z11RTWJ66mnA.png</logo>
      <link href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@viterbi.or.not" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Viterbi Or Not To Be on Medium</subtitle>
      <title>Stories by Viterbi Or Not To Be on Medium</title>
      <updated>2018-04-11T09:00:12Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3753031463594823927.post-2253300890173394060</id>
    <link href="https://cse481n.blogspot.com/2018/04/blog-post-2.html" rel="alternate" type="text/html"/>
    <title>Blog Post #2</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>We’ve mostly settled on working on a single document summarization task. We want to pick a type of document to work on summarizing, although we haven’t decided on one specific category yet. </div><br/> <div>While we narrow down the details of the project, we have been reading a number of papers and other resources to become more familiar with the subject. We have setup PyTorch on our machines, which we are both familiar with, as well as Tensorflow, which we are still playing around with. We’ve found some interesting repositories on GitHub related to SDS that we are trying out: </div> <br/> <div><a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss">https://github.com/tensorflow/models/tree/master/research/textsum</a><br/> <a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss">https://github.com/gregdurrett/berkeley-doc-summarizer</a><br/> <a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss">https://github.com/chakki-works/sumeval</a><br/> <a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss">https://github.com/ceteri/pytextrank</a><br/> <a href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss">https://github.com/adamfabish/Reduction</a><br/></div> <br/><div>Not all of these tools use machine learning - many seem to be heuristic-based sentence extractors. Nonetheless, it is interesting to consider their ideas in the context of neural network approaches. </div><br/> <div>One of the reasons we chose to attack this problem is that there is a rich literature to consult; this problem has been worked on in one form or another since 1958 [1]. As one would imagine, this means that there have been many different approaches to this problem, to varying degrees of success. But unlike other problems, where all current approaches are based on deep learning, there is active research into non-neural solutions to SDS. </div><br/> <div>Many researchers have tried to solve SDS with combinatorial optimization, reducing it to the Knapsack problem, the Maximum Coverage problem, or the Budgeted Median problem. For example, the Maximum Coverage problem is: given a number k and a collection S, of m sets, choose less than k sets in S that maximize the number of covered elements. To frame SDS as a Maximum Coverage problem, you break the document into “conceptual units”. Conceptual units are supposed to represent a single concept - for example, “the man bought a book” and the “the man read a book”. But it’s not clear at what granularity these conceptual units should be defined. One easy (but not especially effective) solution is to simply make each word a conceptual unit. Then, the document = S, and each sentence is a set of words inside S. The problem is now to pick k sentences from the document that maximize the word coverage in the document [2]. </div><br/> <div>One example of a recent non-neural approach is from a paper published 5 years ago [3]. The paper solves SDS by reducing it to the so-called Tree Knapsack Problem. We’ve haven’t fully wrapped our heads around the Tree Knapsack problem (it’s actually not that easy to quickly state), but the researchers’ basically involved representing a document as a Rhetorical Structure Theory-based discourse tree (RST-DT) by “select[ing] textual units according to a preference ranking”. The researchers’ first transform the RST-DT into a dependency-based discourse tree (DEP-DT) in order to get a tree that contains textual units on all nodes (RST-DT only have textual units as leaves), and then trim the DEP-DT using the Tree Knapsack problem.  </div><br/> <div>We aim to find a suitable corpus, and implement multiple models directly from these papers as our baseline models. Hopefully, that will give us insight that’ll help us formulate the problem differently. We also want to explore some neural architectures for single document summarization.  </div><br/> <div>We also have to consider whether we want to build an extractive or abstractive text summarization - the former collects a set of sentences or phrases that summarize the document while the latter tries to “learn the internal language representation to generate more human-like summaries, paraphrasing the intent of the original text” [4]. We’re leaning towards an extractive model, although we may try both. </div><br/> <div>[1] = <a target="">https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119004752.ch3</a> <br/>[2] = <a target="">http://www.anthology.aclweb.org/E/E09/E09-1089.pdf</a> <br/> [3] = <a target="">https://www.semanticscholar.org/paper/Single-Document-Summarization-as-a-Tree-Knapsack-Hirao-Yoshida/ed0c8a7ab911cdb30b7e95edada3a55c01eb22c5</a><br/> [4] = <a target="">https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/</a></div></div>
    </summary>
    <updated>2018-04-06T06:31:00Z</updated>
    <author>
      <name>Ron &amp;amp; Aditya</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3753031463594823927</id>
      <author>
        <name>Ron &amp;amp; Aditya</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="https://cse481n.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss" rel="self" type="application/rss+xml"/>
      <title>PrimeapeNLP</title>
      <updated>2018-04-11T09:00:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://mathstoc.wordpress.com/?p=304</id>
    <link href="https://mathstoc.wordpress.com/2018/04/06/milestone-2-music-as-a-natural-language-task/" rel="alternate" type="text/html"/>
    <title>Milestone #2: Music as a Natural Language Task</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Framing the problem The focus of Natural Language Processing relies on patterns in the structure of language and models that find ways to encode the complexities of these structures. Many forms of music also have large amounts of structure which could potentially be discovered using similar models as a standard natural language. Music datasets for … <a class="more-link" href="https://mathstoc.wordpress.com/2018/04/06/milestone-2-music-as-a-natural-language-task/">Continue reading <span class="screen-reader-text">Milestone #2: Music as a Natural Language Task</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h3>Framing the problem</h3>
<p>The focus of Natural Language Processing relies on patterns in the structure of language and models that find ways to encode the complexities of these structures. Many forms of music also have large amounts of structure which could potentially be discovered using similar models as a standard natural language.</p>
<p>Music datasets for machine learning purposes have recently become available through projects like MusicNet in 2016 [1]. This music is primarily classical, and provided as both audio and MIDI.</p>
<h3>Project ideas</h3>
<p>For our project we are interested in music with lyrical content – both for the potential to create a creative demo and for the interest of making this a language task. The current direction we are most interested in is the generation of lyrics for a song, given its nonlyrical content. This will be broken up into subtasks depending on the feasible scale of the project. Not all of the following points will necessarily be parts of our project, but we will use them as as starting point as we see the success of our models.</p>
<ul>
<li>Creating a machine learning model for MIDI music</li>
<li>Translating MIDI into specific artists or styles</li>
<li>Creating models for the lyrical content of specific artists or styles of music</li>
<li>Generating lyrics given an artist or style</li>
<li>Seq2seq conversion of MIDI into lyrical content</li>
<li>GANs for either side of the conversion – MIDI encoding or lyrical generating</li>
</ul>
<h3>Using MIDIs in RNNs</h3>
<p>Work by Pakhomov [2] has already used RNNs to create models for lyrics. In his <a href="http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/">blog post</a> he additionally discusses a method for forming any MIDI into piano roll format. This is essentially a matrix where each column represents a different time step, and each row represents a different note. Having a 1 corresponds to that note sounding at that time. The individual time vectors can be used as the inputs to an RNN at each time step to create a model representing the various songs.</p>
<p>One possible data source for our project is karaoke data available from various sources online. If available in large enough quantities this could be extremely convenient because it already contains many pairings of MIDI music to their lyrics.</p>
<h3>Azure</h3>
<p>We intend to use PyTorch to train our models, and have begun setting up an instance on Microsoft Azure.</p>
<h3>Relevant work</h3>
<p>[1] <a href="https://homes.cs.washington.edu/~thickstn/musicnet.html" rel="nofollow">https://homes.cs.washington.edu/~thickstn/musicnet.html</a></p>
<p>[2] <a href="http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/" rel="nofollow">http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/</a></p>
<p>[3] Dong, Hao-Wen. 2017. MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment. <a href="https://arxiv.org/pdf/1709.06298" rel="nofollow">https://arxiv.org/pdf/1709.06298</a></p>
<p>[4] Yu, Lantao. 2016. SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. <a href="https://arxiv.org/abs/1609.05473" rel="nofollow">https://arxiv.org/abs/1609.05473</a></p>
<p>[5] Lee, Sang-gil. 2017. A SeqGAN for Polyphonic Music Generation. <a href="https://arxiv.org/abs/1710.11418" rel="nofollow">https://arxiv.org/abs/1710.11418</a></p></div>
    </content>
    <updated>2018-04-06T06:30:14Z</updated>
    <category term="NLP Capstone"/>
    <author>
      <name>Nicholas Ruhland</name>
    </author>
    <source>
      <id>https://mathstoc.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://mathstoc.wordpress.com/category/nlp-capstone/feed/" rel="self" type="application/rss+xml"/>
      <link href="https://mathstoc.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://mathstoc.wordpress.com/osd.xml" rel="search" title="Mathematical Distractions" type="application/opensearchdescription+xml"/>
      <link href="https://mathstoc.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A recreational (occasionally research) blog on topics in maths or computer science - by Kuikui</subtitle>
      <title>NLP Capstone – Mathematical Distractions</title>
      <updated>2018-04-11T09:00:15Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/3a2f40b355a5</id>
    <link href="https://medium.com/nlp-capstone-blog/getting-started-for-the-capstone-software-installation-pipeline-brainstorming-3a2f40b355a5?source=rss----9ba3897b6688---4" rel="alternate" type="text/html"/>
    <title>Getting Started for the Capstone: Software Installation &amp; Pipeline Brainstorming</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Currently, our top two choices for the capstone is</p><ol><li><strong>Machine Dictionary: </strong>learning definitions of technical terms whose semantics are averaged over all places it is mentioned in training (in this case, research publications in the given field of study)</li><li><strong>Visual Reasoning: </strong>Given three windows, each of which containing a random arrangement of colored, geometric shapes, and a statement about the image, predict whether the statement is true or false.</li></ol><p>Despite being problems with very different needs and challenges, the bulk of the tools and frameworks we’ll be using overlap for both tasks. Here, we discuss those tools and frameworks, followed by things we need specific to <strong>Machine Dictionary </strong>and <strong>Visual Reasoning </strong>separately.</p><h3>Resources Used for Both Tasks</h3><h4>PyTorch</h4><p>Given the limited time that we have, a neural-based approach using an established framework is preferred over implementing all of the model architecture from scratch, and to help avoid complications that can accompany other methods such as deriving parameter updates for bayesian models. PyTorch is an excellent framework that abstracts away differentiation and tensor arithmetic while still allowing a healthy amount of flexibility with it’s ability to dynamically produce computation graphs.</p><h4>AllenNLP</h4><p>After the crash course on the framework provided by AI2 in class, along with our experience from using it in the undergraduate NLP class, we’re convinced that the integration of AllenNLP with PyTorch is the best way to be as productive as possible. We plan to use the libraries it provides to make training more streamline and organized.</p><p>Since we’ve taken the undergraduate NLP class, we’ve already installed PyTorch and AllenNLP. We installed PyTorch through conda and AllenNLP through pip. Deep Learning projects also tend to involve complicated models which might require more computing resources, so we will also utilize the Azure credits available through the capstone. This process involved installing PyTorch and AllenNLP on Ubuntu VMs on Azure configured into include NVIDIA GPUs so that we can take advantage of PyTorch’s .</p><h3>Resources Specific to Machine Dictionary</h3><h4>Semantic Scholar Open Research Corpus</h4><p>The best dataset we’ve seen so far for this task is the <a href="http://labs.semanticscholar.org/corpus/">Semantic Scholar Open Research Corpus</a> provided by <a href="http://allenai.org/">AI2</a>. The dataset specifically consists of JSON files with metadata for each publication. The most relevant files will be <em>title, pdfUrls, </em>and <em>year</em>. Given all of the content besides the paper abstract (which is included as a field called <em>paperAbstract</em>), we resort to using the <em>pdfUrls</em> and extracting the text from each.</p><h4>Textract</h4><p><a href="https://github.com/deanmalmgren/textract">Textract</a> is a Python package that allows the extraction of text from PDFs. We plan to rely on this package given that it’s robust to both compiled and scanned PDFs.</p><p>We downloaded Textract to the Azure Linux VM using the Ubuntu installation directions:</p><pre>apt-get install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr \<br/>flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig<br/>pip install textract</pre><p>but ran into an issue that the developers haven’t dealt with yet. In the first line, they are missing a dependency to the libpulse-dev package, which causes the build to fail when downloading Textract. Doing a pip install libpulse-dev takes care of it.</p><p>Grabbing the text from a PDF file is then fairly convenient. We tested this on a <a href="https://www.semanticscholar.org/paper/Effects-of-anthocyanins-on-the-prevention-and-of-Lin-Gong/1bcf9ae84d4ec5c0aba7918e6784dbfd0e8514b6">cancer research paper</a> taken from the Semantic Scholar dataset:</p><pre><strong>import</strong> textract<br/>text = textract.process("path-to-doc.pdf", encoding="ascii")</pre><p>which produced an excellent parse of the PDF.</p><blockquote>b’BJP\n\nBritish Journal of\nPharmacology\n\nBritish Journal of Pharmacology (2016) \n\n1\n\nREVIEW ARTICLE THEMED ISSUE\nEffects of anthocyanins on the prevention and\ntreatment of cancer\nCorrespondence Ying-Yu Cui, Department of Regenerative Medicine, Tongji University School of Medicine, Shanghai 200092,\nChina. E-mail: yycui@tongji.edu.cn\n\nReceived 13 June 2016; Revised 17 August 2016; Accepted 13 September 2016\n\nBo-Wen Lin1, Cheng-Chen Gong1, Hai-Fei Song1 and Ying-Yu Cui1,2,3\n1\n\nDepartment of Regenerative Medicine, Tongji University School of Medicine, Shanghai, China, 2Key Laboratory of Arrhythmias, Ministry of\n\nEducation (Tongji University), Shanghai, China, and 3Institute of Medical Genetics, Tongji University School of Medicine, Shanghai, China\n\nAnthocyanins are a class of water-soluble avonoids, which show a range of pharmacological effects, such as prevention of\ncardiovascular disease, obesity control and antitumour activity.</blockquote><h3>Resources Specific to Visual Reasoning</h3><h4>Cornell NLVR Dataset</h4><p>The dataset for the Visual Reasoning task is easily available through <a href="https://github.com/clic-lab/nlvr">github</a>. This dataset was also very conveniently organized into three folders: testing data, development data, and training data. Each folder contains the actual images which are provided for us to train on.</p><p>In addition to the images, each folder also includes a JSON file which contains basic JSON representations of each image and the sentence that needs to be validated in each data point. This JSON representation is very useful because we do not need to parse the image to retrieve the raw elements (shape, color, location).</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3a2f40b355a5" width="1"/><hr/><p><a href="https://medium.com/nlp-capstone-blog/getting-started-for-the-capstone-software-installation-pipeline-brainstorming-3a2f40b355a5">Getting Started for the Capstone: Software Installation &amp; Pipeline Brainstorming</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>
    </content>
    <updated>2018-04-06T06:06:13Z</updated>
    <category term="python"/>
    <category term="nlp"/>
    <category term="machine-learning"/>
    <category term="deep-learning"/>
    <author>
      <name>Tam Dang</name>
    </author>
    <source>
      <id>https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4</id>
      <logo>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</logo>
      <link href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/nlp-capstone-blog" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>A Journey Through CSE 481N, the Natural Language Processing Capstone Course at the University of Washington - Medium</subtitle>
      <title>NLP Capstone Blog - Medium</title>
      <updated>2018-04-11T09:00:11Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-9203775015655831448.post-34377626932024049</id>
    <link href="https://teamoverfit.blogspot.com/2018/04/2-milestone-warm-up.html" rel="alternate" type="text/html"/>
    <title>#2 Milestone: Warm up</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h2 style="height: 0px;"><span>Team Overfit</span></h2><h3><span><br/></span></h3><h3><span>Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span>Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><br/><div/><br/><div style="text-align: start; text-indent: 0px;"><div><span><b>Blog Post: #2: 04/05/2018</b></span></div><div><span><b><br/></b></span></div><div style="margin: 0px;"/><br/><ul><li><span>We first installed Pytorch 3.6 and we tried to run small programs on our local machines.</span></li><li><span>We then explored the usage of the RNN and seq2seq APIs, which we are going to use for most of our projects ideas.</span></li><ul><li><span>We looked through the tutorial of RNNs/LSTMs/GRUs from the previous 447 class.</span></li></ul><ul><li><span id="docs-internal-guid-97b5af9d-9943-133b-4f16-5d4414eefd5d"><span><a href="https://colab.research.google.com/drive/11iLtGFDpnIuHj5B0rQDGG5lqq6BQ8FRh">https://colab.research.google.com/drive/11iLtGFDpnIuHj5B0rQDGG5lqq6BQ8FRh</a></span></span></li></ul><li><span><span style="white-space: pre-wrap;">We tried to set up an Azure instance for GPU computation</span></span></li><ul><li><span><span style="white-space: pre-wrap;">We installed cuda support for the Pytorch package and it ran successfully with Tesla K80</span></span></li></ul><li><span><span style="white-space: pre-wrap;">We revisited the Recurrent Neural Networks, Attention and Reading Comprehension projects from the last quarter and experimented with other Pytorch features related to our project.</span></span></li></ul><br/></div></div>
    </summary>
    <updated>2018-04-06T04:48:00Z</updated>
    <author>
      <name>Team Overfit</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-9203775015655831448</id>
      <author>
        <name>Team Overfit</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="https://teamoverfit.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="https://teamoverfit.blogspot.com/feeds/posts/default?alt=rss" rel="self" type="application/rss+xml"/>
      <title>NLP Capstone</title>
      <updated>2018-04-11T09:00:14Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/3d3651220219</id>
    <link href="https://medium.com/@ryanp97/project-logistics-and-package-exploration-3d3651220219?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
    <title>Project Logistics and Package Exploration</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Jan Buys has agreed to advise me while I pursue Neural Machine Translation with Semantic Transfer, so this post will mainly focus on the packages and resources available for completing and exploring the minimal viable action plan as described <a href="https://medium.com/@ryanp97/project-ideas-ab3d796c422e">previously</a>.</p><h4>Dataset</h4><p>I’m currently working on cleaning the <a href="http://www.edrdg.org/wiki/index.php/Tanaka_Corpus">Tanaka Corpus</a> so that I can segment the sentences and then parse the graphs. This corpus is small and the sentences are short, so it seems like a good option for development. I’ve cleaned the corpus by removing any sentences that are not in English or Japanese as well as removing any sentences that do not belong to a pair of translations. Each sentence has at least one translation; note that some sentences have more than one translation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/753/1*vajUkTbO551F3mjGQjyHQA.png"/>The Japanese sentence can be translated into the two English sentences. Note that the first translation is more direct, while the second translation seems to be drawing on the author’s bias or some other context.</figure><p>Also, in the Tanaka corpus, I noticed some things of interest. The first thing is that some hard to read Kanji are annotated with pronunciations in Katakana. The next were a couple typos in the translation (i.e. the proper noun “Tatoeba” was spelled “Tatoeb” in the translation). Also, on one line (the only one that I could find), the Japanese sentence had the romanization in parentheses following the translation. For the sake of consistency, I’ve removed this from the dataset for the sake of consistency. Aside from this single change, everything else was left untouched.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/838/1*xhYvt3b5aClaW0-pDdK8Cw.png"/>Example of a translation pair from the Tanaka corpus. Note the translated smiley face!</figure><p>There is also the <a href="https://alaginrc.nict.go.jp/WikiCorpus/index_E.html">Kyoto Corpus</a> which is larger, more diverse, and has a more accurate representation of ‘real’ sentences. However, this dataset is formatted in a slightly more complex way, so I’m still figuring out how to tackle cleaning this corpus. This corpus also provides at least one translation.</p><h4>Word Segmentation</h4><p>There were 3 main programs that I considered and experimented with: <a href="https://github.com/neubig/kytea">KyTea</a> (pronounced “cutie”), <a href="http://www.atilika.org/">Kuromoji</a>, and <a href="http://taku910.github.io/mecab/">MeCab</a>.</p><p>After comparing the segmentation outputs on some small samples, it seemed that KyTea’s outputs often had significant differences between the outputs from Kuromoji and MeCab. After asking a lecturer from the UW Japanese department for their opinion, it seemed that the outputs from Kuromoji and MeCab were closer to the gold standard than the KyTea’s. As implied by the previous sentiment: the data that I will be working with is NOT gold standard, but it is the best available.</p><p>I decided on using MeCab as it had a very easy to use Python interface while Kuromoji did not have a easily accessible Python interface. Following <a href="http://www.robfahey.co.uk/blog/japanese-text-analysis-in-python/">this</a> blog post, I’ve setup MeCab to be able to handle more robust input such as slang and neologisms, though I’m unsure if this will work well with the Jacy grammar mentioned in the Graph Parsing section.</p><p><a href="https://arxiv.org/pdf/1410.0291.pdf">Here</a> is a paper on the accuracy of MeCab when performing segmentation. The paper also gives a nice, brief overview on the differences between English and Japanese. Note the size of the Tanaka corpus is smaller than the current version as translations are added semi-regularly.</p><h4>Graph Parsing</h4><p>For parsing the MRS graphs and converting MRS to DMRS, I used the <a href="https://github.com/goodmami/mrs-to-penman">mrs-to-penmen</a>. It uses the <a href="https://github.com/delph-in/pydelphin">PyDelphin</a> interface which provides a wrapper for the <a href="http://sweaglesw.org/linguistics/ace/">ACE</a> parser. The ACE parser parses the MRS graph, and then mrs-to-penmen should convert the parsed MRS graph to penmen format.</p><p>As for the grammar that ACE uses to parse, I’m using the <a href="http://www.delph-in.net/erg/">English Resource Grammar</a> and <a href="http://moin.delph-in.net/JacyTop">Jacy</a> for English and Japanese respectively.</p><p>After getting the parsed penman format, there is still some cleaning that I have to do and some simplification of the penman format to simplify what the seq2seq model is expected to output.</p><h4>Deep Learning Packages</h4><p>Additionally, since this project will rely on Deep Learning, I’ve installed AllenNLP and PyTorch in anticipation of the seq2seq model that will be trained after linearizing the DMRS graph as well as the TreeLSTM model (stretch goal).</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3d3651220219" width="1"/></div>
    </content>
    <updated>2018-04-06T01:20:18Z</updated>
    <category term="machine-learning"/>
    <author>
      <name>Ryan Pham</name>
    </author>
    <source>
      <id>https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/1*83KfTWByl5pPq7A8_E8ApA.gif</logo>
      <link href="https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@ryanp97" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Ryan Pham on Medium</subtitle>
      <title>Stories by Ryan Pham on Medium</title>
      <updated>2018-04-11T09:00:05Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2</id>
    <link href="http://sarahyu.weebly.com/cse-481n/technical-details-blog-post-2" rel="alternate" type="text/html"/>
    <title>Technical Details (Blog Post #2)</title>
    <summary>For my project I am planning to do some deep learning at the end if I have time and if the results up to that point lead to that track. (I have pytorch installed from NLP so that's nice to have). With that said, I've been working with the Reddit API's and Reddit datadumps to get started on gathering the necessary data for pursuing the Language Accommodation project. I've been trying to figure out if the best approach is to work with the limited requests, the direct json files, or if some of [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="paragraph"><span style="color: rgb(0, 0, 0);">For my project I am planning to do some deep learning at the end if I have time and if the results up to that point lead to that track. (I have pytorch installed from NLP so that's nice to have). <br/><br/>With that said, I've been working with the Reddit API's and Reddit datadumps to get started on gathering the necessary data for pursuing the Language Accommodation project. I've been trying to figure out if the best approach is to work with the limited requests, the direct json files, or if some of the data dumps will suffice. I hope to have most of that and some basic data visualizations ready in the next couple of days to inform some of the choices I should make regarding the data (i.e. what time period to gather data from, what subreddits to pull from, etc.)</span><br/></div></div>
    </content>
    <updated>2018-04-06T00:10:50Z</updated>
    <category term="Uncategorized"/>
    <source>
      <id>http://sarahyu.weebly.com/cse-481n</id>
      <author>
        <name>Sarah Yu &lt;br/&gt; Team Jekyll-Hyde</name>
      </author>
      <link href="http://sarahyu.weebly.com/cse-481n" rel="alternate" type="text/html"/>
      <link href="http://sarahyu.weebly.com/6/feed" rel="self" type="application/rss+xml"/>
      <subtitle>CSE 481N</subtitle>
      <title>Sarah yu - CSE 481N</title>
      <updated>2018-04-06T00:30:16Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/280325a548d1</id>
    <link href="https://medium.com/@be.li.nda/nlp-capstone-blog-2-learning-pytorch-280325a548d1?source=rss-fad49d942bf3------2" rel="alternate" type="text/html"/>
    <title>NLP Capstone Blog #2: Learning PyTorch</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In order to familiarized myself with PyTorch, I built an attentive LSTM RNN for a benchmark dataset (<a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/">Rotten Tomato movie reviews</a>) frequently used for sentiment analysis. It was able to achieve up to 76–78% accuracy when trained to 10 epochs, with a learning rate of 0.05. With more time, a natural next step would have been adapting the model for GPU, implementing dropout, and performing hyper-parameter tuning, but this can all be done when I implement my actual model for my project.</p><h3>Preprocessing the Data</h3><p><strong>Libraries used</strong>: torchtext, and SpaCy.</p><p>After parsing the data files, I split the dataset into train/dev/test datasets using an 80/10/10 ratio</p><p>For the next steps of preprocessing (tokenization, vocab building, embedding, and batching), I utilized the torchtext library extensively. I first defined a custom dataset by inheriting data.Dataset. I define three data.Fields for my dataset (TEXT for the word embeddings, POLARITY for the polarity embeddings, and LABEL for the label of each example). When I defining the TEXT field, I was able to set the tokenizer to SpaCy’s default english tokenizer.</p><p>One of the benefits of using torchtext is the ease of building the vocab, loading the embeddings, and batching. A few simple lines of code were able to encapsulate these functionalities.</p><pre>TEXT.build_vocab(Xtrain) # 1. generate vocabulary<br/>TEXT.vocab.load_vectors(‘glove.6B.’ + str(embedding_dim) + ‘d’) # 2. load GloVe embeddings<br/>Xtrain, Xdev, Xtest = data.Iterator.splits(<br/>    (Xtrain, Xdev, Xtest),<br/>    batch_sizes=(batch_size, len(Xdev), len(Xtest)),<br/>    repeat=False,<br/>    device = -1 # run on CPU<br/>) # 3. build batches</pre><p>One of the troubles I ran into when preprocessing the data was being unable to load two embeddings for each word in a sequence — I couldn’t figure out how to load both the pre-trained GloVe embeddings and the learn-able (untrained) polarity embeddings to encapsulates the prior polarities of words. I eventually resolved this difficulty by adding a Field to the Dataset just for the polarity of each word.</p><pre>POLARITY = data.Field(sequential=True, tokenize=get_polarity)<br/>POLARITY.build_vocab(Xtrain)</pre><h3>Building the Model</h3><p><strong>Libraries used</strong>: PyTorch</p><p>I defined my network as follows —</p><pre>class Model(nn.Module):<br/>    def __init__(self, num_labels, vocab_size, embeddings_size,<br/>                 hidden_dim, word_embeddings, num_features,<br/>                 batch_size):<br/>        # Code omitted, but basically everything should be <br/>        # initialized here, including:<br/>        # 1. The embeddings<br/>        # 2. The LSTM<br/>        # 3. The linear layers, including attention</pre><pre>    def init_hidden(self):<br/>        # initialize a single hidden layer to all 0s (code omitted)</pre><pre>    def forward(self, word_vec, feature_vec):<br/>        # 1. Apply embeddings &amp; prepare input<br/>        word_embeds_vec = self.word_embeds(word_vec)<br/>        feature_embeds_vec = self.feature_embeds(feature_vec)<br/>        lstm_input = \<br/>            torch.cat((word_embeds_vec,feature_embeds_vec),2)</pre><pre>        # 2. Pass through lstm<br/>        lstm_out, self.hidden = self.lstm(lstm_input, self.hidden)</pre><pre>        # 3. Compute and apply weights (attention) to each layer<br/>        alphas = F.softmax(self.attention(lstm_out), dim=0)<br/>        weighted_lstm_out = \<br/>            torch.sum(torch.mul(alphas, lstm_out), dim=0)</pre><pre>        # 4. Get final results, passing in weighted lstm output:<br/>        tag_space = self.hidden2label(weighted_lstm_out)<br/>        log_probs = F.log_softmax(tag_space, dim=1)<br/>        return log_probs</pre><p>As can be seen by the forward algorithm, the model first retrieves the embeddings of each word in the sequence, then passes them through an LSTM, applies attention, and finally maps the outputs to the labels.</p><h3>Training the Model</h3><p><strong>Library used</strong>: PyTorch</p><p>I used the NLLLoss function for loss, which I optimized using the SGD optimizer with a learning rate of 0.05. I trained the model for a maximum of 10 epochs. For each epoch, I iterated across the batches, performing 4 steps:</p><ul><li>Clear the accumulated gradients and detach the hidden state from the last instance.</li></ul><pre>model.zero_grad()<br/>model.hidden = model.init_hidden()</pre><ul><li>Run the forward pass.</li></ul><pre>log_probs = model(words, polarity)</pre><ul><li>Compute the loss and gradients.</li></ul><pre>loss = loss_function(log_probs, label)<br/>loss.backward()</pre><ul><li>Call optimizer.step() to update parameters.</li></ul><h3>Evaluating the Model</h3><p><strong>Library used</strong>: n/a</p><p>To evaluate my model, I simply iterated across the batches of each dataset and counted the number of correctly positive and correctly negative instances in each batch. Then, I put the numbers together for an overall accuracy score. This was a simple process that didn’t depend heavily on utilizing library tools.</p><p>All code I’ve written for this post can be found on Github: <a href="https://github.com/eunsol/document-e2e-sent/tree/master/rt-polarity">https://github.com/eunsol/document-e2e-sent/tree/master/rt-polarity</a></p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=280325a548d1" width="1"/></div>
    </content>
    <updated>2018-04-05T22:48:30Z</updated>
    <category term="machine-learning"/>
    <category term="pytorch"/>
    <category term="naturallanguageprocessing"/>
    <author>
      <name>Belinda Zou Li</name>
    </author>
    <source>
      <id>https://medium.com/@be.li.nda?source=rss-fad49d942bf3------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/0*A16vZtRWBzxYPlmn.</logo>
      <link href="https://medium.com/@be.li.nda?source=rss-fad49d942bf3------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@be.li.nda" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Belinda Zou Li on Medium</subtitle>
      <title>Stories by Belinda Zou Li on Medium</title>
      <updated>2018-04-11T09:00:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://cse481n-capstone.azurewebsites.net/?p=31</id>
    <link href="http://cse481n-capstone.azurewebsites.net/2018/04/05/warm-up/" rel="alternate" type="text/html"/>
    <title>Warm Up!</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Data Collection: We have already begun the process of collecting data from Reddit for our project, using the Reddit API. We want to train our first neural-net model, which will be able to tell if some text is offensive or not, on a large amount of data. Besides using the Reddit API to get posts … <a class="more-link" href="http://cse481n-capstone.azurewebsites.net/2018/04/05/warm-up/">Continue reading<span class="screen-reader-text"> "Warm Up!"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h3><b>Data Collection:</b></h3>
<p><span style="font-weight: 400;">We have already begun the process of collecting data from Reddit for our project, using the Reddit API. We want to train our first neural-net model, which will be able to tell if some text is offensive or not, on a large amount of data.</span></p>
<p><span style="font-weight: 400;">Besides using the Reddit API to get posts from r/MeanJokes, we also use the Reddit submission dataset from </span><a href="https://pushshift.io/"><span style="font-weight: 400;">pushshift.io</span></a><span style="font-weight: 400;"> to get more examples from a wider context. We pre-processed data by filtering out non-text submission and deleted posts. </span></p>
<p><span style="font-weight: 400;">Now that we have the r/meanJokes posts, we want to determine the content phrases of these posts so that we can use them to find similar sentences all over Reddit. We know that the r/meanJokes posts are all offensive, and similar sentences elsewhere in Reddit could give us non-offensive examples. We have found an algorithm for pulling content phrases out of sentences, the </span><span style="font-weight: 400;">Rapid Automatic Keyword Extraction (RAKE) algorithm. We have extracted content phrases from r/meanJokes posts and also want to extract them from the rest of Reddit and the next step is to decide on a way to compare similarity and output the final set of posts we want to train the model on.</span></p>
<h3><b>Deep Learning Tools Set Up:</b></h3>
<p><span style="font-weight: 400;">For our modeling purposes, we are exploring PyTorch and AllenNLP. We learned PyTorch basics and went through Nelson’s tutorial in the undergrad NLP course last quarter. PyTorch would be our weapon of choice if we experiment with novel models.</span></p>
<p><span style="font-weight: 400;">We also installed AllenNLP because it looks like a nice tool to build and evaluate baseline models. We are currently going through AllenNLP’s official tutorial by running some of their existing models and demos.</span></p>
<p><span style="font-weight: 400;">So far, these tools are working as we expected.</span></p>
<p> </p></div>
    </content>
    <updated>2018-04-05T17:41:07Z</updated>
    <category term="Weekly blog"/>
    <author>
      <name>Team Watch Your Language!</name>
    </author>
    <source>
      <id>http://cse481n-capstone.azurewebsites.net</id>
      <link href="http://cse481n-capstone.azurewebsites.net/feed/" rel="self" type="application/rss+xml"/>
      <link href="http://cse481n-capstone.azurewebsites.net" rel="alternate" type="text/html"/>
      <subtitle>Spring2018 CSE481N Capstone</subtitle>
      <title>Team Watch Your Language!</title>
      <updated>2018-04-11T09:00:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/9a0f5382cff5</id>
    <link href="https://medium.com/@viterbi.or.not/preliminary-ideas-9a0f5382cff5?source=rss-c522ef075bb3------2" rel="alternate" type="text/html"/>
    <title>Project Ideas</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5yHodb3mhK0Ec-CEkw4o-A.png"/></figure><p>Welcome to Team Viterbi Or Not To Be’s first blog post, where we will be discussing our top three project ideas and linking them to the preliminary research we’ve done. As a start for our to-be-determined project, we’ve created a <a href="https://github.com/viterbi-or-not-to-be/viterbi-or-not-to-be">git repository</a> where you can access our code and see our progress!</p><p>While considering various ideas to explore, our team ultimately decided to approach the project in “research mode”, with a greater focus on novel models and analysis of their performance. Although it would be exciting to build a complete platform incorporating Natural Language Processing techniques, we decided it would be more educational overall to examine the workings of the models themselves. With that overarching direction in mind, we began to think about the issues in the field that would be most interesting to work with, and settled on a few ideas:</p><h4><strong>Automatic Conversation Summarization</strong></h4><p>One idea is to explore various approaches for the automatic summarization of written conversations. As a topic, the usefulness of written conversation summaries is undeniable — as so many methods of communication are being powered through online e-mail or chat interfaces, there is value in being able to extract relevant topics or action items from conversations just as there is for documents or news articles. As a task, however, there are plenty of challenges to overcome to determine what properties of a topic in a discussion influence its importance to the overall summary and how a summary can be generated based off that data.</p><p>In our preliminary research, we found that relatively few efforts to automatically summarize natural language text have focused on conversational input specifically, but those that have show promise. One paper in particular examines the <a href="https://pdfs.semanticscholar.org/efe0/fffe080ac4b1a943f62cc56f2baa27c6e195.pdf">summarization of both spoken and written conversations</a>, and notes that there are substantial differences in the two types of data. Although the results of the summarization efforts presented in this paper are well below the baseline established by human summarizers, we think it would be interesting to implement a comparable system and explore modifications that could be made or alternative models that could be used to get a more complete picture of the possibilities. By reading the research papers of related projects, we have identified a number of datasets that would allow us to train our model, including <a href="http://groups.inf.ed.ac.uk/ami/corpus/">meeting summaries</a>, <a href="https://www.cs.cmu.edu/~./enron/">summaries of email threads</a>, and <a href="https://flossmole.org/content/software-archaeology-gnue-irc-data-summaries">summaries of chat logs</a>.</p><p>If we were to pursue this option, we would initially focus on replicating the approaches used by previous research projects to automatically summarize conversations as a baseline. Once that is working, we would move toward creating a minimum viable product by examining the features and models used in past approaches and performing an analysis of possible alternatives. Although it may not be within the scope of this quarter-long project to apply an entirely novel model to the problem, at a minimum we would seek to determine the relationship between different features and the various types of conversation available through experimentation.</p><p>Beyond the minimum viable product, there are several stretch goals we would like to tackle. One of the more seemingly impactful would be using model parameters and features extracted from one type of data to improve the model’s performance on another, such as using the result of training a model on spoken meeting data to improve the automatic summarization of emails. In the previously linked research paper, the authors mention that a future goal for their research is to implement such a system, and they assert that preliminary results are promising. Another stretch goal would be attempting to beat previous approaches by focusing on one specific domain and using the unique properties of that data to produce better summaries. An example could be to focus on chat logs from the GNU dataset that deal specifically with bugfixes — by restricting the domain to a set of code-related topics that likely share a much smaller vocabulary and a consistent notion of “importance”, such as action items during the lifecycle of a bugfix, it may be possible to produce better summaries than in the case of general summarization. Finally, we are interested in comparing the results of extractive and abstractive summarization — while the former works by identifying the most important sentences in a text and combining them, the latter attempts to make a more “human” summary by identifying topics in a text and generating new sentences that paraphrase the intent. One extension could therefore be to try extending extractive models proposed previously to be abstractive.</p><p>While our primary interest would be in conversation summarization, we would also consider doing a similar summarization project based on research papers if the conversation data proved to be insufficient. To do so, we would identify corpuses of research papers, and use the author-written abstracts as the summaries for our training data.</p><h4><strong>Multiple Premise Entailment</strong></h4><p>Another idea is to pursue a project that tackles the problem of Multiple Premise Entailment, which involves being able to make inferences based on multiple premises, as shown in the example below. This would contribute to the making of more “knowledgeable” models that aim to use and understand contexts across multiple ideas, a more challenging problem than making inferences from a single sentence as is done in standard entailment tasks.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7M-V9YHW9UtascpkTEK5Nw.png"/>Example of a Multiple Premise Entailment, Lai et al. 2017</figure><p>If we were to take on this challenge, we would start by looking at <a href="http://aclweb.org/anthology/I17-1011">existing research</a> and begin our approach in a similar way, first using baseline neural models for standard entailment. Once this is running on the MPE dataset, we would do some study and error analysis of these runs, then aim to improve upon these models in a way that takes our findings on the baseline models into account and includes adaptations to tackle multiple premises to create a minimum viable product.</p><p>Beyond the minimum viable product, a stretch goal for this problem would be to present a unique model that possibly uses a novel approach from the baseline models to tackle multiple premises in a way that best suits this new challenge, relying less on models for standard entailment.</p><h4><strong>Natural Language Visual Reasoning</strong></h4><p>The last idea we are interested in involves the <a href="http://lic.nlp.cornell.edu/nlvr/">Cornell Natural Language Visual Reasoning dataset</a>, which contains 92.244 pairs of natural language statements grounded in synthetic images like the one shown below. The challenge that involves language is to determine whether a statement about the image is true or false. Doing so would typically involve reasoning based on spatial relations, quantities, and other qualities about sets of objects that might appear in the NLVR dataset images.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Y4-XNexoEp2I5lGKm9MTmg.png"/>Examples of image-statement pairs from the NLVR dataset, Suhr et al. 2017</figure><p>If we were to work on this problem, we would focus first on reimplementation of the <a href="https://arxiv.org/pdf/1511.02799.pdf">state of the art model</a> from UC Berkeley. As noted in the “Future Work” section of the UC Berkeley paper, their model maintains a strict separation between predicting network structures and learning network parameters. As a stretch goal, we could work on integrating the current approach with existing tools for learning semantic parsers to achieve an integration between the two components that would possibly improve performance or make way for a novel approach to the problem.</p><p>Edited 4/5 to fix formatting, revise introduction (plus dog mascot!), and add example images.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9a0f5382cff5" width="1"/></div>
    </content>
    <updated>2018-04-04T07:03:32Z</updated>
    <category term="nlp"/>
    <author>
      <name>Viterbi Or Not To Be</name>
    </author>
    <source>
      <id>https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/1*wiisg40Bu4z11RTWJ66mnA.png</logo>
      <link href="https://medium.com/@viterbi.or.not?source=rss-c522ef075bb3------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@viterbi.or.not" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Viterbi Or Not To Be on Medium</subtitle>
      <title>Stories by Viterbi Or Not To Be on Medium</title>
      <updated>2018-04-11T09:00:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://mathstoc.wordpress.com/?p=277</id>
    <link href="https://mathstoc.wordpress.com/2018/04/04/nlp-capstone-post-1-ideation/" rel="alternate" type="text/html"/>
    <title>NLP Capstone Post #1: Ideation</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In this post, I’d like to briefly discuss three different ideas I have for my capstone project. UPDATE (04/05/2018): I am fortunate to be joined by a fellow student, Nicholas Ruhland, for this capstone project. A Theoretical Analysis of RNNs (Research Mode): A recent paper of Professor Naftali Tishby provided some useful observations on the … <a class="more-link" href="https://mathstoc.wordpress.com/2018/04/04/nlp-capstone-post-1-ideation/">Continue reading <span class="screen-reader-text">NLP Capstone Post #1: Ideation</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In this post, I’d like to briefly discuss three different ideas I have for my capstone project.</p>
<p>UPDATE (04/05/2018): I am fortunate to be joined by a fellow student, Nicholas Ruhland, for this capstone project.</p>
<h1>A Theoretical Analysis of RNNs (Research Mode):</h1>
<p> A recent <a href="https://arxiv.org/abs/1703.00810">paper of Professor Naftali Tishby</a> provided some useful observations on the behavior of feedforward neural networks, and proposed a promising approach to understanding their performance. Earlier empirical work done in the vision community showed that when a convolutional neural network is trained, layers closer to the input learn lower level features (such as edges and corners) and layers closer to the output learn higher level features (“this part of the image resembles a nose, and this other part resembles an eye”). One might expect similar behavior to occur with general feedforward neural networks: that earlier layers learn lower level features of the input and later levels learn higher level features of the input. The key insight here was to think of each layer of a neural network as a Markov chain, where each layer <img alt="L_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{i}"/> is a (vector-valued) random variable that is conditionally independent of <img alt="L_{j}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{j}"/> for all <img alt="j &lt; i - 1" class="latex" src="https://s0.wp.com/latex.php?latex=j+%3C+i+-+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="j &lt; i - 1"/> given <img alt="L_{i-1}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bi-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="L_{i-1}"/>. In this way, information flowing forward in the network can be quantified via notions of entropy from traditional information theory.</p>
<p>The paper contains some empirical work, observing that there are generally two phases to learning artificial neural networks via stochastic gradient descent: the fitting phase, and the compression phase. The fitting phase is the shorter phase, where the model is quickly tuning itself to minimize the empirical loss function. At the end of this phase, we don't necessarily have a model that will generalize to new data. The compression phase is where the model begins to learn the relevant features in the input, with the intuition that there are many irrelevant parts of the input (I don't need to know every atom in an object to identify it). </p>
<p>The goal of this project would be to perform a similar theoretical analysis and empirical work for RNN architectures (whose "natural" Markov chain isn't as simple, as there are cycles) on some traditional NLP task, such as Machine Translation, with the goal of studying the flow of information in an RNN architecture, rather than performing comparably to state-of-the-art Machine Translation models (although this can be a stretch goal).</p>
<p>The relevant steps in this project will likely look like the following:<br/>
1. Reading up on the relevant work by Tishby et. al. (and any other theoretical papers on deep learning).<br/>
2. Understand basic and traditional RNN architectures.<br/>
3. Learning PyTorch.<br/>
4. Implementing several of these architectures and testing (for example, to see if learning also comes in two distinct phases: fitting and compression)<br/>
5. Using these empirical observations, and information theory to analyze these architectures.<br/>
6. Time permitted, play around with new RNN architectures.</p>
<h1>Musical Style Learning from Musical Scores (Research/Start-Up Mode):</h1>
<p> This idea lies somewhat outside traditional NLP in that it tackles the language of music. While the alphabet of a musical score consist chiefly of the 12 musical notes, there is added challenge in that several notes may be played simultaneously, especially if there are several instruments involved or simply the two hands of a pianist. Furthermore, the exact timing of each note played matters, note merely the ordering of the notes.</p>
<p>The idea here is simply to, given the score of a musical piece, represented as a sequence of notes at each time, predict the era (Baroque, Classical, Romantic, etc.) or even, the composer of the piece (Bach, Beethoven, Brahms, etc.) There are several problems to be solved step by step for this project.</p>
<p>1. Data collection from a large library of musical scores (ex: <a href="http://imslp.org/">IMSLP</a>)<br/>
2. Data formatting so as to be usable.<br/>
3. Model selection.<br/>
4. Model implementation (PyTorch).<br/>
5. Model testing.</p>
<p>There are also several extensions that can be viewed as stretch goals. For these, the first two can be reused.</p>
<h3>Musical Score Generation:</h3>
<p> Now, we learn how to compose a piece that “sounds” similar to a given composer. This will involve learning from the pieces written by a given input composer, and outputting a new piece. One core challenge here is ensuring that the output is syntactically correct.</p>
<h1>Story Illustration (Start-Up Mode):</h1>
<p> Given a short story and a specific scene (or place in the text), produce an image that is representative of the scene. This project combines aspects of NLP and vision. This project may also explore generative adversarial methods. One well-known challenge here is convergence.</p>
<p>Here are the general steps for this project:<br/>
1. Data collection (image captioning dataset can be helpful)<br/>
2. Model selection.<br/>
3. Model implementation (PyTorch).<br/>
4. Model testing.</p>
<p>As an extension, one can also generate several frames to form a short “movie”. Another can be comic book pane generation.</p></div>
    </content>
    <updated>2018-04-04T06:53:35Z</updated>
    <category term="NLP Capstone"/>
    <category term="Uncategorized"/>
    <author>
      <name>Kuikui Liu</name>
    </author>
    <source>
      <id>https://mathstoc.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://mathstoc.wordpress.com/category/nlp-capstone/feed/" rel="self" type="application/rss+xml"/>
      <link href="https://mathstoc.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://mathstoc.wordpress.com/osd.xml" rel="search" title="Mathematical Distractions" type="application/opensearchdescription+xml"/>
      <link href="https://mathstoc.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A recreational (occasionally research) blog on topics in maths or computer science - by Kuikui</subtitle>
      <title>NLP Capstone – Mathematical Distractions</title>
      <updated>2018-04-11T09:00:15Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/ee873b6885d5</id>
    <link href="https://medium.com/@halden.lin/nlp-capstone-01-options-ee873b6885d5?source=rss-2759d54493c0------2" rel="alternate" type="text/html"/>
    <title>NLP Capstone | 01: Options</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Hello! This post is the first in a series that will document my progression through CSE 481n, taught by Prof. Yejin Choi at the University of Washington.</p><p>My github for this project can be found at: <a href="https://github.com/haldenl/nlpcapstone">https://github.com/haldenl/nlpcapstone</a></p><h3>What I hope to explore</h3><p>Over the course of the next 10 weeks, I intend to explore the intersection of V<strong>isualization (Vis) and Natural Language Processing (NLP)</strong>. I am particularly excited to explore the avenues through which Vis can be used to augment the interpretability of Neural Networks (NNs). I’ve spent the past year working around Vis (through classes and research) and am excited to bring what I’ve learned to problems in NLP. I intend to take a <strong>research oriented approach</strong> to this project.</p><h3>Relevant Work (a brief and incomplete list)</h3><ul><li><a href="https://arxiv.org/pdf/1506.02078.pdf">Visualizing and Understanding Recurrent Neural Networks, Karpathy et al. (2015)</a></li><li><a href="http://www.aclweb.org/anthology/N16-1082">Visualizing and Understanding Neural Networks in NLP, Li et al. (2016)</a></li><li><a href="https://distill.pub/2018/building-blocks/">The Building Blocks of Interpretability, Olah et al. (2017)</a></li><li><a href="https://arxiv.org/pdf/1612.08220.pdf">Understanding Neural Networks through Representation Erasure, Li et al. (2017)</a></li><li><a href="https://ufal.mff.cuni.cz/pbml/109/art-rikters-fishel-bojar.pdf">Visualizing Neural Machine Translation Attention and Confidence, Rikters et al. (2017)</a></li></ul><h3>Directions for Exploration</h3><h4>Option 1: Attention</h4><p>Visualizing and understanding <strong>attention</strong>, with a focus on its role in text summarization.</p><p>This is my most realized idea at this time, so I will spend more time here explaining my thoughts.</p><p><strong>Background:</strong></p><p>Visualizations of attention are often used in an effort to understand, and subsequently improve, decisions made by neural networks. To my knowledge, the most common way of visualization attention in seq2seq models is via a 2-dimensional heat-map, wherein the attention each decoding unit gives to each input token can be seen.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/612/1*KvfXxucogv8flZHndSJvHg.png"/>Rikters et al. (2017). An attention visualization for a seq2seq problem (in this case, translation). Whiter cells represent higher attention.</figure><p>There are a few issues with this format: (1) it is difficult to fit the words (as seen above) on the x-axis, harming readability; (2) this does not scale well with large input or output (e.g. summarization); and (3) we do not read single-tokens at a time (i.e. y-axis), and input and output are generally not in this format either.</p><p>As a whole, this format, while simple, is lacking in interpretability. The cognitive work-load of a viewer is less than optimal.</p><p>Alternatives can be found in literature, such as the following from Rikters et al. (2017).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jtRrxf5pIB-OCBsoqUjgJQ.png"/>Rikters et al. (2017). The input sequence is seen on top — output on bottom. Thicker lines denote higher attention.</figure><p>However, this also suffers from similar interpretability and scalability issues. Moreover, the thickness of lines as an encoding scheme does not lend itself easily to comparison between words.</p><p><strong>Proposed Exploration:</strong></p><p>Perhaps interaction can be used to create a more insightful and interpretable visualization framework for understanding attention. For example, text heat-maps are already used widely to visualize sentiment analysis.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*lsgeuBXGGBog4YkuQNgJVw.png"/>Lin et al. (2017). Visualization of sentiment analysis on a token-by-token basis.</figure><p>In a static context, using this method for attention would require repeat of the same input sequence for each word in the output sequence. Using interaction, however, a model creator could brush over single or sequences of words in the output sequence to view corresponding soft-alignment in the input sequence. Aggregate visualizations could be shown to supplement this view (either aggregates over a particular input / output sequence, or aggregates over all input / output sequences).</p><p><strong>Baseline: </strong>A user-interface for an interactive visualization of attention for seq2seq models (e.g. text summarization models). Exploration of how this visualization method can be used to either improve an existing models or understand the differences between models. This would, of course, require building a model from which to obtain data.</p><p><strong>Reach: </strong>Integration into an existing platform, e.g. in PyTorch to be viewable in TensorBoard.</p><h4>Option 2: Neural Network Cells</h4><p>Visualizing and understanding the role of cells in a neural network. In particular, the meaning of each cell within a model.</p><p><strong>Background:</strong></p><p>Though more exploration of relevant work will be required if this becomes the path of choice, Kaparthy et al. (2015) present work that serves as the basis for this idea. They show that the role of cells in a neural network can be interpreted by viewing their activations at each point in a given passage. For example, they found in their model a cell that fired at the end of a line (top left).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6yoF78PlE5ETNc9-kbya6w.png"/>Kaparthy et al. (2015). Different cells fire in different contexts.</figure><p><strong>Proposed Exploration:</strong></p><p>Perhaps a deeper exploration of the activations of neural network cells could lead to insights for visualizations that may aid the interpretability of neural networks.</p><p><strong>Baseline: </strong>Create a program or UI that generates a visualization (either interactive or static), of the activations of each cell in a neural network. Use this information to posit the meanings of each cell and build further visualizations as needed for exploration. This would require building a model from which to obtain observations.</p><p><strong>Reach:</strong> This is a little loose, as it is unclear how much exploration will be needed to produce results. Perhaps as in Option 1, integration of whatever tool created into an existing framework.</p><h4>Option 3: Gates and Value flow in RNN models.</h4><p>Visualizing and understanding the flow of values through gates in a Recurrent Neural Network (RNN). In particular, allowing for interpretability of gates and the ‘memory’ of cells.</p><p><strong>Background:</strong></p><p>My motivation for this idea is largely anecdotal. I recall while taking the undergraduate NLP course that the concepts of ‘gates’ and ‘memory’ in RNN models (e.g. GRU, LSTM) were difficult for me to wrap my head around. We have these giant networks with variable length ‘memory’ determined by gates that preform well as a result of this mechanism, but how exactly is this mechanism is used, and on what tokens at what points? Kaparthy et al. (2015) present a way of visualizing the ‘memory’ of cells in an LSTM model via plotting of cells and the time spent right or left saturated (right saturated cells remember values for longer periods of time).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/975/1*Qq9KrefGEl7tM4mPxUi99g.png"/>Kaparthy et al. (2015). Visualizing the cells and their time spent right or left saturated.</figure><p>Data-flow graphs have already been developed for neural networks (e.g. Wongsuphasawat et al. (2017)). An RNN specific model with a focus on interpretability of gates may be valuable for the NLP community.</p><p><strong>Proposed Exploration:</strong></p><p>Perhaps producing visualizations of a network of cells using information similar to above could lead to improvements in interpretability and aid model improvement.</p><p><strong>Baseline: </strong>Create a program or UI to generate a visualization, as described above, of an small RNN models. Use this generate insight into the model’s behavior (e.g. compare between models), and explore further from there.</p><p><strong>Reach: </strong>Support larger-scale networks. Integration into existing frameworks.</p><h3>Works Cited</h3><ul><li><a href="https://arxiv.org/pdf/1506.02078.pdf">Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. “Visualizing and understanding recurrent networks.” <em>arXiv preprint arXiv:1506.02078</em> (2015).</a></li><li><a href="https://arxiv.org/pdf/1506.01066.pdf">Li, Jiwei, <em>et al.</em> “Visualizing and understanding neural models in NLP.” <em>arXiv preprint arXiv:1506.01066</em> (2015).</a></li><li><a href="https://arxiv.org/pdf/1612.08220.pdf">Li, Jiwei, Will Monroe, and Dan Jurafsky. “Understanding neural networks through representation erasure.” <em>arXiv preprint arXiv:1612.08220</em> (2016).</a></li><li><a href="https://ufal.mff.cuni.cz/pbml/109/art-rikters-fishel-bojar.pdf">Rikters, Matīss, Mark Fishel, and Ondřej Bojar. “Visualizing neural machine translation attention and confidence.” <em>The Prague Bulletin of Mathematical Linguistics</em> 109.1 (2017): 39–50.</a></li><li><a href="http://idl.cs.washington.edu/files/2018-TensorFlowGraph-VAST.pdf">K. Wongsuphasawat <em>et al</em>., “Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow,” in <em>IEEE Transactions on Visualization and Computer Graphics</em>, vol. 24, no. 1, pp. 1–12, Jan. 2018.</a></li><li><a href="https://arxiv.org/pdf/1703.03130.pdf">Lin, Zhouhan, <em>et al.</em>, “A structured self-attentive sentence embedding.” <em>arXiv preprint arXiv:1703.03130</em> (2017).</a></li></ul><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ee873b6885d5" width="1"/></div>
    </content>
    <updated>2018-04-04T06:43:21Z</updated>
    <category term="visualization"/>
    <category term="data-visualization"/>
    <category term="machine-learning"/>
    <category term="nlp"/>
    <category term="computer-science"/>
    <author>
      <name>Halden Lin</name>
    </author>
    <source>
      <id>https://medium.com/@halden.lin?source=rss-2759d54493c0------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/1*Hzu2ZqgloT0I1F6Kwg8OPA.jpeg</logo>
      <link href="https://medium.com/@halden.lin?source=rss-2759d54493c0------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@halden.lin" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Halden Lin on Medium</subtitle>
      <title>Stories by Halden Lin on Medium</title>
      <updated>2018-04-11T09:00:14Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-9203775015655831448.post-3003438141513431489</id>
    <link href="https://teamoverfit.blogspot.com/2018/04/1-initial-project-ideas.html" rel="alternate" type="text/html"/>
    <title>#1 Initial Project Ideas</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h2 style="height: 0px;"><span style="font-family: Arial, Helvetica, sans-serif;">Team Overfit</span></h2><h3><span style="color: #999999; font-family: Arial, Helvetica, sans-serif;"><br/></span></h3><h3><span style="color: #999999; font-family: Arial, Helvetica, sans-serif;">Project repo: <span style="font-size: 18.72px;"><a href="https://github.com/pinyiw/nlpcapstone-teamoverfit">https://github.com/pinyiw/nlpcapstone-teamoverfit</a></span></span></h3><h4><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Team members: Dawei Shen, Pinyi Wang, Xukai Liu</span></h4><div><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><b>Blog Post: #1: 04/03/2018</b></span></div><div><span style="color: #444444;"><br/></span></div><div><span style="font-family: Arial, Helvetica, sans-serif;">We are likely going to choose the start-up mode and actually build some cool projects! Here's some of our initial project ideas:</span></div><div><ul><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Twitter Hate Speech Detection</b></span></li><ul><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Dataset:</b> <a href="https://github.com/zeerakw/hatespeech">https://github.com/zeerakw/hatespeech</a></span></li><ul><li><span style="font-family: Arial, Helvetica, sans-serif;">This dataset contains the tweets ID that are labelled as either Raicist, Sexist or Neither Racist or Sexist.</span></li></ul><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Minimal viable plan:</b> We could use LSTM sequence to vector encoding to extract critical features of the tweets. Then, we use RNN and attention mechanism to output the label of the speech and the hate score of tweets in each category.</span></li><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Stretch goals:</b> </span></li><ul><li><span style="font-family: Arial, Helvetica, sans-serif;">We could experiment with different model configurations to improve the performance of the model.</span></li><li><span style="font-family: Arial, Helvetica, sans-serif;">We could build a twitter bot that can collect reports or hateful speech from user, which can be used to train on, so that the model can adapt the model to the latest slangs.</span></li></ul></ul></ul><span style="font-family: Arial, Helvetica, sans-serif;"><br/></span><ul><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Virtual Date chat bot</b></span></li><ul><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Description: </b></span><span id="docs-internal-guid-6febb8b7-8f58-f3ab-f461-d5c0a5077f07"><span style="font-family: Arial; vertical-align: baseline; white-space: pre-wrap;">Use conversations between couples to train a chat bot that user can flirt with when they feel lonely.</span></span></li><li><span><span style="font-family: Arial; vertical-align: baseline; white-space: pre-wrap;"><span id="docs-internal-guid-6febb8b7-8f59-3e54-c748-e595571ddaaa"><span style="font-weight: 700; vertical-align: baseline;">Minimal viable plan:</span><span style="vertical-align: baseline;"> Use neural machine translation and attention mechanism to train a generative chat bot. Some training data can be obtained from romantic movies’ dialogue or from Twitter. The challenge would be how to use NLP to classify whether a conversation is flirt or not.</span></span></span></span></li><li><span><span style="font-family: Arial; vertical-align: baseline; white-space: pre-wrap;"><span><span style="vertical-align: baseline;"><span id="docs-internal-guid-6febb8b7-8f5a-50ce-dc63-78e864ac8b2e"><span style="font-weight: 700; vertical-align: baseline;">Stretch goals:</span><span style="vertical-align: baseline;"> Train the chat bot so that it has consistent personality and have long term memory of chat history.</span></span></span></span></span></span></li></ul></ul><span style="font-family: Arial;"><span style="white-space: pre-wrap;"><br/></span></span><ul><li><span id="docs-internal-guid-6febb8b7-8f5a-cf38-76f0-a21179c20dde"><span style="font-family: Arial; vertical-align: baseline; white-space: pre-wrap;"><b>Detect Violations of Laws (Virtual Lawyer)</b></span></span></li><ul><li><span><span style="font-family: Arial; vertical-align: baseline; white-space: pre-wrap;"><b>Description:</b> The project is to try with different models and learn to detect the laws broke given the description of behaviors or given the sworn testimonies in the court.</span></span></li><li><span><span style="font-family: Arial; vertical-align: baseline; white-space: pre-wrap;"><span id="docs-internal-guid-6febb8b7-8f5b-2fce-e8fd-4e47759e160c"><span style="vertical-align: baseline;"><b>Minimal viable plan:</b> Experiment with LSTM and attention mechanism</span><span style="background-color: white; color: #757575; vertical-align: baseline;">.</span><span style="vertical-align: baseline;"> Given the testimony, detects whether the law is broken.</span></span></span></span></li><li><span style="font-family: Arial, Helvetica, sans-serif;"><b>Stretch goals:</b></span></li><ul><li><span style="font-family: Arial, Helvetica, sans-serif;">Given the testimony, detects and return the list of laws have been broken.</span></li><li><span style="font-family: Arial, Helvetica, sans-serif;">Generate texts to offend the testimony.</span></li><li><span style="font-family: Arial, Helvetica, sans-serif;">Generate texts to defend the testimony.</span></li></ul></ul></ul><span style="font-family: Arial, Helvetica, sans-serif;"><br/></span></div></div>
    </summary>
    <updated>2018-04-04T06:36:00Z</updated>
    <author>
      <name>Team Overfit</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-9203775015655831448</id>
      <author>
        <name>Team Overfit</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="https://teamoverfit.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="https://teamoverfit.blogspot.com/feeds/posts/default?alt=rss" rel="self" type="application/rss+xml"/>
      <title>NLP Capstone</title>
      <updated>2018-04-11T09:00:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://deeplearningturingtest.wordpress.com/?p=3</id>
    <link href="https://deeplearningturingtest.wordpress.com/2018/04/04/the-journey-begins/" rel="alternate" type="text/html"/>
    <title>Top 3 Project Ideas I’m Excited For</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Visualizing a text-based description: Train a model to learn a language to image mapping with simple descriptions. The minimum plan would be to feed the model short text descriptions like “white hat” and “black cat” with their corresponding visual outputs. Then, if the text “white cat” is input at test time, the model should output … <a class="more-link" href="https://deeplearningturingtest.wordpress.com/2018/04/04/the-journey-begins/">Continue reading <span class="screen-reader-text">Top 3 Project Ideas I’m Excited For</span> <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ol>
<li><strong>Visualizing a text-based description:</strong> Train a model to learn a language to image mapping with simple descriptions. The minimum plan would be to feed the model short text descriptions like “white hat” and “black cat” with their corresponding visual outputs. Then, if the text “white cat” is input at test time, the model should output the cat with the same shade of white as the hat. Stretch goals include visualizing multiple objects in the same picture and/or visualizing them in different spatial orientations with respect to each other (e.g. on top of, inside, underneath, next to, etc.).</li>
<li><strong>Generating Multimodal Word Embeddings:</strong> The goal is to create word embeddings that describe a word more holistically from multiple modalities like audio and visual inputs. One possible approach is to concatenate pre-trained word embeddings (e.g. GloVe vector) with additional features based on what context the word is in the present sentence. Then, concatenate this with features generated from a deep fully connected layer of a ConvNet where the input is an image of the actual word (e.g. car). Stretch goals include using these augmented embeddings to enhance performance in applications like sentiment analysis or further augmenting these embeddings with audio features of the word being pronounced (which can help distinguish different meanings of the word).</li>
<li><strong>Dialogue and Information State Tracking:</strong> The goal is to create a model that can either receive contextual information as text or probe the environment with questions and receive an answer as text. This text can either be input into a linear or tree LSTM for entity extraction, coreference resolution, parsing, and/or other algorithms which can extract valuable contextual information. Then, this text can be used to update the current dialogue and/or information state using deep reinforcement learning. The policy then uses the updated state to choose the next action and hopefully keep repeating this until the text is satisfactorily understood. Stretch goals include using this model to answer test questions about the reading material using a machine comprehension model like the ReasoNet architecture.</li>
</ol>
<p> </p>
<p>Git Repo URL: <a href="https://gitlab.cs.washington.edu/ananthgo/cse481n-capstone" rel="nofollow">https://gitlab.cs.washington.edu/ananthgo/cse481n-capstone</a></p>
<p>This project is in research mode with a heavy focus on making improvements in keeping track of the meaning behind lines of input text.</p></div>
    </content>
    <updated>2018-04-04T05:42:14Z</updated>
    <category term="Uncategorized"/>
    <author>
      <name>ananthgo</name>
    </author>
    <source>
      <id>https://deeplearningturingtest.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://deeplearningturingtest.wordpress.com/feed/" rel="self" type="application/rss+xml"/>
      <link href="https://deeplearningturingtest.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://deeplearningturingtest.wordpress.com/osd.xml" rel="search" title="NLP Capstone Project Updates - Ananth" type="application/opensearchdescription+xml"/>
      <link href="https://deeplearningturingtest.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>NLP Capstone Project Updates – Ananth</title>
      <updated>2018-04-11T09:00:12Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/93d82eed396c</id>
    <link href="https://medium.com/nlp-capstone-blog/a-discussion-of-language-tasks-for-the-nlp-capstone-blog-post-1-93d82eed396c?source=rss----9ba3897b6688---4" rel="alternate" type="text/html"/>
    <title>A Discussion of Language Tasks for the NLP Capstone (Blog Post #1)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Welcome to Team Illimitatum! In this first blog post, we’ll discuss the top three/four ideas that emerged over several days of brainstorming. We started the week with seven exciting potential projects and carefully narrowed them down to these three ideas after taking into consideration feasibility, dataset availability, scope, and complexity for each project. Here they are!</p><h3>1. Machine Dictionary</h3><h4>Motivation &amp; Context</h4><p>The breadth and ubiquity of data and online material is both a gift and a curse. Availability of resources ultimately leads to better-informed decisions, but puts an enormous strain on a researcher to</p><ul><li>Search for sources while having to determine which ones are credible and which ones are not</li><li>Read passages and excerpts of these sources</li><li>Reach an understanding that is consistent with all of the sources they have seen</li></ul><p>On one hand, a concept or idea embedded in lengthy text (a textbook, article, or research paper, etc.), makes it so the reader has to achieve some basic understanding of the context surrounding the concept of interest as well as find all related details in order to learn the concept. On the other hand, basic web definitions and blogs <em>may</em> provide a more presentable and explicit definition of a particular concept, but may be lacking in the amount detail a researcher is looking for along with confidence in its credibility.</p><p>Summarization is a task in NLP that aims to reduce passages to a more condense form without sacrificing semantic meaning. The trouble then, of needing to make sense of lengthy text can be alleviated by solving this task.</p><p>Entity recognition is another task in NLP where the goal is to highlight “entities” or figures of significance in text that conventially refer to nouns people, places, and organizations. In a medical context, entities could include chemical names, technical terms pertaining to the anatomy of the human body, and diseases.</p><p>We aim to combine aspects of both summarization and entity recognition to solve the three “maladies” of research we’ve outlined above: to produce a model that produces the most probable definition for a given entity that would be consistent with as much of the data as possible.</p><p>With the intention of training such a model only on publications and research papers, we hope to pave the way for “specialized dictionaries”. Training the data on all publications from a particular field of study would simultaneously restrict the search space to only credible and relevant sources for any definition contained. Providing a generated definition would also prevent the need for the researcher to hunt for it in every source and make sense of it, themselves.</p><h4>Minimum Viable Product</h4><p>Steps toward the realization of this model involve finding a <strong>dataset</strong>, defining <strong>metrics of accuracy and performance</strong>, and determining a reasonable <strong>baseline</strong>.</p><p>Semantic Scholar’s <a href="http://labs.semanticscholar.org/corpus/">Open Research Corpus</a> provides over 20 million publications in Computer Science, Neuroscience, and Medicine. While also providing the metadata for each publication, the dataset is primed for the task as it provides credible sources while opening up the possibility of testing the model in three different fields of study.</p><p>Metrics of accuracy and performance will be difficult to define. At the moment, we plan on using precision and recall of relevant terms found in Google, Oxford, and Webster definitions and whether they appear in a generated definition.</p><p>As for determining a baseline, we plan to implement a standard approach to the Question-Answering task, where the question is always “What is X?”. Ideally, our model should be able to provide more complete definitions with more tangential detail than a standard QA model.</p><h4>Stretch Goal: Ontology Matching</h4><p>If we were to succeed at the task, applications of our methodology could lead to advancement in both summarization and entity recognition. However, a working version of the model could also pave the way for a novel approach to Ontology Matching.</p><p>We could extend our model to solve Ontology Matching by first extracting all significant entities from the corpora. We could then assign each of these entities a definition through our model, and cluster terms based on the definitions accordingly. By doing so, we can in a sense eliminate entities that are linearly dependent with another, reducing the set of entities to one in which their definitions are unique and different enough to be stand-alone definitions.</p><p>Through this, we could achieve a new means of standardizing vocabulary in order to clean up knowledge bases or assist in the advent of a new field. Deep Learning in particular has experienced a lot of redundancy in notation and definitions and would benefit from this application.</p><h3><strong>2. Natural Language Visual Reasoning</strong></h3><h4><strong>Motivation &amp; Context</strong></h4><p>Natural Language Visual Reasoning is a task which involves answering questions about an image. Extracting information from an image is an incredibly important but difficult task. Pictures provide a plethora of context about the world, especially about the relationships between objects. It’s also important to be able to extract knowledge about these relationships through natural language. If we can successfully reason about image content, we can change the way we train models and how they integrate with the physical world.</p><p>This task is also exciting because it combines Computer Vision and Natural Language Processing. While this goal is fairly lofty, researchers at Cornell and Facebook have collaborated to create a new visual reasoning language dataset (<a href="http://lic.nlp.cornell.edu/nlvr/">http://lic.nlp.cornell.edu/nlvr/</a>), which is a simplified set of images (which contain shapes and colors of a certain format) and sentences. One of the major motivations of this project is the fact that this dataset is so vast and complete.</p><h4><strong>Minimum Viable Product</strong></h4><p>The Visual Reasoning task can be described as follows. The below image represents one instance of training data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/924/1*O1uEL-Ljs3OgfkVeTJZJuQ.png"/>Example training instance</figure><p>As you can see, each piece of data consists of a statement and an image of three boxes. If the statement is true of the image, then we output true, otherwise false. There are two viable courses of action for this problem. In the first approach, we can take the current best performing model (<a href="https://arxiv.org/pdf/1511.02799.pdf">Neural Module Networks</a>) and improve the model to produce better results. In the second approach, we can start from scratch and try to develop a model that is uniquely trained for this dataset. It’s important to note that the Neural Module Network can be applied to any image, whereas we can train a model that can only be applied to images of this type.</p><p>Putting it all together, the end product should be able to take the sentence and image as input, extract all relevant information from the image into some intermediate form, validate the criteria described in the sentence for each image, and output the correct answer. We can start by coming up with a basic model which represents each image as a list of objects (yellow triangle at (a, b), blue square at (c, d), black circle at (e, f)) and finds a relationship between the sentence and the list of objects. A more effective model might try to represent the image in a different way, or utilize some of the techniques from the Neural Module Network paper.</p><p>The evaluation metrics for this project are very simple. The output of the model will always be true or false and we can do a simple accuracy test to determine the strength of our model.</p><h4><strong>Stretch Goals</strong></h4><p>A stretch goal for this project would be to generalize to different pictures of the same type. For example, what would happen if we changed the shapes and colors in the picture? What would happen if we changed the type of picture? Trying to achieve a high accuracy on different types of pictures would definitely be a stretch for this project.</p><h3><strong>3. You Can Read This, Machines Can’t</strong></h3><h4><strong>Motivation and Context</strong></h4><p>The motivation for this project idea came from one of the datasets that Yonatan introduced to the class. Research has shown that humans are able to read the following sentence, but it turns out that computers are not able to do the same:</p><blockquote>“Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn’t mttaer in waht oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and lsat ltteer be at the rghit pclae”</blockquote><p>This is an interesting project because it explores how robust Machine Learning models can be to error (like misspellings). It also explores how much computers might be able to infer given surrounding words.</p><p>Another interesting aspect of this project is the available data. We can constantly generate new scrambled sentences with a simple algorithm.</p><h4><strong>Minimum Viable Product</strong></h4><p>The goal of this task is to take a scrambled English sentence and produce the unscrambled version. In particular, each word in the English sentence will have the correct first and last letter but the letters in between might all be incorrect. There are several variations of this data. In the easiest version, the letters in the middle of each word are all correct but they are scrambled. A slightly more difficult challenge would be evaluating sentences where the letters in the middle of each word are semi-random and the number of letters may not correspond to the number of letters in the actual word.</p><p>In order to approach this problem, we can start with a very basic approach (for the easier version of the dataset) where we create a very efficient dictionary search algorithm. This approach would not use any Machine Learning and might be very slow. A better approach might be to train a very effective sequence to sequence model which maps a scrambled sentence to the correct sentence. Since we can mutate the data in many ways, we can train several different versions of the same sentence.</p><p>We could also explore a more effective way of training the model, rather than training on several mutations of the same sentence. One approach would be to represent words as a volume in N-Dimensional space, so a model would be able to recognize any data point within that volume as the original word. Another interesting approach would be to infer what the next word might be based on the most likely translation of the previous word. This would not only involve choosing a likely translation of the word, but also choosing the most likely translation of the sentence. A third approach might be to treat the the scrambled sentence as a language and use Machine Translation techniques to “translate” the sentence to English.</p><p>The performance metric for this would be the number of correctly predicted words in each sentence.</p><h4><strong>Stretch Goals</strong></h4><p>The stretch goal for this project would be to recognize any mutated sentence as the original sentence. This includes random letters, swapped letters, dropped letters, etc. We also could generate new datasets in different languages pretty easily. It might be interesting to see how our model performs on languages like Spanish or French.</p><h3>Conclusion</h3><p>Questioning the immediate vs. long-term impact, along with the tractability and scope of each task, helped define our motivations while ensuring to the best of our knowledge, the viability of each potential project. We asked each other questions along the lines of “why is this task important?”,<br/>“if other people have done this before, how can we do it better?”, and “can it be done in 10 weeks?”.</p><p>Ultimately, we want our efforts and insights to have potentially long-term and significant consequences. Without consciously keeping track of whether an idea was research or industry/product oriented, we both came up with only research-oriented proposals. These proposals, if executed properly, would have impacts on related works, having fundamental and methodological influence through novel approaches and perspectives rather than aiding in the downstream application of any particular product. For the time being, we consider ourselves a research mode team.</p><p>Link to Code: <a href="https://github.com/NLP-Capstone-Project">https://github.com/NLP-Capstone-Project</a></p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=93d82eed396c" width="1"/><hr/><p><a href="https://medium.com/nlp-capstone-blog/a-discussion-of-language-tasks-for-the-nlp-capstone-blog-post-1-93d82eed396c">A Discussion of Language Tasks for the NLP Capstone (Blog Post #1)</a> was originally published in <a href="https://medium.com/nlp-capstone-blog">NLP Capstone Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></div>
    </content>
    <updated>2018-04-04T03:24:13Z</updated>
    <category term="machine-learning"/>
    <category term="university-of-washington"/>
    <category term="naturallanguageprocessing"/>
    <category term="capstone"/>
    <category term="software"/>
    <author>
      <name>Karishma Mandyam</name>
    </author>
    <source>
      <id>https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4</id>
      <logo>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</logo>
      <link href="https://medium.com/nlp-capstone-blog?source=rss----9ba3897b6688---4" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/nlp-capstone-blog" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>A Journey Through CSE 481N, the Natural Language Processing Capstone Course at the University of Washington - Medium</subtitle>
      <title>NLP Capstone Blog - Medium</title>
      <updated>2018-04-11T09:00:11Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/f7412889d221</id>
    <link href="https://medium.com/@be.li.nda/nlp-capstone-blog-1-f7412889d221?source=rss-fad49d942bf3------2" rel="alternate" type="text/html"/>
    <title>NLP Capstone Blog #1</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Brainstorming project ideas for my CSE 481N NLP Capstone class.</p><p>The following project ideas are all research-oriented (research mode).</p><h3>Document Level Entity-Entity Sentiment Analysis</h3><p><strong>Github URL</strong>: <a href="https://github.com/eunsol/document-e2e-sent">https://github.com/eunsol/document-e2e-sent</a></p><p>Given a document and named entities within the documents, we try to model the sentiment between the various entities in a document, categorizing the sentiments as “positive,” “negative,” or “neutral/no sentiment.” We focus especially on world news, which usually involve many named entities (i.e. countries, world leaders, etc.). For example, given such a document and the named entities in the document:</p><blockquote>Iran’s President <strong>Mahmoud Ahmadinejad</strong> said <strong>Iran</strong> opposes the “aggressive and arrogant policies” of the <strong>United States</strong>, local satellite <strong>Press TV</strong> reported Sunday.</blockquote><blockquote>“The <strong>Iranian nation</strong> opposes the aggressive and arrogant policies of the <strong>U.S. government</strong> and will stand against them forever,” <strong>Ahmadinejad</strong> told a religious conference Saturday.</blockquote><blockquote><strong>Iranian president</strong> also urged the <strong>United States</strong> and other western countries to change their attitude.</blockquote><blockquote>“You should change your attitude,” <strong>Ahmadinejad</strong> said.</blockquote><blockquote><strong>Iran</strong> has constantly accused the <strong>United States</strong> and western powers of earmarking funds to stir up protests against the country and its way of dealing with human rights.</blockquote><p>We should be able to generate the following set of relations between the <strong>bolded</strong>, named entities:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/290/1*ThoNzALuI1a78NOYImbGiw.png"/>Graphical representation of the annotations for the document above. Note that the <strong>red edges</strong> represent <strong>negative</strong> sentiment, <strong>green edges </strong>represent <strong>positive</strong> sentiment, <strong>light green edges</strong> represent <strong>non-negative</strong> sentiments, and <strong>light red edges</strong> represent <strong>non-positive</strong> sentiments.</figure><h4>Minimal Viable Action Plan</h4><ul><li>Fortunately, I already have a pre-existing, labelled dataset I can work with (<a href="https://homes.cs.washington.edu/~eunsol/project_page/acl16/index.html">some examples here</a>), so I don’t have worry about procuring data.</li><li>Implement and train a basic LSTM. This will serve as the baseline model for which to compare results.</li><li>Improve the LSTM with attention, word and sentiment embeddings, dropout, etc. Experiment with various hyper-parameter settings such as learning rate, dropout rate, batch sizes, etc. to maximize performance.</li></ul><h4>Stretch Goals</h4><ul><li>Implement and train a bi-affine relation attention network as delineated in <a href="https://arxiv.org/pdf/1802.10569.pdf">this paper</a>, and adapt it for sentiment analysis between entities.</li><li>Experiment with new network architectures for the bi-affine network to hopefully improve results.</li></ul><p>As I am 99.99% sure that I will be doing entity-entity sentiment analysis for this class, the two ideas below are just here to represent research directions that I’m generally interested in, and how I would approach working on them.</p><h3>Generating Conversational Responses</h3><p>Given an input sentence, generate an appropriate and believable sentence in response. As chat bots are on the rise, these sorts of problems are becoming increasingly pervasive.</p><h4>Minimal Viable Action Plan</h4><ul><li>Procure data from some pre-existing dataset. <a href="http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">This movie dialogue corpus</a> could be a good source. Note though that movie dialogue can be written quite differently from everyday dialogue, and may vary depending on movie genre and setting.</li><li>Implement beam search as described in the lecture slides and in <a href="https://geekyisawesome.blogspot.com/2016/10/using-beam-search-to-generate-most.html">the referenced blog post</a>. This will be used as the benchmark model to which the performance of other models will be compared to.</li></ul><h4>Stretch Goals</h4><ul><li>Implement the semantically-conditioned LSTM-based model as described in <a href="https://arxiv.org/pdf/1508.01745.pdf">the following paper</a>.</li><li>Attempt to improve the model in some way.</li></ul><h3>Fake News/Misinformation Detection</h3><p>Given quotes from news articles, notable politicians, or online communities, categorize it as “true,” “mostly true,” “half true,” “mostly false,” “false,” or “pants on fire.”</p><h4>Minimal Viable Action Plan</h4><ul><li>Politifact is a great data source, and an inspiration for this project.</li><li>Based off of <a href="https://arxiv.org/pdf/1803.03786.pdf">this paper</a>, implement and train a model that combines a bidirectional attention-based LSTM with an SVM. In particular, the model works by feeding the output of the last hidden layer in the LSTM network into an SVM, which then outputs the label.</li></ul><h4>Stretch Goals</h4><ul><li>Since the source paper applied the model to news articles, the model may or may not adapt well to quotes. It would be good to experiment with the architecture of the LSTM and/or determine whether the LSTM + SVM model actually performs better than just an LSTM or just an SVM.</li><li>Experiment with various combinations of LSTM and other machine learning models in an attempt to improve performance.</li></ul><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f7412889d221" width="1"/></div>
    </content>
    <updated>2018-04-04T00:34:38Z</updated>
    <category term="machine-learning"/>
    <category term="deep-learning"/>
    <category term="naturallanguageprocessing"/>
    <author>
      <name>Belinda Zou Li</name>
    </author>
    <source>
      <id>https://medium.com/@be.li.nda?source=rss-fad49d942bf3------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/0*A16vZtRWBzxYPlmn.</logo>
      <link href="https://medium.com/@be.li.nda?source=rss-fad49d942bf3------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@be.li.nda" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Belinda Zou Li on Medium</subtitle>
      <title>Stories by Belinda Zou Li on Medium</title>
      <updated>2018-04-11T09:00:11Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-5600014144802012716.post-4534461914269998368</id>
    <link href="https://nlpcapstonesemparse.blogspot.com/2018/04/blog-post-2-warmup.html" rel="alternate" type="text/html"/>
    <title>Blog Post 2: Warmup</title>
    <summary>I'm working on my project in my fork of Allennlp. I got the dataset and have written the DatasetReader code to preprocess and index the dataset. I plan to add tests for this, and I need to add further preprocessing code such as splitting variables on camel casing.</summary>
    <updated>2018-04-03T21:53:00Z</updated>
    <author>
      <name>nlpcapstone</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-5600014144802012716</id>
      <author>
        <name>nlpcapstone</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="https://nlpcapstonesemparse.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="https://nlpcapstonesemparse.blogspot.com/feeds/posts/default?alt=rss" rel="self" type="application/rss+xml"/>
      <title>NlpCapstone</title>
      <updated>2018-04-11T09:00:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://sarahyu.weebly.com/cse-481n/inaugural-blog-post</id>
    <link href="http://sarahyu.weebly.com/cse-481n/inaugural-blog-post" rel="alternate" type="text/html"/>
    <title>Inaugural Blog Post</title>
    <summary>Welcome to the first blog post of Jekyll-Hyde (my very cool and somewhat related group-of-1 name).As the name might reveal, I’m interested in using NLP to uncover the duality of language, the ability to simultaneously present both sides of a coin, whether in everyday conversation or more curated prose. Because language matters; so much so that we mend and mold our language to navigate the different social environments and spaces we inhabit, whether for power, survival, or acceptance,  [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="paragraph">Welcome to the first blog post of <u><em>Jekyll-Hyde</em></u><em> </em><u>(</u>my very cool and somewhat related group-of-1 name).<br/>As the name might reveal, I’m interested in using NLP to uncover the duality of language, the ability to simultaneously present both sides of a coin, whether in everyday conversation or more curated prose. Because language matters; so much so that we mend and mold our language to navigate the different social environments and spaces we inhabit, whether for power, survival, or acceptance, and often in the most primitive and subconscious ways. And because in today’s (supposedly) civilized world, the language we employ with another can be a sort of proxy for the relationship we share. I'd like to get at some of these ideas through the NLP capstone and think the next three topics are a potential start. <br/><br/>1) Language Accommodation (or my unlikely paper title: <em>Nice Guy by Day, A**hole by Night: Language Accommodation for Self-Presentation in Subreddit Communities)</em><ul><li>​MVP: Scrape Reddit user data, identify language baselines for a given subreddit or capture linguistic differences of a single user across subreddits</li><li>Stretch Goals: Do both (subreddit baselines and user difference) and not only identify a user's language accommodation, but how they fall in line with the communities' baseline (a kind of hive mentality)</li></ul><br/>2) Identifying Condescension (another working title: "<em>Well, Actually": Identifying Ambiguities in emails from 'helpful' colleagues</em>)<ul><li>​MVP: Identify an appropriate data source (ideally emails or more personal interactions), manually identify possible ambiguities, train model (maybe one that doesn't require a large dataset) to identify ambiguous spans of a sentence. </li><li>Stretch Goals: Begin identifying entity-entity-relationships with cues from ambiguous interactions or maybe something cooler about context and the different meanings if ambiguous...</li></ul><br/>3) Identifying Disrespect (might as well for consistency: <em>Linguistic (dis)R.E.S.P.E.C.T. - Addressing 90% of Comment Sections)</em><ul><li>​MVP: Scrape Youtube comment data, classify comments as hateful/not hateful, train model on classified comments, test and tune</li><li>Stretch Goals: Train a portable model that can work with content from other mediums such as Twitter and Reddit</li></ul><br/>I'll be pursuing one of these ideas in <strong>research mode</strong> and you can follow along at:<br/>                                      https://github.com/sarahyu17/481n</div>  <div class="wsite-spacer" style="height: 50px;"/>  <div> 				<form action="http://www.weebly.com/weebly/apps/formSubmit.php" enctype="multipart/form-data" id="form-147197638403517045" method="POST"> 					<div class="wsite-form-container" id="147197638403517045-form-parent" style="margin-top: 10px;"> 						<ul class="formlist" id="147197638403517045-form-list"> 							<h2 class="wsite-content-title">Any Favorite Paper Titles?</h2>  <label class="wsite-form-label wsite-form-fields-required-label"><span class="form-required">*</span> Indicates required field</label><div><div class="wsite-form-field" style="margin: 5px 0px 0px 0px;">   <label class="wsite-form-label" for="input-789590629342031487">Paper Title <span class="form-required">*</span></label>   <div class="wsite-form-radio-container">     <span class="form-radio-container"><input id="radio-0-_u789590629342031487" name="_u789590629342031487" type="radio" value="#1"/><label for="radio-0-_u789590629342031487">#1</label></span><span class="form-radio-container"><input id="radio-1-_u789590629342031487" name="_u789590629342031487" type="radio" value="#2"/><label for="radio-1-_u789590629342031487">#2</label></span><span class="form-radio-container"><input id="radio-2-_u789590629342031487" name="_u789590629342031487" type="radio" value="#3"/><label for="radio-2-_u789590629342031487">#3</label></span>   </div>   <div class="wsite-form-instructions" id="instructions-Paper Title" style="display: none;"/> </div></div> 						</ul> 					</div> 					<div style="display: none;"> 						<input name="weebly_subject" type="text"/> 					</div> 					<div style="text-align: left; margin-top: 10px; margin-bottom: 10px;"> 						<input name="form_version" type="hidden" value="2"/> 						<input id="weebly-approved" name="weebly_approved" type="hidden" value="approved"/> 						<input name="ucfid" type="hidden" value="147197638403517045"/> 						<input name="recaptcha_token" type="hidden"/> 						<input name="opted_in" type="hidden" value="0"/> 						<input type="submit"/> 						<a class="wsite-button"> 							<span class="wsite-button-inner">vote</span> 						</a> 					</div> 				</form> 				<div class="recaptcha" id="g-recaptcha-147197638403517045"/> 			  			</div></div>
    </content>
    <updated>2018-04-03T07:00:00Z</updated>
    <category term="Uncategorized"/>
    <source>
      <id>http://sarahyu.weebly.com/cse-481n</id>
      <author>
        <name>Sarah Yu &lt;br/&gt; Team Jekyll-Hyde</name>
      </author>
      <link href="http://sarahyu.weebly.com/cse-481n" rel="alternate" type="text/html"/>
      <link href="http://sarahyu.weebly.com/6/feed" rel="self" type="application/rss+xml"/>
      <subtitle>CSE 481N</subtitle>
      <title>Sarah yu - CSE 481N</title>
      <updated>2018-04-06T00:30:16Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/5258ddd9eedd</id>
    <link href="https://medium.com/@hongnin1/nlp-5258ddd9eedd?source=rss-c450eb982161------2" rel="alternate" type="text/html"/>
    <title>NLP</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is a blog about Natural Language Processing.</p><p>Group member: Ning Hong, Zichun Liu, Zhou Sujie</p><p>Team name: The Bugless</p><p>Three topics our team is considering doing (All start-up mode):</p><ol><li><strong>Movie summarization (imdb)/Food review summarization (yelp)</strong></li></ol><p>minimal viable action plan: scrap data from twitter/imdb/rotten tomato/yelp and train our model to be able to summarize the reviews for movies (or food if we are scrapping data from yelp). The summarization of the movie is how the audience feel about the movie in general, for example, given a movie title, our model should be able to produce something like this: It is violent but good.</p><p>stretch goals: Output a overall review for the movie instead of simple sentences, for example, given a movie title as input, our model should output: “The Terror got my full attention from beginning to end. I couldn’t turn away. I didn’t want to turn away. For me, that’s extremely rare.”</p><p>Another stretch goal is to be able to detect sentiment not only in the US market, but also in China market by using data from DouBan (one of the largest movie review site for China), and compare the sentiment between US and China for a certain movie.</p><p><strong>2.Chinese Phoneticization mapping:</strong></p><p>The way input Chinese to machine is by typing Pinyin, a kind of phoneticization for Chinese sentence and words, which almost every boy in China know about. However, one Chinese character may have many phoneticization can one Chinese phoneticization sequence may map to many Chinese sentences. When typing Chinese to a machine by Pinyin, the machine will rank the potential Chinese sentences by preferences. However, the ranking may not be so consistent to the context. Therefore, we want to generate a language model that map from Pinyin (English character) to Chinese words and sentences depend on context and speaking habit of this person. In addition, this can be potentially turned into an online learn algorithm.</p><p>First step: finding data, where input is Chinese phoneticization (pinyin) and output is Chinese sentences and words. Also, we need to find a good algorithm to do that, and onlint learning algorithm will be better.</p><p>Ideas come from Neural Input Method Engine of last quarter: <a href="https://www.dropbox.com/sh/z3idncggfpwm8rs/AAAnHVvHIPvt_CxTXsaDVqvda?dl=0&amp;preview=teamverynatural_3417269_43042970_Very+Natural+Final+Report.pdf">https://www.dropbox.com/sh/z3idncggfpwm8rs/AAAnHVvHIPvt_CxTXsaDVqvda?dl=0&amp;preview=teamverynatural_3417269_43042970_Very+Natural+Final+Report.pdf</a></p><p><strong>3. Flirt tutor:</strong></p><p>When you chat with your beloved boy/girl, you must have some hard time picking interesting and flirting word to response. Then let the machine teach you! Specifically, we want to build a model that can generate cute response giving the context of chatting. This model is neural based.</p><p>First step: figuring out what is the right dataset and find such. Meanwhile, consider the right model to use and find paper/resources about is.</p><p><strong>APIs:</strong></p><p>Weibo API:</p><p><a href="https://www.cs.cmu.edu/~lingwang/weiboguide/">Sina Weibo API Guide</a></p><p>Allen NLP API:</p><p><a href="https://allenai.github.io/allennlp-docs/">Home - AllenNLP 0.4.1 documentation</a></p><p>PyTorch basic functions:</p><p><a href="http://pytorch.org/docs/0.3.1/torch.html">torch - PyTorch master documentation</a></p><p>PyTorch examples:</p><p><a href="https://github.com/pytorch/examples">pytorch/examples</a></p><p>More detailed examples on PyTorch:</p><p><a href="https://colab.research.google.com/drive/11iLtGFDpnIuHj5B0rQDGG5lqq6BQ8FRh">https://colab.research.google.com/drive/11iLtGFDpnIuHj5B0rQDGG5lqq6BQ8FRh</a></p><p>PyTorch Tutorial:</p><ul><li><a href="https://courses.cs.washington.edu/courses/cse446/18wi/sections/section7/446_pytorch_tutorial.html">446_pytorch _tutorial</a></li><li><a href="http://pytorch.org/tutorials/">Welcome to PyTorch Tutorials - PyTorch Tutorials 0.3.1.post2 documentation</a></li></ul><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5258ddd9eedd" width="1"/></div>
    </content>
    <updated>2018-04-03T02:48:17Z</updated>
    <category term="nlp"/>
    <author>
      <name>Ning Hong</name>
    </author>
    <source>
      <id>https://medium.com/@hongnin1?source=rss-c450eb982161------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/0*d6QC_ngideag3rTN.</logo>
      <link href="https://medium.com/@hongnin1?source=rss-c450eb982161------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@hongnin1" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Ning Hong on Medium</subtitle>
      <title>Stories by Ning Hong on Medium</title>
      <updated>2018-04-11T09:00:06Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/ab3d796c422e</id>
    <link href="https://medium.com/@ryanp97/project-ideas-ab3d796c422e?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
    <title>Capstone Ideas</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I plan on following a research track and hope to pursue one of the following ideas:</p><h4><strong>Neural Machine Translation with Semantic Transfer</strong> (as outlined by Jan Buys)</h4><p><em>Minimal Viable Action Plan:</em> <br/>1) Use statistical parser (<a href="http://sweaglesw.org/linguistics/ace/">ACE</a>) to get MRS graphs of English/Japanese sentences and convert to DMRS graph (using <a href="https://github.com/delph-in/pydelphin">PyDelphin</a> interface to do parsing and conversion)<br/>2) Linearize DMRS graph<br/>3) Train a seq2seq that takes linearized English DMRS graph and outputs linearized Japanese DMRS graph</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*J3l_TjWr3A-jOAcrCznqvA.png"/>Example of penmen format that DMRS can be represented with (non-linearized). Note that the representation shown is an AMR graph, not a DMRS graph. Figure taken from this <a href="https://arxiv.org/pdf/1704.08381.pdf">paper</a>.</figure><p><em>Stretch Goals:<br/></em>1) Explore different architectures for semantic transfer (e.g. TreeLSTM as opposed to seq2seq)<br/>2) Explore ways to learn correspondences between semantic concepts in the two languages</p><h4>DMRS to Text Generation (as outlined by Jan Buys)</h4><p><em>Minimal Viable Action Plan:<br/></em>1) Generate DMRS graph serialization similar to what was outlined above<br/>2) Train seq2seq model to generate text directly from graph serialization</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Bndfdhs3ixwG6-JUQPC72A.png"/>Example of different graph representations. Figure taken from this <a href="http://www.lrec-conf.org/proceedings/lrec2016/pdf/634_Paper.pdf">paper</a>.</figure><p><em>Stretch Goals:</em><br/>1) Experiment with different seq2seq architectures<br/>2) Attempt semi-supervised training using a high-precision grammar-based parser</p><h4>Reproduce results / expand on a Paper</h4><p>In this option, I would be attempting to expand on this <a href="https://arxiv.org/pdf/1704.04859.pdf">paper</a> regarding hybrid models. The paper attempts to alleviate and/or solve the issue of out-of-vocabulary words and characters, specifically in Chinese and Japanese. It describes using a CNN in conjunction with an RNN to do so; the CNN learns the radicals of characters and attempts to choose characters with similar radicals since the meaning of radicals remains constant between characters.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/822/1*5GyInYVxc8ifP_YRCHiHHA.png"/>Image taken from the linked paper.</figure><p><em>Minimal Viable Action Plan:<br/></em>1) Obtain/create a visual dataset of the characters in order to train the CNN<br/>2) Design and explore different methods for joining the two models such as described in the paper<br/>3) Train the model end-to-end</p><p><em>Stretch Goals:<br/></em>1) Error analysis on different methods for joining and potential points of error<br/>2) Exploration of model variations and architecture (incremental changes similar to this <a href="https://arxiv.org/pdf/1708.04755.pdf">paper</a>)</p><p>For current progress, visit the <a href="https://github.com/ryanp97/NeuralEmpty">repo</a>.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ab3d796c422e" width="1"/></div>
    </content>
    <updated>2018-04-03T00:55:49Z</updated>
    <category term="machine-learning"/>
    <author>
      <name>Ryan Pham</name>
    </author>
    <source>
      <id>https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/1*83KfTWByl5pPq7A8_E8ApA.gif</logo>
      <link href="https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@ryanp97" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Ryan Pham on Medium</subtitle>
      <title>Stories by Ryan Pham on Medium</title>
      <updated>2018-04-11T09:00:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://cse481n-capstone.azurewebsites.net/?p=15</id>
    <link href="http://cse481n-capstone.azurewebsites.net/2018/04/02/first-blog-post/" rel="alternate" type="text/html"/>
    <title>First Blog Post!</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Team Name:  Team Watch Your Language! Three Project Ideas: Offensive Text Recognition Our minimal viable plan is to create two models, one which determines if text is offensive or not, and another which determines if any particular group is targeted by the text, such as a racial, religious, or political grouping. Our stretch goal is … <a class="more-link" href="http://cse481n-capstone.azurewebsites.net/2018/04/02/first-blog-post/">Continue reading<span class="screen-reader-text"> "First Blog Post!"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h3><strong>Team Name: </strong></h3>
<p><span style="font-weight: 400;">Team Watch Your Language!</span></p>
<h3><strong>Three Project Ideas:</strong></h3>
<p><span style="text-decoration: underline;">Offensive Text Recognition</span></p>
<p><span style="font-weight: 400;">Our minimal viable plan is to create two models, one which determines if text is offensive or not, and another which determines if any particular group is targeted by the text, such as a racial, religious, or political grouping. </span></p>
<p><span style="font-weight: 400;">Our stretch goal is to use these models to make a third model which can use the first two outputs as assumptions to then provide human-readable explanations as to why that particular text was labeled the way it was. This way we can determine if text is offensive or not and provide reasons for that labeling. This can be used to assist in teaching conversational agents common sense about what to say.</span></p>
<p><span style="text-decoration: underline;"><span style="font-weight: 400;">Domain-Specific Conversational Agent</span></span></p>
<p><span style="font-weight: 400;">Our minimal viable plan is to create a conversational agent which can provide information and hold a conversation with a well-intentioned user about a particular domain. </span></p>
<p><span style="font-weight: 400;">The stretch goal here would be to just continually make it better at conversing, at least in the particular domain it is trained to be good at talking about.</span></p>
<p><span style="text-decoration: underline;"><span style="font-weight: 400;">Image Description</span></span></p>
<p><span style="font-weight: 400;">Our minimal viable plan is to create a model that can describe a simple image correctly, like generating a sentence describing the spatial relation between a box and a cylinder.</span></p>
<p><span style="font-weight: 400;">For the stretch goal, we would like to improve our model that can describe a unique pattern of an image among 3 (or multiple) other images. For example, if we have 3 images A, B, and C, we would like to come up with a model to generate a sentence that describes image A but not image B or C.</span></p>
<h3><strong>GitLab Repo:</strong></h3>
<p><span style="font-weight: 400;"> </span><a href="https://gitlab.cs.washington.edu/danielby/nlp-capstone"><span style="font-weight: 400;">https://gitlab.cs.washington.edu/danielby/nlp-capstone</span></a></p>
<h3><strong>Mode:</strong></h3>
<p><span style="font-weight: 400;">We will be tackling the Offensive Text Recognition task in</span> <em>research mode</em><span style="font-weight: 400;">!</span></p></div>
    </content>
    <updated>2018-04-02T06:58:42Z</updated>
    <category term="Weekly blog"/>
    <author>
      <name>Team Watch Your Language!</name>
    </author>
    <source>
      <id>http://cse481n-capstone.azurewebsites.net</id>
      <link href="http://cse481n-capstone.azurewebsites.net/feed/" rel="self" type="application/rss+xml"/>
      <link href="http://cse481n-capstone.azurewebsites.net" rel="alternate" type="text/html"/>
      <subtitle>Spring2018 CSE481N Capstone</subtitle>
      <title>Team Watch Your Language!</title>
      <updated>2018-04-11T09:00:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3753031463594823927.post-6307882466820480344</id>
    <link href="https://cse481n.blogspot.com/2018/04/blog-post-1.html" rel="alternate" type="text/html"/>
    <title>Blog Post #1</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div style="font-family: Verdana; text-align: left;">    <div>Temporary GitHub URL: <a href="https://github.com/rococode/primeapeNLP" target="_blank">https://github.com/rococode/primeapeNLP</a></div>    <div>We plan to be in “research mode” for this capstone.</div>    <div style="font-size: 1.1rem;">Three possible ideas:</div>     <div style="font-size: 1.1rem;">1. New evaluation methods for text generation models</div>    <div>There don’t seem to be many good ways to evaluate the output of machine generated text (such as in the problem of creating hotel reviews). We want our models to generate text that’s indistinguishable from human text, but it’s hard to quantify how similar generated text is to human text, so it’s hard to see when progress is being made. However, it feels like the problem of actually generating human-like text should be harder than the problem of just checking if that text seems human. As a baseline approach, we would like to explore building a regression model for the “humanness” of a piece of unfamiliar text.</div>    <div><strong>        M<span style="font-size: 0.7rem; font-weight: normal;">(inimal)</span>        V<span style="font-size: 0.7rem; font-weight: normal;">(iable)</span>        P<span style="font-size: 0.7rem; font-weight: normal;">(lan)</span>    :</strong></div>    <ol>        <li>Build a reasonably large dataset using a combination of web scraping, existing datasets, and existing generative models</li>        <li>Build a regression model with decent performance on training set</li>        <li>Build a regression model with decent performance on development and test sets</li>    </ol>    <div><strong>Stretch goals:</strong></div>    <ol>        <li>Use our model to score various generative models and compare our model’s rankings for these generative models to commonly agreed-upon rankings by researchers</li>        <li>Integrate model scores into a “generate a lot of possibilities, then search for the best one” approach for text generation</li>    </ol>    <div>We would also explore non-neural methods for evaluating Natural Language Generation (NLG). One idea would be to compare the probability distribution of generated sentences to real sentences. A common problem is that generated text repeats the most probable sentences over and over. Thus, the distribution of sentences is front-loaded. We hypothesize that generated text with a sentence distribution that closely mirrors real text would be more difficult to distinguish from real text. However, we first need to investigate what real world sentence distributions look like. It may be likely that individual sentences are not likely to repeat - in that case, the distribution would be more-or-less uniform across all sentences. We may need to come up with a novel method to categorize similar sentences. We would have to decide whether to use semantic similarity or syntactic similarity.</div>    <div><strong>MVP:</strong></div>    <ol>        <li>Gather a data set - perhaps the data set Ari showed us for hotel reviews.</li>        <li>Examine the existing probability distribution at the sentence-level in the corpus. If the distribution is too uniform, design a method to place similar sentences in the same “bucket” and re-compute the probability distribution.</li>        <li>Design metrics to compute the similarity of two distributions; if a model is “good”, the distribution of the generated sentences will match the distribution of the training corpus.</li>        <li>Use our metric to score generated text models found in the literature.</li>    </ol>    <div><strong>Stretch goals:</strong></div>    <ul>        <li>Use insights from our metrics to improve on current approaches to NLG.</li>    </ul>    <div style="font-size: 1.1rem;">2. Single-document summarization</div>    <div>Single document summarization (SDS) models typically label each sentence in the document as in the summary or not in the summary. The problem then becomes a binary classification problem, and many people train a NN with supervised learning. However, there are other non-neural approaches to this problem that have been tried successfully. One paper shows how SDS can be thought of as a tree based Knapsack problem, which is then solved by a Integer Linear Programming (ILP) solver (see: <a href="https://www.semanticscholar.org/paper/Single-Document-Summarization-as-a-Tree-Knapsack-Hirao-Yoshida/ed0c8a7ab911cdb30b7e95edada3a55c01eb22c5">https://www.semanticscholar.org/paper/Single-Document-Summarization-as-a-Tree-Knapsack-Hirao-Yoshida/ed0c8a7ab911cdb30b7e95edada3a55c01eb22c5</a>). We would like to explore both neural models and non-neural approaches to SDS. </div>    <div><strong>MVP:</strong></div>    <ol>        <li>As a baseline approach, build neural models from current papers that solve SDS. Evaluate the performance using F1 as well as ROUGE metrics (see: <a href="https://en.wikipedia.org/wiki/ROUGE_(metric)">https://en.wikipedia.org/wiki/ROUGE_(metric)</a>).</li>        <li>Build neural models from current papers that focus on non-neural approaches, such as the combinatorial approach described above.</li>        <li>Evaluate neural and non-neural approaches: what do these approaches have in common? What key insights are they leveraging? Are there any generalizations of the problem that can be extracted? Hopefully, this study will allow us to either improve the SOTA neural models or fine tune some non-neural approach.</li>    </ol>    <div><strong>Stretch goal:</strong></div>    <ul>        <li>Use insights from our studies to formulate and solve SDS in a unique way. Evaluate our approach using F1 and ROUGE metrics.</li>    </ul>    <div style="font-size: 1.1rem;">3. Multi-span comprehension</div>    <div>Reading comprehension models generally operate by extracting an answer to a question by outputting a start and end index on the original passage. This is not a very human-like way of answering questions, and makes it impossible to generate good answers to some simple factual questions using common sentence structures. We would like to explore ways of answering questions from passages without being limited to a single span from the passage text.</div>    <div><strong>MVP:</strong></div>    <ol>        <li>Implement a baseline model that performs near state of the art levels on the SQuAD dataset.</li>        <li>Build a dataset that requires information from multiple spans to answer the questions well. We will likely create this dataset by hand.</li>        <li>Build a model that answers questions about a passage by generating multiple spans. We envision designing our model to output a sequence of indices such that every pair of indices corresponds to one part of the answer. This model should perform almost as well as the baseline model on SQuAD, since that’s just a specific case (one span) of a multi-span answer.</li>        <li>Build upon the previous model by using the output spans to generate a formal answer through a generative model, using the output spans and the question sentence as inputs.</li>    </ol>    <div><strong>Stretch goal:</strong></div>    <ul>        <li>Optimize model to actually be effective at answering multispan questions. We expect this to be quite difficult, so while building a functioning model is part of the MVP, actually having it perform comparatively well is a stretch goal.</li>    </ul></div></div>
    </summary>
    <updated>2018-04-02T04:32:00Z</updated>
    <author>
      <name>Ron &amp;amp; Aditya</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3753031463594823927</id>
      <author>
        <name>Ron &amp;amp; Aditya</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="https://cse481n.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="https://cse481n.blogspot.com/feeds/posts/default?alt=rss" rel="self" type="application/rss+xml"/>
      <title>PrimeapeNLP</title>
      <updated>2018-04-11T09:00:13Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-5600014144802012716.post-6768023392170538237</id>
    <link href="https://nlpcapstonesemparse.blogspot.com/2018/03/blog-post-1.html" rel="alternate" type="text/html"/>
    <title>Blog Post 1: Project Ideas</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span><span style="font-size: 14.6667px; white-space: pre;">I am interested in working in the code generation/semantic parsing space on the research track. My code </span></span><br/><span><span style="font-size: 14.6667px; white-space: pre;">will be in various branches of my fork of allennlp (https://github.com/rajasagashe/allennlp). I will keep you</span></span><br/><span><span style="font-size: 14.6667px; white-space: pre;"> updated on which branch/commits I worked on during each blog post. Also note that project idea 1 has </span></span><br/><span><span style="font-size: 14.6667px; white-space: pre;">the most detail since I have picked it as my project!</span></span></div><h2 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;"><span>Project Idea 1</span></h2><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Minimum Viable Plan:</span><span> Implement the model in the recent UW paper which introduces the task of </span><br/><span>generating the code for a java function from a natural language description. To further aid code </span><br/><span>generation, the class in which the generated function is to reside is provided, i.e. the class variables </span><br/><span>and methods. Thus the encoder encodes the class as well as the utterance and the decoder uses a </span><br/><span>two step attention mechanism and decodes through the java grammar production rules. </span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Stretch Goals:</span><span> Reproduce the state of the art results in the paper. I’m putting this in the stretch goals </span><br/><span>since successfully implementing a neural semantic parser with type constraints is pretty challenging. </span><br/><span>In addition, I hope to experiment with other improvements like encoding the entire class method body</span><br/><span> which wasn’t done.</span></div><h2 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;"><span>Project Idea 2</span></h2><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Minimum Viable Plan: </span><span>Implement this paper: </span><a href="https://arxiv.org/pdf/1704.01696.pdf" style="text-decoration: none;"><span>https://arxiv.org/pdf/1704.01696.pdf</span></a><span>. The model is </span><br/><span>similar to that of the previous idea, but the datasets are for python and ifttt instead.</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Stretch Goals: </span><span>Improve the paper’s result.</span></div><h2 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;"><span>Project Idea 3</span></h2><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Minimum Viable Plan: </span><span>Perform transfer learning across several code generation tasks by using the </span><br/><span>same encoder for them all. This technique would be similar to what was used in the Cove paper </span><br/><a href="https://arxiv.org/pdf/1708.00107.pdf" style="text-decoration: none;"><span>https://arxiv.org/pdf/1708.00107.pdf</span></a><span>. </span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Stretch Goals: </span><span>Improve the individual paper results with this technique.</span></div><div><span><br/></span></div></div>
    </summary>
    <updated>2018-03-30T22:41:00Z</updated>
    <author>
      <name>nlpcapstone</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-5600014144802012716</id>
      <author>
        <name>nlpcapstone</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="https://nlpcapstonesemparse.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="https://nlpcapstonesemparse.blogspot.com/feeds/posts/default?alt=rss" rel="self" type="application/rss+xml"/>
      <title>NlpCapstone</title>
      <updated>2018-04-11T09:00:03Z</updated>
    </source>
  </entry>
</feed>
