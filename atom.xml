<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>NLP Capstone Spring 2018</title>
  <updated>2018-04-04T04:00:19Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Nelson Liu</name>
    <email>nfliu[at]cs.washington.edu</email>
  </author>
  <id>https://nlpcapstone.github.io/atom.xml</id>
  <link href="https://nlpcapstone.github.io/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="https://nlpcapstone.github.io/" rel="alternate"/>

  <entry>
    <id>https://medium.com/p/f7412889d221</id>
    <link href="https://medium.com/@be.li.nda/nlp-capstone-blog-1-f7412889d221?source=rss-fad49d942bf3------2" rel="alternate" type="text/html"/>
    <title>NLP Capstone Blog #1</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Brainstorming project ideas for my CSE 481N NLP Capstone class.</p><p>The following project ideas are all research-oriented (research mode).</p><h3>Document Level Entity-Entity Sentiment Analysis</h3><p><strong>Github URL</strong>: <a href="https://github.com/eunsol/document-e2e-sent">https://github.com/eunsol/document-e2e-sent</a></p><p>Given a document and named entities within the documents, we try to model the sentiment between the various entities in a document, categorizing the sentiments as “positive,” “negative,” or “neutral/no sentiment.” We focus especially on world news, which usually involve many named entities (i.e. countries, world leaders, etc.). For example, given such a document and the named entities in the document:</p><blockquote>Iran’s President <strong>Mahmoud Ahmadinejad</strong> said <strong>Iran</strong> opposes the “aggressive and arrogant policies” of the <strong>United States</strong>, local satellite <strong>Press TV</strong> reported Sunday.</blockquote><blockquote>“The <strong>Iranian nation</strong> opposes the aggressive and arrogant policies of the <strong>U.S. government</strong> and will stand against them forever,” <strong>Ahmadinejad</strong> told a religious conference Saturday.</blockquote><blockquote><strong>Iranian president</strong> also urged the <strong>United States</strong> and other western countries to change their attitude.</blockquote><blockquote>“You should change your attitude,” <strong>Ahmadinejad</strong> said.</blockquote><blockquote><strong>Iran</strong> has constantly accused the <strong>United States</strong> and western powers of earmarking funds to stir up protests against the country and its way of dealing with human rights.</blockquote><p>We should be able to generate the following set of relations between the <strong>bolded</strong>, named entities:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/290/1*ThoNzALuI1a78NOYImbGiw.png"/>Graphical representation of the annotations for the document above. Note that the <strong>red edges</strong> represent <strong>negative</strong> sentiment, <strong>green edges </strong>represent <strong>positive</strong> sentiment, <strong>light green edges</strong> represent <strong>non-negative</strong> sentiments, and <strong>light red edges</strong> represent <strong>non-positive</strong> sentiments.</figure><h4>Minimal Viable Action Plan</h4><ul><li>Fortunately, I already have a pre-existing, labelled dataset I can work with (<a href="https://homes.cs.washington.edu/~eunsol/project_page/acl16/index.html">some examples here</a>), so I don’t have worry about procuring data.</li><li>Implement and train a basic LSTM. This will serve as the baseline model for which to compare results.</li><li>Improve the LSTM with attention, word and sentiment embeddings, dropout, etc. Experiment with various hyper-parameter settings such as learning rate, dropout rate, batch sizes, etc. to maximize performance.</li></ul><h4>Stretch Goals</h4><ul><li>Implement and train a bi-affine relation attention network as delineated in <a href="https://arxiv.org/pdf/1802.10569.pdf">this paper</a>, and adapt it for sentiment analysis between entities.</li><li>Experiment with new network architectures for the bi-affine network to hopefully improve results.</li></ul><p>As I am 99.99% sure that I will be doing entity-entity sentiment analysis for this class, the two ideas below are just here to represent research directions that I’m generally interested in, and how I would approach working on them.</p><h3>Generating Conversational Responses</h3><p>Given an input sentence, generate an appropriate and believable sentence in response. As chat bots are on the rise, these sorts of problems are becoming increasingly pervasive.</p><h4>Minimal Viable Action Plan</h4><ul><li>Procure data from some pre-existing dataset. <a href="http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">This movie dialogue corpus</a> could be a good source. Note though that movie dialogue can be written quite differently from everyday dialogue, and may vary depending on movie genre and setting.</li><li>Implement beam search as described in the lecture slides and in <a href="https://geekyisawesome.blogspot.com/2016/10/using-beam-search-to-generate-most.html">the referenced blog post</a>. This will be used as the benchmark model to which the performance of other models will be compared to.</li></ul><h4>Stretch Goals</h4><ul><li>Implement the semantically-conditioned LSTM-based model as described in <a href="https://arxiv.org/pdf/1508.01745.pdf">the following paper</a>.</li><li>Attempt to improve the model in some way.</li></ul><h3>Fake News/Misinformation Detection</h3><p>Given quotes from news articles, notable politicians, or online communities, categorize it as “true,” “mostly true,” “half true,” “mostly false,” “false,” or “pants on fire.”</p><h4>Minimal Viable Action Plan</h4><ul><li>Politifact is a great data source, and an inspiration for this project.</li><li>Based off of <a href="https://arxiv.org/pdf/1803.03786.pdf">this paper</a>, implement and train a model that combines a bidirectional attention-based LSTM with an SVM. In particular, the model works by feeding the output of the last hidden layer in the LSTM network into an SVM, which then outputs the label.</li></ul><h4>Stretch Goals</h4><ul><li>Since the source paper applied the model to news articles, the model may or may not adapt well to quotes. It would be good to experiment with the architecture of the LSTM and/or determine whether the LSTM + SVM model actually performs better than just an LSTM or just an SVM.</li><li>Experiment with various combinations of LSTM and other machine learning models in an attempt to improve performance.</li></ul><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f7412889d221" width="1"/></div>
    </content>
    <updated>2018-04-04T00:34:38Z</updated>
    <category term="machine-learning"/>
    <category term="deep-learning"/>
    <category term="naturallanguageprocessing"/>
    <author>
      <name>Belinda Zou Li</name>
    </author>
    <source>
      <id>https://medium.com/@be.li.nda?source=rss-fad49d942bf3------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/0*A16vZtRWBzxYPlmn.</logo>
      <link href="https://medium.com/@be.li.nda?source=rss-fad49d942bf3------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@be.li.nda" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Belinda Zou Li on Medium</subtitle>
      <title>Stories by Belinda Zou Li on Medium</title>
      <updated>2018-04-04T04:00:19Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-5600014144802012716.post-4534461914269998368</id>
    <link href="https://nlpcapstonesemparse.blogspot.com/2018/04/blog-post-2-warmup.html" rel="alternate" type="text/html"/>
    <title>Blog Post 2: Warmup</title>
    <summary>I'm working on my project in my fork of Allennlp. I got the dataset and have written the DatasetReader code to preprocess and index the dataset. I plan to add tests for this, and I need to add further preprocessing code such as splitting variables on camel casing.</summary>
    <updated>2018-04-03T21:53:00Z</updated>
    <author>
      <name>nlpcapstone</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-5600014144802012716</id>
      <author>
        <name>nlpcapstone</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="https://nlpcapstonesemparse.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="https://nlpcapstonesemparse.blogspot.com/feeds/posts/default?alt=rss" rel="self" type="application/rss+xml"/>
      <title>NlpCapstone</title>
      <updated>2018-04-03T22:00:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/5258ddd9eedd</id>
    <link href="https://medium.com/@hongnin1/nlp-5258ddd9eedd?source=rss-c450eb982161------2" rel="alternate" type="text/html"/>
    <title>NLP</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is a blog about Natural Language Processing.</p><p>Group member: Ning Hong, Zichun Liu, Zhou Sujie</p><p>Team name: The Bugless</p><p>Three topics our team is considering doing (All start-up mode):</p><ol><li>Movie summarization (imdb)/Food review summarization (yelp)</li></ol><p>minimal viable action plan: scrap data from twitter/imdb/rotten tomato/yelp and train our model to be able to summarize the reviews for movies (or food if we are scrapping data from yelp). The summarization of the movie is how the audience feel about the movie in general, for example, given a movie title, our model should be able to produce something like this: It is violent but good.</p><p>stretch goals: Output a overall review for the movie instead of simple sentences, for example, given a movie title as input, our model should output: “The Terror got my full attention from beginning to end. I couldn’t turn away. I didn’t want to turn away. For me, that’s extremely rare.”</p><p>Another stretch goal is to be able to detect sentiment not only in the US market, but also in China market by using data from DouBan (one of the largest movie review site for China), and compare the sentiment between US and China for a certain movie.</p><p>2.Chinese Phoneticization mapping:</p><p>The way input Chinese to machine is by typing Pinyin, a kind of phoneticization for Chinese sentence and words, which almost every boy in China know about. However, one Chinese character may have many phoneticization can one Chinese phoneticization sequence may map to many Chinese sentences. When typing Chinese to a machine by Pinyin, the machine will rank the potential Chinese sentences by preferences. However, the ranking may not be so consistent to the context. Therefore, we want to generate a language model that map from Pinyin (English character) to Chinese words and sentences depend on context and speaking habit of this person. In addition, this can be potentially turned into an online learn algorithm.</p><p>First step: finding data, where input is Chinese phoneticization (pinyin) and output is Chinese sentences and words. Also, we need to find a good algorithm to do that, and onlint learning algorithm will be better.</p><p>Ideas come from Neural Input Method Engine of last quarter: <a href="https://www.dropbox.com/sh/z3idncggfpwm8rs/AAAnHVvHIPvt_CxTXsaDVqvda?dl=0&amp;preview=teamverynatural_3417269_43042970_Very+Natural+Final+Report.pdf">https://www.dropbox.com/sh/z3idncggfpwm8rs/AAAnHVvHIPvt_CxTXsaDVqvda?dl=0&amp;preview=teamverynatural_3417269_43042970_Very+Natural+Final+Report.pdf</a></p><p>3. Flirt tutor:</p><p>When you chat with your beloved boy/girl, you must have some hard time picking interesting and flirting word to response. Then let the machine teach you! Specifically, we want to build a model that can generate cute response giving the context of chatting. This model is neural based.</p><p>First step: figuring out what is the right dataset and find such. Meanwhile, consider the right model to use and find paper/resources about is.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5258ddd9eedd" width="1"/></div>
    </content>
    <updated>2018-04-03T02:48:17Z</updated>
    <category term="nlp"/>
    <author>
      <name>Ning Hong</name>
    </author>
    <source>
      <id>https://medium.com/@hongnin1?source=rss-c450eb982161------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/0*d6QC_ngideag3rTN.</logo>
      <link href="https://medium.com/@hongnin1?source=rss-c450eb982161------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@hongnin1" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Ning Hong on Medium</subtitle>
      <title>Stories by Ning Hong on Medium</title>
      <updated>2018-04-04T04:00:06Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/ab3d796c422e</id>
    <link href="https://medium.com/@ryanp97/project-ideas-ab3d796c422e?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
    <title>Capstone Ideas</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I plan on following a research track and hope to pursue one of the following ideas:</p><h4><strong>Neural Machine Translation with Semantic Transfer</strong> (as outlined by Jan Buys)</h4><p><em>Minimal Viable Action Plan:</em> <br/>1) Use statistical parser (<a href="http://sweaglesw.org/linguistics/ace/">ACE</a>) to get MRS graphs of English/Japanese sentences and convert to DMRS graph (using <a href="https://github.com/delph-in/pydelphin">PyDelphin</a> interface to do parsing and conversion)<br/>2) Linearize DMRS graph<br/>3) Train a seq2seq that takes linearized English DMRS graph and outputs linearized Japanese DMRS graph</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*J3l_TjWr3A-jOAcrCznqvA.png"/>Example of penmen format that DMRS can be represented with (non-linearized). Note that the representation shown is an AMR graph, not a DMRS graph. Figure taken from this <a href="https://arxiv.org/pdf/1704.08381.pdf">paper</a>.</figure><p><em>Stretch Goals:<br/></em>1) Explore different architectures for semantic transfer (e.g. TreeLSTM as opposed to seq2seq)<br/>2) Explore ways to learn correspondences between semantic concepts in the two languages</p><h4>DMRS to Text Generation (as outlined by Jan Buys)</h4><p><em>Minimal Viable Action Plan:<br/></em>1) Generate DMRS graph serialization similar to what was outlined above<br/>2) Train seq2seq model to generate text directly from graph serialization</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Bndfdhs3ixwG6-JUQPC72A.png"/>Example of different graph representations. Figure taken from this <a href="http://www.lrec-conf.org/proceedings/lrec2016/pdf/634_Paper.pdf">paper</a>.</figure><p><em>Stretch Goals:</em><br/>1) Experiment with different seq2seq architectures<br/>2) Attempt semi-supervised training using a high-precision grammar-based parser</p><h4>Reproduce results / expand on a Paper</h4><p>In this option, I would be attempting to expand on this <a href="https://arxiv.org/pdf/1704.04859.pdf">paper</a> regarding hybrid models. The paper attempts to alleviate and/or solve the issue of out-of-vocabulary words and characters, specifically in Chinese and Japanese. It describes using a CNN in conjunction with an RNN to do so; the CNN learns the radicals of characters and attempts to choose characters with similar radicals since the meaning of radicals remains constant between characters.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/822/1*5GyInYVxc8ifP_YRCHiHHA.png"/>Image taken from the linked paper.</figure><p><em>Minimal Viable Action Plan:<br/></em>1) Obtain/create a visual dataset of the characters in order to train the CNN<br/>2) Design and explore different methods for joining the two models such as described in the paper<br/>3) Train the model end-to-end</p><p><em>Stretch Goals:<br/></em>1) Error analysis on different methods for joining and potential points of error<br/>2) Exploration of model variations and architecture (incremental changes similar to this <a href="https://arxiv.org/pdf/1708.04755.pdf">paper</a>)</p><p>For current progress, visit the <a href="https://github.com/ryanp97/NeuralEmpty">repo</a>.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ab3d796c422e" width="1"/></div>
    </content>
    <updated>2018-04-03T00:55:49Z</updated>
    <category term="machine-learning"/>
    <author>
      <name>Ryan Pham</name>
    </author>
    <source>
      <id>https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/1*83KfTWByl5pPq7A8_E8ApA.gif</logo>
      <link href="https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@ryanp97" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Ryan Pham on Medium</subtitle>
      <title>Stories by Ryan Pham on Medium</title>
      <updated>2018-04-04T04:00:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://cse481n-capstone.azurewebsites.net/?p=15</id>
    <link href="http://cse481n-capstone.azurewebsites.net/2018/04/02/first-blog-post/" rel="alternate" type="text/html"/>
    <title>First Blog Post!</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Team Name:  Team Watch Your Language! Three Project Ideas: Offensive Text Recognition Our minimal viable plan is to create two models, one which determines if text is offensive or not, and another which determines if any particular group is targeted by the text, such as a racial, religious, or political grouping. Our stretch goal is … <a class="more-link" href="http://cse481n-capstone.azurewebsites.net/2018/04/02/first-blog-post/">Continue reading<span class="screen-reader-text"> "First Blog Post!"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h3><strong>Team Name: </strong></h3>
<p><span style="font-weight: 400;">Team Watch Your Language!</span></p>
<h3><strong>Three Project Ideas:</strong></h3>
<p><span style="text-decoration: underline;">Offensive Text Recognition</span></p>
<p><span style="font-weight: 400;">Our minimal viable plan is to create two models, one which determines if text is offensive or not, and another which determines if any particular group is targeted by the text, such as a racial, religious, or political grouping. </span></p>
<p><span style="font-weight: 400;">Our stretch goal is to use these models to make a third model which can use the first two outputs as assumptions to then provide human-readable explanations as to why that particular text was labeled the way it was. This way we can determine if text is offensive or not and provide reasons for that labeling. This can be used to assist in teaching conversational agents common sense about what to say.</span></p>
<p><span style="text-decoration: underline;"><span style="font-weight: 400;">Domain-Specific Conversational Agent</span></span></p>
<p><span style="font-weight: 400;">Our minimal viable plan is to create a conversational agent which can provide information and hold a conversation with a well-intentioned user about a particular domain. </span></p>
<p><span style="font-weight: 400;">The stretch goal here would be to just continually make it better at conversing, at least in the particular domain it is trained to be good at talking about.</span></p>
<p><span style="text-decoration: underline;"><span style="font-weight: 400;">Image Description</span></span></p>
<p><span style="font-weight: 400;">Our minimal viable plan is to create a model that can describe a simple image correctly, like generating a sentence describing the spatial relation between a box and a cylinder.</span></p>
<p><span style="font-weight: 400;">For the stretch goal, we would like to improve our model that can describe a unique pattern of an image among 3 (or multiple) other images. For example, if we have 3 images A, B, and C, we would like to come up with a model to generate a sentence that describes image A but not image B or C.</span></p>
<h3><strong>GitLab Repo:</strong></h3>
<p><span style="font-weight: 400;"> </span><a href="https://gitlab.cs.washington.edu/danielby/nlp-capstone"><span style="font-weight: 400;">https://gitlab.cs.washington.edu/danielby/nlp-capstone</span></a></p>
<h3><strong>Mode:</strong></h3>
<p><span style="font-weight: 400;">We will be tackling the Offensive Text Recognition task in</span> <em>research mode</em><span style="font-weight: 400;">!</span></p></div>
    </content>
    <updated>2018-04-02T06:58:42Z</updated>
    <category term="Weekly blog"/>
    <author>
      <name>Team Watch Your Language!</name>
    </author>
    <source>
      <id>http://cse481n-capstone.azurewebsites.net</id>
      <link href="http://cse481n-capstone.azurewebsites.net/feed/" rel="self" type="application/rss+xml"/>
      <link href="http://cse481n-capstone.azurewebsites.net" rel="alternate" type="text/html"/>
      <subtitle>Spring2018 CSE481N Capstone</subtitle>
      <title>Team Watch Your Language!</title>
      <updated>2018-04-03T17:12:08Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-5600014144802012716.post-6768023392170538237</id>
    <link href="https://nlpcapstonesemparse.blogspot.com/2018/03/blog-post-1.html" rel="alternate" type="text/html"/>
    <title>Blog Post 1: Project Ideas</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span><span style="font-size: 14.6667px; white-space: pre;">I am interested in working in the code generation/semantic parsing space on the research track. My code </span></span><br/><span><span style="font-size: 14.6667px; white-space: pre;">will be in various branches of my fork of allennlp (https://github.com/rajasagashe/allennlp). I will keep you</span></span><br/><span><span style="font-size: 14.6667px; white-space: pre;"> updated on which branch/commits I worked on during each blog post. Also note that project idea 1 has </span></span><br/><span><span style="font-size: 14.6667px; white-space: pre;">the most detail since I have picked it as my project!</span></span></div><h2 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;"><span>Project Idea 1</span></h2><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Minimum Viable Plan:</span><span> Implement the model in the recent UW paper which introduces the task of </span><br/><span>generating the code for a java function from a natural language description. To further aid code </span><br/><span>generation, the class in which the generated function is to reside is provided, i.e. the class variables </span><br/><span>and methods. Thus the encoder encodes the class as well as the utterance and the decoder uses a </span><br/><span>two step attention mechanism and decodes through the java grammar production rules. </span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Stretch Goals:</span><span> Reproduce the state of the art results in the paper. I’m putting this in the stretch goals </span><br/><span>since successfully implementing a neural semantic parser with type constraints is pretty challenging. </span><br/><span>In addition, I hope to experiment with other improvements like encoding the entire class method body</span><br/><span> which wasn’t done.</span></div><h2 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;"><span>Project Idea 2</span></h2><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Minimum Viable Plan: </span><span>Implement this paper: </span><a href="https://arxiv.org/pdf/1704.01696.pdf" style="text-decoration: none;"><span>https://arxiv.org/pdf/1704.01696.pdf</span></a><span>. The model is </span><br/><span>similar to that of the previous idea, but the datasets are for python and ifttt instead.</span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Stretch Goals: </span><span>Improve the paper’s result.</span></div><h2 dir="ltr" style="line-height: 1.38; margin-bottom: 6pt; margin-top: 18pt;"><span>Project Idea 3</span></h2><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Minimum Viable Plan: </span><span>Perform transfer learning across several code generation tasks by using the </span><br/><span>same encoder for them all. This technique would be similar to what was used in the Cove paper </span><br/><a href="https://arxiv.org/pdf/1708.00107.pdf" style="text-decoration: none;"><span>https://arxiv.org/pdf/1708.00107.pdf</span></a><span>. </span></div><div dir="ltr" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;"><span>Stretch Goals: </span><span>Improve the individual paper results with this technique.</span></div><div><span><br/></span></div></div>
    </summary>
    <updated>2018-03-30T22:41:00Z</updated>
    <author>
      <name>nlpcapstone</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-5600014144802012716</id>
      <author>
        <name>nlpcapstone</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="https://nlpcapstonesemparse.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="https://nlpcapstonesemparse.blogspot.com/feeds/posts/default?alt=rss" rel="self" type="application/rss+xml"/>
      <title>NlpCapstone</title>
      <updated>2018-04-03T22:00:03Z</updated>
    </source>
  </entry>
</feed>
