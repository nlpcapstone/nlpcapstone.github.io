<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>NLP Capstone Spring 2018</title>
  <updated>2018-07-30T20:02:34Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Nelson Liu</name>
    <email>nfliu[at]cs.washington.edu</email>
  </author>
  <id>https://nlpcapstone.github.io/atom.xml</id>
  <link href="https://nlpcapstone.github.io/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="https://nlpcapstone.github.io/" rel="alternate"/>

  <entry>
    <id>https://medium.com/p/81ba6be6634</id>
    <link href="https://medium.com/@ryanp97/data-subsets-parent-feeding-and-future-work-81ba6be6634?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
    <title>Data Subsets, Parent Feeding, and Future Work</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the last week, I’ve been training more models on different subsets of the data and began working on implementing parent feeding. Though I don’t think I will have parent feeding done in time for the final presentation, it’ll be a nice checkpoint to start at after this capstone finishes.</p><h4>Data Subsets</h4><p>The model trained on all of Michael Goodman’s data last week had an issue with SMATCH (which is why the SMATCH score was omitted in last week’s blogpost). It turns out that there were a couple translation pairs in which the unicode character U+3000 (ideographic/wide space) was used as a token causing it to appear as a surface predicate in some development examples. As a result, SMATCH had issues dealing with these and crashed. Considering how infrequently this surface predicate actually occurred, I decided to invalidate the graphs that contained them during post-processing. This model ‘achieved’ a SMATCH score of 0.54. For reference, when treating the predicates as a bag of words, the model had an F1 score of 0.52.</p><p>Considering the Kyoto Corpus suffers the issue of having many uncommon named entities, I decided to train a model on just the Japanese WordNet corpus. This dataset is significantly smaller with ~105,000 training examples compared to ~325,000 training examples for the combined corpus. I figured this dataset would have significantly fewer named entities and not mis-predict the named abstract predicate so often. This model did, in fact, achieve a better SMATCH score with a F1 of 0.57 and an F1 score of 0.54 when treating the predicates as a bag of words. Notably, the named abstract predicate was mis-predicted less often, though it was still in the top 10 mis-predicted predicates. This resulted in a higher abstract predicate precision, ~0.04 above. Surprisingly, however, the surface predicate precision drop ~0.04. I’m not entirely sure why quite yet, but it may be due to the ratio of number of surface predicates to number of abstract predicates in each dataset.</p><p>I’m currently training a model with all of the data from Michael Goodman and adding all the training examples I had parsed from the Tanaka Corpus. In theory this model should perform slightly better than the model trained with solely Michael Goodman’s dataset, though I won’t be able to tell until late tomorrow considering the time it takes to train a single epoch.</p><h4>Parent Feeding</h4><p>Working with OpenNMT’s codebase has been quite a pain. Though I’ve implemented a short method to calculate the parent indicies of a single graph, I have had lots of trouble figuring out where exactly they should be calculated and how they will be stored. For now I’ve placed it as a step in ShardedTextCorpusIterator. So the pipeline for generating the input to OpenNMT is still the same. The preprocess.py script takes the same inputs and outputs as usual. The only thing that is different now is the saved files will now also contain parent indicies for each example.</p><p>I haven’t been able to figure out how batching will work with this quite yet, so I’m meeting up with Jan later this week to discuss how we should do batching for this. Once Jan clears up how batching works on Wednesday, I should have a good enough understanding to attempt to modify/write a decoder that also uses parent feeding.</p><h4>ELMo Embeddings</h4><p>After discussing with Jan more, I likely would not see significantly improved results with ELMo embeddings unless I was able to scrape more data to train these embeddings with. Since the vocabulary the model is trying to predict are predicates and edges, I would have to generate graphs after scraping more data for both languages. Considering how little time I have left, I decided to leave this for future work.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=81ba6be6634" width="1"/></div>
    </content>
    <updated>2018-05-29T00:20:11Z</updated>
    <category term="machine-learning"/>
    <author>
      <name>Ryan Pham</name>
    </author>
    <source>
      <id>https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/1*83KfTWByl5pPq7A8_E8ApA.gif</logo>
      <link href="https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@ryanp97" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Ryan Pham on Medium</subtitle>
      <title>Stories by Ryan Pham on Medium</title>
      <updated>2018-07-30T19:52:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://medium.com/p/ff44ae10d41e</id>
    <link href="https://medium.com/@ryanp97/incorporating-more-data-ff44ae10d41e?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
    <title>Incorporating more Data</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Based on the results of last week’s hyper-parameter tuning, I wanted to incorporate more data to see if the issue was with the dataset or not. So this week I worked on adding the Kyoto corpus as well as Japanese WordNet (parallel corpus) definitions and examples into my dataset.</p><p>I began parsing the graphs from the Kyoto Corpus similar to how I did with the Tanaka Corpus earlier this week until Michael Goodman, the linguistics grad student I have also been working with gave me access to his preprocessed version. Michael had split up the data in many tiny chunks for each corpus such that each subdirectory was a subset of the actual data. Also the data was stored in a different format than I was expecting, but it was easily converted to the Penman format using the mrs-to-penman script mentioned in earlier blog posts.</p><p>In total, this allowed me to triple the size of my dataset from ~124,000 examples to ~325,000 examples (the result of combining Tanaka, Kyoto, and WordNet corpora from Michael). Although this is not as significant as I was hoping, it adds a lot of variety to the types of sentences and graphs that the model has been trained on up until this point.</p><p>The Tanaka Corpus is very casual in nature since it was essentially crowd-sourced by a teacher asking his students to translate sentences for him. As a result, the sentences are usually in casual speech form, some examples are from songs, some of the translations include mistakes, etc. The Kyoto Corpus, on the other hand, was created from manual translation of Wikipedia articles with the purpose of the data being used for travel brochures and similar tasks in mind. As a result, a translation pair from the Kyoto Corpus is likely to contain named entities that are not often found in the other translation pairs in the dataset, which causes errors mentioned later in this post. The Japanese WordNet examples are the definitions of words and examples of the words being used in sentences. This also seems like it may contribute to the issues mentioned later.</p><p>Something to note is that the preprocessed data I got from Michael contained a different version of the Tanaka Corpus than the one I am currently using. It seems to have been segmented differently and/or seems that it might be a non-current version since it was shipped with the Jacy grammar. So the Tanaka Corpus that came with Michael’s data accounts for ~1,000/325,000 examples which is significantly fewer parsed graphs than I was able to obtain.</p><p>In the coming week, I want to experiment with training a model with the Michael’s dataset combined with my current dataset as well as different subsets of the data (i.e. just the Kyoto Corpus or just the Japanese WordNet corpus).</p><h4>Retraining the Baseline</h4><p>After I preprocessed the data from Michael, I chose the best model from my previous attempts (though they were all pretty similar in performance), which happened to be my initial baseline, and trained it on Michael’s combined dataset. I was optimistically hoping for improved performance in terms of predicate precision and recall, but the model performed much worse.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*IhuwDSj7OrdW4n_Z9bivNA.png"/></figure><p>Using the same concept as last week, I calculated the most commonly mis-predicted/overly-predicted predicates:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XGtwfRJBrQcPyptwMBth5Q.png"/></figure><p>Just like with the previous models and dataset, the model still has issues mis-predicting abstract predicates and grammar concepts like particles. Something new to the list is the named abstract predicate. This was not even in the top 20 mis-predicted predicates in any of the previous models, but it shot up to the top of the charts with this new dataset. Like I mentioned earlier, the cause for this can most likely be attributed to the Kyoto Corpus and Japanese WordNet translation pairs containing many more named entities compared to the Tanaka Corpus.</p><p>Something else that I noticed is that this new dataset resulted in many more predictions having large length differences. In the baseline model, ~2,200/12,000 translation pairs differed by 5 or more predicates. With this new dataset, ~4,400/13,000 translation pairs differed by 5 or more predicates. One possible reason for this is the Kyoto Corpus — the Kyoto Corpus has translation pairs for both titles and summaries. The length difference between these is usually quite large since summaries are just longer by nature and carry much more semantic meaning.</p><h4>Future Work</h4><p>Like I mentioned earlier, I want to continue experimenting with different subsets of the data for training the model. I mentioned last week that I wanted to try implementing parent feeding into the decoder to try and force the model to really learn the semantic meanings of the non-terminals. I didn’t have time to do that for this blog post, but it is something that is a possibility for the next blogpost and/or the final presentation. Something else that I plan to try is ELMo embeddings as suggested by Yejin. OpenNMT currently does not have ELMo embedding support, as far as I am aware, so I may switch back over to AllenNLP to explore this addition to the model.</p><img height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ff44ae10d41e" width="1"/></div>
    </content>
    <updated>2018-05-23T04:48:56Z</updated>
    <category term="machine-learning"/>
    <author>
      <name>Ryan Pham</name>
    </author>
    <source>
      <id>https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2</id>
      <logo>https://cdn-images-1.medium.com/fit/c/150/150/1*83KfTWByl5pPq7A8_E8ApA.gif</logo>
      <link href="https://medium.com/@ryanp97?source=rss-6378d85d3a9b------2" rel="alternate" type="text/html"/>
      <link href="https://medium.com/feed/@ryanp97" rel="self" type="application/rss+xml"/>
      <link href="http://medium.superfeedr.com" rel="hub" type="text/html"/>
      <subtitle>Stories by Ryan Pham on Medium</subtitle>
      <title>Stories by Ryan Pham on Medium</title>
      <updated>2018-07-30T19:53:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://cse481n-capstone.azurewebsites.net/?p=105</id>
    <link href="http://cse481n-capstone.azurewebsites.net/2018/05/15/advanced-attempt-ii-2/" rel="alternate" type="text/html"/>
    <title>Advanced Attempt II</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Data Collection In our last post, we described our final collection of Reddit sentences that we would send for labeling. We decided to add subreddit metadata to each sentence before sending it off, but this created an unexpected slight setback because of the way we spliced posts into sentences after throwing away all of the … <a class="more-link" href="http://cse481n-capstone.azurewebsites.net/2018/05/15/advanced-attempt-ii-2/">Continue reading<span class="screen-reader-text"> "Advanced Attempt II"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h3>Data Collection</h3>
<p><span style="font-weight: 400;">In our last post, we described our final collection of Reddit sentences that we would send for labeling. We decided to add subreddit metadata to each sentence before sending it off, but this created an unexpected slight setback because of the way we spliced posts into sentences after throwing away all of the metadata associated with the original dump. We did finish connecting each sentence to its originating subreddit however, and from doing so discovered a slight issue with our data. It turned out that from doing the hate lexicon matching described earlier, we got a few posts that looked like: “[uncensored-r/Bitcoin] &lt;</span><i><span style="font-weight: 400;">some original post</span></i><span style="font-weight: 400;">&gt; The following post by vichuu is being replicated because the post has been silently removed.” In the end we decided to add the subreddit “noncensored_bitcoin” to our blacklist and removed all its sentences from our dataset, but this decreased our dataset size to ~18k, down 2k from the last blog post. </span></p>
<p><span style="font-weight: 400;">In our sixth blog post we showed the top 10 subreddits for quantity of posts. Here are the top 10 subreddits just in our dataset. </span></p>
<p><span style="font-weight: 400;">As expected, we have a lot of MeanJoke sentences as well as sentences from other joke subreddits that probably have similar language structure. The inclusion of the hate lexicon set of sentences most likely helped bring diversity to this final set of sentences. Otherwise, it looks like a good collection of discussion subreddits. As well, since there are 18k sentences and the top 10 subreddits only sum to ~6000 of them we once again see the sparse nature of our data’s origins. </span></p>
<table>
<tbody>
<tr>
<td>Subreddit</td>
<td>Number of Sentences</td>
</tr>
<tr>
<td>MeanJokes</td>
<td>2964</td>
</tr>
<tr>
<td>AskReddit</td>
<td>706</td>
</tr>
<tr>
<td>Jokes</td>
<td>565</td>
</tr>
<tr>
<td>darkjokes</td>
<td>387</td>
</tr>
<tr>
<td>depression</td>
<td>284</td>
</tr>
<tr>
<td>relationships</td>
<td>280</td>
</tr>
<tr>
<td>DebateConservatives</td>
<td>253</td>
</tr>
<tr>
<td>offmychest</td>
<td>227</td>
</tr>
<tr>
<td>raisedbynarcissists</td>
<td>168</td>
</tr>
<tr>
<td>relationship_advice</td>
<td>166</td>
</tr>
</tbody>
</table>
<h3>Advanced Model Attempt</h3>
<p><span style="font-weight: 400;">We implemented a new model which incorporates attention.</span></p>
<p><span style="font-weight: 400;">The way we did it is by writing an seq2seq (in this case, is LSTM) encoder with attention and then take the last dimension of each sequence as our vector representation of the whole sentence.</span></p>
<p><span style="font-weight: 400;">For example, if we have (batch_size, sentence_length, dim), our vector representation will become (batch_size, dim).</span></p>
<p><span style="font-weight: 400;">After that, we passed it through a 2-layer feedforward neural network and map the output to a 2-dimensional vector which represents each class (hate, none in this case). Then we do a softmax and pick the one with the highest probability as our prediction.</span></p>
<p><b>Statistics</b></p>
<table>
<tbody>
<tr>
<td><span>50d</span></td>
<td><span>Without ELMo</span></td>
</tr>
<tr>
<td><span>F1</span></td>
<td><span>0.7907</span></td>
</tr>
<tr>
<td><span>Precision</span></td>
<td><span>0.7961</span></td>
</tr>
<tr>
<td><span>Recall</span></td>
<td><span>0.7862</span></td>
</tr>
<tr>
<td><span>Accuracy</span></td>
<td><span>0.8194</span></td>
</tr>
</tbody>
</table>
<p> </p>
<table>
<tbody>
<tr>
<td><span>100d</span></td>
<td><span>Without ELMo</span></td>
</tr>
<tr>
<td><span>F1</span></td>
<td><span>0.7907</span></td>
</tr>
<tr>
<td><span>Precision</span></td>
<td><span>0.8076</span></td>
</tr>
<tr>
<td><span>Recall</span></td>
<td><span>0.7798</span></td>
</tr>
<tr>
<td><span>Accuracy</span></td>
<td><span>0.8242</span></td>
</tr>
</tbody>
</table>
<p> </p>
<table>
<tbody>
<tr>
<td><span>200d</span></td>
<td><span>Without ELMo</span></td>
</tr>
<tr>
<td><span>F1</span></td>
<td><span>0.7944</span></td>
</tr>
<tr>
<td><span>Precision</span></td>
<td><span>0.7902</span></td>
</tr>
<tr>
<td><span>Recall</span></td>
<td><span>0.7995</span></td>
</tr>
<tr>
<td><span>Accuracy</span></td>
<td><span>0.8166</span></td>
</tr>
</tbody>
</table>
<p><b>Unexpected Problem encountered:</b></p>
<p><span style="font-weight: 400;">There was a bug in pytorch saying “fn” is undefined when we want to train LSTM with ELMo using CUDA, and we have no luck on fixing that. After googling it, it turns out to be a problem related to pytorch 0.3.1 and it’s fixed in a later version like 0.4.0. Since Allennlp requires pytorch 0.3.1, we may have to try something else instead.</span></p>
<p><span style="font-weight: 400;">We will try to use GRU instead (hopefully it works) and report all metrics in detail in next week’s blog post.</span></p>
<h3>Demo Frontend</h3>
<p><span style="font-weight: 400;">Previously, we used default AllenNLP frontend for demoing purpose. Although it was easy to set up and use by us the developers who knew exactly what numbers we are looking for, it was not intuitive to use at all for other people. The default demo page looked like this:</span></p>
<p><img alt="" class="alignnone size-full wp-image-107" height="922" src="http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/old_demo.png" width="2712"/></p>
<p><span style="font-weight: 400;">In order to make the demo page more straightforward, we decided to only keep the most relevant fields of the model output: predicted label and class probabilities. Since it is also interesting to see how prediction changes every time a new word is entered, we designed the page to make a prediction everytime a blank space or ENTER is entered in the text input box (in addition to the Predict button being hit). Our new demo page looked like this (serving a different model):</span></p>
<p><img alt="" class="alignnone size-full wp-image-108" height="1034" src="http://cse481n-capstone.azurewebsites.net/wp-content/uploads/2018/05/new_demo.png" width="2758"/></p>
<p><span style="font-weight: 400;">As shown above, while the overall page format looked the same (the left half is input, the right half is output), the output display changed quite a bit. The predicted label is nicely put into a sentence and highlighted under “Prediction”, and the class probabilities are in a table with the “Hate probability” entry highlighted under “Summary Breakdown”. We also added hate probability visualization at the end. A linear gradient from bright green to red represented hate probability from 0% to 100%, a dark vertical line representing the predicted hate probability of the current sentence, and a small label under the dark line showing the hate probability. </span></p>
<p><span style="font-weight: 400;">Not only is the new demo page easier to read and see the changes when each word is entered, this simple output display could potentially be ported to AMT surveys in the future if we want turkers to come up with hard to detect examples.</span></p>
<h3>Next Steps:</h3>
<p>On the data collection front, we will push forward in the process of getting our reddit dataset labeled.  For modeling, we would perform more hyperparameter tuning and error analysis as well as incorporating ELMo into the current model, and hopefully, we get to explore potential ways to make neural nets work with relatively small datasets like these twitter datasets that we have.</p></div>
    </content>
    <updated>2018-05-16T03:27:56Z</updated>
    <category term="Weekly blog"/>
    <author>
      <name>Team Watch Your Language!</name>
    </author>
    <source>
      <id>http://cse481n-capstone.azurewebsites.net</id>
      <link href="http://cse481n-capstone.azurewebsites.net/feed/" rel="self" type="application/rss+xml"/>
      <link href="http://cse481n-capstone.azurewebsites.net" rel="alternate" type="text/html"/>
      <subtitle>Spring2018 CSE481N Capstone</subtitle>
      <title>Team Watch Your Language!</title>
      <updated>2018-07-30T19:54:51Z</updated>
    </source>
  </entry>
</feed>
